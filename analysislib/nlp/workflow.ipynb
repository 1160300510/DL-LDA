{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 2, 3, 0, 1, 1, 1, 0, 1, 0, 4, 3, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 2, 1, 3, 0, 2, 3, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 3, 1, 0, 0, 1, 0, 1, 1, 0, 0, 2, 3, 1, 3, 1, 0, 2, 0, 2, 2, 1, 0, 3, 0, 0, 0, 2, 0, 2, 2, 1, 2, 1, 1, 0, 0, 0, 3, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 2, 0, 2, 2, 4, 0, 0, 2, 1, 2, 3, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 3, 1, 2, 0, 1, 2, 0, 5, 0, 1, 0, 3, 3, 2, 1, 0, 2, 0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 3, 1, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 3, 2, 1, 2, 1, 3, 1, 3, 0, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 3, 4, 0, 1, 0, 1, 2, 1, 1, 2, 1, 2, 0, 2, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 2, 3, 0, 2, 1, 3, 1, 2, 0, 0, 2, 2, 2, 3, 2, 2, 1, 1, 2, 3, 0, 1, 2, 3, 1, 3, 1, 2, 3, 3, 0, 0, 1, 0, 0, 2, 2, 2, 3, 0, 2, 2, 2, 2, 3, 3, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 0, 1, 2, 2, 2, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 3, 1, 2, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 1, 0, 4, 0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 3, 1, 2, 0, 0, 4, 0, 1, 2, 2, 2, 2, 0, 1, 3, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 3, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1, 3, 0, 0, 0, 3, 2, 1, 0, 1, 0, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 3, 0, 0, 1, 2, 2, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 3, 2, 4, 0, 0, 1, 3, 1, 0, 0, 0, 2, 1, 4, 2, 0, 1, 0, 1, 0, 1, 0, 4, 0, 0, 2, 0, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 0, 0, 2, 0, 2, 2, 0, 3, 2, 0, 3, 0, 0, 3, 0, 0, 0, 1, 3, 0, 0, 1, 3, 0, 3, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 0, 2, 3, 2, 2, 3, 1, 4, 0, 0, 0, 0, 3, 0, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 3, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 2, 2, 3, 0, 0, 2, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 1, 5, 1, 1, 0, 0, 0, 2, 2, 0, 1, 1, 0, 4, 1, 0, 3, 2, 3, 0, 0, 0, 1, 1, 1, 3, 3, 1, 0, 0, 2, 1, 1, 3, 0, 1, 3, 0, 3, 2, 2, 1, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 3, 2, 0, 2, 0, 0, 3, 2, 4, 0, 2, 0, 2, 1, 1, 2, 1, 1, 3, 0, 0, 1, 0, 4, 2, 3, 2, 2, 0, 2, 2, 3, 1, 2, 3, 2, 0, 1, 0, 1, 0, 2, 1, 3, 0, 4, 1, 2, 2, 1, 3, 2, 1, 2, 1, 0, 2, 0, 0, 1, 2, 3, 4, 0, 3, 2, 4, 2, 1, 0, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2, 0, 3, 1, 2, 2, 4, 1, 3, 2, 1, 4, 3, 1, 4, 1, 1, 1, 3, 2, 0, 3, 2, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2, 0, 0, 3, 0, 4, 1, 3, 0, 2, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 3, 1, 0, 4, 1, 1, 0, 1, 1, 3, 2, 1, 0, 4, 2, 1, 3, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 2, 3, 0, 2, 0, 1, 0, 0, 6, 2, 4, 1, 0, 2, 0, 1, 2, 2, 1, 2, 0, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 2, 0, 3, 2, 3, 0, 5, 0, 0, 1, 1, 2, 0, 3, 1, 2, 1, 3, 0, 1, 0, 0, 0, 1, 0, 3, 3, 0, 2, 0, 1, 1, 0, 2, 3, 1, 0, 1, 2, 2, 0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 3, 2, 2, 3, 0, 2, 1, 0, 5, 1, 2, 1, 3, 1, 1, 0, 3, 3, 0, 4, 1, 0, 2, 3, 1, 2, 2, 2, 2, 1, 1, 5, 0, 0, 3, 1, 2, 2, 2, 0, 2, 1, 0, 0, 3, 1, 3, 1, 3, 1, 2, 5, 2, 2, 2, 5, 3, 1, 0, 0, 2, 1, 1, 1, 3, 3, 0, 0, 3, 2, 1, 0, 0, 2, 1, 2, 1, 1, 0, 2, 3, 0, 3, 1, 1, 0, 3, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 4, 1, 1, 1, 1, 3, 1, 3, 1, 0, 4, 0, 3, 5, 1, 0, 0, 0, 2, 4, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 3, 2, 1, 3, 1, 2, 0, 0, 3, 2, 2, 0, 0, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 3, 2, 1, 1, 3, 4, 1, 1, 2, 0, 3, 0, 2, 5, 1, 2, 3, 0, 0, 0, 0, 0, 2, 0, 2, 2, 3, 2, 0, 3, 0, 2, 0, 1, 2, 1, 0, 1, 3, 1, 2, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 4, 0, 2, 1, 0, 3, 1, 0, 1, 2, 0, 0, 0, 1, 2, 0, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 3, 3, 2, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 3, 1, 1, 1, 1, 3, 0, 0, 3, 3, 1, 1, 0, 2, 1, 1, 1, 3, 3, 3, 0, 2, 2, 3, 1, 2, 3, 0, 3, 0, 1, 3, 0, 0, 2, 0, 1, 4, 3, 1, 1, 0, 1, 0, 3, 2, 1, 0, 1, 0, 3, 0, 3, 0, 0, 1, 1, 0, 2, 3, 0, 2, 0, 0, 1, 1, 3, 1, 1, 2, 6, 0, 1, 3, 0, 0, 3, 1, 0, 2, 0, 2, 2, 2, 5, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 3, 2, 3, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 3, 1, 0, 4, 1, 3, 1, 1, 0, 1, 2, 3, 0, 0, 3, 0, 0, 2, 1, 2, 0, 3, 1, 0, 0, 1, 0, 1, 3, 0, 0, 0, 1, 1, 0, 3, 3, 1, 2, 0, 0, 0, 3, 0, 0, 3, 1, 2, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 0, 1, 1, 3, 1, 0, 2, 2, 1, 1, 0, 1, 0, 3, 2, 0, 0, 1, 2, 0, 0, 0, 3, 1, 1, 3, 0, 2, 0, 1, 1, 3, 3, 2, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 2, 2, 0, 4, 2, 2, 2, 1, 3, 2, 0, 1, 2, 0, 1, 0, 1, 2, 2, 3, 0, 3, 0, 1, 1, 0, 2, 0, 3, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 2, 3, 1, 3, 3, 2, 2, 1, 1, 2, 4, 1, 0, 0, 2, 0, 2, 1, 2, 0, 3, 3, 3, 1, 2, 0, 0, 2, 1, 0, 1, 0, 4, 1, 3, 1, 1, 0, 3, 0, 2, 1, 0, 0, 2, 1, 1, 3, 1, 2, 0, 2, 3, 3, 1, 2, 1, 2, 0, 0, 3, 2, 1, 2, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, 0, 3, 4, 0, 2, 2, 2, 1, 2, 0, 2, 3, 2, 2, 2, 1, 2, 3, 2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 3, 0, 2, 2, 0, 1, 1, 3, 1, 0, 0, 1, 0, 1, 0, 3, 1, 1, 1, 0, 0, 2, 2, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 3, 2, 0, 3, 0, 0, 0, 2, 3, 3, 1, 3, 1, 1, 2, 0, 2, 2, 0, 4, 2, 0, 1, 4, 0, 3, 3, 2, 1, 2, 2, 0, 2, 2, 3, 1, 0, 3, 4, 1, 0, 3, 1, 3, 0, 3, 0, 3, 0, 1, 0, 2, 3, 0, 0, 1, 1, 0, 3, 1, 2, 2, 4, 0, 0, 2, 1, 1, 2, 0, 0, 0, 0, 4, 2, 0, 1, 1, 1, 3, 1, 0, 0, 3, 4, 2, 3, 1, 0, 2, 1, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 4, 2, 3, 1, 1, 3, 1, 0, 0, 2, 3, 1, 2, 2, 2, 2, 1, 3, 1, 0, 3, 1, 2, 1, 0, 3, 0, 0, 4, 1, 0, 1, 1, 2, 3, 2, 0, 0, 2, 1, 2, 2, 0, 0, 1, 2, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0, 2, 1, 4, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 2, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 3, 0, 0, 4, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 3, 1, 1, 2, 1, 0, 0, 1, 1, 3, 3, 3, 0, 1, 0, 2, 1, 1, 3, 0, 3, 2, 2, 1, 0, 3, 2, 2, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 0, 1, 2, 1, 1, 1, 0, 0, 0, 3, 1, 1, 3, 2, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 1, 2, 0, 3, 0, 1, 0, 2, 1, 2, 5, 2, 2, 1, 2, 3, 0, 1, 2, 0, 2, 0, 0, 2, 4, 3, 2, 3, 1, 0, 1, 1, 2, 2, 0, 1, 1, 2, 3, 0, 2, 0, 0, 0, 1, 1, 3, 0, 0, 0, 2, 3, 0, 2, 2, 1, 0, 0, 2, 1, 1, 0, 0, 3, 1, 2, 0, 4, 0, 1, 2, 1, 0, 0, 2, 0, 3, 1, 1, 1, 2, 1, 1, 1, 0, 0, 4, 3, 4, 1, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 1, 2, 1, 2, 1, 5, 0, 2, 0, 3, 3, 2, 3, 0, 0, 2, 2, 0, 0, 1, 1, 1, 1, 0, 1, 1, 3, 0, 3, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 1, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 3, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 3, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 0, 0, 2, 2, 0, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "\n",
    "fname = datapath(\"nlp_gensim_7.model\")\n",
    "lda = LdaModel.load(\"nlp_gensim_7.model\")\n",
    "\n",
    "corpus = np.load('gensim.npy', allow_pickle=True).tolist()\n",
    "\n",
    "doc_topic_max = []\n",
    "for doc in lda.get_document_topics(corpus):\n",
    "    doc_topic = []\n",
    "    for topic in doc:\n",
    "        doc_topic.append(topic[1])\n",
    "    doc_topic_max.append(doc_topic.index(max(doc_topic)))\n",
    "\n",
    "print(doc_topic_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.049*\"vec\" + 0.046*\"doc\" + 0.042*\"document\" + 0.035*\"model\" + 0.026*\"word\" '\n",
      "  '+ 0.025*\"training\" + 0.014*\"doc2vec\" + 0.013*\"using\" + 0.013*\"parameter\" + '\n",
      "  '0.011*\"vector\" + 0.011*\"result\" + 0.011*\"train\" + 0.010*\"data\" + '\n",
      "  '0.010*\"sentence\" + 0.009*\"use\"'),\n",
      " (1,\n",
      "  '0.136*\"topic\" + 0.105*\"lda\" + 0.043*\"model\" + 0.026*\"modeling\" + '\n",
      "  '0.024*\"using\" + 0.023*\"score\" + 0.023*\"document\" + 0.019*\"coherence\" + '\n",
      "  '0.017*\"number\" + 0.017*\"python\" + 0.017*\"get\" + 0.011*\"probability\" + '\n",
      "  '0.010*\"modelling\" + 0.010*\"want\" + 0.009*\"text\"'),\n",
      " (2,\n",
      "  '0.030*\"corpus\" + 0.026*\"python\" + 0.021*\"dictionary\" + 0.021*\"model\" + '\n",
      "  '0.020*\"document\" + 0.018*\"using\" + 0.017*\"matrix\" + 0.015*\"text\" + '\n",
      "  '0.013*\"use\" + 0.011*\"nlp\" + 0.011*\"term\" + 0.010*\"tf\" + 0.010*\"idf\" + '\n",
      "  '0.010*\"similarity\" + 0.010*\"like\"'),\n",
      " (3,\n",
      "  '0.055*\"similarity\" + 0.040*\"sentence\" + 0.019*\"list\" + 0.019*\"cosine\" + '\n",
      "  '0.016*\"nlp\" + 0.015*\"python\" + 0.014*\"would\" + 0.014*\"time\" + 0.013*\"word\" '\n",
      "  '+ 0.013*\"two\" + 0.012*\"find\" + 0.011*\"like\" + 0.011*\"using\" + 0.011*\"data\" '\n",
      "  '+ 0.010*\"text\"'),\n",
      " (4,\n",
      "  '0.022*\"python\" + 0.022*\"text\" + 0.020*\"code\" + 0.018*\"data\" + 0.018*\"like\" '\n",
      "  '+ 0.015*\"list\" + 0.014*\"input\" + 0.011*\"error\" + 0.011*\"get\" + '\n",
      "  '0.011*\"using\" + 0.010*\"dataframe\" + 0.009*\"column\" + 0.009*\"dictionary\" + '\n",
      "  '0.009*\"following\" + 0.009*\"array\"'),\n",
      " (5,\n",
      "  '0.049*\"python\" + 0.047*\"error\" + 0.037*\"file\" + 0.026*\"code\" + '\n",
      "  '0.019*\"model\" + 0.019*\"import\" + 0.015*\"package\" + 0.014*\"following\" + '\n",
      "  '0.013*\"user\" + 0.012*\"trying\" + 0.012*\"run\" + 0.011*\"py\" + 0.011*\"line\" + '\n",
      "  '0.010*\"using\" + 0.010*\"tried\"'),\n",
      " (6,\n",
      "  '0.108*\"word\" + 0.053*\"model\" + 0.039*\"vec\" + 0.039*\"vector\" + '\n",
      "  '0.018*\"embedding\" + 0.016*\"using\" + 0.015*\"trained\" + 0.015*\"word2vec\" + '\n",
      "  '0.014*\"python\" + 0.013*\"use\" + 0.012*\"fasttext\" + 0.012*\"want\" + '\n",
      "  '0.009*\"sentence\" + 0.009*\"train\" + 0.009*\"get\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=7, num_words=15)\n",
    "pprint(topicwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[815, 708, 481, 232, 51, 12, 2]\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "for i in range(7):\n",
    "    topics.append(0)\n",
    "for topic in doc_topic_max:\n",
    "    topics[topic]  += 1\n",
    "print(topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.354\n",
      "0.308\n",
      "0.209\n",
      "0.101\n",
      "0.022\n",
      "0.005\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "for i in topics:\n",
    "    print('%.3f' %(i/len(doc_topic_max)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.032*\"java\" + 0.029*\"error\" + 0.028*\"python\" + 0.024*\"corenlp\" + '\n",
      "  '0.021*\"code\" + 0.018*\"using\" + 0.014*\"file\" + 0.013*\"get\" + 0.012*\"run\" + '\n",
      "  '0.012*\"use\" + 0.010*\"trying\" + 0.010*\"following\" + 0.010*\"nltk\" + '\n",
      "  '0.009*\"library\" + 0.009*\"package\"'),\n",
      " (1,\n",
      "  '0.053*\"sentiment\" + 0.034*\"analysis\" + 0.022*\"sentence\" + 0.020*\"language\" '\n",
      "  '+ 0.016*\"using\" + 0.016*\"word\" + 0.010*\"like\" + 0.010*\"score\" + '\n",
      "  '0.010*\"question\" + 0.010*\"opennlp\" + 0.009*\"learning\" + 0.009*\"would\" + '\n",
      "  '0.009*\"find\" + 0.009*\"use\" + 0.008*\"document\"'),\n",
      " (2,\n",
      "  '0.047*\"entity\" + 0.044*\"ner\" + 0.030*\"model\" + 0.025*\"training\" + '\n",
      "  '0.024*\"data\" + 0.022*\"named\" + 0.018*\"name\" + 0.015*\"train\" + 0.015*\"text\" '\n",
      "  '+ 0.014*\"classifier\" + 0.013*\"recognition\" + 0.013*\"using\" + 0.013*\"like\" + '\n",
      "  '0.010*\"feature\" + 0.009*\"person\"'),\n",
      " (3,\n",
      "  '0.035*\"word\" + 0.021*\"po\" + 0.020*\"text\" + 0.020*\"tag\" + 0.016*\"sentence\" + '\n",
      "  '0.015*\"using\" + 0.015*\"token\" + 0.013*\"tagger\" + 0.013*\"like\" + 0.012*\"get\" '\n",
      "  '+ 0.010*\"use\" + 0.009*\"would\" + 0.009*\"way\" + 0.009*\"want\" + '\n",
      "  '0.009*\"example\"'),\n",
      " (4,\n",
      "  '0.055*\"parser\" + 0.045*\"tree\" + 0.040*\"dependency\" + 0.025*\"parse\" + '\n",
      "  '0.021*\"java\" + 0.020*\"parsing\" + 0.017*\"get\" + 0.016*\"np\" + '\n",
      "  '0.016*\"sentence\" + 0.014*\"nltk\" + 0.014*\"edu\" + 0.012*\"python\" + '\n",
      "  '0.012*\"using\" + 0.012*\"node\" + 0.011*\"output\"'),\n",
      " (5,\n",
      "  '0.039*\"sentence\" + 0.025*\"corenlp\" + 0.018*\"using\" + 0.018*\"text\" + '\n",
      "  '0.014*\"result\" + 0.013*\"output\" + 0.012*\"extract\" + 0.011*\"like\" + '\n",
      "  '0.011*\"time\" + 0.011*\"wa\" + 0.010*\"code\" + 0.010*\"get\" + 0.009*\"one\" + '\n",
      "  '0.009*\"phrase\" + 0.009*\"way\"'),\n",
      " (6,\n",
      "  '0.044*\"model\" + 0.042*\"file\" + 0.036*\"java\" + 0.027*\"corenlp\" + 0.027*\"edu\" '\n",
      "  '+ 0.018*\"pipeline\" + 0.016*\"ner\" + 0.015*\"command\" + '\n",
      "  '0.014*\"stanfordcorenlp\" + 0.013*\"server\" + 0.013*\"using\" + 0.012*\"property\" '\n",
      "  '+ 0.012*\"error\" + 0.011*\"english\" + 0.011*\"jar\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel.load(\"nlp_stanfordnlp_7.model\")\n",
    "\n",
    "corpus = np.load('stanfordnlp.npy', allow_pickle=True).tolist()\n",
    "\n",
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=7, num_words=15)\n",
    "pprint(topicwords)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.027*\"code\" + 0.025*\"class\" + 0.025*\"data\" + 0.023*\"text\" + 0.018*\"using\" '\n",
      "  '+ 0.017*\"model\" + 0.017*\"pipeline\" + 0.016*\"analysis\" + 0.013*\"feature\" + '\n",
      "  '0.013*\"classifier\" + 0.013*\"sentiment\" + 0.012*\"error\" + 0.012*\"tweet\" + '\n",
      "  '0.011*\"classification\" + 0.011*\"dataset\"'),\n",
      " (1,\n",
      "  '0.055*\"idf\" + 0.053*\"tf\" + 0.026*\"scikit\" + 0.025*\"matrix\" + 0.024*\"learn\" '\n",
      "  '+ 0.023*\"word\" + 0.021*\"tfidfvectorizer\" + 0.018*\"document\" + 0.016*\"tfidf\" '\n",
      "  '+ 0.014*\"count\" + 0.013*\"nlp\" + 0.013*\"value\" + 0.012*\"using\" + '\n",
      "  '0.011*\"sklearn\" + 0.011*\"term\"'),\n",
      " (2,\n",
      "  '0.027*\"error\" + 0.023*\"file\" + 0.020*\"nltk\" + 0.017*\"spacy\" + 0.016*\"code\" '\n",
      "  '+ 0.014*\"text\" + 0.014*\"use\" + 0.013*\"nlp\" + 0.012*\"sentence\" + '\n",
      "  '0.011*\"using\" + 0.011*\"package\" + 0.011*\"get\" + 0.010*\"function\" + '\n",
      "  '0.010*\"model\" + 0.009*\"txt\"'),\n",
      " (3,\n",
      "  '0.038*\"model\" + 0.029*\"data\" + 0.021*\"text\" + 0.017*\"classification\" + '\n",
      "  '0.017*\"dataset\" + 0.016*\"label\" + 0.013*\"using\" + 0.013*\"trained\" + '\n",
      "  '0.013*\"training\" + 0.012*\"predict\" + 0.012*\"set\" + 0.012*\"test\" + '\n",
      "  '0.011*\"scikit\" + 0.011*\"learn\" + 0.011*\"sentiment\"'),\n",
      " (4,\n",
      "  '0.037*\"word\" + 0.019*\"text\" + 0.014*\"vector\" + 0.014*\"learn\" + '\n",
      "  '0.014*\"scikit\" + 0.013*\"using\" + 0.012*\"classification\" + 0.011*\"one\" + '\n",
      "  '0.011*\"feature\" + 0.011*\"would\" + 0.011*\"nlp\" + 0.010*\"similarity\" + '\n",
      "  '0.010*\"like\" + 0.010*\"document\" + 0.010*\"input\"'),\n",
      " (5,\n",
      "  '0.058*\"topic\" + 0.047*\"lda\" + 0.029*\"learn\" + 0.029*\"scikit\" + '\n",
      "  '0.024*\"model\" + 0.022*\"result\" + 0.022*\"id\" + 0.019*\"gensim\" + '\n",
      "  '0.017*\"using\" + 0.017*\"jupyter\" + 0.017*\"doc\" + 0.016*\"sklearn\" + '\n",
      "  '0.014*\"corpus\" + 0.013*\"modeling\" + 0.011*\"nmf\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel.load(\"nlp_sklearn_6.model\")\n",
    "\n",
    "corpus = np.load('sklearn.npy', allow_pickle=True).tolist()\n",
    "\n",
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=6, num_words=15)\n",
    "pprint(topicwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[574, 587, 545, 309, 82, 1]\n",
      "0.274\n",
      "0.280\n",
      "0.260\n",
      "0.147\n",
      "0.039\n",
      "0.000\n"
     ]
    }
   ],
   "source": [
    "doc_topic_max = []\n",
    "for doc in lda.get_document_topics(corpus):\n",
    "    doc_topic = []\n",
    "    for topic in doc:\n",
    "        doc_topic.append(topic[1])\n",
    "    doc_topic_max.append(doc_topic.index(max(doc_topic)))\n",
    "\n",
    "topics = []\n",
    "for i in range(6):\n",
    "    topics.append(0)\n",
    "for topic in doc_topic_max:\n",
    "    topics[topic]  += 1\n",
    "print(topics)\n",
    "\n",
    "for i in topics:\n",
    "    print('%.3f' %(i/len(doc_topic_max)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.033*\"model\" + 0.024*\"code\" + 0.021*\"error\" + 0.019*\"http\" + 0.013*\"nlp\" + '\n",
      "  '0.013*\"python\" + 0.011*\"example\" + 0.011*\"use\" + 0.011*\"com\" + '\n",
      "  '0.010*\"google\" + 0.010*\"using\" + 0.010*\"wa\" + 0.009*\"io\" + 0.009*\"github\" + '\n",
      "  '0.009*\"following\"'),\n",
      " (1,\n",
      "  '0.063*\"word\" + 0.032*\"vector\" + 0.020*\"document\" + 0.017*\"similarity\" + '\n",
      "  '0.017*\"using\" + 0.016*\"nlp\" + 0.013*\"text\" + 0.013*\"way\" + 0.013*\"python\" + '\n",
      "  '0.013*\"use\" + 0.011*\"list\" + 0.010*\"vec\" + 0.010*\"embedding\" + 0.010*\"get\" '\n",
      "  '+ 0.010*\"one\"'),\n",
      " (2,\n",
      "  '0.055*\"python\" + 0.042*\"error\" + 0.026*\"model\" + 0.018*\"en\" + 0.017*\"using\" '\n",
      "  '+ 0.016*\"core\" + 0.014*\"tried\" + 0.014*\"install\" + 0.013*\"nlp\" + '\n",
      "  '0.013*\"web\" + 0.013*\"get\" + 0.013*\"code\" + 0.012*\"version\" + 0.012*\"load\" + '\n",
      "  '0.012*\"package\"'),\n",
      " (3,\n",
      "  '0.079*\"file\" + 0.041*\"python\" + 0.041*\"program\" + 0.038*\"user\" + '\n",
      "  '0.034*\"package\" + 0.033*\"lib\" + 0.033*\"rasa\" + 0.030*\"py\" + 0.027*\"site\" + '\n",
      "  '0.019*\"line\" + 0.018*\"microsoft\" + 0.017*\"local\" + 0.017*\"nlu\" + '\n",
      "  '0.016*\"appdata\" + 0.016*\"server\"'),\n",
      " (4,\n",
      "  '0.063*\"model\" + 0.046*\"training\" + 0.040*\"data\" + 0.033*\"train\" + '\n",
      "  '0.024*\"ner\" + 0.019*\"text\" + 0.016*\"custom\" + 0.014*\"dataset\" + '\n",
      "  '0.012*\"format\" + 0.012*\"file\" + 0.012*\"label\" + 0.012*\"using\" + '\n",
      "  '0.011*\"code\" + 0.011*\"classification\" + 0.010*\"like\"'),\n",
      " (5,\n",
      "  '0.033*\"sentence\" + 0.028*\"nlp\" + 0.026*\"python\" + 0.023*\"word\" + '\n",
      "  '0.018*\"noun\" + 0.018*\"text\" + 0.017*\"using\" + 0.014*\"dependency\" + '\n",
      "  '0.012*\"like\" + 0.012*\"want\" + 0.011*\"code\" + 0.011*\"get\" + 0.011*\"object\" + '\n",
      "  '0.010*\"verb\" + 0.010*\"po\"'),\n",
      " (6,\n",
      "  '0.030*\"token\" + 0.021*\"sentence\" + 0.021*\"pattern\" + 0.019*\"python\" + '\n",
      "  '0.017*\"like\" + 0.015*\"matcher\" + 0.014*\"list\" + 0.014*\"match\" + '\n",
      "  '0.013*\"column\" + 0.012*\"want\" + 0.012*\"code\" + 0.012*\"one\" + 0.011*\"using\" '\n",
      "  '+ 0.010*\"text\" + 0.010*\"rule\"'),\n",
      " (7,\n",
      "  '0.073*\"entity\" + 0.030*\"ner\" + 0.022*\"nlp\" + 0.022*\"python\" + 0.022*\"text\" '\n",
      "  '+ 0.021*\"named\" + 0.020*\"name\" + 0.019*\"using\" + 0.013*\"recognition\" + '\n",
      "  '0.013*\"extract\" + 0.013*\"sentence\" + 0.012*\"like\" + 0.011*\"new\" + '\n",
      "  '0.010*\"example\" + 0.010*\"use\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel.load(\"nlp_spacy_8.model\")\n",
    "\n",
    "corpus = np.load('spacy.npy', allow_pickle=True).tolist()\n",
    "\n",
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=8, num_words=15)\n",
    "pprint(topicwords)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[834, 810, 471, 191, 51, 20, 0, 0]\n",
      "0.351\n",
      "0.341\n",
      "0.198\n",
      "0.080\n",
      "0.021\n",
      "0.008\n",
      "0.000\n",
      "0.000\n"
     ]
    }
   ],
   "source": [
    "doc_topic_max = []\n",
    "for doc in lda.get_document_topics(corpus):\n",
    "    doc_topic = []\n",
    "    for topic in doc:\n",
    "        doc_topic.append(topic[1])\n",
    "    doc_topic_max.append(doc_topic.index(max(doc_topic)))\n",
    "\n",
    "topics = []\n",
    "for i in range(8):\n",
    "    topics.append(0)\n",
    "for topic in doc_topic_max:\n",
    "    topics[topic]  += 1\n",
    "print(topics)\n",
    "\n",
    "for i in topics:\n",
    "    print('%.3f' %(i/len(doc_topic_max)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.033*\"corenlp\" + 0.025*\"error\" + 0.024*\"file\" + 0.022*\"model\" + '\n",
      "  '0.022*\"java\" + 0.020*\"python\" + 0.018*\"using\" + 0.016*\"code\" + 0.013*\"jar\" '\n",
      "  '+ 0.013*\"use\" + 0.012*\"run\" + 0.012*\"server\" + 0.011*\"following\" + '\n",
      "  '0.011*\"command\" + 0.010*\"http\"'),\n",
      " (1,\n",
      "  '0.065*\"sentiment\" + 0.037*\"analysis\" + 0.032*\"dependency\" + 0.015*\"text\" + '\n",
      "  '0.015*\"data\" + 0.014*\"get\" + 0.014*\"using\" + 0.013*\"corenlp\" + '\n",
      "  '0.011*\"score\" + 0.010*\"result\" + 0.010*\"type\" + 0.009*\"negative\" + '\n",
      "  '0.009*\"sentence\" + 0.009*\"format\" + 0.009*\"http\"'),\n",
      " (2,\n",
      "  '0.058*\"parser\" + 0.051*\"tree\" + 0.032*\"dependency\" + 0.030*\"parse\" + '\n",
      "  '0.028*\"sentence\" + 0.023*\"parsing\" + 0.020*\"output\" + 0.018*\"using\" + '\n",
      "  '0.018*\"np\" + 0.018*\"python\" + 0.017*\"get\" + 0.016*\"nltk\" + 0.014*\"node\" + '\n",
      "  '0.012*\"nn\" + 0.011*\"code\"'),\n",
      " (3,\n",
      "  '0.031*\"sentence\" + 0.018*\"text\" + 0.018*\"using\" + 0.017*\"word\" + 0.017*\"po\" '\n",
      "  '+ 0.016*\"tag\" + 0.016*\"like\" + 0.014*\"get\" + 0.013*\"extract\" + '\n",
      "  '0.012*\"example\" + 0.010*\"tagger\" + 0.010*\"noun\" + 0.010*\"want\" + '\n",
      "  '0.009*\"java\" + 0.009*\"python\"'),\n",
      " (4,\n",
      "  '0.028*\"word\" + 0.017*\"sentence\" + 0.016*\"text\" + 0.014*\"language\" + '\n",
      "  '0.011*\"using\" + 0.011*\"one\" + 0.011*\"use\" + 0.010*\"would\" + 0.010*\"like\" + '\n",
      "  '0.009*\"wa\" + 0.008*\"way\" + 0.008*\"corenlp\" + 0.007*\"need\" + 0.007*\"want\" + '\n",
      "  '0.006*\"time\"'),\n",
      " (5,\n",
      "  '0.058*\"java\" + 0.028*\"file\" + 0.027*\"edu\" + 0.020*\"pipeline\" + '\n",
      "  '0.014*\"error\" + 0.013*\"corenlp\" + 0.013*\"stanfordcorenlp\" + 0.012*\"using\" + '\n",
      "  '0.011*\"code\" + 0.011*\"annotator\" + 0.010*\"run\" + 0.010*\"line\" + '\n",
      "  '0.009*\"tagger\" + 0.009*\"memory\" + 0.009*\"exception\"'),\n",
      " (6,\n",
      "  '0.066*\"ner\" + 0.052*\"entity\" + 0.043*\"model\" + 0.028*\"training\" + '\n",
      "  '0.025*\"named\" + 0.022*\"data\" + 0.020*\"train\" + 0.019*\"classifier\" + '\n",
      "  '0.015*\"file\" + 0.014*\"recognition\" + 0.013*\"using\" + 0.012*\"custom\" + '\n",
      "  '0.011*\"like\" + 0.011*\"crf\" + 0.011*\"feature\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel.load(\"nlp_stanfordnlp_7.model\")\n",
    "\n",
    "corpus = np.load('stanfordnlp.npy', allow_pickle=True).tolist()\n",
    "\n",
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=7, num_words=15)\n",
    "pprint(topicwords)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1445, 969, 519, 147, 12, 4, 1]\n",
      "0.467\n",
      "0.313\n",
      "0.168\n",
      "0.047\n",
      "0.004\n",
      "0.001\n",
      "0.000\n"
     ]
    }
   ],
   "source": [
    "doc_topic_max = []\n",
    "for doc in lda.get_document_topics(corpus):\n",
    "    doc_topic = []\n",
    "    for topic in doc:\n",
    "        doc_topic.append(topic[1])\n",
    "    doc_topic_max.append(doc_topic.index(max(doc_topic)))\n",
    "\n",
    "topics = []\n",
    "for i in range(7):\n",
    "    topics.append(0)\n",
    "for topic in doc_topic_max:\n",
    "    topics[topic]  += 1\n",
    "print(topics)\n",
    "\n",
    "for i in topics:\n",
    "    print('%.3f' %(i/len(doc_topic_max)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.033*\"text\" + 0.021*\"nlp\" + 0.016*\"name\" + 0.016*\"like\" + 0.015*\"would\" + '\n",
      "  '0.013*\"entity\" + 0.012*\"using\" + 0.012*\"question\" + 0.010*\"extract\" + '\n",
      "  '0.010*\"example\" + 0.009*\"use\" + 0.009*\"language\" + 0.009*\"way\" + '\n",
      "  '0.008*\"data\" + 0.007*\"want\"'),\n",
      " (1,\n",
      "  '0.046*\"sentiment\" + 0.039*\"analysis\" + 0.028*\"classifier\" + 0.023*\"data\" + '\n",
      "  '0.020*\"classification\" + 0.019*\"using\" + 0.018*\"tweet\" + 0.018*\"review\" + '\n",
      "  '0.017*\"code\" + 0.015*\"text\" + 0.015*\"feature\" + 0.013*\"positive\" + '\n",
      "  '0.013*\"score\" + 0.012*\"negative\" + 0.011*\"learning\"'),\n",
      " (2,\n",
      "  '0.109*\"sentence\" + 0.028*\"nlp\" + 0.028*\"tree\" + 0.022*\"using\" + '\n",
      "  '0.015*\"like\" + 0.015*\"spacy\" + 0.015*\"grammar\" + 0.014*\"want\" + '\n",
      "  '0.013*\"parse\" + 0.013*\"example\" + 0.012*\"parser\" + 0.011*\"stanford\" + '\n",
      "  '0.011*\"chunk\" + 0.011*\"phrase\" + 0.010*\"parsing\"'),\n",
      " (3,\n",
      "  '0.080*\"file\" + 0.035*\"column\" + 0.027*\"code\" + 0.027*\"dataframe\" + '\n",
      "  '0.025*\"text\" + 0.024*\"data\" + 0.022*\"error\" + 0.020*\"line\" + 0.017*\"csv\" + '\n",
      "  '0.017*\"row\" + 0.016*\"pandas\" + 0.016*\"panda\" + 0.015*\"object\" + 0.015*\"txt\" '\n",
      "  '+ 0.013*\"self\"'),\n",
      " (4,\n",
      "  '0.063*\"word\" + 0.054*\"list\" + 0.027*\"text\" + 0.024*\"string\" + 0.020*\"like\" '\n",
      "  '+ 0.019*\"output\" + 0.019*\"tokenize\" + 0.017*\"code\" + 0.016*\"remove\" + '\n",
      "  '0.014*\"want\" + 0.013*\"stop\" + 0.013*\"using\" + 0.013*\"get\" + '\n",
      "  '0.012*\"sentence\" + 0.011*\"token\"'),\n",
      " (5,\n",
      "  '0.057*\"noun\" + 0.034*\"verb\" + 0.027*\"word\" + 0.023*\"lemmatization\" + '\n",
      "  '0.022*\"nlp\" + 0.018*\"form\" + 0.016*\"using\" + 0.015*\"adjective\" + '\n",
      "  '0.014*\"keywords\" + 0.014*\"want\" + 0.013*\"extract\" + 0.012*\"df\" + '\n",
      "  '0.012*\"lemmatize\" + 0.012*\"array\" + 0.011*\"like\"'),\n",
      " (6,\n",
      "  '0.100*\"model\" + 0.045*\"document\" + 0.033*\"using\" + 0.029*\"gensim\" + '\n",
      "  '0.027*\"topic\" + 0.026*\"learn\" + 0.025*\"data\" + 0.024*\"scikit\" + 0.022*\"tf\" '\n",
      "  '+ 0.022*\"idf\" + 0.021*\"matrix\" + 0.021*\"train\" + 0.021*\"training\" + '\n",
      "  '0.017*\"use\" + 0.016*\"spacy\"'),\n",
      " (7,\n",
      "  '0.092*\"word\" + 0.043*\"wordnet\" + 0.030*\"synset\" + 0.028*\"dictionary\" + '\n",
      "  '0.020*\"similarity\" + 0.018*\"find\" + 0.016*\"list\" + 0.015*\"get\" + '\n",
      "  '0.015*\"english\" + 0.013*\"two\" + 0.013*\"synonym\" + 0.011*\"using\" + '\n",
      "  '0.011*\"like\" + 0.011*\"way\" + 0.010*\"use\"'),\n",
      " (8,\n",
      "  '0.047*\"error\" + 0.021*\"code\" + 0.015*\"using\" + 0.014*\"import\" + '\n",
      "  '0.013*\"package\" + 0.012*\"run\" + 0.012*\"get\" + 0.012*\"following\" + '\n",
      "  '0.011*\"http\" + 0.011*\"tried\" + 0.011*\"trying\" + 0.010*\"module\" + '\n",
      "  '0.010*\"file\" + 0.010*\"py\" + 0.009*\"use\"'),\n",
      " (9,\n",
      "  '0.044*\"word\" + 0.038*\"corpus\" + 0.028*\"frequency\" + 0.027*\"bigram\" + '\n",
      "  '0.025*\"count\" + 0.024*\"gram\" + 0.015*\"code\" + 0.015*\"output\" + 0.015*\"text\" '\n",
      "  '+ 0.014*\"get\" + 0.014*\"find\" + 0.013*\"want\" + 0.012*\"using\" + 0.011*\"would\" '\n",
      "  '+ 0.010*\"nlp\"'),\n",
      " (10,\n",
      "  '0.077*\"tag\" + 0.072*\"po\" + 0.047*\"tagger\" + 0.031*\"corpus\" + 0.026*\"part\" + '\n",
      "  '0.025*\"nn\" + 0.025*\"tagging\" + 0.023*\"speech\" + 0.023*\"tagged\" + '\n",
      "  '0.017*\"nlp\" + 0.014*\"word\" + 0.013*\"using\" + 0.013*\"pos\" + 0.013*\"stanford\" '\n",
      "  '+ 0.010*\"sentence\"'),\n",
      " (11,\n",
      "  '0.071*\"word\" + 0.021*\"vec\" + 0.016*\"number\" + 0.016*\"feature\" + '\n",
      "  '0.015*\"probability\" + 0.014*\"function\" + 0.014*\"class\" + 0.013*\"stemming\" + '\n",
      "  '0.013*\"result\" + 0.012*\"stem\" + 0.012*\"get\" + 0.012*\"value\" + '\n",
      "  '0.010*\"vector\" + 0.010*\"code\" + 0.009*\"learning\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "lda = LdaModel.load(\"nlp_nltk_12_6.model\")\n",
    "\n",
    "corpus = np.load('mynltk.npy', allow_pickle=True).tolist()\n",
    "\n",
    "from pprint import pprint\n",
    "topicwords = lda.print_topics(num_topics=12, num_words=15)\n",
    "pprint(topicwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2095, 1805, 1409, 888, 424, 159, 52, 21, 4, 1, 1, 1]\n",
      "0.305\n",
      "0.263\n",
      "0.205\n",
      "0.129\n",
      "0.062\n",
      "0.023\n",
      "0.008\n",
      "0.003\n",
      "0.001\n",
      "0.000\n",
      "0.000\n",
      "0.000\n"
     ]
    }
   ],
   "source": [
    "doc_topic_max = []\n",
    "for doc in lda.get_document_topics(corpus):\n",
    "    doc_topic = []\n",
    "    for topic in doc:\n",
    "        doc_topic.append(topic[1])\n",
    "    doc_topic_max.append(doc_topic.index(max(doc_topic)))\n",
    "\n",
    "topics = []\n",
    "for i in range(12):\n",
    "    topics.append(0)\n",
    "for topic in doc_topic_max:\n",
    "    topics[topic]  += 1\n",
    "print(topics)\n",
    "\n",
    "for i in topics:\n",
    "    print('%.3f' %(i/len(doc_topic_max)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fb4d7dd3",
   "language": "python",
   "display_name": "PyCharm (DL-LDA)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}