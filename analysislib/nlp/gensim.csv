,Id,CreationDate,DeletionDate,Body,LastEditDate,LastActivityDate,Title,Tags,ClosedDate,CommunityOwnedDate,ContentLicense,nltk,spacy,gensim,stanford-nlp,scikit-learn
61,15670525,2013-03-27 22:12:52,,"<p>I have X as a csr_matrix that I obtained using scikit's tfidf vectorizer, and y which is an array</p>

<p>My plan is to create features using LDA, however, I failed to find how to initialize a gensim's corpus variable with X as a csr_matrix. In other words, I don't want to download a corpus as shown in gensim's documentation nor convert X to a dense matrix, since it would consume a lot of memory and the computer could hang.</p>

<p>In short, my questions are the following,</p>

<ol>
<li>How do you initialize a gensim corpus given that I have a csr_matrix (sparse) representing the whole corpus?</li>
<li>How do you use LDA to extract features?</li>
</ol>
",2013-03-27 22:13:56,2013-03-28 23:27:52,How do you initialize a gensim corpus variable with a csr_matrix?,<python><scikit-learn><document-classification><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
221,9969599,2012-04-02 00:31:23,,"<p>I'm trying to retrieve list of topics from a large corpus of news articles, I'm planning to use gensim to extract a topic distribution for each document using LDA. I want to know the format of processed articles required by gensim implementation of lda and how to convert raw articles to that format. I saw this link about using lda on wikipedia dump but I found the corpus to be in a processed state whose format was not mentioned anywhere</p>
",,2012-11-23 16:35:09,How to use gensim for lda on news articles?,<machine-learning><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
300,13913142,2012-12-17 11:20:56,,"<p>I have trained LDA model using gensim on a text_corpus.</p>

<pre><code>&gt;lda_model = gensim.models.ldamodel.LdaModel(text_corpus, 10)
</code></pre>

<p>Now if a new text document text_sparse_vector has to be inferred I have to do </p>

<pre><code>&gt;lda_model[text_sparse_vector]
[(0, 0.036479568280206563), (3, 0.053828073308160099), (7, 0.021936618544365804), (11, 0.017499953446152686), (15, 0.010153090454090822), (16, 0.35967516223499041), (19, 0.098570351997275749), (26, 0.068550060242800928), (27, 0.08371562828754453), (28, 0.14110945630261607), (29, 0.089938130046832571)]
</code></pre>

<p>But how do I get the word distribution for each of the corresponding topics. For example, How do I know top 20 words for topic number 16 ?</p>

<p>The class gensim.models.ldamodel.LdaModel has method called show_topics(topics=10, topn=10, log=False, formatted=True), but the as the documentation says it shows randomly selected list of topics. </p>

<p>Is there a way to link or print I can map the inferred topic numbers to word distributions ?</p>
",,2019-04-01 16:35:17,How do I get topic numbers in LDA model in gensim,<python><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
524,15016025,2013-02-22 02:47:42,,"<p>Using <code>gensim</code> I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models?</p>

<p>When printing the <code>lda.print_topics(10)</code> the code gave the following error because <code>print_topics()</code> return a <code>NoneType</code>:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/alvas/workspace/XLINGTOP/xlingtop.py"", line 93, in &lt;module&gt;
    for top in lda.print_topics(2):
TypeError: 'NoneType' object is not iterable
</code></pre>

<p>The code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# I can print out the topics for LSA
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus]

for l,t in izip(corpus_lsi,corpus):
  print l,""#"",t
print
for top in lsi.print_topics(2):
  print top

# I can print out the documents and which is the most probable topics for each doc.
lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
corpus_lda = lda[corpus]

for l,t in izip(corpus_lda,corpus):
  print l,""#"",t
print

# But I am unable to print out the topics, how should i do it?
for top in lda.print_topics(10):
  print top
</code></pre>
",,2019-08-28 04:07:15,How to print the LDA topics models from gensim? Python,<python><nlp><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
568,15036048,2013-02-23 01:40:55,,"<p>Why did the tf-idf model in <code>gensim</code> throws away the terms and counts after i transform the corpus?</p>

<p>My code:</p>

<pre><code>from gensim import corpora, models, similarities

# Let's say you have a corpus made up of 2 documents.
doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]

# Train a tfidf model using the corpus
tfidf = models.TfidfModel(corpus)

# Now if you print the corpus, it still remains as the flat frequency counts.
for d in corpus:
  print d
print 

# To convert the corpus into tfidf, re-initialize the corpus 
# according to the model to get the normalized frequencies.
corpus = tfidf[corpus]

for d in corpus:
  print d
</code></pre>

<p>Outputs:</p>

<pre><code>[(0, 1.0), (1, 1.0)]
[(0, 1.0)]
[(0, 1.0), (1, 1.0)]
[(0, 3.0), (1, 1.0)]

[(1, 1.0)]
[]
[(1, 1.0)]
[(1, 1.0)]
</code></pre>
",,2013-03-14 07:35:48,Why did the tf-idf model in `gensim` throws away the terms and counts after i transform the corpus?,<python><nlp><information-retrieval><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
600,15067734,2013-02-25 13:08:28,,"<p>I am using python <code>gensim</code> to train an Latent Dirichlet Allocation (LDA) model from a small corpus of 231 sentences. However, each time i repeat the process, it generates different topics. </p>

<p><strong>Why does the same LDA parameters and corpus generate different topics everytime?</strong></p>

<p><strong>And how do i stabilize the topic generation?</strong></p>

<p>I'm using this corpus (<a href=""http://pastebin.com/WptkKVF0"">http://pastebin.com/WptkKVF0</a>) and this list of stopwords (<a href=""http://pastebin.com/LL7dqLcj"">http://pastebin.com/LL7dqLcj</a>) and here's my code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip
from collections import defaultdict
import codecs, os, glob, math

stopwords = [i.strip() for i in codecs.open('stopmild','r','utf8').readlines() if i[0] != ""#"" and i != """"]

def generateTopics(corpus, dictionary):
    # Build LDA model using the above corpus
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
    corpus_lda = lda[corpus]

    # Group topics with similar words together.
    tops = set(lda.show_topics(50))
    top_clusters = []
    for l in tops:
        top = []
        for t in l.split("" + ""):
            top.append((t.split(""*"")[0], t.split(""*"")[1]))
        top_clusters.append(top)

    # Generate word only topics
    top_wordonly = []
    for i in top_clusters:
        top_wordonly.append("":"".join([j[1] for j in i]))

    return lda, corpus_lda, top_clusters, top_wordonly

####################################################################### 

# Read textfile, build dictionary and bag-of-words corpus
documents = []
for line in codecs.open(""./europarl-mini2/map/coach.en-es.all"",""r"",""utf8""):
    lemma = line.split(""\t"")[3]
    documents.append(lemma)
texts = [[word for word in document.lower().split() if word not in stopwords]
             for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda, corpus_lda, topic_clusters, topic_wordonly = generateTopics(corpus, dictionary)

for i in topic_wordonly:
    print i
</code></pre>
",,2020-03-12 20:09:37,LDA model generates different topics everytime i train on the same corpus,<python><nlp><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
794,15184655,2013-03-03 10:20:55,,"<p><strong>How do I load an LDA transformed corpus from python's <code>gensim</code> ?</strong> What i've tried:</p>

<pre><code>from gensim import corpora, models
import numpy.random
numpy.random.seed(10)

doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]
dictionary = corpora.Dictionary(corpus)

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf.save('x.corpus_tfidf')

# To access the tfidf fitted corpus i've saved i used corpora.MmCorpus.load()
corpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lda = lda[corpus]
corpus_lda.save('x.corpus_lda')

for i,j in enumerate(corpus_lda):
  print j, corpus[i]
</code></pre>

<p>The above code will output:</p>

<pre><code>[(0, 0.54259038344543631), (1, 0.45740961655456358)] [(0, 1), (1, 1)]
[(0, 0.56718063124157458), (1, 0.43281936875842542)] [(0, 1)]
[(0, 0.54255407573666647), (1, 0.45744592426333358)] [(0, 1), (1, 1)]
[(0, 0.75229707773868093), (1, 0.2477029222613191)] [(0, 3), (1, 1)]

# [(&lt;topic_number_from x.corpus_lda model&gt;, 
#   &lt;probability of this topic for this document&gt;), 
#  (&lt;topic# from lda model&gt;, &lt;prob of this top for this doc&gt;)] [&lt;document[i] from corpus&gt;]
</code></pre>

<p><strong>If i want to load the saved LDA transformed corpus, which class from <code>gensim</code> should i be using to load?</strong></p>

<p>I have tried using <code>corpora.MmCorpus.load()</code>, it doesn't give me the same output of the transformed corpus as shown above:</p>

<pre><code>&gt;&gt;&gt; lda_corpus = corpora.MmCorpus.load('x.corpus_lda')
&gt;&gt;&gt; for i,j in enumerate(lda_corpus):
...   print j, corpus[i]
... 
[(0, 0.55087839240547309), (1, 0.44912160759452685)] [(0, 1), (1, 1)]
[(0, 0.56715974584850259), (1, 0.43284025415149735)] [(0, 1)]
[(0, 0.54275680271070581), (1, 0.45724319728929413)] [(0, 1), (1, 1)]
[(0, 0.75233330695720912), (1, 0.24766669304279079)] [(0, 3), (1, 1)]
</code></pre>
",2013-03-19 12:29:25,2014-04-02 22:34:32,Which gensim corpora class should I use to load an LDA transformed corpus? - Python,<python><nlp><corpus><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
904,15260864,2013-03-07 00:24:44,,"<p>All,</p>

<p>This is a re-post to what I responded to over in <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">this thread</a>. I am getting some totally screwy results with trying to print LSI topics in gensim. Here is my code:</p>

<pre><code>try:
    from gensim import corpora, models
except ImportError as err:
    print err

class LSI:
    def topics(self, corpus):
        tfidf = models.TfidfModel(corpus)
        corpus_tfidf = tfidf[corpus]
        dictionary = corpora.Dictionary(corpus)
        lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)
        print lsi.show_topics()

if __name__ == '__main__':
    data = '../data/data.txt'
    corpus = corpora.textcorpus.TextCorpus(data)
    LSI().topics(corpus)
</code></pre>

<p>This prints the following to the console.</p>

<pre><code>-0.804*""(5, 1)"" + -0.246*""(856, 1)"" + -0.227*""(145, 1)"" + ......
</code></pre>

<p>I would like to be able to print out the topics like @2er0 did <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">over here</a> but I am getting results like these. See below and note that the second item that is printed is a tuple and I have no idea where it came from. data.txt is a text file with several paragraphs in it. That is all.</p>

<p>Any thoughts on this would be fantastic! Adam</p>
",2017-05-23 12:11:05,2013-03-12 02:34:07,Gensim topic printing errors/issues,<python><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
971,9470479,2012-02-27 18:48:16,,"<p>From the documents which i found out from the net i figured out the expression used to determine the Term Frequency and Inverse Document frequency weights of terms in a corpus to be</p>

<p>tf-idf(wt)= tf * log(|N|/d);</p>

<p>I was going through the implementation of tf-idf mentioned in gensim.
The example given in the documentation is</p>

<pre><code>&gt;&gt;&gt; doc_bow = [(0, 1), (1, 1)]
&gt;&gt;&gt; print tfidf[doc_bow] # step 2 -- use the model to transform vectors
[(0, 0.70710678), (1, 0.70710678)] 
</code></pre>

<p>Which apparently does not follow the standard implementation of Tf-IDF.
What is the difference between both the models?</p>

<p>Note: 0.70710678 is the value 2^(-1/2) which is used usually in eigen value calculation.
So how does eigen value come into the TF-IDF model?</p>
",2012-03-20 11:28:36,2015-06-13 18:27:33,How is TF-IDF implemented in gensim tool in python?,<python><tf-idf><latent-semantic-indexing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
972,9470899,2012-02-27 19:20:44,,"<p>The popular topic model, Latent Dirichlet Allocation (LDA), which when used to extract topics from a corpus, returns different topics with different probability distributions over the dictionary words.</p>

<p>Whereas Latent Semantic Indexing (LSI) gives the same topics and same distributions after every iteration.</p>

<p>In reality LDA is widely used to extract topics. 
How does LDA maintain consistency if it returns different topic distribution every time a classification is made?</p>

<p>Consider this simple example.
A sample of documents are taken where D represents a document:</p>

<pre><code>D1: Linear Algebra techniques for dimensionality reduction
D2: dimensionality reduction of a sample database
D3: An introduction to linear algebra
D4: Measure of similarity and dissimilarity of different web documents
D5: Classification of data using database sample
D6: overfitting due lack of representative samples
D7: handling overfitting in descision tree
D8: proximity measure for web documents
D9: introduction to web query classification
D10: classification using LSI 
</code></pre>

<p>Each line represents a document.
On the above corpus the LDA model is used to generate the topics from the document.
Gensim is used for LDA, batch LDA is performed where number of topics chosen are 4 and number of passes are 20.</p>

<p>Now on the original corpus the batch LDA is performed and the topics generated after 20 passes are:</p>

<pre><code>topic #0: 0.045*query + 0.043*introduction + 0.042*similarity + 0.042*different + 0.041*reduction + 0.040*handling + 0.039*techniques + 0.039*dimensionality + 0.039*web + 0.039*using

topic #1: 0.043*tree + 0.042*lack + 0.041*reduction + 0.040*measure + 0.040*descision + 0.039*documents + 0.039*overfitting + 0.038*algebra + 0.038*proximity + 0.038*query

topic #2: 0.043*reduction + 0.043*data + 0.042*proximity + 0.041*linear + 0.040*database + 0.040*samples + 0.040*overfitting + 0.039*lsi + 0.039*introduction + 0.039*using

topic #3: 0.046*lsi + 0.045*query + 0.043*samples + 0.040*linear + 0.040*similarity + 0.039*classification + 0.039*algebra + 0.039*documents + 0.038*handling + 0.037*sample
</code></pre>

<p>Now batch LDA is performed on the same original corpus again and the topics generated in that case are:</p>

<pre><code>topic #0: 0.041*data + 0.041*descision + 0.041*linear + 0.041*techniques + 0.040*dimensionality + 0.040*dissimilarity + 0.040*database + 0.040*reduction + 0.039*documents + 0.038*proximity

topic #1: 0.042*dissimilarity + 0.041*documents + 0.041*dimensionality + 0.040*tree + 0.040*proximity + 0.040*different + 0.038*descision + 0.038*algebra + 0.038*similarity + 0.038*techniques

topic #2: 0.043*proximity + 0.042*data + 0.041*database + 0.041*different + 0.041*tree + 0.040*techniques + 0.040*linear + 0.039*classification + 0.038*measure + 0.038*representative

topic #3: 0.043*similarity + 0.042*documents + 0.041*algebra + 0.041*web + 0.040*proximity + 0.040*handling + 0.039*dissimilarity + 0.038*representative + 0.038*tree + 0.038*measure
</code></pre>

<p>The word distribution in each topic is not same in both the cases.
In fact, the word distribution is never the same.</p>

<p>So how does LDA work effectively if it doesn't have the same word distribution in its topics like LSI?</p>
",2018-10-14 11:54:19,2019-05-22 10:12:59,How does LDA give consistent results?,<nlp><lda><topic-modeling><latent-semantic-indexing>,,,CC BY-SA 3.0,False,False,True,False,False
983,16254207,2013-04-27 16:05:52,,"<p>I have to apply LDA (Latent Dirichlet Allocation) to get the possible topics from a data base of 20,000 documents that I collected.</p>

<p>How can I use these documents rather than the other corpus available like the Brown Corpus or English Wikipedia as training corpus ?</p>

<p>You can refer <a href=""http://radimrehurek.com/gensim/models/ldamodel.html"">this</a> page.</p>
",2013-04-27 16:23:15,2017-11-07 15:56:26,Can we use a self made corpus for training for LDA using gensim?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
985,16259652,2013-04-28 04:37:21,,"<p>I have corpus with around 20,000 documents and I have to train that data set for topic modelling using LDA.</p>

<pre><code>import logging, gensim

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
id2word = gensim.corpora.Dictionary('questions.dict')
mm = gensim.corpora.MmCorpus('questions.mm')
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, chunksize=3000, passes=20)
lda.print_topics(20)
</code></pre>

<p>Whenever I run this program I come across this error:</p>

<pre><code>2013-04-28 09:57:09,750 : INFO : adding document #0 to Dictionary(0 unique tokens)
2013-04-28 09:57:09,759 : INFO : built Dictionary(11 unique tokens) from 14 documents (total 14 corpus positions)
2013-04-28 09:57:09,785 : INFO : loaded corpus index from questions.mm.index
2013-04-28 09:57:09,790 : INFO : initializing corpus reader from questions.mm
2013-04-28 09:57:09,796 : INFO : accepted corpus with 19188 documents, 15791 features, 106222 non-zero entries
2013-04-28 09:57:09,802 : INFO : using serial LDA version on this node
2013-04-28 09:57:09,808 : INFO : running batch LDA training, 100 topics, 20 passes over the supplied corpus of 19188 documents, updating model once every 19188 documents
2013-04-28 09:57:10,267 : INFO : PROGRESS: iteration 0, at document #3000/19188

Traceback (most recent call last):
File ""C:/Users/Animesh/Desktop/NLP/topicmodel/lda.py"", line 10, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, chunksize=3000, passes=20)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 265, in __init__
self.update(corpus)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 445, in update
self.do_estep(chunk, other)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 365, in do_estep
gamma, sstats = self.inference(chunk, collect_sstats=True)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 318, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index (11) out of range (0&lt;=index&lt;10) in dimension 1
</code></pre>

<p>I even tried to change the values in <code>LdaModel</code> function but I always get the same error!</p>

<p>What should be done ?</p>
",,2013-05-01 12:51:59,Applying LDA to a corpus for training using gensim,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
995,16262016,2013-04-28 10:39:43,,"<p>I have trained a corpus for LDA topic modelling using gensim.</p>

<p>Going through the tutorial on the gensim website (this is not the whole code):</p>

<pre><code>question = 'Changelog generation from Github issues?';

temp = question.lower()
for i in range(len(punctuation_string)):
    temp = temp.replace(punctuation_string[i], '')

words = re.findall(r'\w+', temp, flags = re.UNICODE | re.LOCALE)
important_words = []
important_words = filter(lambda x: x not in stoplist, words)
print important_words
dictionary = corpora.Dictionary.load('questions.dict')
ques_vec = []
ques_vec = dictionary.doc2bow(important_words)
print dictionary
print ques_vec
print lda[ques_vec]
</code></pre>

<p>This is the output that I get:</p>

<pre><code>['changelog', 'generation', 'github', 'issues']
Dictionary(15791 unique tokens)
[(514, 1), (3625, 1), (3626, 1), (3627, 1)]
[(4, 0.20400000000000032), (11, 0.20400000000000032), (19, 0.20263215848547525), (29, 0.20536784151452539)]
</code></pre>

<p>I don't know how the last output is going to help me find the possible topic for the <code>question</code> !!!</p>

<p>Please help!</p>
",2013-04-28 10:48:48,2016-06-18 05:00:52,How to predict the topic of a new query using a trained LDA model using gensim?,<python><nlp><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1064,11471376,2012-07-13 13:22:10,,"<p>I am using Gensim to do some large-scale topic modeling. I am having difficulty understanding how to determine predicted topics for an unseen (non-indexed) document. For example: I have 25 million documents which I have converted to vectors in LSA (and LDA) space. I now want to figure out the topics of a new document, lets call it x.</p>

<p>According to the Gensim documentation, I can use:</p>

<pre><code>topics = lsi[doc(x)]
</code></pre>

<p>where doc(x) is a function that converts x into a vector.</p>

<p>The problem is, however, that the above variable, topics, returns a vector. The vector is useful if I am comparing x to additional documents because it allows me to find the cosine similarity between them, but I am unable to actually return specific words that are associated with x itself.</p>

<p>Am I missing something, or does Gensim not have this capability?</p>

<p>Thank you,</p>

<p><strong>EDIT</strong></p>

<p>Larsmans has the answer.</p>

<p>I was able to show the topics by using:</p>

<pre><code>for t in topics:
    print lsi.show_topics(t[0])
</code></pre>
",2012-07-14 13:02:21,2014-05-17 16:43:02,Finding topics of an unseen document via Gensim,<python><nlp><latent-semantic-indexing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1075,16309798,2013-04-30 22:10:04,,"<p>I'm using Gensim to calculate the similarity between 2 documents. For some reason the line tfidf[corpus] returns an empty list. I'm not sure why though</p>

<pre><code>    articles = []
#make a corpus by adding each of the top 25 documents to a list
for x in range(0,25):
    articles.append(str(WikiDoc(sorted_links[0]).jsonify()['text']))
#puts all of the top 25 documents into a list
texts = [[word for word in document.lower().split()] for document in articles]
print texts
#load precomputed dictionary
articles_dict = corpora.Dictionary(texts)
articles_dict.save('./articles.dict')
articles_dict = Dictionary.load('./articles.dict')
#articles_corpus = [articles_dict.doc2bow(text) for text in texts]
#corpora.MmCorpus.serialize('./articles.mm', articles_corpus)
corpus = [articles_dict.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('./articles.mm', corpus)
corpus = corpora.MmCorpus('./articles.mm')
#build the tfidf model based on the 25 documents so that we can find similarities 
#with respect to each of these documents
tfidf = models.TfidfModel(corpus)
#get the other document and process to produce dictionary representation
one_doc_bow = WikiDoc('SpongeBob')
one_doc_bow = articles_dict.doc2bow(one_doc_bow.jsonify()['text'].lower().split())
print tfidf[one_doc_bow]
top = tfidf[one_doc_bow]
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>When I print the dictionary I get: Dictionary(2204 unique tokens)
When I print the MmCorpus I get: MmCorpus(25 documents, 2204 features, 55100 non-zero entries)
tfidf[corpus] yield [].
Can anyone diagnose my problem? Thanks a lot!</p>
",,2013-04-30 22:10:04,Calculating TF-IDF Similarity Between 2 Documents Using Gensim,<python><nlp><similarity><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1163,10559591,2012-05-11 23:00:15,,"<p>Jep still playing around with Python. </p>

<p>I decided to try out Gensim, a tool to find out topics for a choosen word &amp; context. </p>

<p>So I wondered how to find a word in a section of text and extract 20 words together with it (as in 10 words before that spectic word and 10 words after that specific word) then to save it together with other such extractions so Gensim could be run on it.</p>

<p>What seems to be hard for me is to find a way to extract the 10 before and after words when the choosen word is found. I played with nltk before and by just tokenizing the text into words or sentences it was easy to get hold of the sentences. Still getting those words or the sentences before and after that specific sentence seems hard for me to figure out how to do.</p>

<p>For those who are confused (it's 1am here so I may be confusing) I'll show it with an example:</p>

<blockquote>
  <p>As soon as it had finished, all her blood rushed to her heart, for she
  was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will I make something which shall destroy her
  completely."" Thus saying, she made a poisoned comb by arts which she
  understood, and then, disguising herself, she took the form of an old
  widow. She went over the seven hills to the house of the seven Dwarfs,
  and[15] knocking at the door, called out, ""Good wares to sell to-day!""</p>
</blockquote>

<p>If we say the word is Snow-White then I'd want to get this part extracted:</p>

<blockquote>
  <p>her heart, for she was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will</p>
</blockquote>

<p>10 word before and after Snow-White. </p>

<p>It is also cool enough to instead get the sentence before and after the sentence Snow-White appeared in if this can be done in nltk and is easier. </p>

<p>I mean whatever works best I shall be happy with one of the two solutions if someone could help me.</p>

<p>If this can be done with Gensim too...and that is easier, then I shall be happy with that too. So any of the 3 ways will be fine...I just want to try and see how this can be done because atm my head is blank.</p>
",,2018-07-02 09:45:34,Extracting a word plus 20 more from a section (python),<python><nltk><extraction><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
1398,16509883,2013-05-12 17:00:00,,"<p>First, is this the right way to get the topic distributions of the corpus on which LDA was performed?</p>

<pre><code>lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2)
#get the topics distribution of the corpus
result=lda[corpus]
</code></pre>

<p>Now the issue occurs when I add the alpha parameter to the LDA and try to convert the corpus to a sparse matrix as follows:</p>

<pre><code>  1- lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2,alpha=0.5)
  2- result=lda[corpus]
  3- gensim.matutils.corpus2csc(result).T
</code></pre>

<p>During the conversion from gensim corpus to the sparse matrix as in line 3, I get the error <code>ValueError: invalid shape</code></p>

<p>I only get this problem when I add the ALPHA parameter!</p>

<p>The complete traceback:</p>

<pre><code>    ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-788-7fb54d5da9fb&gt; in &lt;module&gt;()
----&gt; 1 xp,xc=issam.lda(c)

C:\Anaconda\lib\issamKit.py in lda(X)
   1745      corpus=gensim.matutils.Sparse2Corpus(X.T)
   1746      lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2,alpha=1)
-&gt; 1747      return lda,gensim.matutils.corpus2csc(lda[corpus]).T
   1748 def lsi(X):
   1749      import gensim

C:\Anaconda\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\matutils.pyc in corpus2csc(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)
     97         data = numpy.asarray(data, dtype=dtype)
     98         indices = numpy.asarray(indices)
---&gt; 99         result = scipy.sparse.csc_matrix((data, indices, indptr), shape=(num_terms, num_docs), dtype=dtype)
    100     return result
    101 

C:\Anaconda\lib\site-packages\scipy\sparse\compressed.pyc in __init__(self, arg1, shape, dtype, copy)
     66         # Read matrix dimensions given, if any
     67         if shape is not None:
---&gt; 68             self.shape = shape   # spmatrix will check for errors
     69         else:
     70             if self.shape is None:

C:\Anaconda\lib\site-packages\scipy\sparse\base.pyc in set_shape(self, shape)
     69 
     70         if not (shape[0] &gt;= 1 and shape[1] &gt;= 1):
---&gt; 71             raise ValueError('invalid shape')
     72 
     73         if (self._shape != shape) and (self._shape is not None):

ValueError: invalid shape
</code></pre>
",,2013-12-04 22:25:18,"(Gensim) ValueError: invalid shape, with the alpha parameter",<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1454,16553252,2013-05-14 21:35:04,,"<p>I am on Mac OS X 10.8.3 (Mountain Lion) and am trying to run a script in PyCharm.  Python 2.7.2 is installed, I have installed Canopy and Gensim.  I just do not understand what could be causing the error that I'm getting.</p>

<pre><code>scipy.__version__ 
</code></pre>

<p>shows that v 0.11 is installed.</p>

<p>Here is the entirety of my output following a run of the script:</p>

<pre><code>/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python ""/util/LSA/Base LSA.py""

Traceback (most recent call last):

File ""/util/LSA/Base LSA.py"", line 8, in &lt;module&gt;
    from gensim import corpora, models, similarities, matutils
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/__init__.py"", line 7, in &lt;module&gt;
    import utils, matutils, interfaces, corpora, models, similarities
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/matutils.py"", line 20, in &lt;module&gt;
    import scipy.sparse
ImportError: No module named scipy.sparse

Process finished with exit code 1
</code></pre>
",,2013-05-15 01:44:50,Gensim ImportError in PyCharm: No module named scipy.sparse,<python><scipy><pycharm><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1461,12713797,2012-10-03 17:32:57,,"<p>I am looking to compute similarities between users and text documents using their topic representations. I.e. each document and user is represented by a vector of topics (e.g. Neuroscience, Technology, etc) and how relevant that topic is to the user/document.</p>

<p>My goal is then to compute the similarity between these vectors, so that I can find similar users, articles and recommended articles.</p>

<p>I have tried to use Pearson Correlation but it ends up taking too much memory and time once it reaches ~40k articles and the vectors' length is around 10k.</p>

<p>I am using numpy.</p>

<p>Can you imagine a better way to do this? or is it inevitable (on a single machine)?</p>

<p>Thank you</p>
",2012-10-05 10:29:10,2013-02-25 12:18:03,Topic-based text and user similarity,<python><numpy><recommendation-engine><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1572,12763608,2012-10-06 20:31:45,,"<p>I'm trying to come up with a topic-based recommender system to suggest relevant text documents to users.</p>

<p>I trained a latent semantic indexing model, using gensim, on the wikipedia corpus. This lets me easily transform documents into the LSI topic distributions. My idea now is to represent users the same way. However, of course, users have a history of viewed articles, as well as ratings of articles.</p>

<p>So my question is: how to represent the users?</p>

<p>An idea I had is the following: represent a user as the aggregation of all the documents viewed. But how to take into account the rating?</p>

<p>Any ideas?</p>

<p>Thanks</p>
",2012-10-06 20:50:44,2013-01-29 13:08:48,User profiling for topic-based recommender system,<python><machine-learning><recommendation-engine><latent-semantic-indexing><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
1580,16645799,2013-05-20 08:51:42,,"<p>From <a href=""https://stackoverflow.com/questions/15502802/creating-a-subset-of-words-from-a-corpus-in-r"">Creating a subset of words from a corpus in R</a>, the answerer can easily convert a <code>term-document matrix</code> into a word cloud easily.</p>

<p>Is there a similar function from python libraries that takes either a raw word textfile or <code>NLTK</code> corpus or <code>Gensim</code> Mmcorpus into a word cloud?</p>

<p>The result will look somewhat like this:
<img src=""https://i.stack.imgur.com/ieYK2.png"" alt=""enter image description here""></p>
",2017-05-23 12:10:24,2020-09-09 14:19:47,How to create a word cloud from a corpus in Python?,<python><nltk><corpus><gensim><word-cloud>,,,CC BY-SA 3.0,True,False,True,False,False
1586,23176061,2014-04-19 21:58:42,,"<p>I have a python code that uses sklearn and gensim libraries for tf-idf and LDA(Latent Dirichlet Allocation). Now that I want to migrate to Google app engine I can't use any of these two libraries because they are not supported yet. Is there any service already included in Google app engine that I can use instead of these two libraries to do tf-idf and LDA?</p>
",,2014-04-30 04:33:58,tf-idf and LDA on Google App Engine,<google-app-engine><scikit-learn><tf-idf><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
1672,17662916,2013-07-15 20:06:08,,"<p>The <code>lda.show_topics</code> module from the following code only prints the distribution of the top 10 words for each topic, how do i print out the full distribution of all the words in the corpus?</p>

<pre><code>from gensim import corpora, models

documents = [""Human machine interface for lab abc computer applications"",
""A survey of user opinion of computer system response time"",
""The EPS user interface management system"",
""System and human system engineering testing of EPS"",
""Relation of user perceived response time to error measurement"",
""The generation of random binary unordered trees"",
""The intersection graph of paths in trees"",
""Graph minors IV Widths of trees and well quasi ordering"",
""Graph minors A survey""]

stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)

for i in lda.show_topics():
    print i
</code></pre>
",2015-08-15 04:25:46,2017-10-19 18:32:08,How to print out the full distribution of words in an LDA topic in gensim?,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1681,22079418,2014-02-27 20:18:06,,"<p>The gensim dictionary object has a very nice filtering function to remove tokens that appear in fewer than a set amount of documents. However, I am looking to remove tokens that occur exactly once in the <em>corpus</em>. Does anyone know of a quick and easy way to do this?</p>
",,2017-02-07 21:33:53,filter out tokens that occur exactly once in a gensim dictionary,<python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1703,20953143,2014-01-06 15:21:46,,"<p>I see the following script snippet from the <a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim tutorial page</a>.</p>

<p>What's the syntax of <strong>word for word</strong> in below Python script?</p>

<pre><code>&gt;&gt; texts = [[word for word in document.lower().split() if word not in stoplist]
&gt;&gt;          for document in documents]
</code></pre>
",,2014-01-06 15:43:39,"What does ""word for word"" syntax mean in Python?",<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1709,20954805,2014-01-06 16:42:42,,"<p><strong>Big picture goal:</strong> I am making an LDA model of product reviews in Python using NLTK and Gensim. I want to run this on varying n-grams. </p>

<p><strong>Problem:</strong> Everything is great with unigrams, but when I run with bigrams, I start to get topics with repeated information. For example, Topic 1 might contain: <code>['good product', 'good value']</code>, and Topic 4 might contain: <code>['great product', 'great value']</code>. To a human these are obviously conveying the same information, but obviously <code>'good product'</code> and <code>'great product'</code> are distinct bigrams. How do I algorithmically determine that <code>'good product'</code> and <code>'great product'</code> are similar enough, so I can translate all occurrences of one of them to the other (maybe the one that appears more often in the corpus)?</p>

<p><strong>What I've tried:</strong> I played around with WordNet's Synset tree, with little luck. It turns out that <code>good</code> is an 'adjective', but <code>great</code> is an 'adjective satellite', and therefore return <code>None</code> for path similarity. My thought process was to do the following:</p>

<ol>
<li>Part of speech tag the sentence</li>
<li>Use these POS to find the correct Synset</li>
<li>Compute similarity of the two Synsets</li>
<li>If they are above some threshold, compute occurrences of both words</li>
<li>Replace the least occurring word with the most occurring word</li>
</ol>

<p>Ideally, though, I'd like an algorithm that can determine that <code>good</code> and <code>great</code> are similar <em>in my corpus</em> (perhaps in a co-occurring sense), so that it can be extended to words that aren't part of the general English language, but appear in my corpus, and so that it can be extended to n-grams (maybe <code>Oracle</code> and <code>terrible</code> are synonymous in my corpus, or <code>feature engineering</code> and <code>feature creation</code> are similar).</p>

<p>Any suggestions on algorithms, or suggestions to get WordNet synset to behave?</p>
",,2014-01-07 10:13:49,NLTK - Automatically translating similar words,<python><algorithm><nltk><wordnet><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
1769,23270933,2014-04-24 13:51:10,,"<p>I am using <code>GENSIM</code> on a corpus of 50000 documents along with a dictionary of around 4000 features. I also have a <code>LSI</code> model already prepared for the same. </p>

<p>Now I want to find the highest matching features for each of the added documents. To find the best features in a particular document, I am running gensim's similarity module for each of the features on all the documents. This gives us a score for each of the feature that we want to use later on. But as you can imagine, this is a costly process as we have to iterate over 50000 indices and run 4000 iterations of similarity on each. </p>

<p>I need a better way of doing this as I run out of 8 GB memory on my system at around 1000 iterations. There's actually no reason for the memory to keep rising as I am only reallocating it during the iterations. Surprisingly the memory starts rising only after around 200 iterations. </p>

<ol>
<li>Why the memory issue? How can it be solved?</li>
<li>Is there a better way of finding the highest scored features in a particular document (not topics)?</li>
</ol>

<p>Here's a snippet of the code that runs out of memory:</p>

<pre><code>dictionary = corpora.Dictionary.load('features-dict.dict')
corpus = corpora.MmCorpus('corpus.mm')
lsi = models.LsiModel.load('model.lsi')
corpus_lsi = lsi[corpus]
index = similarities.MatrixSimilarity(list(corpus_lsi))
newDict = dict()

for feature in dictionary.token2id.keys():
  vec_bow = dictionary.doc2bow([feature])
  vec_lsi = lsi[vec_bow]
  sims = index[vec_lsi]
  li = sorted(enumerate(sims * 100), key=lambda item: -item[1])

  for data in li:
    dict[data[0]] = (feature,data[1]) # Store feature and score for each document


# Do something with the dict created above
</code></pre>

<p>EDIT: </p>

<p>The memory issue was resolved using a memory profiler. There was something else in that loop that caused it to rise drastically.</p>

<p>Let me explain the purpose in detail. Imagine we are dealing with various recipes (each recipe is document) and each item in our dictionary is an ingredient. Find six such recipes below.</p>

<p><code>corpus = [[Olive Oil, Tomato, Brocolli, Oregano], [Garlic, Olive Oil, Bread, Cheese, Oregano], [Avocado, Beans, Cheese, Lime], [Jalepeneo, Lime, Tomato, Tortilla, Sour Cream], [Chili Sauce, Vinegar, Mushrooms, Rice], [Soy Sauce, Noodles, Brocolli, Ginger, Vinegar]]</code></p>

<p>There are thousands of such recipes. What I am trying to achieve is to assign a weight between 0 and 100 to each of the ingredient (where higher weighted ingredient is the most important or most unique). What would be the best way to achieve this.</p>
",2014-05-09 06:27:16,2014-05-09 06:27:16,Use Gensim for scoring features in each document. Also a Python memory issue,<python><memory-management><dictionary><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1773,22121028,2014-03-01 22:08:11,,"<p>I have a word2vec model in gensim trained over 98892 documents. For any given sentence that is not present in the sentences array (i.e. the set over which I trained the model), I need to update the model with that sentence so that querying it next time gives out some results. I am doing it like this:</p>

<pre><code>new_sentence = ['moscow', 'weather', 'cold']
model.train(new_sentence)
</code></pre>

<p>and its printing this as logs:</p>

<pre><code>2014-03-01 16:46:58,061 : INFO : training model with 1 workers on 98892 vocabulary and 100 features
2014-03-01 16:46:58,211 : INFO : reached the end of input; waiting to finish 1 outstanding jobs
2014-03-01 16:46:58,235 : INFO : training on 10 words took 0.1s, 174 words/s
</code></pre>

<p>Now, when I query with similar new_sentence for most positives (as <code>model.most_similar(positive=new_sentence)</code>) it gives out error:</p>

<pre><code>Traceback (most recent call last):
 File ""&lt;pyshell#220&gt;"", line 1, in &lt;module&gt;
 model.most_similar(positive=['moscow', 'weather', 'cold'])
 File ""/Library/Python/2.7/site-packages/gensim/models/word2vec.py"", line 405, in most_similar
 raise KeyError(""word '%s' not in vocabulary"" % word)
  KeyError: ""word 'cold' not in vocabulary""
</code></pre>

<p>Which indicates that the word 'cold' is not part of the vocabulary over which i trained the thing (am I right)?</p>

<p>So the question is: How to update the model so that it gives out all the possible similarities for the given new sentence?</p>
",,2019-09-25 01:49:40,Update gensim word2vec model,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
1777,20984841,2014-01-08 00:30:08,,"<p>I am able to run the LDA code from gensim and got the top 10 topics with their respective keywords.</p>

<p>Now I would like to go a step further to see how accurate the LDA algo is by seeing which document they cluster into each topic. Is this possible in gensim LDA?</p>

<p>Basically i would like to do something like this, but in python and using gensim.</p>

<p><a href=""https://stackoverflow.com/questions/14875493/lda-with-topicmodels-how-can-i-see-which-topics-different-documents-belong-to"">LDA with topicmodels, how can I see which topics different documents belong to?</a></p>
",2017-05-23 12:34:08,2020-08-28 21:15:20,Topic distribution: How do we see which document belong to which topic after doing LDA in python,<python><nltk><lda><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
1780,22129943,2014-03-02 16:04:53,,"<p>According to the <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Gensim Word2Vec</a>, I can use the word2vec model in gensim package to calculate the similarity between 2 words.</p>

<p>e.g.</p>

<pre><code>trained_model.similarity('woman', 'man') 
0.73723527
</code></pre>

<p>However, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?</p>
",2016-04-12 13:10:22,2020-01-08 06:38:49,How to calculate the sentence similarity using word2vec model of gensim with python,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
1904,22196248,2014-03-05 11:20:51,,"<p>I'm trying to use <code>gensim's lda</code> model. If I create the lda model with a given corpus, and then I want to update it with a new corpus that contains words that aren't seen in the first corpus, how do I do this? When I try to just call <code>lda_model.update(new_corpus)</code>, I get the following error:</p>

<pre><code>/Library/Python/2.7/site-packages/gensim/models/ldamodel.pyc in inference(self, chunk, collect_sstats)
    361             Elogthetad = Elogtheta[d, :]
    362             expElogthetad = expElogtheta[d, :]
 --&gt;363             expElogbetad = self.expElogbeta[:, ids]
    364 
    365             # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.
   IndexError: index 57 is out of bounds for axis 1 with size 57
</code></pre>

<p>I initialized lda_model with a corpus consisting of only 57 words, so that's why we see the size <code>57</code> bound. Then I wanted to call update on it with a corpus of many more words, and this fails.</p>

<p>How do I get around this? I want to be able to update my lda model with a new corpus with new words is this possible?</p>
",2014-03-05 11:41:42,2017-06-05 08:31:15,gensim lda model - calling update on a corpus with unseen words,<lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1939,23348819,2014-04-28 18:41:57,,"<p>I believe my issue is that python does not play nicely with the character encoding of a column in a SQL table:</p>

<pre><code>| column | varchar(255) | latin1_swedish_ci | YES  |     | NULL              |                             | select,insert,update,references |    | 
</code></pre>

<p>The above shows the output for this column. It has type <code>varchar(255)</code> and has encoding <code>latin1_swedish_ci.</code> </p>

<p>Now when I try to make python play with this data, I am getting the following error: </p>

<pre><code> dictionary = gs.corpora.Dictionary(tweets)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 50, in __init__
    self.add_documents(documents)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 97, in add_documents
    _ = self.doc2bow(document, allow_update=True) # ignore the result, here we only care about updating token ids
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in doc2bow
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in &lt;genexpr&gt;
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/utils.py"", line 164, in any2utf8
    return unicode(text, encoding, errors=errors).encode('utf8')
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 0: invalid start byte
</code></pre>

<p><code>gs</code> is the <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">gensim</a> topic modeling library. I believe that the problem is that gensim requires unicode encodings. </p>

<ol>
<li>How can I change the character encoding (collation?) for this column in my database?</li>
<li>Is there an alternative solution?</li>
</ol>

<p>Thanks for all the help!</p>
",,2018-12-07 08:15:15,Python MySQLdb change string encoding,<python><mysql><encoding><collation><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1953,17765509,2013-07-20 18:45:27,,"<p>I am using this <a href=""http://radimrehurek.com/gensim/tut3.html#similarity-interface"" rel=""nofollow"">gensim</a> tutorial to find similarities between texts. Here is the code </p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey"",
              ""red cow butter oil""]
'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

#print corpus

tfidf = models.TfidfModel(corpus)

#print tfidf

corpus_tfidf = tfidf[corpus]

#print corpus_tfidf

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lsi.print_topics(1)

lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lda.print_topics(1)

corpora.MmCorpus.serialize('dict.mm', corpus)
corpus = corpora.MmCorpus('dict.mm')
#print corpus

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
doc = ""human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
#print vec_lsi

index = similarities.MatrixSimilarity(lsi[corpus])
index.save('dict.index')
index = similarities.MatrixSimilarity.load('dict.index')

sims = index[vec_lsi]
#print list(enumerate(sims))

sims = sorted(enumerate(sims),key=lambda item: -item[1])
for sim in sims:
  print documents[sim[0]], "" ==&gt; "", sim[1]
</code></pre>

<p>There are two documents here. One has 10 texts and another has 2. One is commented out. If I use the first documents list everything goes fine and generates meaningful output. If I use the second document list(having 2 texts) an error occured. Here is it </p>

<pre><code>/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py:122: UserWarning: indices array has non-integer dtype (float64)
% self.indices.dtype.name )
</code></pre>

<p>What is the reason behind this error and how can I fix it?
I am using a 64bit machine.</p>
",,2013-12-04 22:44:49,python gensim: indices array has non-integer dtype (float64),<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
1957,14468078,2013-01-22 21:11:09,,"<p>I am using Gensim python toolkit to build tf-idf model for documents. So I need to create a dictionary for all documents first. However, I found Gensim does not use stemming before creating the dictionary and corpus. Am I right ? </p>
",2013-01-22 21:12:28,2016-02-25 05:36:03,Is stemming used when gensim creates a dictionary for tf-idf model?,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2051,22272370,2014-03-08 17:07:51,,"<p>I am trying to train a word2vec model on very short phrases (5 grams). Since each sentence or example is very short, I believe the window size I can use can atmost be 2. I am trying to understand what the implications of such a small window size are on the quality of the learned model, so that I can understand whether my model has learnt something meaningful or not. I tried training a word2vec model on 5-grams but it appears the learnt model does not capture semantics etc very well.</p>

<p>I am using the following test to evaluate the accuracy of model:
<a href=""https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt"" rel=""noreferrer"">https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a></p>

<p>I used gensim.Word2Vec to train a model and here is a snippet of my accuracy scores (using a window size of 2)</p>

<pre><code>[{'correct': 2, 'incorrect': 304, 'section': 'capital-common-countries'},
 {'correct': 2, 'incorrect': 453, 'section': 'capital-world'},
 {'correct': 0, 'incorrect': 86, 'section': 'currency'},
 {'correct': 2, 'incorrect': 703, 'section': 'city-in-state'},
 {'correct': 123, 'incorrect': 183, 'section': 'family'},
 {'correct': 21, 'incorrect': 791, 'section': 'gram1-adjective-to-adverb'},
 {'correct': 8, 'incorrect': 544, 'section': 'gram2-opposite'},
 {'correct': 284, 'incorrect': 976, 'section': 'gram3-comparative'},
 {'correct': 67, 'incorrect': 863, 'section': 'gram4-superlative'},
 {'correct': 41, 'incorrect': 951, 'section': 'gram5-present-participle'},
 {'correct': 6, 'incorrect': 1089, 'section': 'gram6-nationality-adjective'},
 {'correct': 171, 'incorrect': 1389, 'section': 'gram7-past-tense'},
 {'correct': 56, 'incorrect': 936, 'section': 'gram8-plural'},
 {'correct': 52, 'incorrect': 705, 'section': 'gram9-plural-verbs'},
 {'correct': 835, 'incorrect': 9973, 'section': 'total'}]
</code></pre>

<p>I also tried running the demo-word-accuracy.sh script outlined here with a window size of 2 and get poor accuracy as well:</p>

<pre><code>Sample output:
    capital-common-countries:
    ACCURACY TOP1: 19.37 %  (98 / 506)
    Total accuracy: 19.37 %   Semantic accuracy: 19.37 %   Syntactic accuracy: -nan % 
    capital-world:
    ACCURACY TOP1: 10.26 %  (149 / 1452)
    Total accuracy: 12.61 %   Semantic accuracy: 12.61 %   Syntactic accuracy: -nan % 
    currency:
    ACCURACY TOP1: 6.34 %  (17 / 268)
    Total accuracy: 11.86 %   Semantic accuracy: 11.86 %   Syntactic accuracy: -nan % 
    city-in-state:
    ACCURACY TOP1: 11.78 %  (185 / 1571)
    Total accuracy: 11.83 %   Semantic accuracy: 11.83 %   Syntactic accuracy: -nan % 
    family:
    ACCURACY TOP1: 57.19 %  (175 / 306)
    Total accuracy: 15.21 %   Semantic accuracy: 15.21 %   Syntactic accuracy: -nan % 
    gram1-adjective-to-adverb:
    ACCURACY TOP1: 6.48 %  (49 / 756)
    Total accuracy: 13.85 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 6.48 % 
    gram2-opposite:
    ACCURACY TOP1: 17.97 %  (55 / 306)
    Total accuracy: 14.09 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 9.79 % 
    gram3-comparative:
    ACCURACY TOP1: 34.68 %  (437 / 1260)
    Total accuracy: 18.13 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 23.30 % 
    gram4-superlative:
    ACCURACY TOP1: 14.82 %  (75 / 506)
    Total accuracy: 17.89 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.78 % 
    gram5-present-participle:
    ACCURACY TOP1: 19.96 %  (198 / 992)
    Total accuracy: 18.15 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.31 % 
    gram6-nationality-adjective:
    ACCURACY TOP1: 35.81 %  (491 / 1371)
    Total accuracy: 20.76 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.14 % 
    gram7-past-tense:
    ACCURACY TOP1: 19.67 %  (262 / 1332)
    Total accuracy: 20.62 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 24.02 % 
    gram8-plural:
    ACCURACY TOP1: 35.38 %  (351 / 992)
    Total accuracy: 21.88 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.52 % 
    gram9-plural-verbs:
    ACCURACY TOP1: 20.00 %  (130 / 650)
    Total accuracy: 21.78 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.08 % 
    Questions seen / total: 12268 19544   62.77 % 
</code></pre>

<p>However the word2vec site claims its possible to obtain an accuracy of ~60% on these tasks.
Hence I would like to gain some insights into the effect of these hyperparameters like window size and how they affect quality of learnt models.</p>
",,2019-09-09 09:26:15,Word2Vec: Effect of window size used,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
2066,22283396,2014-03-09 14:25:43,,"<p>I am trying to get related documents for a list of 10,000 documents from the same set of 10,000 docs. I am using two algorithms for testing: gensim lsi and gensim similarity. Both give terrible results. How can I improve it?</p>

<pre><code>from gensim import corpora, models, similarities
from nltk.corpus import stopwords
import re

def cleanword(word):
    return re.sub(r'\W+', '', word).strip()

def create_corpus(documents):

    # remove common words and tokenize
    stoplist = stopwords.words('english')
    stoplist.append('')
    texts = [[cleanword(word) for word in document.lower().split() if cleanword(word) not in stoplist]
             for document in documents]

    # remove words that appear only once
    all_tokens = sum(texts, [])
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)

    texts = [[word for word in text if word not in tokens_once] for text in texts]

    dictionary = corpora.Dictionary(texts)
    corp = [dictionary.doc2bow(text) for text in texts]

def create_lsi(documents):

    corp = create_corpus(documents)
    # extract 400 LSI topics; use the default one-pass algorithm
    lsi = models.lsimodel.LsiModel(corpus=corp, id2word=dictionary, num_topics=400)
    # print the most contributing words (both positively and negatively) for each of the first ten topics
    lsi.print_topics(10)

def create_sim_index(documents):
    corp = create_corpus(documents)
    index = similarities.Similarity('/tmp/tst', corp, num_features=12)
    return index
</code></pre>
",,2015-10-16 07:03:51,Document Similarity Gensim,<python><nlp><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
2088,22286488,2014-03-09 18:47:40,,"<p>Hello GenSim community. 
I am working with Python 2.7.5, and Sublime Text 2. I need to install Gensim. I have tried Enthought but it does not support NLTK. I need to import GenSim in Sublime Text 2. I tried all the command line instructions, to no success. Can someone please tell me how to get GenSim in ST2? I already have Scipy and Numpy installed. I have tried following instructions on the Radim Rehurek webpage, easy_install, and pip install etc. </p>
",,2014-03-10 07:18:28,Python with NLTK and GenSim,<python><sublimetext2><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
2104,18840537,2013-09-17 02:50:55,,"<p>What I want to do is, get a text training set (natural language) and increase this set with automatically created text that tries to mimic the text content. I'm using a bag-of-words assumption, sequence doesn't matter, syntax doesn't matter, I just want to create text that contains words that is pertinent with the general topic of the base.</p>

<p>Right now I'm using <strong>Latent Dirichlet Allocation</strong> to classify my documents in topics distributions, average the topic distribution of my set, and generate documents from these topic distribution.</p>

<p>I want to know two things:</p>

<blockquote>
  <p>1- Is there a better way to do that?</p>
  
  <p>2- Can I train LDA with texts that are not of the domain of my set,
  without tainting my topics: Eg. The set that I want to increase has
  texts about politics. Can I train my model with any kind of text
  (cars, fashion, musics) and classificates my base of politics text get its topics distributions and generates similar text from this distribution.</p>
</blockquote>

<p>I'm using python 2.7 and gensim.</p>
",2013-09-17 03:03:36,2013-09-17 16:56:07,How to generate pertinent text?,<algorithm><language-agnostic><nlp><probability-theory><gensim>,2014-04-24 05:58:27,,CC BY-SA 3.0,False,False,True,False,False
2158,18867516,2013-09-18 08:39:47,,"<p>I can save a serialized corpus into <code>foobar.mm</code> but when i try to load it, it gives <code>UnpicklingError</code>. Loading the dictionary seems fine though. <strong>Anyone knows how to resolve this? And why does this occur?</strong></p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; docs = [""this is a foo bar"", ""you are a foo""]
&gt;&gt;&gt; texts = [[i for i in doc.lower().split()] for doc in docs]
&gt;&gt;&gt; print texts
[['this', 'is', 'a', 'foo', 'bar'], ['you', 'are', 'a', 'foo']]

&gt;&gt;&gt; dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; dictionary.save('foobar.dic')
&gt;&gt;&gt; print dictionary
Dictionary(7 unique tokens)
&gt;&gt;&gt; corpora.Dictionary.load('foobar.dic')
&lt;gensim.corpora.dictionary.Dictionary object at 0x329f910&gt;

&gt;&gt;&gt; corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; corpora.MmCorpus.serialize('foobar.mm', corpus)
&gt;&gt;&gt; corpus = corpora.MmCorpus.load('foobar.mm')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 166, in load
    return unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 492, in unpickle
    return cPickle.load(open(fname, 'rb'))
cPickle.UnpicklingError: invalid load key, '%'.
</code></pre>
",,2013-12-04 22:15:34,How to resolve the unpicklingerror in loading gensim corpus? - python,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2164,23473844,2014-05-05 13:34:04,,"<p>Problem statement: I have several documents(20k documents). I need to apply Topic modelling to find similar documents and then analyze those similar documents to find how those are different from each other. 
Q: Could anyone suggest me any Topic modelling package through which I can achieve this. I am exploring Mallet and Gensim Python. Not sure which would best fit in my requirement. </p>

<p>Any help would be highly appreciated. </p>
",,2014-06-17 15:19:06,Topic Modelling and finding similarity in topics,<topic-modeling><gensim><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
2213,23509699,2014-05-07 05:48:16,,"<p>I tried to examine the contents of the BOW corpus vs. the LDA[BOW Corpus] (transformed by LDA model trained on that corpus with, say,  35 topics)
I found the following output:</p>

<pre><code>DOC 1 : [(1522, 1), (2028, 1), (2082, 1), (6202, 1)]  
LDA 1 : [(29, 0.80571428571428572)]  
DOC 2 : [(1522, 1), (5364, 1), (6202, 1), (6661, 1), (6983, 1)]  
LDA 2 : [(29, 0.83809523809523812)]  
DOC 3 : [(3079, 1), (3395, 1), (4874, 1)]  
LDA 3 : [(34, 0.75714285714285712)]  
DOC 4 : [(1482, 1), (2806, 1), (3988, 1)]  
LDA 4 : [(22, 0.50714288283121989), (32, 0.25714283145449457)]  
DOC 5 : [(440, 1), (533, 1), (1264, 1), (2433, 1), (3012, 1), (3902, 1), (4037, 1), (4502, 1), (5027, 1), (5723, 1)]  
LDA 5 : [(12, 0.075870715371114297), (30, 0.088821329943986921), (31, 0.75219107156801579)]  
DOC 6 : [(705, 1), (3156, 1), (3284, 1), (3555, 1), (3920, 1), (4306, 1), (4581, 1), (4900, 1), (5224, 1), (6156, 1)]  
LDA 6 : [(6, 0.63896110435842401), (20, 0.18441557445724915), (28, 0.09350643806744402)]  
DOC 7 : [(470, 1), (1434, 1), (1741, 1), (3654, 1), (4261, 1)]  
LDA 7 : [(5, 0.17142855723258577), (13, 0.17142856888458904), (19, 0.50476192150187316)]  
DOC 8 : [(2227, 1), (2290, 1), (2549, 1), (5102, 1), (7651, 1)]  
LDA 8 : [(12, 0.16776844589094803), (19, 0.13980868559963203), (22, 0.1728575716782704), (28, 0.37194624921210206)]  
</code></pre>

<p>Where, 
    DOC N is the document from the BOW corpus 
    LDA N is the transformation of DOC N by that LDA model</p>

<p>Am I correct in understanding the output for each transformed document ""LDA N"" to be the topics that the document N belongs to? By that understanding, I can see some documents like 4, 5, 6, 7 and 8 to belong to more than 1 topic like DOC 8 belongs to topics 12, 19, 22 and 28 with the respective probabilities.</p>

<p>Could you please explain the output of LDA N and correct my understanding of this output, especially since in another thread <a href=""https://groups.google.com/forum/#!msg/gensim/Vv7SSC8HR8k/nQGNRE9HjacJ"" rel=""noreferrer"">HERE</a> - by the creator of Gensim himself, it's been mentioned that a document belongs to ONE topic? </p>
",2014-05-07 17:47:43,2018-05-16 10:20:33,Understanding LDA Transformed Corpus in Gensim,<python><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2225,22361438,2014-03-12 19:06:18,,"<p>I have bunch of html documents 10-15 on which i have to apply LDA algorithm in gensim
I am stuck on creating the corpus as i don't understand how i design a corpus for a collection of html documents. The example on the site shows the creation of them on wikipedia compressed file .xml.bz</p>

<p>Anyone please guide me how can i apply LDA on bunch of html documents.
Thanks in advance</p>
",2014-03-12 20:49:18,2014-03-18 23:45:01,LDA for Html Documents in Genism,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2355,22433884,2014-03-16 06:51:25,,"<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.
After studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>
",,2020-06-10 12:47:30,Python Gensim: how to calculate document similarity using the LDA model?,<python><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2381,21313493,2014-01-23 16:09:48,,"<p>I have a total of 54892 documents which have 360331 unique tokens. The length of the dictionary is 88.</p>

<pre><code>mm = corpora.MmCorpus('PRC.mm')
dictionary = corpora.Dictionary('PRC.dict')
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=50, update_every=0, chunksize=19188, passes=650)
</code></pre>

<p>Whenever I run this script I get this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\modelDeTopics.py"", line 19, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=50, update_every=0, chunksize=19188, passes=650)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 265, in __init__
self.update(corpus)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 445, in update
self.do_estep(chunk, other)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 365, in do_estep
gamma, sstats = self.inference(chunk, collect_sstats=True)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 318, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index 8 is out of bounds for axis 1 with size 8
</code></pre>

<p>I check on the internet, it is mentioned that i might be related to the RAM the computer has. I am using Windows 7 32-bit with 4 GB RAM. What change should I make in the script?</p>

<p>Please help!</p>
",,2014-01-30 20:35:36,IndexError while using Gensim package for LDA Topic Modelling,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2399,18988886,2013-09-24 18:07:35,,"<p>In the <code>gensim</code> library, there is a <code>MmReader</code> class that converts a <a href=""http://bickson.blogspot.de/2012/02/matrix-market-format.html"" rel=""nofollow"">matrix market format</a> file into a python object. Sometimes it is necessary to <a href=""https://en.wikipedia.org/wiki/Transpose"" rel=""nofollow"">transpose the matrix</a>, hence the transposed parameter was introduced in the <code>MmReader</code>.</p>

<p>However, I am confused about why is it that at lines <code>525-526</code> and <code>567-568</code> of <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py"" rel=""nofollow"">https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py</a> , the inversion of term-document values and id happens when <code>transposed == False</code>.</p>

<p>Anyone familiar with term-document matrices in information retrieval care to enlighten me?</p>

<pre><code>class MmReader(object):
    """"""
    Wrap a term-document matrix on disk (in matrix-market format), and present it
    as an object which supports iteration over the rows (~documents).

    Note that the file is read into memory one document at a time, not the whole
    matrix at once (unlike scipy.io.mmread). This allows us to process corpora
    which are larger than the available RAM.
    """"""
    def __init__(self, input, transposed=True):
        """"""
        Initialize the matrix reader.

        The `input` refers to a file on local filesystem, which is expected to
        be in the sparse (coordinate) Matrix Market format. Documents are assumed
        to be rows of the matrix (and document features are columns).

        `input` is either a string (file path) or a file-like object that supports
        `seek()` (e.g. gzip.GzipFile, bz2.BZ2File).
        """"""
        logger.info(""initializing corpus reader from %s"" % input)
        self.input, self.transposed = input, transposed
        if isinstance(input, basestring):
            input = open(input)
        header = input.next().strip()
        if not header.lower().startswith('%%matrixmarket matrix coordinate real general'):
            raise ValueError(""File %s not in Matrix Market format with coordinate real general; instead found: \n%s"" %
                             (self.input, header))
        self.num_docs = self.num_terms = self.num_nnz = 0
        for lineno, line in enumerate(input):
            if not line.startswith('%'):
                self.num_docs, self.num_terms, self.num_nnz = map(int, line.split())
                if not self.transposed: ## line 525
                    self.num_docs, self.num_terms = self.num_terms, self.num_docs
                break
        logger.info(""accepted corpus with %i documents, %i features, %i non-zero entries"" %
                     (self.num_docs, self.num_terms, self.num_nnz))

    def __len__(self):
        return self.num_docs

    def __str__(self):
        return (""MmCorpus(%i documents, %i features, %i non-zero entries)"" %
                (self.num_docs, self.num_terms, self.num_nnz))

    def skip_headers(self, input_file):
        """"""
        Skip file headers that appear before the first document.
        """"""
        for line in input_file:
            if line.startswith('%'):
                continue
            break

    def __iter__(self):
        """"""
        Iteratively yield vectors from the underlying file, in the format (row_no, vector),
        where vector is a list of (col_no, value) 2-tuples.

        Note that the total number of vectors returned is always equal to the
        number of rows specified in the header; empty documents are inserted and
        yielded where appropriate, even if they are not explicitly stored in the
        Matrix Market file.
        """"""
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input
            fin.seek(0)
        self.skip_headers(fin)

        previd = -1
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed:
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                # change of document: return the document read so far (its id is prevId)
                if previd &gt;= 0:
                    yield previd, document

                # return implicit (empty) documents between previous id and new id
                # too, to keep consistent document numbering and corpus length
                for previd in xrange(previd + 1, docid):
                    yield previd, []

                # from now on start adding fields to a new document, with a new id
                previd = docid
                document = []

            document.append((termid, val,)) # add another field to the current document

        # handle the last document, as a special case
        if previd &gt;= 0:
            yield previd, document

        # return empty documents between the last explicit document and the number
        # of documents as specified in the header
        for previd in xrange(previd + 1, self.num_docs):
            yield previd, []


    def docbyoffset(self, offset):
        """"""Return document at file offset `offset` (in bytes)""""""
        # empty documents are not stored explicitly in MM format, so the index marks
        # them with a special offset, -1.
        if offset == -1:
            return []
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input

        fin.seek(offset) # works for gzip/bz2 input, too
        previd, document = -1, []
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed: ## line 567
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                if previd &gt;= 0:
                    return document
                previd = docid

            document.append((termid, val,)) # add another field to the current document
        return document
#endclass MmReader
</code></pre>
",,2013-09-25 09:19:00,Transposed parameter in Matrix Market Format of gensim - python,<python><matrix><information-retrieval><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2436,22469506,2014-03-18 02:52:52,,"<p>I'm trying to model twitter stream data with topic models. Gensim, being an easy to use solution, is impressive in it's simplicity. It has a truly online implementation for LSI, but not for LDA. For a changing content stream like twitter, Dynamic Topic Models are ideal. Is there any way, or even a hack - an implementation or even a strategy, using which I can utilize Gensim for this purpose?</p>

<p>Are there any other python implementations which derive (preferably) from Gensim or independent? I am preferring python, since I want to get started asap, but if there is an optimum solution with some work, please mention it.</p>

<p>Thanks.</p>
",,2016-06-02 18:31:50,"Are there any efficient python libraries for Dynamic Topic Models, preferably extending Gensim?",<python><lda><text-analysis><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2531,21403839,2014-01-28 11:04:12,,"<p>I want to cluster words based on their semantic similarity. Currently I have a list of documents with detected noun phrases in them. I want to make cluster out of these obtained nouns within the documents and unsupervisedly cluster them semantically?</p>

<p>I have looked at wordnet and gensim libraries. Any suggestions as to which can really help in getting the required cluster of words based on their semantic similarity?</p>
",2014-01-29 08:57:33,2014-01-30 20:25:14,Unsupervised Clustering of Words in a document semantically,<python><cluster-analysis><semantics><wordnet><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2565,21440132,2014-01-29 18:57:53,,"<p>I am learning Latent semantic analysis (LSA) and I am able to construct term-document matrix and find its SVD decomposition. How can I get the topics from that decomposition?</p>

<p>For example, in gensim:</p>

<pre><code>topic #0(332.762): 0.425*""utc"" + 0.299*""talk"" + 0.293*""page"" + 0.226*""article"" + 0.224*""delete"" + 0.216*""discussion"" + 0.205*""deletion"" + 0.198*""should"" + 0.146*""debate"" + 0.132*""be""
topic #1(201.852): 0.282*""link"" + 0.209*""he"" + 0.145*""com"" + 0.139*""his"" + -0.137*""page"" + -0.118*""delete"" + 0.114*""blacklist"" + -0.108*""deletion"" + -0.105*""discussion"" + 0.100*""diff""
topic #2(191.991): -0.565*""link"" + -0.241*""com"" + -0.238*""blacklist"" + -0.202*""diff"" + -0.193*""additions"" + -0.182*""users"" + -0.158*""coibot"" + -0.136*""user"" + 0.133*""he"" + -0.130*""resolves""
</code></pre>
",2014-01-30 20:48:42,2014-01-31 12:45:54,Latent semantic analysis in finding topics,<algorithm><svd><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2590,23735576,2014-05-19 10:37:21,,"<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia
""<a href=""http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>""</p>

<p>However, I am not sure what is the best preprocessing for this corpus.</p>

<p><code>gensim</code> model accepts a list of tokenized sentences.
My first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>

<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>

<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>

<p>This the code of my first trial:</p>

<pre><code>from gensim.corpora import WikiCorpus
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
corpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)
max_sentence = -1

def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence==-1:
            yield text
        else:
            break

from gensim.models.word2vec import BrownCorpus, Word2Vec
model = Word2Vec() 
model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
model.train(generate_lines(),chunksize=500)
</code></pre>
",2019-09-09 13:26:10,2019-09-09 13:26:10,Gensim train word2vec on wikipedia - preprocessing and parameters,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
2655,18183810,2013-08-12 09:38:20,,"<p>I was just curious about the gensim dictionary implementation. I have the following code:</p>

<pre><code>    def build_dictionary(documents):
        dictionary = corpora.Dictionary(documents)
        dictionary.save('/tmp/deerwester.dict') # store the dictionary
        return dictionary    
</code></pre>

<p>and I looked inside the file deerwester.dict and it looks like this:</p>

<pre><code>8002 6367 656e 7369 6d2e 636f 7270 6f72
612e 6469 6374 696f 6e61 7279 0a44 6963
7469 6f6e 6172 790a 7101 2981 7102 7d71
0328 5508 6e75 6d5f 646f 6373 7104 4b09
5508 ...
</code></pre>

<p>the following code, however,</p>

<pre><code>my_dict = dictionary.load('/tmp/deerwester.dict') 
print my_dict.token2id #view dictionary
</code></pre>

<p>yields this:</p>

<pre><code>{'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
</code></pre>

<p>So my question is, since I don't see the actual words inside the .dict file, what are all of the hexadecimal values stored there? Is this some kind of super compressed format? I'm curious because I feel like if it is, I should consider using it from now on.</p>
",2013-09-18 10:01:44,2013-09-18 10:01:44,Gensim Dictionary Implementation,<python><nlp><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2663,21498633,2014-02-01 13:28:14,,"<p>I am trying to classify emails based on the subject-line, and I have to get the LSI in order to train the classifier. I am getting tf-idf and further trying to get LSI model. However, It does not do any processing/write to any file at all. My code is as below:</p>

<pre><code>#reading the list of subjects for features
f = open('subject1000.csv','rb')
f500 = open('subject500.csv','wb')

with open('subject1000.csv') as myfile:
    head=list(islice(myfile,500))#only 500 subjects for training

for h in head:
    f500.write(h)
    #print h

f500.close()    
texts = (line.lower().split() for line in head) #creating texts of subjects

dictionary = corpora.Dictionary(texts) #all the words used to create dictionary
dictionary.compactify()
print dictionary #checkpoint - 2215 unique tokens -- 2215 unique words to 1418 for 500 topics

#corpus streaming 
class MyCorpus(object):
    def __iter__(self):
        for line in open('subject500.csv','rb'): #supposed to be one document per line -- open('subject1000.csv','rb')
            yield dictionary.doc2bow(line.lower().split())  #every line - converted to bag-of-words format = list of (token_id, token_count) 2-tuples          
print 'corpus created'
corpus = MyCorpus() # object created

for vector in corpus:
    print vector

tfidf = models.TfidfModel(corpus)
corpus_tfidf= tfidf[corpus]  #re-initialize the corpus according to the model to get the normalized frequencies.
corpora.MmCorpus.serialize('subject500-tfidf', corpus_tfidf)  #store to disk for later use

print 'TFIDF complete!' #check - till here its ok

lsi300 = models.LsiModel(corpus_tfidf, num_topics=300, id2word=dictionary) #using the trained corpus to use LSI indexing
corpus_lsi300 = lsi300[corpus_tfidf]
print corpus_lsi300 #checkpoint
lsi300.print_topics(10,5) #checks
corpora.BleiCorpus.serialize('subjects500-lsi-300', corpus_lsi300)
</code></pre>

<p>I get the output till 'TFIDF complete!' but then the program does not return anything for LSI. I am running through 500 subject lines for the above. Any ideas on what might be going wrong will be very much appreciated! Thanks.</p>

<p>The logged data is as below:</p>

<pre><code>INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens)
INFO:gensim.corpora.dictionary:built Dictionary(1418 unique tokens) from 500 documents (total 3109 corpus positions)
DEBUG:gensim.corpora.dictionary:rebuilding dictionary, shrinking gaps
INFO:gensim.models.tfidfmodel:collecting document frequencies
INFO:gensim.models.tfidfmodel:PROGRESS: processing document #0
INFO:gensim.models.tfidfmodel:calculating IDF weights for 500 documents and 1418 features (3081 matrix non-zeros)
INFO:gensim.corpora.mmcorpus:storing corpus in Matrix Market format to subject500-tfidf
INFO:gensim.matutils:saving sparse matrix to subject500-tfidf
INFO:gensim.matutils:PROGRESS: saving document #0
INFO:gensim.matutils:saved 500x1418 matrix, density=0.435% (3081/709000)
DEBUG:gensim.matutils:closing subject500-tfidf
DEBUG:gensim.matutils:closing subject500-tfidf
INFO:gensim.corpora.indexedcorpus:saving MmCorpus index to subject500-tfidf.index
INFO:gensim.models.lsimodel:using serial LSI version on this node
INFO:gensim.models.lsimodel:updating model with new documents
INFO:gensim.models.lsimodel:preparing a new chunk of documents
DEBUG:gensim.models.lsimodel:converting corpus to csc format
INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations
INFO:gensim.models.lsimodel:1st phase: constructing (1418, 400) action matrix
INFO:gensim.models.lsimodel:orthonormalizing (1418, 400) action matrix
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
DEBUG:gensim.models.lsimodel:running 2 power iterations
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
INFO:gensim.models.lsimodel:2nd phase: running dense svd on (400, 500) matrix
</code></pre>
",2014-02-01 19:08:10,2014-06-02 19:47:02,Python LSI using gensim not working,<python><text-processing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2682,17310933,2013-06-26 03:13:39,,"<p>I've derived a LDA topic model using a toy corpus as follows:</p>

<pre><code>documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = corpora.Dictionary(texts)

id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
</code></pre>

<p>I found that when I use a small number of topics to derive the model, Gensim yields a full report of topical distribution over all potential topics for a test document. E.g.:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=5, id2word=id2word)
test_lda[dictionary.doc2bow('human system')]

Out[314]: [(0, 0.59751626959781134),
(1, 0.10001902477790173),
(2, 0.10001375856907335),
(3, 0.10005453508763221),
(4, 0.10239641196758137)]
</code></pre>

<p>However when I use a large number of topics, the report is no longer complete:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=100, id2word=id2word)

test_lda[dictionary.doc2bow('human system')]
Out[315]: [(73, 0.50499999999997613)]
</code></pre>

<p>It seems to me that topics with a probability less than some threshold (I observed 0.01 to be more specific) are omitted form the output.</p>

<p>I'm wondering if this behaviour is due to some aesthetic considerations? And how can I get the distribution of the probability mass residual over all other topics?</p>

<p>Thank you for your kind answer! </p>
",,2015-09-18 12:22:05,Document topical distribution in Gensim LDA,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2712,20349958,2013-12-03 11:31:04,,"<p>I am trying to understand how gensim package in Python implements Latent Dirichlet Allocation. I am doing the following:</p>

<p>Define the dataset</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]             
</code></pre>

<p>After removing stopwords, I create the dictionary and the corpus:</p>

<pre><code>texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then I define the LDA model.</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, update_every=1, chunksize=10000, passes=1)
</code></pre>

<p>Then I print the topics:</p>

<pre><code>&gt;&gt;&gt; lda.print_topics(5)
['0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product', '0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new', '0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is', '0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new', '0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft']
2013-12-03 13:26:21,878 : INFO : topic #0: 0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product
2013-12-03 13:26:21,880 : INFO : topic #1: 0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new
2013-12-03 13:26:21,880 : INFO : topic #2: 0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is
2013-12-03 13:26:21,881 : INFO : topic #3: 0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new
2013-12-03 13:26:21,881 : INFO : topic #4: 0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft
&gt;&gt;&gt; 
</code></pre>

<p>I'm not able to understand much out of this result. Is it providing with a probability of the occurrence of each word? Also, what's the meaning of topic #1, topic #2 etc? I was expecting something more or less like the most important keywords.</p>

<p>I already checked the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">gensim tutorial</a> but it didn't really help much.</p>

<p>Thanks.</p>
",2015-08-22 14:44:31,2019-03-22 16:08:29,Understanding LDA implementation using gensim,<python><topic-modeling><gensim><dirichlet>,,,CC BY-SA 3.0,False,False,True,False,False
2745,20362993,2013-12-03 22:25:56,,"<p>I am trying to use the <a href=""http://radimrehurek.com/gensim/models/word2vec.html""><code>word2vec</code></a> module from <code>gensim</code> natural language processing library in Python.</p>

<p>The docs say to initialize the model:</p>

<pre class=""lang-python prettyprint-override""><code>from gensim.models import word2vec
model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>What format does <code>gensim</code> expect for the input sentences?  I have raw text</p>

<pre><code>""the quick brown fox jumps over the lazy dogs""
""Then a cop quizzed Mick Jagger's ex-wives briefly.""
etc.
</code></pre>

<p>What additional processing do I need to post into <code>word2fec</code>?</p>

<hr>

<p><strong>UPDATE:</strong> Here is what I have tried.  When it loads the sentences, I get nothing.</p>

<pre><code>&gt;&gt;&gt; sentences = ['the quick brown fox jumps over the lazy dogs',
             ""Then a cop quizzed Mick Jagger's ex-wives briefly.""]
&gt;&gt;&gt; x = word2vec.Word2Vec()
&gt;&gt;&gt; x.build_vocab([s.encode('utf-8').split( ) for s in sentences])
&gt;&gt;&gt; x.vocab
{}
</code></pre>
",2013-12-03 23:33:41,2017-03-31 09:18:06,How to load sentences into Python gensim?,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2760,22674660,2014-03-26 22:47:51,,"<p>Once I get topics for a document with lda[doc] I can print each topic using lda.print_topic(topic_id).</p>

<p>What's the equivalent way to retrieve topics for HdpModel? </p>

<p>One way I can think of is using hdp_to_lda to create an LdaModel. Is there a more straightforward way?</p>
",,2014-05-11 02:23:58,How to get the specific topics on HDP,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2762,21552518,2014-02-04 12:25:15,,"<p>I am trying to recycle scikit-learn vectorizer objects with gensim topic models. The reasons are simple: first of all, I already have a great deal of vectorized data; second, I prefer the interface and flexibility of scikit-learn vectorizers; third, even though topic modelling with gensim is very fast, computing its dictionaries (<code>Dictionary()</code>) is relatively slow in my experience.</p>

<p>Similar questions have been asked before, <a href=""https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda"">especially here</a> and <a href=""https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix"">here</a>, and the bridging solution is gensim's <code>Sparse2Corpus()</code> function which transforms a Scipy sparse matrix into a gensim corpus object.</p>

<p>However, this conversion does not make use of the <code>vocabulary_</code> attribute of sklearn vectorizers, which holds the mapping between words and feature ids. This mapping is necessary in order to print the discriminant words for each topic (<code>id2word</code> in gensim topic models, described as ""a a mapping from word ids (integers) to words (strings)"").</p>

<p>I am aware of the fact that gensim's <code>Dictionary</code> objects are much more complex (and slower to compute) than scikit's <code>vect.vocabulary_</code> (a simple Python <code>dict</code>)...</p>

<p>Any ideas to use <code>vect.vocabulary_</code> as <code>id2word</code> in gensim models?</p>

<p>Some example code:</p>

<pre><code># our data
documents = [u'Human machine interface for lab abc computer applications',
        u'A survey of user opinion of computer system response time',
        u'The EPS user interface management system',
        u'System and human system engineering testing of EPS',
        u'Relation of user perceived response time to error measurement',
        u'The generation of random binary unordered trees',
        u'The intersection graph of paths in trees',
        u'Graph minors IV Widths of trees and well quasi ordering',
        u'Graph minors A survey']

from sklearn.feature_extraction.text import CountVectorizer
# compute vector space with sklearn
vect = CountVectorizer(min_df=1, ngram_range=(1, 1), max_features=25000)
corpus_vect = vect.fit_transform(documents)
# each doc is a scipy sparse matrix
print vect.vocabulary_
#{u'and': 1, u'minors': 20, u'generation': 9, u'testing': 32, u'iv': 15, u'engineering': 5, u'computer': 4, u'relation': 28, u'human': 11, u'measurement': 19, u'unordered': 37, u'binary': 3, u'abc': 0, u'for': 8, u'ordering': 23, u'graph': 10, u'system': 31, u'machine': 17, u'to': 35, u'quasi': 26, u'time': 34, u'random': 27, u'paths': 24, u'of': 21, u'trees': 36, u'applications': 2, u'management': 18, u'lab': 16, u'interface': 13, u'intersection': 14, u'response': 29, u'perceived': 25, u'in': 12, u'widths': 40, u'well': 39, u'eps': 6, u'survey': 30, u'error': 7, u'opinion': 22, u'the': 33, u'user': 38}

import gensim
# transform sparse matrix into gensim corpus
corpus_vect_gensim = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)
lsi = gensim.models.LsiModel(corpus_vect_gensim, num_topics=4)
# I instead would like something like this line below
# lsi = gensim.models.LsiModel(corpus_vect_gensim, id2word=vect.vocabulary_, num_topics=2)
print lsi.print_topics(2)
#['0.622*""21"" + 0.359*""31"" + 0.256*""38"" + 0.206*""29"" + 0.206*""34"" + 0.197*""36"" + 0.170*""33"" + 0.168*""1"" + 0.158*""10"" + 0.147*""4""', '0.399*""36"" + 0.364*""10"" + -0.295*""31"" + 0.245*""20"" + -0.226*""38"" + 0.194*""26"" + 0.194*""15"" + 0.194*""39"" + 0.194*""23"" + 0.194*""40""']
</code></pre>
",2017-05-23 12:26:26,2019-07-31 18:31:07,Using scikit-learn vectorizers and vocabularies with gensim,<python><scikit-learn><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
2783,14705944,2013-02-05 11:01:37,,"<p>I am working on tf-idf model. I have little confusion as how this model is implemented. I have constructed model now when I am trying to print the model it is giving different value for the same term. For following two term are giving these result:</p>

<pre><code>doc_bow = [(0, 1), (1, 1)]
val1= tf_idf_corpus[doc_bow] 

doc_bow = [(0,1)]
val2=tf_idf_corpus[doc_bow] 
</code></pre>

<p>Following is the result:</p>

<pre><code>val1= [(0, 0.56486634414605663), (1, 0.82518241210720711)]
val2=[(0, 1.0)]
</code></pre>

<p>I am just curious to know, why tf-idf value of term 0 is 0.5648 in val1 and 1.0 in val2.</p>
",2013-02-23 01:47:29,2013-12-04 22:09:11,Little confusion about how tf-idf model is implemented in gensim,<python><nlp><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2791,23844457,2014-05-24 11:29:06,,"<p>I am trying to save a word2vec to a file.</p>

<pre><code>model = Word2Vec(sentences, size=vector_size, window=5, min_count=5, workers=4)
fo = open(""foo.txt"", ""wb"")
model.save(fo)
</code></pre>

<p>I am getting the following error in genericpath.py</p>

<pre><code>File ""word2Vec_impl.py"", line 39, in &lt;module&gt;
model.save(fo, separately=None)
File ""C:\Python27\lib\site-packages\gensim\models\word2vec.py"", line 669, in s
ave
super(Word2Vec, self).save(*args, **kwargs)
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 269, in save
pickle(self, fname)
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 608, in pickle
with smart_open(fname, 'wb') as fout: # 'b' for binary, needed on Windows
File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 596, in smart_open
_, ext = path.splitext(fname)
File ""C:\Python27\lib\ntpath.py"", line 190, in splitext
return genericpath._splitext(p, sep, altsep, extsep)
File ""C:\Python27\lib\genericpath.py"", line 91, in _splitext
sepIndex = p.rfind(sep)
AttributeError: 'file' object has no attribute 'rfind'
</code></pre>

<p>Where I am going wrong?</p>
",2014-05-24 18:29:37,2014-05-24 18:29:37,'file' object has no attribute 'rfind',<python><word2vec>,2014-05-25 00:30:22,,CC BY-SA 3.0,False,False,True,False,False
2810,23853828,2014-05-25 09:23:37,,"<p><a href=""https://stackoverflow.com/questions/21313493/indexerror-while-using-gensim-package-for-lda-topic-modelling"">Another thread</a> has a similar question to mine but leaves out reproducible code. </p>

<p>The goal with the script in question is to create a process that is as memory efficient as possible. So I tried to write a the class <code>corpus()</code> to take advantage of gensims' capabilities. However, I am running into an IndexError that I'm not sure how to resolve when creating <code>lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))</code>. </p>

<p>The documents that I am using are the same as used in the gensim tutorial, which I placed into tutorial_example.txt:</p>

<pre><code>$ cat tutorial_example.txt 
Human machine interface for lab abc computer applications
A survey of user opinion of computer system response time
The EPS user interface management system
System and human system engineering testing of EPS
Relation of user perceived response time to error measurement
The generation of random binary unordered trees
The intersection graph of paths in trees
Graph minors IV Widths of trees and well quasi ordering
Graph minors A survey
</code></pre>

<h2>Error received</h2>

<pre><code>$./gensim_topic_modeling.py -mn2 -w'english' -l1 tutorial_example.txt 
Traceback (most recent call last):
  File ""./gensim_topic_modeling.py"", line 98, in &lt;module&gt;
    lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 306, in __init__
    self.update(corpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 543, in update
    self.log_perplexity(chunk, total_docs=lencorpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 454, in log_perplexity
    perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 630, in bound
    gammad, _ = self.inference([doc])
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 366, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 7 is out of bounds for axis 1 with size 7
</code></pre>

<p>Below is the <code>gensim_topic_modeling.py</code> script:</p>

<pre><code>##gensim_topic_modeling.py

#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import sys
import re
import codecs
import logging
import fileinput
from operator import *
from itertools import *
from sklearn.cluster import KMeans
from gensim import corpora, models, similarities, matutils
import argparse
from nltk.corpus import stopwords

reload(sys)
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
sys.stdin = codecs.getreader('utf-8')(sys.stdin)


##defs

def stop_word_gen():
    nltk_langs=['danish', 'dutch', 'english', 'french', 'german', 'italian','norwegian', 'portuguese', 'russian', 'spanish', 'swedish']
    stoplist = []
    for lang in options.stop_langs.split("",""):
        if lang not in nltk_langs:
            sys.stderr.write('\n'+""Language {0} not supported"".format(lang)+'\n')
            continue
        stoplist.extend(stopwords.words(lang))
    return stoplist


def clean_texts(texts):
    # remove tokens that appear only once
    all_tokens = sum(texts, [])
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
    return [[word for word in text if word not in tokens_once] for text in texts]

##class

class corpus(object):
    """"""sparse vector matrix and dictionary""""""
    def __iter__(self):
        first=True
        for line in fileinput.FileInput(options.input, openhook=fileinput.hook_encoded(""utf-8"")):
            # assume there's one document per line; tokenizer option determines how to split
            if options.space_tokenizer:
                rl = re.compile('\s+', re.UNICODE).split(unicode(line,'utf-8'))
            else:
                rl = re.compile('\W+', re.UNICODE).split(tagRE.sub(' ',line)) 
            # create dictionary
            tokens=[token.strip().lower() for token in rl if token != '' and token.strip().lower() not in stoplist]
            if first:
                first=False
                self.dictionary=corpora.Dictionary([tokens])
            else:
                self.dictionary.add_documents([tokens])
                self.dictionary.compactify
            yield self.dictionary.doc2bow(tokens)


##main 

if __name__ == '__main__':
    ##parser
    parser = argparse.ArgumentParser(
                description=""Topic model from a column of text.  Each line is a document in the corpus"")
    parser.add_argument(""input"", metavar=""args"")
    parser.add_argument(""-l"", ""--document-frequency-limit"", dest=""doc_freq_limit"", default=1,
                help=""Remove all tokens less than or equal to limit (default 1)"")
    parser.add_argument(""-m"", ""--create-model"", dest=""create_model"", default=False, action=""store_true"",
                help=""Create and save a model from existing dictionary and input corpus."")
    parser.add_argument(""-n"", ""--number-of-topics"", dest=""number_of_topics"", default=2,
                help=""Number of topics (default 2)"")
    parser.add_argument(""-t"", ""--space-tokenizer"", dest=""space_tokenizer"", default=False, action=""store_true"", 
                help=""Use alternate whitespace tokenizer"")
    parser.add_argument(""-w"", ""--stop-word-languages"", dest=""stop_langs"", default=""danish,dutch,english,french,german,italian,norwegian,portuguese,russian,spanish,swedish"",
                help=""Desired languages for stopword lists"")
    options = parser.parse_args()

    ##globals

    stoplist=set(stop_word_gen())  
    tagRE = re.compile(r'&lt;.*?&gt;', re.UNICODE)    # Remove xml/html tags
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename=""topic-modeling-log"")
    logr = logging.getLogger(""topic_model"")
    logr.info(""#""*15 + "" started "" + ""#""*15)

    ##instance of class 

    checker=corpus()
    logr.info(""#""*15 + "" SPARSE MATRIX (pre-filter)"" + ""#""*15)

    ##view sparse matrix and dictionary

    for vector in checker: 
        logr.info(vector)
    logr.info(""#""*15 + "" DICTIONARY (pre-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)
    #filter
    checker.dictionary.filter_extremes(no_below=int(options.doc_freq_limit)+1)
    logr.info(""#""*15 + "" DICTIONARY (post-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)

    ##Create lda model

    if options.create_model:     
        tfidf = models.TfidfModel(checker,normalize=False)
        print tfidf
        logr.info(""#""*15 + "" corpus_tfidf "" + ""#""*15)
        corpus_tfidf = tfidf[checker]
        logr.info(""#""*15 + "" lda "" + ""#""*15)
        lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
        logr.info(""#""*15 + "" corpus_lda "" + ""#""*15)
        corpus_lda = lda[corpus_tfidf] 

        ##Evaluate topics based on threshold

        scores = list(chain(*[[score for topic,score in topic] \
                      for topic in [doc for doc in corpus_lda]]))
        threshold = sum(scores)/len(scores)
        print ""threshold:"",threshold
        print
        cluster1 = [j for i,j in zip(corpus_lda,documents) if i[0][1] &gt; threshold]
        cluster2 = [j for i,j in zip(corpus_lda,documents) if i[1][1] &gt; threshold]
        cluster3 = [j for i,j in zip(corpus_lda,documents) if i[2][1] &gt; threshold]
</code></pre>

<p>The resulting <code>topic-modeling-log</code> file is below. Thanks in advance for any help!</p>

<h1>topic-modeling-log</h1>

<pre><code>2014-05-25 02:58:50,482 : INFO : ############### started ###############
2014-05-25 02:58:50,483 : INFO : ############### SPARSE MATRIX (pre-filter)###############
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,483 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,483 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (27, 1), (28, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(9, 1), (26, 1), (30, 1)]
2014-05-25 02:58:50,485 : INFO : ############### DICTIONARY (pre-filter)###############
2014-05-25 02:58:50,485 : INFO : Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : {'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
2014-05-25 02:58:50,486 : INFO : keeping 12 tokens which were in no less than 2 and no more than 4 (=50.0%) documents
2014-05-25 02:58:50,486 : INFO : resulting dictionary: Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : ############### DICTIONARY (post-filter)###############
2014-05-25 02:58:50,486 : INFO : Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : {'minors': 0, 'graph': 1, 'system': 2, 'trees': 3, 'eps': 4, 'computer': 5, 'survey': 6, 'user': 7, 'human': 8, 'time': 9, 'interface': 10, 'response': 11}
2014-05-25 02:58:50,486 : INFO : collecting document frequencies
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,486 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,486 : INFO : PROGRESS: processing document #0
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,486 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,488 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)
2014-05-25 02:58:50,488 : INFO : ############### corpus_tfidf ###############
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,488 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : ############### lda ###############
2014-05-25 02:58:50,489 : INFO : using symmetric alpha at 0.5
2014-05-25 02:58:50,489 : INFO : using serial LDA version on this node
2014-05-25 02:58:50,489 : WARNING : input corpus stream has no len(); counting documents
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,489 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,489 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,491 : INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50 with a convergence threshold of 0
2014-05-25 02:58:50,491 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,491 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
</code></pre>
",2017-05-23 12:08:50,2015-12-12 02:45:06,python IndexError using gensim for LDA Topic Modeling,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,True,False,True,False,True
2832,17354417,2013-06-27 22:39:49,,"<p>last parts of the code:   </p>

<pre><code>lda = LdaModel(corpus=corpus,id2word=dictionary, num_topics=2)
print lda
</code></pre>

<p>bash output:</p>

<pre><code>INFO : adding document #0 to Dictionary(0 unique tokens)
INFO : built Dictionary(18 unique tokens) from 5 documents (total  20 corpus positions)
INFO : using serial LDA version on this node
INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 5 documents, updating model once every 5 documents
WARNING : too few updates, training might not converge; consider increasing the number of passes to improve accuracy
INFO : PROGRESS: iteration 0, at document #5/5
INFO : 2/5 documents converged within 50 iterations
INFO : topic #0: 0.079*cute + 0.076*broccoli + 0.070*adopted + 0.069*yesterday + 0.069*eat + 0.069*sister + 0.068*kitten + 0.068*kittens + 0.067*bananas + 0.067*chinchillas
INFO : topic #1: 0.082*broccoli + 0.079*cute + 0.071*piece + 0.070*munching + 0.069*spinach + 0.068*hamster + 0.068*ate + 0.067*banana + 0.066*breakfast + 0.066*smoothie
INFO : topic diff=0.470477, rho=1.000000
&lt;gensim.models.ldamodel.LdaModel object at 0x10f1f4050&gt;
</code></pre>

<p>So I'm wondering i'm able to save the resulting topics that it generated, to a readable format. I've tried the <code>.save()</code> methods, but it always outputs something unreadable. </p>
",2016-08-16 14:42:13,2018-08-02 19:00:42,"Gensim: How to save LDA model's produced topics to a readable format (csv,txt,etc)?",<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
2875,23877375,2014-05-26 20:35:36,,"<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
",2018-04-13 14:37:46,2019-04-16 06:55:47,word2vec lemmatization of corpus before training,<nlp><word2vec><gensim><lemmatization>,,,CC BY-SA 3.0,False,False,True,False,False
2926,19315338,2013-10-11 09:58:14,,"<p>Im trying to get started by loading the pretrained .bin files from the google word2vec site ( freebase-vectors-skipgram1000.bin.gz) into the gensim implementation of word2vec. The model loads fine, </p>

<p>using ..</p>

<pre><code>model = word2vec.Word2Vec.load_word2vec_format('...../free....-en.bin', binary= True)
</code></pre>

<p>and creates a </p>

<pre><code>&gt;&gt;&gt; print model
&lt;gensim.models.word2vec.Word2Vec object at 0x105d87f50&gt;
</code></pre>

<p>but when I run the most similar function. It cant find the words in the vocabulary. My error code is below.</p>

<p>Any ideas where Im going wrong?</p>

<pre><code>&gt;&gt;&gt; model.most_similar(['girl', 'father'], ['boy'], topn=3)
2013-10-11 10:22:00,562 : WARNING : word girl not in vocabulary; ignoring it
2013-10-11 10:22:00,562 : WARNING : word father not in vocabulary; ignoring it
2013-10-11 10:22:00,563 : WARNING : word boy not in vocabulary; ignoring it
Traceback (most recent call last):
File , line 1, in
File /....../anaconda/python.app/Contents/lib/python2.7/site-packages/gensim-0.8.7/py2.7.egg/gensim/models/word2vec.py, line 312, in most_similar
raise ValueError(cannot compute similarity with no input)
ValueError: cannot compute similarity with no input
</code></pre>
",2014-01-16 05:40:33,2015-06-10 16:16:26,Working with google word2vec .bin files in gensim python,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
3036,23971900,2014-05-31 15:49:27,,"<p>I do as the gensim tutorial to run LAD on 195145 documents, 6636308 features, 188901082 non-zero entries.
The code is simple:</p>

<pre><code>from gensim import corpora, models, similarities
class MyCorpus(object):
    def __iter__(self):
        for line in open('/home/pda/xxz149/LDA/DrugPatents.csv'):
            # assume there's one document per line, tokens separated by ','
            yield dictionary.doc2bow(line.lower().split(','))
dictionary = corpora.Dictionary.load('/home/pda/xxz149/LDA/DrugPatent.dict')
corpus = MyCorpus()
lda = models.ldamodel.LdaModel(corpus, num_topics = 300,id2word=dictionary,distributed = False,chunksize = 15, passes = 1 )
lda.save('/home/pda/xxz149/LDA/lda_DrugPatent.model')
</code></pre>

<p>But I meet the value error:</p>

<pre><code>  File ""/usr/lib/python2.6/site-packages/gensim-0.10.0rc1-py2.6.egg/gensim/models/ldamodel.py"", line 79, in __init__

    self.sstats = numpy.zeros(shape)
ValueError: array is too big.
</code></pre>

<p>The gensim is memory-friendly, Why this happen? How can I get through?</p>
",2014-10-05 14:54:06,2014-10-05 14:54:06,"How to handle ""ValueError: array is too big."" in Gensim when run LDA?",<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3143,24430238,2014-06-26 12:13:15,,"<p>EDIT: I've found an interesting issue here. <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus/15069580#15069580"">This link</a> shows that gensim uses randomness in both training and inference steps. So what it suggested here is to set a fixed seed in order to get same results every time. Why however am I getting for every topic the same probability?</p>

<p>What I want to do is to find for every twitter user her topics and  calculate the similarity between twitter users based on similarities in topics. Is there any possibility to calculate the same topics for every user in gensim or do I have to calculate a dictionary of topics and cluster every user topic?</p>

<p>In general, which is the best way to compare two twitter users based on topic-models extraction in gensim? My code is the following:</p>

<pre><code>   def preprocess(id): #Returns user word list (or list of user tweet)

        user_list =  user_corpus(id, 'user_'+str(id)+'.txt')
        documents = []
        for line in open('user_'+str(id)+'.txt'):
                 documents.append(line)
        #remove stop words
        lines = [line.rstrip() for line in open('stoplist.txt')]
        stoplist= set(lines)  
        texts = [[word for word in document.lower().split() if word not in stoplist]
                   for document in documents]
        # remove words that appear only once
        all_tokens = sum(texts, [])
        tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) &lt; 3)
        texts = [[word for word in text if word not in tokens_once]
                   for text in texts]
        words = []
        for text in texts:
            for word in text:
                words.append(word)

        return words


    words1 = preprocess(14937173)
    words2 = preprocess(15386966)
    #Load the trained model
    lda = ldamodel.LdaModel.load('tmp/fashion1.lda')
    dictionary = corpora.Dictionary.load('tmp/fashion1.dict') #Load the trained dict

    corpus = [dictionary.doc2bow(words1)]
    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    corpus_lda = lda[corpus_tfidf]

    list1 = []
    for item in corpus_lda:
      list1.append(item)

    print lda.show_topic(0)
    corpus2 = [dictionary.doc2bow(words2)]
    tfidf2 = models.TfidfModel(corpus2)
    corpus_tfidf2 = tfidf2[corpus2]
    corpus_lda2 = lda[corpus_tfidf2]

    list2 = []
    for it in corpus_lda2:
      list2.append(it)

    print corpus_lda.show_topic(0)  
</code></pre>

<p>Returned topic probabilities for user corpus (when using as a corpus a list of user words):</p>

<pre><code> [(0, 0.10000000000000002), (1, 0.10000000000000002), (2, 0.10000000000000002),
  (3, 0.10000000000000002), (4, 0.10000000000000002), (5, 0.10000000000000002),
  (6, 0.10000000000000002), (7, 0.10000000000000002), (8, 0.10000000000000002),
  (9, 0.10000000000000002)]
</code></pre>

<p>In the case where I use a list of user tweets, I get back calculated topics for every tweet.</p>

<p>Question 2: Does the following make sense: training LDA model with several twitter users and calculating the topic for every user (with every user corpus), using the LDA model calculated before?</p>

<p>In provided example, <code>list[0]</code> returns topic distribution with equal probabilities 0.1. 
Basically, every line of text corresponds to a different tweet. If I calculate corpus with <code>corpus = [dictionary.doc2bow(text) for text in texts]</code> it will give me the probabilities for every tweet separately. On the other hand, if I use <code>corpus = [dictionary.doc2bow(words)]</code> like the example, I'll have just all user words as corpus. In the second case, gensim returns the same probabilities for all topics. Thus, for both users I am getting the same topic distributions.</p>

<p>Should user text corpus be a list of words or a list of sentences (a list of tweets)?</p>

<p>Regarding the implementation of Qi He and Jianshu Weng in <a href=""http://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1503&amp;context=sis_research"" rel=""nofollow noreferrer"">twitterRank approach</a> in page 264 it says that: we aggregate the tweets published by individual twitterer into a big document. Thus, each document corresponds to a twitterer. Ok I am confused, if document will be all user tweets then what should the corpus contain??</p>
",2017-05-23 11:44:06,2017-12-20 04:52:18,"LDA gensim implementation, distance between two different docs",<python><probability><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3256,19474333,2013-10-20 05:46:31,,"<p>I am new to Python and Gensim.  I am currently working through one of the tutorials on <code>gensim</code> (<a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html</a>).  I have two question about this line of code:</p>

<pre><code># collect statistics about all tokens
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
</code></pre>

<p>1) Is the file <code>mycorpus.txt</code> fully loaded into memory before the Dictionary starts to get built?    The tutorial explicitly says no:</p>

<pre><code>Similarly, to construct the dictionary without loading all texts into memory
</code></pre>

<p>but when I monitor RAM usage in my Activity Monitor, the Python process hits 1 gig for a 3 gig file (I killed the process midway).  This is strange, as I assumed the dictionary for my 3 gig text file would be MUCH smaller.  Can someone clarify this point for me?</p>

<p>2) How can I recode this line so that I can do stuff between each line read?  I want to print to screen to see the progress.  Here is my attempt:</p>

<pre><code>i = 1

for line in f:
    if i % 1000 == 0:
        print i
    dictionary = corpora.Dictionary([line.lower().split()])
    i += 1
</code></pre>

<p>This doesn't work because dictionary is being reinitialized for every line.</p>

<p>I realize these are very n00b questions - appreciate your help and patience.</p>
",,2013-10-20 05:55:13,Build Dictionary without Loading All Texts,<python><dictionary><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3331,19504898,2013-10-21 21:16:10,,"<p>I've used various versions of TFIDF in scikit learn to model some text data.</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=1,stop_words='english')
</code></pre>

<p>The resulting data X is in this format:</p>

<pre><code>&lt;rowsxcolumns sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with xyz stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>I wanted to experiment with LDA as a way to do reduce dimensionality of my sparse matrix.
Is there a simple way to feed the NumPy sparse matrix X into a gensim LDA model?</p>

<pre><code>lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100)
</code></pre>

<p>I can ignore scikit and go the way the gensim tutorial outlines, but I like the simplicity of the scikit vectorizers and all of its parameters.</p>
",2018-12-09 05:04:37,2018-12-09 05:04:37,Use scikit-learn TfIdf with gensim LDA,<python><scikit-learn><text-mining><lda>,,,CC BY-SA 4.0,False,False,True,False,True
3372,19522258,2013-10-22 15:34:33,,"<p>When trying to install gensim (with pip install and setup install), it gives me this error:</p>

<pre><code>Traceback (most recent call last):
  File ""setup.py"", line 19, in &lt;module&gt;
    import ez_setup
  File ""C:\Users\User\Desktop\gensim-0.8.7\ez_setup.py"", line 106
    except pkg_resources.VersionConflict, e:
                                        ^
SyntaxError: invalid syntax
</code></pre>

<p>How can I solve this</p>
",,2013-10-22 15:54:18,Can't install gensim,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3390,24126187,2014-06-09 18:08:45,,"<p>I was following the instructions on this link (""<a href=""http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/"" rel=""nofollow"">http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/</a>""), however I came across an error when I tried to train the model:</p>

<pre><code>    model = models.LdaMallet(mallet_path, corpus, num_topics =10, id2word = corpus.dictionary)
    IOError: [Errno 2] No such file or directory: 'c:\\users\\brlu\\appdata\\local\\temp\\c6a13a_state.mallet.gz'
</code></pre>

<p>Please share any thoughts you might have. </p>

<p>Thanks.</p>
",,2018-10-13 01:33:20,Error when implementing gensim.LdaMallet,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3499,24178843,2014-06-12 07:33:21,,"<p>I use <a href=""/questions/tagged/gensim"" class=""post-tag"" title=""show questions tagged &#39;gensim&#39;"" rel=""tag"">gensim</a> to build dictionary from a collection of documents. Each document is a list of tokens. this my code</p>

<pre><code>def constructModel(self, docTokens):
    """""" Given document tokens, constructs the tf-idf and similarity models""""""

    #construct dictionary for the BOW (vector-space) model : Dictionary = a mapping between words and their integer ids = collection of (word_index,word_string) pairs
    #print ""dictionary""
    self.dictionary = corpora.Dictionary(docTokens)

    # prune dictionary: remove words that appear too infrequently or too frequently
    print ""dictionary size before filter_extremes:"",self.dictionary#len(self.dictionary.values())
    #self.dictionary.filter_extremes(no_below=1, no_above=0.9, keep_n=100000)
    #self.dictionary.compactify()

    print ""dictionary size after filter_extremes:"",self.dictionary

    #construct the corpus bow vectors; bow vector = collection of (word_id,word_frequency) pairs
    corpus_bow = [self.dictionary.doc2bow(doc) for doc in docTokens]


    #construct the tf-idf model 
    self.model = models.TfidfModel(corpus_bow,normalize=True)
    corpus_tfidf = self.model[corpus_bow]   # first transform each raw bow vector in the corpus to the tfidf model's vector space
    self.similarityModel = similarities.MatrixSimilarity(corpus_tfidf)  # construct the term-document index
</code></pre>

<p>my question is how to add a new doc (tokens) to this dictionary and update it. I searched in gensim documents but I didn't find a solution </p>
",2020-01-30 18:07:54,2020-01-30 18:07:54,how to add tokens to gensim dictionary,<python><gensim><topic-modeling><topicmodels>,,,CC BY-SA 3.0,False,False,True,False,False
3548,23032745,2014-04-12 15:59:46,,"<p>I know that after training the lda model for gensim, we can get the topic for an unseen document by:</p>

<pre><code>lda = LdaModel(corpus, num_topics=10)
doc_lda = lda[doc_bow]
</code></pre>

<p>But how about the documents that are already used for training? I mean is there a way to get the topic for a document in corpus that was used in training without treating it like a new document?</p>
",,2014-04-17 13:18:16,Gensim get topic for a document (seen document),<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3589,19615951,2013-10-27 08:11:23,,"<p>I've been experimenting with LDA topic modelling using <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">Gensim</a>. I couldn't seem to find any topic model evaluation facility in Gensim, which could report on the perplexity of a topic model on held-out evaluation texts thus facilitates subsequent fine tuning of LDA parameters (e.g. number of topics). It would be greatly appreciated if anyone could shed some light on how I can perform topic model evaluation in Gensim. This question has also been posted on <a href=""http://metaoptimize.com/qa/questions/14332/topic-models-evaluation-in-gensim"" rel=""nofollow"">metaoptimize</a>.</p>
",2013-10-27 09:07:27,2013-11-04 05:03:29,Topic models evaluation in Gensim,<lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3599,24212368,2014-06-13 19:09:45,,"<p>I am working on an LDA model with gensim. For this, I am basically opening text files, building a dictionary, and then running the model. </p>

<p>To open the files I use this: </p>

<pre><code>files = [codecs.open(infile, 'r', 'utf-16', 'ignore') for infile in sample_list] 
</code></pre>

<p>in which sample_list is a list of paths to files. I need to use codecs.open because the texts are in a different language (and I haven't updated Python). </p>

<p>My problem is that I don't know how to close all the files after using them. Any ideas? I've tried a couple of things. I cannot use a regular loop here because of my following step is:</p>

<pre><code>texts = ["" "".join(file.readlines()[0:]) for file in files]
</code></pre>

<p>When I use over 5,000 files I get the error '' IOError: [Errno 24] Too many open files '' I am thinking that I could open a number of files at a time,  join them, close them, and repeat. Also, keeping the files open is just bad. 
Thank you!</p>
",2014-06-13 19:10:52,2014-06-13 19:37:10,Python- Closing bunch of text files opened at the same time with list comprehensions,<python><loops><text><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3621,25643004,2014-09-03 11:20:13,,"<p>I've been trying to install word2vec on my Windows 7 machine using my Python2.7 interpreter: <a href=""https://github.com/danielfrg/word2vec"" rel=""noreferrer"">https://github.com/danielfrg/word2vec</a> </p>

<p>I've tried downloading the zip &amp; running python <code>setup.py</code> install from the unzipped directory and running <code>pip install</code>. however in both instances it returns the below errors:</p>

<pre><code>Downloading/unpacking word2vec
  Downloading word2vec-0.5.1.tar.gz
  Running setup.py egg_info for package word2vec
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 16, in &lt;module&gt;
      File ""c:\users\georgioa\appdata\local\temp\pip_build_georgioa\word2vec\setup.py"", line 17, in &lt;module&gt;
        subprocess.call(['make', '-C', 'word2vec-c'])
      File ""C:\Python27\lib\subprocess.py"", line 524, in call
        return Popen(*popenargs, **kwargs).wait()
      File ""C:\Python27\lib\subprocess.py"", line 711, in __init__
        errread, errwrite)
      File ""C:\Python27\lib\subprocess.py"", line 948, in _execute_child
        startupinfo)
    WindowsError: [Error 2] The system cannot find the file specified
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 16, in &lt;module&gt;
  File ""c:\users\georgioa\appdata\local\temp\pip_build_georgioa\word2vec\setup.py"", line 17, in &lt;module&gt;
    subprocess.call(['make', '-C', 'word2vec-c'])
  File ""C:\Python27\lib\subprocess.py"", line 524, in call
    return Popen(*popenargs, **kwargs).wait()
  File ""C:\Python27\lib\subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""C:\Python27\lib\subprocess.py"", line 948, in _execute_child
    startupinfo)
WindowsError: [Error 2] The system cannot find the file specified
</code></pre>

<p>There seemed to be a problem accessing <code>subprocess.call()</code>, so after a bit of googling I managed to add <code>shell=True</code> to the line the the word2vec <code>setup.py</code> and it then throws this error:</p>

<pre><code>'make' is not recognized as an internal or external command,
operable program or batch file.
C:\Python27\lib\distutils\dist.py:267: UserWarning: Unknown distribution option: 'install_requires'
  warnings.warn(msg)
running install
running build
running build_py
running install_lib
running install_data
error: can't copy 'bin\word2vec': doesn't exist or not a regular file 
</code></pre>

<p>To be honest I'm not even sure where I should go from here. I've also tried installing make and setting the path variable to the .exe file in the install, any advice would be greatly appreciated, thanks.</p>

<p><strong>UPDATE:</strong></p>

<p>While the word2vec module wouldn't work a package called <code>genism</code> seems to work pretty well, it's got some great other NLP functionality too <a href=""http://radimrehurek.com/gensim/"" rel=""noreferrer"">http://radimrehurek.com/gensim/</a> </p>
",2017-10-22 03:26:12,2019-04-16 06:06:42,python word2vec not installing,<python><pip><gnuwin32><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
3653,19652908,2013-10-29 08:15:26,,"<p>I have been using LsiModel in gensim for modelling topics from a corpus of 10000 mails. I am able to get the words and word scores for each topic  and store them in a file. I have tried using <strong>print_topics()</strong> and <strong>show_topics()</strong> but both return only the words &amp; score associated with those words. But I also need the topic score that it outputs to the log file, I want those values in a variable.  Like for this example log output below: </p>

<pre><code>2010-11-03 16:08:27,602 : INFO : topic #0(200.990): -0.475*""delete"" + -0.383*""deletion"" + -0.275*""debate"" + -0.223*""comments"" + -0.220*""edits"" + -0.213*""modify"" + -0.208*""appropriate"" + -0.194*""subsequent"" + -0.155*""wp"" + -0.117*""notability""
2010-11-03 16:08:27,626 : INFO : topic #1(143.129): -0.320*""diff"" + -0.305*""link"" + -0.199*""image"" + -0.171*""www"" + -0.162*""user"" + 0.149*""delete"" + -0.147*""undo"" + -0.144*""contribs"" + -0.122*""album"" + 0.113*""deletion""
2010-11-03 16:08:27,651 : INFO : topic #2(135.665): -0.437*""diff"" + -0.400*""link"" + -0.202*""undo"" + -0.192*""user"" + -0.182*""www"" + -0.176*""contribs"" + 0.168*""image"" + -0.109*""added"" + 0.106*""album"" + 0.097*""copyright""
2010-11-03 16:08:27,677 : INFO : topic #3(125.027): -0.354*""image"" + 0.239*""age"" + 0.218*""median"" + -0.213*""copyright"" + 0.204*""population"" + -0.195*""fair"" + 0.195*""income"" + 0.167*""census"" + 0.165*""km"" + 0.162*""households""
2010-11-03 16:08:27,701 : INFO : topic #4(116.927): -0.307*""image"" + 0.195*""players"" + 0.184*""median"" + -0.184*""copyright"" + -0.181*""age"" + -0.167*""fair"" + -0.162*""income"" + -0.151*""population"" + -0.136*""households"" + -0.134*""census""
</code></pre>

<p>I need these score in a variable.   </p>

<pre><code>topic #0 : 200.990 
topic #1 : 143.129
topic #2 : 135.665
topic #3 : 125.027
topic #4 : 116.927
</code></pre>

<p>Is there any method in the package to get these outputs? Please help. </p>
",,2013-12-09 19:40:18,How to obtain the topic score in LSI model of Gensim?,<python><gensim><latent-semantic-indexing>,,,CC BY-SA 3.0,False,False,True,False,False
3664,24242287,2014-06-16 11:06:53,,"<p>Does gensim give us hierarchy of topics?
I write a code to calculate topic of some documents, the output is words of each topic.
But I want hierarchy of topics.
this is my code:</p>

<p><a href=""https://gist.github.com/anonymous/2e3b2f3866e5029c55c3"" rel=""nofollow"">https://gist.github.com/anonymous/2e3b2f3866e5029c55c3</a></p>

<p>and this is output:</p>

<pre><code>2014-06-16 13:02:22,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-06-16 13:02:38,162 : INFO : built Dictionary(324451 unique tokens: [u'considered,', u'\x00\x00', u'\ufb90\ufee0\ufbff\ufeea', u'\u0627\ufee7\ufed4\ufb91\ufe8e\u0643', u'\u0627\u0628\u0631\u0631\u0627\u06cc\u0627\u0646\u0647']...) from 885 documents (total 3885556 corpus positions)
2014-06-16 13:02:38,545 : INFO : storing corpus in Matrix Market format to corpus.mm
2014-06-16 13:02:38,546 : INFO : saving sparse matrix to corpus.mm
2014-06-16 13:02:38,554 : INFO : PROGRESS: saving document #0
2014-06-16 13:02:45,290 : INFO : saved 884x79405 matrix, density=0.514% (360672/70194020)
2014-06-16 13:02:45,292 : INFO : saving MmCorpus index to corpus.mm.index
2014-06-16 13:02:45,293 : INFO : loaded corpus index from corpus.mm.index
2014-06-16 13:02:45,293 : INFO : initializing corpus reader from corpus.mm
2014-06-16 13:02:45,293 : INFO : accepted corpus with 884 documents, 79405 features, 360672 non-zero entries
2014-06-16 13:03:06,913 : INFO : topic 0: 0.010* + 0.006* + 0.006* + 0.006* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004* + 0.004*
2014-06-16 13:03:07,097 : INFO : topic 1: 0.000* + 0.000* + 0.000*Single + 0.000* + 0.000* + 0.000* + 0.000*352 + 0.000* + 0.000* + 0.000* + 0.000* + 0.000* + 0.000*    + 0.000*. + 0.000*19-20. + 0.000*67 + 0.000* + 0.000* + 0.000* + 0.000*
</code></pre>

<p>Is there any way to get the hierarchy of topics?</p>
",,2016-07-26 19:00:57,Get hierarchy of topic from gensim,<python><scipy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3832,26812617,2014-11-08 01:13:02,,"<p>I read the docs I have</p>

<pre><code>corpusObj.readDocsSample(sampleFile)
</code></pre>

<p>Next,</p>

<pre><code>dictionary = corpusObj.buildDictionary()
</code></pre>

<p>Then I build a corpus:</p>

<pre><code>corpus = corpusObj.buildCorpus()
</code></pre>

<p>Definition of buildDictionary and buildCorpus:</p>

<pre><code>def buildDictionary(self):

         texts = [[word for word in self.docs[i]] for i in self.docs]
         self.dictionary = corpora.Dictionary(texts)
         return self.dictionary


def buildCorpus(self):
         return [self.dictionary.doc2bow(words) for words in self.docs.itervalues()]
</code></pre>

<p>Then I do stop words stuff:</p>

<pre><code>stop = corpus.readStopWords()
stopids = [dictionary.token2id[stopword] for stopword in stop
         if stopword in dictionary.token2id]
dictionary.filter_tokens(stopids)
dictionary.compactify()
</code></pre>

<p>Then I call:</p>

<pre><code> lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100, update_every=1, chunksize=1000, passes=1)
</code></pre>

<p>Here is the error:</p>

<pre><code> Traceback (most recent call last):
File ""/Users/jsuit/PycharmProjects/MyGensimPlaything/GensimPlayToy.py"", line 33, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100,     update_every=1, chunksize=1000, passes=1)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 313, in __init__
self.update(corpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 553, in update
self.log_perplexity(chunk, total_docs=lencorpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 464, in log_perplexity
perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 639, in bound
gammad, _ = self.inference([doc])
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 376, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index 46979 is out of bounds for axis 1 with size 46979
</code></pre>

<p>The logging information below shows that it gets started but then crashes.</p>

<pre><code>2014-11-07 19:31:56,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-11-07 19:32:00,458 : INFO : built Dictionary(47445 unique tokens: [u'Szczecin', u'pro-Soviet', u'Negroponte', u'1,800', u'woods']...) from 2250 documents (total 1050902 corpus positions)
2014-11-07 19:32:08,192 : DEBUG : rebuilding dictionary, shrinking gaps
2014-11-07 19:32:08,237 : INFO : using symmetric alpha at 0.01
2014-11-07 19:32:08,237 : INFO : using serial LDA version on this node
2014-11-07 19:32:08,856 : INFO : running online LDA training, 100 topics, 1 passes over   
the supplied corpus of 2250 documents, updating model once every 1000 documents,           
evaluating perplexity every 2250 documents, iterating 50x with a convergence threshold of 
0.001000
2014-11-07 19:32:08,856 : WARNING : too few updates, training might not converge; 
consider increasing the number of passes or iterations to improve accuracy
2014-11-07 19:32:08,931 : INFO : PROGRESS: pass 0, at document #1000/2250
2014-11-07 19:32:08,931 : DEBUG : performing inference on a chunk of 1000 documents
2014-11-07 19:32:15,414 : DEBUG : 22/1000 documents converged within 50 iterations
2014-11-07 19:32:15,432 : DEBUG : updating topics
2014-11-07 19:32:15,476 : INFO : merging changes from 1000 documents into a model of 2250  
documents
2014-11-07 19:32:16,222 : INFO : topic #60 (0.010): 0.057*Neeman + 0.042*woods + 0.039*needed + 0.024*timeout + 0.020*reggae + 0.020*Shocked + 0.019*Dexter + 0.015*nonsensical + 0.014*3-to-1 + 0.011*Mauritius
2014-11-07 19:32:16,237 : INFO : topic #45 (0.010): 0.049*needed + 0.047*Neeman + 0.042*woods + 0.024*reggae + 0.023*timeout + 0.022*Dexter + 0.022*Shocked + 0.019*nonsensical + 0.012*3-to-1 + 0.011*mid-week
2014-11-07 19:32:16,251 : INFO : topic #86 (0.010): 0.049*needed + 0.048*Neeman + 0.047*woods + 0.029*Shocked + 0.023*timeout + 0.017*nonsensical + 0.016*reggae + 0.016*3-to-1 + 0.014*Dexter + 0.014*Mauritius
2014-11-07 19:32:16,265 : INFO : topic #92 (0.010): 0.017*Neeman + 0.016*needed + 0.014*woods + 0.011*Dexter + 0.010*timeout + 0.009*reggae + 0.006*Shocked + 0.006*nonsensical + 0.005*22-month-old + 0.004*3-to-1
2014-11-07 19:32:16,279 : INFO : topic #95 (0.010): 0.045*needed + 0.041*woods + 0.032*Shocked + 0.028*Neeman + 0.022*timeout + 0.020*nonsensical + 0.018*reggae + 0.017*Dexter + 0.013*Mauritius + 0.010*3-to-1
2014-11-07 19:32:16,294 : INFO : topic #30 (0.010): 0.054*needed + 0.052*Neeman + 0.033*woods + 0.024*timeout + 0.022*nonsensical + 0.021*Dexter + 0.021*Shocked + 0.016*reggae + 0.013*Mauritius + 0.012*3-to-1
2014-11-07 19:32:16,307 : INFO : topic #51 (0.010): 0.000*expands + 0.000*Promotion + 0.000*Arnold + 0.000*1,320.75 + 0.000*credits + 0.000*tuition + 0.000*_Or + 0.000*Hunt + 0.000*Futrell + 0.000*stagecoaches
2014-11-07 19:32:16,321 : INFO : topic #41 (0.010): 0.045*Neeman + 0.032*needed + 0.031*woods + 0.016*Dexter + 0.013*nonsensical + 0.013*Shocked + 0.013*timeout + 0.011*reggae + 0.009*peux + 0.009*Mauritius
2014-11-07 19:32:16,336 : INFO : topic #28 (0.010): 0.052*Neeman + 0.046*needed + 0.040*woods + 0.030*timeout + 0.026*Shocked + 0.019*nonsensical + 0.018*Dexter + 0.014*reggae + 0.011*3-to-1 + 0.010*crouch
2014-11-07 19:32:16,351 : INFO : topic #11 (0.010): 0.046*Neeman + 0.044*woods + 0.037*needed + 0.031*Shocked + 0.021*Dexter + 0.021*reggae + 0.017*nonsensical + 0.017*timeout + 0.012*3-to-1 + 0.010*Mauritius
2014-11-07 19:32:16,365 : INFO : topic #20 (0.010): 0.067*Neeman + 0.036*woods + 0.035*needed + 0.028*timeout + 0.020*reggae + 0.018*Dexter + 0.016*Mauritius + 0.015*Shocked + 0.015*nonsensical + 0.014*mid-week
2014-11-07 19:32:16,379 : INFO : topic #31 (0.010): 0.001*Neeman + 0.001*woods + 0.001*timeout + 0.001*reggae + 0.000*needed + 0.000*Dexter + 0.000*3-to-1 + 0.000*Shocked + 0.000*romped + 0.000*1,800
2014-11-07 19:32:16,393 : INFO : topic #80 (0.010): 0.043*woods + 0.042*Neeman + 0.037*needed + 0.029*timeout + 0.024*Shocked + 0.017*nonsensical + 0.015*reggae + 0.014*Dexter + 0.011*1,800 + 0.011*3-to-1
2014-11-07 19:32:16,407 : INFO : topic #58 (0.010): 0.029*Neeman + 0.027*needed + 0.019*woods + 0.019*timeout + 0.013*Dexter + 0.010*Shocked + 0.008*nonsensical + 0.008*mid-week + 0.007*reggae + 0.007*3-to-1
 2014-11-07 19:32:16,421 : INFO : topic #79 (0.010): 0.002*woods + 0.002*needed + 0.002*sustaining + 0.001*Neeman + 0.001*timeout + 0.001*Godchaux + 0.001*Dexter + 0.001*dozen + 0.001*rumor + 0.001*Miami-based
 2014-11-07 19:32:16,437 : INFO : topic diff=78.067282, rho=1.000000  
 2014-11-07 19:32:16,523 : INFO : PROGRESS: pass 0, at document #2000/2250
 2014-11-07 19:32:16,523 : DEBUG : performing inference on a chunk of 1000 documents
 2014-11-07 19:32:22,841 : DEBUG : 38/1000 documents converged within 50 iterations
 2014-11-07 19:32:22,862 : DEBUG : updating topics
 2014-11-07 19:32:22,919 : INFO : merging changes from 1000 documents into a model of     
 2250 documents
 2014-11-07 19:32:23,640 : INFO : topic #63 (0.010): 0.017*Neeman + 0.016*needed + 0.014*autobiography + 0.013*teacher + 0.012*woods + 0.011*Mauritius + 0.010*Shocked + 0.009*timeout + 0.007*mid-week + 0.007*CFC
 2014-11-07 19:32:23,654 : INFO : topic #8 (0.010): 0.028*Neeman + 0.027*woods + 0.024*Shocked + 0.023*needed + 0.016*timeout + 0.013*Dexter + 0.010*reggae + 0.010*nonsensical + 0.007*Mauritius + 0.007*65-plus
 2014-11-07 19:32:23,669 : INFO : topic #85 (0.010): 0.054*needed + 0.041*woods + 0.036*Neeman + 0.036*Shocked + 0.031*timeout + 0.023*nonsensical + 0.016*reggae + 0.014*Dexter + 0.011*3-to-1 + 0.011*crouch
 2014-11-07 19:32:23,683 : INFO : topic #18 (0.010): 0.017*needed + 0.013*woods + 0.011*Neeman + 0.010*timeout + 0.009*Shocked + 0.008*reggae + 0.008*Guttierez + 0.006*livid + 0.006*Vermont + 0.006*Dexter
  2014-11-07 19:32:23,697 : INFO : topic #49 (0.010): 0.028*needed + 0.028*Neeman + 0.026*woods + 0.024*timeout + 0.019*Dexter + 0.017*nonsensical + 0.012*reggae + 0.009*Shocked + 0.007*3-to-1 + 0.007*crouch
 2014-11-07 19:32:23,712 : INFO : topic #53 (0.010): 0.035*mid-week + 0.034*Mauritius + 0.028*Neeman + 0.028*needed + 0.027*woods + 0.024*Tourism + 0.023*macho + 0.014*Shocked + 0.013*nonsensical + 0.012*timeout
 2014-11-07 19:32:23,726 : INFO : topic #32 (0.010): 0.071*Neeman + 0.031*woods + 0.022*needed + 0.015*timeout + 0.013*Shocked + 0.012*Dexter + 0.012*Mauritius + 0.009*nonsensical + 0.009*mid-week + 0.007*reggae
 2014-11-07 19:32:23,740 : INFO : topic #78 (0.010): 0.040*needed + 0.039*Neeman + 0.039*woods + 0.019*timeout + 0.019*Shocked + 0.017*Dexter + 0.017*reggae + 0.016*nonsensical + 0.015*3-to-1 + 0.011*mid-week
 2014-11-07 19:32:23,754 : INFO : topic #94 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*timeout + 0.001*Dexter + 0.001*nonsensical + 0.001*Shocked + 0.001*3-to-1 + 0.001*reggae + 0.000*dozen
 2014-11-07 19:32:23,768 : INFO : topic #17 (0.010): 0.023*needed + 0.022*woods + 0.018*Neeman + 0.018*timeout + 0.012*1,800 + 0.011*reggae + 0.011*Shocked + 0.010*Dexter + 0.008*Anderson + 0.007*Bovek
 2014-11-07 19:32:23,781 : INFO : topic #73 (0.010): 0.001*woods + 0.001*timeout + 0.001*Neeman + 0.001*Shocked + 0.001*reggae + 0.001*Falcon + 0.001*Dexter + 0.001*needed + 0.001*dozes + 0.001*dozen
 2014-11-07 19:32:23,796 : INFO : topic #96 (0.010): 0.049*Neeman + 0.048*needed + 0.043*woods + 0.025*timeout + 0.019*nonsensical + 0.018*Shocked + 0.015*Dexter + 0.013*3-to-1 + 0.011*reggae + 0.010*crouch
 2014-11-07 19:32:23,810 : INFO : topic #46 (0.010): 0.042*needed + 0.035*Neeman + 0.033*woods + 0.025*Shocked + 0.019*3-to-1 + 0.016*reggae + 0.016*timeout + 0.013*Dexter + 0.012*nonsensical + 0.011*Mauritius
 2014-11-07 19:32:23,824 : INFO : topic #39 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*Shocked + 0.001*Dexter + 0.001*mid-week + 0.001*timeout + 0.001*18th + 0.001*nonsensical + 0.001*Mauritius
 2014-11-07 19:32:23,838 : INFO : topic #66 (0.010): 0.036*Neeman + 0.032*woods + 0.021*needed + 0.020*timeout + 0.019*Shocked + 0.019*Blank + 0.014*cares + 0.013*Dexter + 0.011*reggae + 0.010*nonsensical
 2014-11-07 19:32:23,855 : INFO : topic diff=5.923197, rho=0.707107
 2014-11-07 19:32:24,260 : DEBUG : bound: at document #0
</code></pre>

<p>And then we get the error message I posted above.</p>
",,2014-12-03 00:50:48,Index Error when running LDA in gensim,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3869,27615804,2014-12-23 07:21:19,,"<p>I am trying to install Gensim which I downloaded from <a href=""https://pypi.python.org/pypi/gensim#downloads"" rel=""nofollow noreferrer"">this</a> site. The installation using pip is also not working for me. I am getting the error below.</p>

<pre><code>C:\Users\Anirudh\Downloads\gensim-0.10.3&gt;python setup.py install
running install
running bdist_egg
running egg_info
writing requirements to gensim.egg-info\requires.txt
writing gensim.egg-info\PKG-INFO
writing top-level names to gensim.egg-info\top_level.txt
writing dependency_links to gensim.egg-info\dependency_links.txt
reading manifest file 'gensim.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
no previously-included directories found matching 'docs\src*'
writing manifest file 'gensim.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_py
running build_ext
building 'gensim.models.word2vec_inner' extension
Unable to find vcvarsall.bat
an integer is required
Traceback (most recent call last):
  File ""setup.py"", line 166, in &lt;module&gt;
    include_package_data=True,
  File ""C:\Python27\lib\distutils\core.py"", line 151, in setup
    dist.run_commands()
  File ""C:\Python27\lib\distutils\dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install.py"", line 73, in run
    self.do_egg_install()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install.py"", line 93, in do_egg_install
    self.run_command('bdist_egg')
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\bdist_egg.py"", line 179, in run
    cmd = self.call_command('install_lib', warn_dir=0)
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\bdist_egg.py"", line 166, in call_command
    self.run_command(cmdname)
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install_lib.py"", line 20, in run
    self.build()
  File ""C:\Python27\lib\distutils\command\install_lib.py"", line 111, in build
    self.run_command('build_ext')
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""setup.py"", line 71, in run
    ""There was an issue with your platform configuration - see above."")
TypeError: an integer is required
</code></pre>

<p>I have seen the <a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">same question</a> in stackoverflow. I also did what was mentioned in <a href=""https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat"">this</a> question. I have installed Microsoft c++ compiler for python 2.7. Is there any windows binary for Gensim? How can I install it.</p>
",2017-05-23 11:52:16,2014-12-31 14:07:59,Difficulty installing Gensim using from source and pip,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3899,27801403,2015-01-06 15:05:56,,"<p>I'm trying to run this example code in Python 2.7 for LSI text clustering.</p>

<pre><code>import gensim
from gensim import corpora, models, similarities

documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)

texts = [[word for word in text if word not in tokens_once] for text in texts]

dictionary = corpora.Dictionary(texts)
corp = [dictionary.doc2bow(text) for text in texts]

# extract 400 LSI topics; use the default one-pass algorithm
lsi = gensim.models.lsimodel.LsiModel(corpus=corp, id2word=dictionary, num_topics=400)

# print the most contributing words (both positively and negatively) for each of the first ten topics
lsi.print_topics(10)
</code></pre>

<p>But it returns this error for the 2nd last command.</p>

<pre><code>Warning (from warnings module):
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 2499
    VisibleDeprecationWarning)
VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
</code></pre>

<p>Please let me know what I am doing wrong or if i need to update anything to make it work.</p>
",,2015-01-06 15:05:56,LSI Clustering using gensim in python,<python><python-2.7><cluster-analysis><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3918,27789298,2015-01-05 22:52:13,,"<p>I posted this issue on github (<a href=""https://github.com/piskvorky/gensim/issues/274"" rel=""nofollow"">https://github.com/piskvorky/gensim/issues/274</a>)</p>

<p>However, I need help with how to actually use the compatibility with numpy that gensim has.</p>

<p>I tried passing in None, <code>len(corpus)</code>, and 0-2 all failing.</p>

<p>The following is the corpus:</p>

<pre><code>[(0, 1.0), (1, 1.0), (2, 1.0)]
[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]
[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]
[(1, 1.0), (5, 2.0), (8, 1.0)]
[(3, 1.0), (6, 1.0), (7, 1.0)]
[(9, 1.0)]
[(9, 1.0), (10, 1.0)]
[(9, 1.0), (10, 1.0), (11, 1.0)]
[(4, 1.0), (10, 1.0), (11, 1.0)]
</code></pre>

<p>This is the code which doesn't work in my iPython notebook:</p>

<pre><code>from gensim import matutils
corpus = corpora.MmCorpus('/tmp/corpus.mm')
import numpy
numpy_matrix = matutils.corpus2dense(corpus)
</code></pre>

<p>Which throws IndexErrors</p>
",,2015-01-05 23:17:12,"corpus2dense requires two arguments, but tutorial example only uses one",<python><numpy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3932,27632404,2014-12-24 06:17:47,,"<p>I am trying to use gensim word2vec. I am unable to train the model based on Brown Corpus. Here is my code.</p>

<pre><code>from gensim import models

model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
model.save(""E:\\data.bin"")
</code></pre>

<p>I downloaded nltk_data using <code>nltk.download()</code>.  I am getting the error below.</p>

<pre><code>C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py:401: UserWarning: Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`
  warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
Traceback (most recent call last):
  File ""E:\eclipse_workspace\Python_files\Test\Test.py"", line 8, in &lt;module&gt;
    model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 276, in __init__
    self.train(sentences)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 407, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>What am I doing wrong?</p>
",,2018-01-17 17:53:24,How should I train gensim on Brown corpus,<python><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
3964,25798954,2014-09-12 00:31:16,,"<p>I am trying to use word2vec in Windows 7. I have 24GB of RAM, and i7 processor, and I am using 64-bit python. I am trying to follow the tutorial by Radim. I want to access the vectors in the google 3 billion file provided by the word2vec original page.
When I run the line:</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 536, in load_word2vec_format
result.syn0 = zeros((vocab_size, layer1_size), dtype=REAL)
MemoryError
</code></pre>

<p>I am not sure how to solve this problem since the file is only 1.3GB and I have plenty of free memory space.</p>
",2014-09-12 00:51:43,2014-09-12 00:51:43,MemoryError in reading word2vc data file to python,<python><memory><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
3966,27659985,2014-12-26 17:25:45,,"<p>I am trying to do the following <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial"" rel=""noreferrer"">kaggle assignmnet</a>. I am using gensim package to use word2vec. I am able to create the model and store it to disk. But when I am trying to load the file back I am getting the error below.</p>

<pre><code>    -HP-dx2280-MT-GR541AV:~$ python prog_w2v.py 
Traceback (most recent call last):
  File ""prog_w2v.py"", line 7, in &lt;module&gt;
    models = gensim.models.Word2Vec.load_word2vec_format('300features_40minwords_10context.txt', binary=True)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 579, in load_word2vec_format
    header = utils.to_unicode(fin.readline())
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 190, in any2unicode
    return unicode(text, encoding, errors=errors)
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>I find similar question. But I was unable to solve the problem. My prog_w2v.py is as below.</p>

<pre><code>import gensim
import time
start = time.time()    
models = gensim.models.Word2Vec.load_word2vec_format('300features_40minwords_10context.txt', binary=True) 
end = time.time()   
print end-start,""   seconds""
</code></pre>

<p>I am trying to generate the model using <a href=""http://ideone.com/9DXo4w"" rel=""noreferrer"">code here</a>. The program takes about half an hour to generate the model. Hence I am unable to run it many times to debug it. </p>
",2017-03-02 17:58:03,2018-12-03 06:21:07,Error: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte,<python><character-encoding><gensim><word2vec><kaggle>,,,CC BY-SA 3.0,False,False,True,False,False
3974,27957112,2015-01-15 05:04:05,,"<p>This is my first question at Stack Overflow.</p>

<p>I have a DataFrame of Pandas like this.</p>

<pre><code>        a   b   c   d
one     0   1   2   3
two     4   5   6   7
three   8   9   0   1
four    2   1   1   5
five    1   1   8   9
</code></pre>

<p>I want to extract the pairs of column name and data whose data is 1 and each index is separate at array.</p>

<pre><code>[ [(b,1.0)], [(d,1.0)], [(b,1.0),(c,1.0)], [(a,1.0),(b,1.0)] ]
</code></pre>

<p>I want to use gensim of python library which requires corpus as this form.</p>

<p>Is there any smart way to do this or to apply gensim from pandas data?</p>
",,2017-08-09 20:00:54,"Extract array (column name, data) from Pandas DataFrame",<python><pandas><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
3979,25803267,2014-09-12 07:46:07,,"<p><strong>Situation:</strong></p>

<p>I have a numpy term-document matrix 
example: [[0,1,0,0....],....[......0,0,0,0]].</p>

<p>I have plugged in the above matrix to the ldamodel method of the gensim. And it is working fine with the lad method <code>lda = LdaModel(corpus, num_topics=10)</code>. 
<code>corpus</code> is my term-document matrix mentioned above.
I needed two intermediate matrices( <strong>topic-word array &amp; document-topic array</strong>) for research purpose.</p>

<blockquote>
  <p>1) per document-topic probability matrix (p_d_t)</p>
  
  <p>2) per topic-word probability matrix (p_w_t)</p>
</blockquote>

<p><strong>Question:</strong></p>

<p>How to get those array from the gensim <code>LdaModel()</code> function.? Kindly help me with getting those matrices.</p>
",2014-09-12 22:24:29,2014-09-21 02:45:02,retrieve topic-word array & document-topic array from lda gensim,<lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4045,24688116,2014-07-10 23:53:45,,"<p>I am using <code>gensim</code> for some NLP task. I've created a corpus from <code>dictionary.doc2bow</code> where <code>dictionary</code> is an object of <code>corpora.Dictionary</code>. Now I want to filter out the terms with low tf-idf values before running an LDA model. I looked into the <a href=""http://radimrehurek.com/gensim/corpora/mmcorpus.html"" rel=""noreferrer"">documentation</a> of the corpus class but cannot find a way to access the terms. Any ideas? Thank you.</p>
",2014-07-11 00:00:06,2018-11-01 03:30:20,How to filter out words with low tf-idf in a corpus with gensim?,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4067,28034688,2015-01-19 22:35:40,,"<p>I am trying to install <a href=""http://radimrehurek.com/gensim/index.html"" rel=""nofollow"">Gensim</a> but I am getting the errors displayed below. I am running Anaconda 2.1.0 with Python 2.7.8 and NumPy 1.9.0 on a Windows 8.1 machine. I already have the Windows SDK 8.1.</p>

<p>It says something about a deprecated NumPy version 1.7, which seems odd because I am running NumPy 1.9.0.</p>

<p>I also have a Anaconda3 installation with Python 3.4, though I removed those from my PATH in order to be able to run Python 2 in cmd, because I need to work on a project in Python 2. Running <code>python --version</code> returns ""Python 2.7.8"".</p>

<pre><code>In [9]: %run setup.py install
running install
running bdist_egg
running egg_info
writing requirements to gensim.egg-info\requires.txt
writing gensim.egg-info\PKG-INFO
writing top-level names to gensim.egg-info\top_level.txt
writing dependency_links to gensim.egg-info\dependency_links.txt
reading manifest file 'gensim.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
no previously-included directories found matching 'docs\src*'
writing manifest file 'gensim.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_py
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\gcc.exe -DMS_WIN64
-mdll -O -Wall -IC:\Users\Robert-Jan\Downloads\gensim-0.10.3\gensim-0.10.3\gensi
m\models -IC:\Anaconda\include -IC:\Anaconda\PC -IC:\Anaconda\lib\site-packages\
numpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2
.7\Release\.\gensim\models\word2vec_inner.o
In file included from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rraytypes.h:1804:0,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rrayobject.h:17,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/arr
ayobject.h:4,
                 from ./gensim/models/word2vec_inner.c:232:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/npy_1_7_deprecated_api.h:
12:9: note: #pragma message: C:\Anaconda\lib\site-packages\numpy\core\include/nu
mpy/npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disa
ble it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseArgtupleInvalid':
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: format '%.1s' expects type 'c
har *', but argument 5 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: too many arguments for format

./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseTooManyValuesError':
./gensim/models/word2vec_inner.c:10235:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10235:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseNeedMoreValuesError':
./gensim/models/word2vec_inner.c:10241:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10241:18: warning: format '%.1s' expects type '
char *', but argument 3 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:10241:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: At top level:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__multiarray_api.h:1629:1
: warning: '_import_array' defined but not used
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__ufunc_api.h:241:1: warn
ing: '_import_umath' defined but not used
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_train_sentence_sg':
./gensim/models/word2vec_inner.c:5271:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:5274:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:5275:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:5276:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:5277:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_2train_sentence_cbow':
./gensim/models/word2vec_inner.c:6080:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:6083:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:6084:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:6085:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:6086:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
writing build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.def
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\dllwrap.exe -DMS_WI
N64 -mdll -static --entry _DllMain@12 --output-lib build\temp.win-amd64-2.7\Rele
ase\.\gensim\models\libword2vec_inner.a --def build\temp.win-amd64-2.7\Release\.
\gensim\models\word2vec_inner.def -s build\temp.win-amd64-2.7\Release\.\gensim\m
odels\word2vec_inner.o -LC:\Anaconda\libs -LC:\Anaconda\PCbuild\amd64 -lpython27
 -lmsvcr90 -o build\lib.win-amd64-2.7\gensim\models\word2vec_inner.pyd
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x23fb): undefined reference to `_imp__PyExc_TypeError'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2406): undefined reference to `_imp__PyErr_Format'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2447): undefined reference to `_imp__PyDict_Next'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x246f): undefined reference to `_imp__PyString_Type'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x24aa): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x251f): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2555): undefined reference to `_imp__PyUnicodeUCS2_Compare'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2561): undefined reference to `_imp__PyErr_Occurred'
[...]
</code></pre>

<p>...and it goes on throwing up undefined references.</p>
",,2015-12-22 13:37:31,Gensim not installing - Windows 8.1 - Python,<python><numpy><windows-8.1><anaconda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4076,25829768,2014-09-14 03:21:13,,"<p>I have to admit I am not a programmer, but I am in charge of deployment. Now I met a big problem. 
In our production environment, the loading of corpus dictionary always fails and the error is </p>

<pre><code>  File ""/django/rcxue/osqa/rcxue/QuestMatchV2.py"", line 121, in loadCorpus
    corpus = [dictionary.doc2bow(text) for text in corpus_m] 

  AttributeError: 'NoneType' object has no attribute 'doc2bow'
</code></pre>

<p>However, the same codes work fine in both our test environment and in developer's local environment. 
I make test server connect to production database and the loading works, which means the database is ok.</p>

<p>I checked every setting file in every directory.The files required are all there and all 'path' are ok. I re-installed all the dependencies frozen from test environment. But I am unable to find the root cause. </p>

<p>May anyone give me some advice how should I proceed my troubleshooting.</p>
",2014-09-14 03:41:48,2014-09-14 03:41:48,python corpus: error reported when loading dictionary : 'NoneType' object has no attribute 'doc2bow',<python><django><dictionary><corpus><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4089,26902048,2014-11-13 05:36:55,,"<p>I know from using scikit learn i could use,</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=2,ngram_range=(1, 2),norm='l2')

corpus = vectorizer.fit_transform(text)
</code></pre>

<p>This piece of code. But how could i do this with gensim?</p>
",,2020-07-01 15:19:53,how to tokenize a set of documents into unigram + bigram bagofwords using gensim?,<python-2.7><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
4154,28155313,2015-01-26 17:22:32,,"<p>I have been trying to install <code>gensim</code> using the following command:</p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>I got the following error messages:</p>

<pre><code>Downloading/unpacking gensim
Cannot fetch index base URL https://pypi.python.org/simple/
Could not find any downloads that satisfy the requirement gensim
Cleaning up...
No distributions at all found for gensim
Storing debug log for failure in /home/users/.pip/pip.log
</code></pre>

<p>How can I fix this problem?</p>

<p>Here is the version of Ubuntu, which is installed on Windows through VmWare</p>

<pre><code>No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.1 LTS
Release:    14.04
Codename:   trusty
</code></pre>
",2015-01-26 18:11:58,2016-02-02 02:54:34,Problems when installing gensim on Ubuntu,<python><ubuntu><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4240,28394805,2015-02-08 14:04:04,,"<p>I am trying to cluster text documents using gaac in nltk .The code is as below . </p>

<pre><code>from nltk.corpus import PlaintextCorpusReader
from nltk.cluster import GAAClusterer
from gensim import corpora
import numpy
import gensim
import itertools


filepath='C:\ISSS609\Forum'
corpus=PlaintextCorpusReader(filepath,'.*')
fids=corpus.fileids()
docs=[corpus.words(f) for f in fids]
dictionary=corpora.Dictionary(docs)
vec=[dictionary.doc2bow(doc) for doc in docs]
vec2=list(itertools.chain(*vec))
vectors = [numpy.array(f) for f in vec2]
clusterer = GAAClusterer()
clusterer.cluster(vectors,False)
clusterer.dendrogram()
</code></pre>

<p>below is the error that I get </p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Aditya/PycharmProjects/untitled1/test2.py"", line 21, in &lt;module&gt;
    clusterer.cluster(vectors,False)
  File ""C:\Python34\lib\site-packages\nltk\cluster\gaac.py"", line 41, in cluster
    return VectorSpaceClusterer.cluster(self, vectors, assign_clusters, trace)
  File ""C:\Python34\lib\site-packages\nltk\cluster\util.py"", line 57, in cluster
    self.cluster_vectorspace(vectors, trace)
  File ""C:\Python34\lib\site-packages\nltk\cluster\gaac.py"", line 52, in cluster_vectorspace
    dist = numpy.ones(dims, dtype=numpy.float)*numpy.inf
  File ""C:\Python34\lib\site-packages\numpy\core\numeric.py"", line 178, in ones
    a = empty(shape, dtype, order)
ValueError: array is too big.
</code></pre>

<p>Kindly suggest a work around . </p>
",,2015-02-08 23:56:29,converting vector form of text to numpy : array is too big,<python><numpy><nltk>,,,CC BY-SA 3.0,True,False,True,False,False
4288,26977042,2014-11-17 16:19:21,,"<p>I am working on a project and I would like to use Latent Dirichlet Allocation in order to extract topics from a large amount of articles.</p>

<p>My code is this:</p>

<pre><code>import gensim
import csv
import json
import glob
from gensim import corpora, models
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from time import gmtime, strftime

tokenizer = RegexpTokenizer(r'\w+')
cachedStopWords = set(stopwords.words(""english""))
body = []
processed = []

with open('//file.json') as j:
    data = json.load(j)

for i in range(0,len(data)):
    body.append(data[i]['text'].lower())

for entry in body:
    row = tokenizer.tokenize(entry)
    processed.append([word for word in row if word not in cachedStopWords])

dictionary = corpora.Dictionary(processed)
corpus = [dictionary.doc2bow(text) for text in processed]
lda = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50, update_every=1, passes=1)
topics = lda.show_topics(num_topics=50, num_words=8)

other_doc = ""After being jailed for life in 1964, Nelson Mandela became a worldwide symbol of resistance to apartheid. But his opposition to racism began many years before.""
print lda[other_doc]

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-  packages/gensim/models/ldamodel.py"", line 714, in __getitem__
gamma, _ = self.inference([bow])
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site
packages/gensim/models/ldamodel.py"", line 361, in inference ids = [id for id, _ in doc]
ValueError: need more than 1 value to unpack
</code></pre>

<p>I also tried to use LdaMulticore in 3 different ways :</p>

<pre><code>lda = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
lda = gensim.models.ldamodel.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
</code></pre>

<p>And every time I got this error :</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute LdaMulticore'
</code></pre>

<p>Any ideas?</p>

<p>Thank you in advance.</p>
",,2016-06-21 13:51:35,Using Latent Dirichlet Allocation with Gensim,<python><lda><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
4336,28488714,2015-02-12 22:04:23,,"<p>I am using Gensim for some topic modelling and I have gotten to the point where I am doing similarity queries using the LSI and tf-idf models. I get back the set of IDs and similarities, eg. <code>(299501, 0.64505910873413086)</code>. </p>

<p>How do I get the text document that is related to the ID, in this case 299501?</p>

<p>I have looked at the docs for corpus, dictionary, index, and the model and cannot seem to find it.</p>
",2017-10-10 05:36:55,2017-10-10 06:28:46,Retrieve string version of document by ID in Gensim,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4348,28508548,2015-02-13 20:53:48,,"<p>I am trying to use <em>word2vec</em> for a project and after training I get:</p>

<pre><code>INFO : not storing attribute syn0norm
</code></pre>

<p>Is there any way I could save the <code>syn0norm</code>.</p>

<p>How can I do so?</p>
",2018-11-14 13:42:47,2018-11-14 13:42:47,Gensim Word2vec storing attribute syn0norm,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
4358,25915441,2014-09-18 14:28:04,,"<p>I am using the gensim library to apply LDA to a set of documents. Using gensim I can apply LDA to a corpus  whatever the term weights are: binary, tf, tf-idf...</p>

<p>My question is, what is the term weighting that should be used for the original <a href=""http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"" rel=""nofollow"">LDA</a>? If I have understood correctly the weights should be term frequencies, but I am not sure.</p>
",,2014-09-21 03:20:12,Term weighting for original LDA in gensim,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4391,25936354,2014-09-19 14:32:16,,"<p>I'm trying to install <code>scipy</code>, <code>scikit-learn</code>, and <code>gensim</code> on Windows 7 with Python 3.3.
If I try any of these:
pip install sci</p>

<pre><code>pip install scipy
pip install scikit-learn
pip install gensim
</code></pre>

<p>I end up with an ImportError similar to:</p>

<pre><code>ImportError: no module named numpy
</code></pre>

<p>And yes, I have installed <code>numpy</code> - it works fine if I try to import it in Python. I've managed to install <code>scipy</code> and <code>scikit-learn</code> by downloading executable installers, but <code>gensim</code> doesn't have one...
I've also tried using <code>easy_install</code> for all three, but that doesn't work either.</p>

<p>Is it something to do with the Python installation? Any ideas? Thanks a lot in advance!</p>
",,2014-09-19 14:32:16,"Python pip not working for scipy, scikit-learn and gensim",<windows-7><pip><python-3.3><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
4438,28677350,2015-02-23 15:34:17,,"<p>Latent Dirichlet Allocation(LDA) is a topic model to find latent variable (topics) underlying a bunch of documents. I'm using python gensim package and having two problems: </p>

<ol>
<li><p>I printed out the most frequent words for each topic (I tried 10,20,50 topics), and found out that the distribution over words is very ""flat"": meaning even the most frequent word has only 1% probability... </p></li>
<li><p>Most of the topics are similar: meaning the most frequent words for each of the topics overlap a lot and the topics share almost the same set of words for their high frequency words...</p></li>
</ol>

<p>I guess the problem is probably due to my documents: my documents actually belong to a specific category, for example, they are all documents introducing different online games. For my case, will LDA still work, since the documents themselves are quite similar, so a model based on ""bag of words"" may not be a good way to try? </p>

<p>Could anyone give me some suggestions? Thank you!</p>
",2019-03-10 15:17:32,2019-03-10 15:17:32,"Using LDA(topic model) : the distrubution of each topic over words are similar and ""flat""",<python><lda><topic-modeling><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
4459,27032517,2014-11-20 05:40:50,,"<p><a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a> is a open source tool by Google: </p>

<ul>
<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>
<li><p>There is also a paper on <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>
</ul>
",2016-11-03 16:44:38,2018-07-30 07:18:51,what does the vector of a word in word2vec represents?,<machine-learning><nlp><neural-network><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4530,28823948,2015-03-03 04:10:03,,"<p>I have list of docs in <code>postCorp</code>. I am trying to get topics and corresponding probability using <code>lda model</code> that i have already trained using <code>gensim</code>.
Below is the code snippet where i am:
Getting each document
Converting it to BOW
Passing it to lda to return topics and 
And also calculating mean probability of each topic over all the document.</p>

<pre><code>with open('ldaOutputPost','w') as out:
    topicMean = defaultdict(float)
    count = 0
    for corp in postCorp:
        count += 1
        for (t,p) in lda[dictionary.doc2bow(corp)]:
            topicMean[t] += p 
        out.write(str(lda[dictionary.doc2bow(corp)])+'\n')
</code></pre>

<p>This works fine if i just pass bow of one document. But when I loop over all the documents it gives me following error which I am unable to find out why?</p>

<pre><code>Traceback (most recent call last):
  File ""C:\test.py"", line 110, in &lt;module&gt;
    for (t,p) in lda[dictionary.doc2bow(corp)]:
  File ""C:\Python27\lib\site-packages\gensim\models\ldamodel.py"", line 752, in __getitem__
    gamma, _ = self.inference([bow])
  File ""C:\Python27\lib\site-packages\gensim\models\ldamodel.py"", line 380, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 6770 is out of bounds for axis 1 with size 6770
</code></pre>

<p>Can any one please help.</p>

<p>P.S: There is no issue in BOW as <code>lda[dictionary.doc2bow(preCorp[0])</code> works fine and gives me correct output.</p>

<p>EDIT:
<code>postCorp</code> is list of list where inner list represents a list of all words in a document. Whereas dictionary is <code>dictionary = corpora.Dictionary(combined)</code> here combined is all corpus on which topic model was trained.</p>
",2015-03-03 05:00:42,2015-03-06 01:23:16,Error while while running trained topic model on new document,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4557,26010645,2014-09-24 07:08:51,,"<pre><code>def n_similarity(self, ws1, ws2):
    v1 = [self[word] for word in ws1]
    v2 = [self[word] for word in ws2]
    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))
</code></pre>

<p>This is the code I excerpt from gensim.word2Vec, I know that two single words' similarity can be calculated by cosine distances, but what about two word sets? The code seems to use the mean of each wordvec and then calculated on the two mean vectors' cosine distance. I know few in word2vec, is there some foundations of such process?</p>
",,2014-09-24 09:16:38,Why the similarity beteween two bag-of-words in gensim.word2vec calculated this way?,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
4567,24816912,2014-07-18 03:47:19,,"<p>I'm using gensim's package to implement LSI on a corpus. My goal is to find out the most frequently occurring distinct topics that appear in the corpus.</p>

<p>If I don't know the number of topics that are in the corpus (I'd estimate anywhere from 5 to 20), what is the best approach in setting the number of topics that LSI should search for? Is it better to look for a large number of topics (20-30), or a small number of topics (~5)? </p>
",,2014-11-10 06:34:11,Number of Latent Semantic Indexing topics,<topic-modeling><gensim><latent-semantic-indexing>,,,CC BY-SA 3.0,False,False,True,False,False
4633,27097779,2014-11-24 04:15:12,,"<p>I am new to python and trying to implement topic modelling. I am successful in implementing LDA in pything using gensim , but I am not able to give any label/name to these topics. 
How do we name these topics? please help out with the best way to implement in python. 
My LDA output is somewhat like this(please let me know if you need the code) :-</p>

<p>0.024*research + 0.021*students + 0.019*conference + 0.019*chi + 0.017*field + 0.014*work + 0.013*student + 0.013*hci + 0.013*group + 0.013*researchers
0.047*research + 0.034*students + 0.020*ustars + 0.018*underrepresented + 0.017*participants + 0.012*researchers + 0.012*mathematics + 0.012*graduate + 0.012*mathematical + 0.012*conference
0.027*students + 0.026*research + 0.018*conference + 0.017*field + 0.015*new + 0.014*participants + 0.013*chi + 0.012*robotics + 0.010*researchers + 0.010*student
0.023*students + 0.019*robotics + 0.018*conference + 0.017*international + 0.016*interact + 0.016*new + 0.016*ph.d. + 0.016*meet + 0.016*ieee + 0.015*u.s.
0.033*research + 0.030*flow + 0.028*field + 0.023*visualization + 0.020*challenges + 0.017*students + 0.015*project + 0.013*shape + 0.013*visual + 0.012*data
0.044*research + 0.020*mathematics + 0.017*program + 0.014*june + 0.014*conference + 0.014*- + 0.013*mathematicians + 0.013*conferences + 0.011*field + 0.011*mrc
0.023*research + 0.021*students + 0.015*field + 0.014*hovering + 0.014*mechanisms + 0.014*dpiv + 0.013*aerodynamic + 0.012*unsteady + 0.012*conference + 0.012*hummingbirds
0.031*research + 0.018*mathematics + 0.016*program + 0.014*flow + 0.014*mathematicians + 0.012*conferences + 0.011*field + 0.011*june + 0.010*visualization + 0.010*communities
0.028*students + 0.028*research + 0.018*ustars + 0.018*mathematics + 0.015*underrepresented + 0.010*program + 0.010*encouraging + 0.010*'', + 0.010*participants + 0.010*conference
0.049*research + 0.021*conference + 0.021*program + 0.020*mathematics + 0.014*mathematicians + 0.013*field + 0.013*- + 0.011*conferences + 0.010*areas</p>
",,2014-11-24 04:57:07,Naming LDA topics in Python,<python><label><lda>,,,CC BY-SA 3.0,False,False,True,False,False
4635,27104847,2014-11-24 12:26:04,,"<p><code>Scikit-learn</code> is a machine learning library that requires these dependencies, <a href=""http://scikit-learn.org/stable/install.html"" rel=""nofollow"">http://scikit-learn.org/stable/install.html</a>: </p>

<pre><code>sudo apt-get install build-essential python-dev python-setuptools \
                     python-numpy python-scipy \
                     libatlas-dev libatlas3gf-base
</code></pre>

<p>before a user can:</p>

<pre><code>pip install -U scikit-learn
</code></pre>

<p>And <code>gensim</code> requires no additional dependencies, <a href=""http://radimrehurek.com/gensim/install.html"" rel=""nofollow"">http://radimrehurek.com/gensim/install.html</a> and simply do: </p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>I have my <code>setup.py</code> as such without <code>scikit</code>, its dependencies and <code>gensim</code> from <a href=""https://github.com/alvations/pywsd/blob/master/setup.py"" rel=""nofollow"">https://github.com/alvations/pywsd/blob/master/setup.py</a>:</p>

<pre><code>from distutils.core import setup

setup(
    name='pywsd',
    version='0.1',
    packages=['pywsd',],
    long_description='Python Implementations of Word Sense Disambiguation (WSD) technologies',
)
</code></pre>

<p>How do I add <code>scikit-learn</code> and <code>gensim</code> into my <code>setup.py</code>?</p>
",,2014-11-24 12:26:04,How to add scikit-learn and gensim into my libraries' setup.py?,<python><installation><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
4648,28979559,2015-03-11 05:55:35,,"<pre><code>corpus = PlaintextCorpusReader(""path"",'.*',encoding=""latin1"")
docs = [corpus.words(f)for f in corpus.fileids()]
docs2 = [[w.lower()for w in doc]for doc in docs]
docs3 = [[w for w in doc if re.search('^[a-z]+$', w)]for doc in docs2]
from nltk.corpus import stopwords
stop_list = stopwords.words('english')

docs4 = [[w for w in doc if w not in stop_list]for doc in docs3]
</code></pre>

<p>I have written the following code , which reads a corpus of files. FOllowed by that i have done some preprocessing steps ir removing punctuations , stopwords etc. I would now like to perform a word count and find most frequent words used in the text. I used the following code below to do so so.
for word in docs4:</p>

<pre><code>if word in word_counter:
    word_counter[word] += 1
else:
    word_counter[word] = 1

popular_words = sorted(word_counter, key = word_counter.get, reverse = True)
</code></pre>

<p>However i get the following error. --</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/rohanhm.2014/PycharmProjects/untitled1/bp.py"", line 18, in &lt;module&gt;
    if word in word_counter:
TypeError: unhashable type: 'list'
</code></pre>

<p>Any suggestions?</p>
",,2015-03-11 09:19:10,Unhashable type 'list' - Wordcount,<python><regex><nltk><word-count><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
4652,29083865,2015-03-16 18:03:10,,"<p>My ultimate goal is to produce a *.csv file containing <strong>labeled</strong> binary term vectors for each document. In essence, a term document matrix. </p>

<p>Using gensim, I can produce a file with an unlabeled term matrix.</p>

<p>I do this by essentially copying and pasting code from here: <a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html</a></p>

<p>Given a list of documents called ""texts"".</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in texts]
print(corpus)
[(0, 1), (1, 1), (2, 1)]
[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
[(2, 1), (5, 1), (7, 1), (8, 1)]
[(1, 1), (5, 2), (8, 1)]
[(3, 1), (6, 1), (7, 1)]
[(9, 1)]
[(9, 1), (10, 1)]
[(9, 1), (10, 1), (11, 1)]
[(4, 1), (10, 1), (11, 1)]
</code></pre>

<p>To convert the above vectors into a numpy matrix, I use:</p>

<pre><code>scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)
</code></pre>

<p>I then convert the sparse numpy matrix to a full array:</p>

<pre><code>full_matrix = csc_matrix(scipy_csc_matrix).toarray()
</code></pre>

<p>Finally, I output this to a file:</p>

<pre><code>with open('file.csv','wb') as f:
    writer = csv.writer(f)
    writer.writerows(full_matrix)
</code></pre>

<p>This produces a matrix of binomial vectors, but I do not know which vector represents which word. Is there an accurate way of matching words to vectors?</p>

<p>I've tried parsing the dictionary to creative a list of words which I would glue to the above full_matrix. </p>

<pre><code>#Retrive dictionary
tokenIDs = dictionary.token2id

#Retrieve keys from dictionary and concotanate those to full_matrix
for key, value in tokenIDs.iteritems():
    temp1 = unicodedata.normalize('NFKD', key).encode('ascii','ignore')
    temp = [temp1]
    dictlist.append(temp)

Keys = np.asarray(dictlist)

#Combine Keys and Matrix
labeled_full_matrix = np.concatenate((Keys, full_matrix), axis=1)
</code></pre>

<p>However, this does not work. The word ids (Keys) are not matched to the appropriate vectors. </p>

<p>I am under the assumption a much simpler and more elegant approach is possible. But after some time, I haven't been able to find it. Maybe someone here can help, or point me to something fundamental I've missed.</p>
",,2015-06-09 01:06:23,Word Labels for Document Matrix in Gensim,<python-2.7><numpy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4659,26031958,2014-09-25 06:23:31,,"<p>I'm trying to reproduce the results of Graber et al. in showing that when LDA is used with a multilingual corpus, the most probable terms for a topic (say, top 10) will come from a single language. Their paper is <a href=""http://arxiv.org/pdf/1205.2657.pdf"" rel=""nofollow"">here</a>.  </p>

<p>This is a reasonable sanity check to perform IMO, but I'm having difficulty. </p>

<p>I'm using the same corpus they used, the <a href=""http://www.statmt.org/europarl/"" rel=""nofollow"">Europarl corpus</a>, with the corpus composed of Bulgarian and English.  I concatenated the Bulgarian and English corpuses with </p>

<pre><code>cat corpusBg.txt corpusEn.txt &gt;&gt; corpusMixed.txt.  
</code></pre>

<p>This contains a sentence on each line, with the collection of lines in Bulgarian and the second collection in English. When I fit an LDA model with 4 topics, 3 contain only English terms in the top 10, and the fourth is mixed between English and Bulgarian. I'm using the default settings for LDA:</p>

<pre><code>texts = [[word for word in doc.lower().split()] for doc in open('corpusMixed.txt', 'r')]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(doc) for doc in texts]
lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics = 4)
topics = lda.print_topics(lda.num_topics)

for t in topics:
    print t
</code></pre>

<p>Note that I have not removed stopwords or sparse terms, but I think that this shouldn't matter. There should intuitively be some topics with terms only in bulgarian and others with terms only in English, no? </p>
",2014-09-29 19:10:22,2014-09-29 19:10:22,Lda on Bi(multi)lingual Corpus,<lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4733,27139908,2014-11-26 01:35:41,,"<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>

<p>Thanks! </p>
",,2018-12-15 00:16:31,Load PreComputed Vectors Gensim,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
4753,27145452,2014-11-26 09:26:51,,"<p>In the LDA model these are the two methods to inference the new documents using existing model i think. what are the differences between these two methods ?</p>
",,2017-08-09 13:16:03,What is the deference between lda[doc_bow] and lda.inference(corpus)?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4755,27147690,2014-11-26 11:14:04,,"<p>i am just wondering whether its either TFIDF corpus to be used or just corpus to be used when we are inference documents using LDA in gensim</p>

<p>Here is an example</p>

<pre><code>from gensim import corpora, models
import numpy.random
numpy.random.seed(10)

doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)] 
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]
dictionary = corpora.Dictionary(corpus)

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf.save('x.corpus_tfidf')

corpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)

#which one i should use from this   
**corpus_lda = lda[corpus]**          #this one 
**corpus_LDA = lda[corpus_tfidf ]**   #or this one?


corpus_lda.save('x.corpus_lda')

for i,j in enumerate(corpus_lda):
    print j, corpus[i]
</code></pre>
",,2014-12-03 00:30:07,should i use tfidf corpus or just corpus to inference documents using LDA?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4768,24861239,2014-07-21 09:03:08,,"<p>I'm using Django for a search engine. Requests are made by POST, the server treats them and answers with JSON format. In order to be fast, I need to have an index file loaded at the beginning (with manage.py runserver) and a way to access it when a view is called.</p>

<p>Does anyone know how to do it ?</p>

<p>Thanks in advance !</p>
",,2014-07-21 09:59:16,Load file with django manage.py runserver,<python><django><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4777,29259416,2015-03-25 15:08:27,,"<p>I'm trying to replicate the tutorial for the Mallet wrapper in gensim. <a href=""http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/"" rel=""nofollow"">http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/</a></p>

<p>When I fit the model with</p>

<pre><code>model = models.LdaMallet(mallet_path, corpus, num_topics=10, id2word=corpus.dictionary)
</code></pre>

<p>I get an error message:</p>

<pre><code>C:\Anaconda\lib\site-packages\gensim\models\ldamallet.py:234: RuntimeWarning: invalid value encountered in divide topic = topic / topic.sum() # normalize to probability dist
</code></pre>

<p>When I use the model to infer the topic distribution of the example the distribution is uniform:</p>

<pre><code>doc = ""Don't sell coffee, wheat nor sugar; trade gold, oil and gas instead.""
bow = corpus.dictionary.doc2bow(utils.simple_preprocess(doc))
print model[bow]
</code></pre>

<p>My output:</p>

<pre><code>[(0, 0.10000000000000002), (1, 0.10000000000000002), (2, 0.10000000000000002), (3, 0.10000000000000002), (4, 0.10000000000000002), (5, 0.10000000000000002), (6, 0.10000000000000002), (7, 0.10000000000000002), (8, 0.10000000000000002), (9, 0.10000000000000002)]
</code></pre>

<p>Is this a problem in the functioning of the wrapper or in mallet? I've managed to replicate the mallet tutorial here: <a href=""http://programminghistorian.org/lessons/topic-modeling-and-mallet"" rel=""nofollow"">http://programminghistorian.org/lessons/topic-modeling-and-mallet</a></p>
",,2015-03-25 15:08:27,Gensim LdaMallet division error,<python><machine-learning><topic-modeling><gensim><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
4829,29372611,2015-03-31 15:36:52,,"<p>I have the following code, to run an LDA analysis on Tweets:</p>

<pre><code>import logging, gensim, bz2
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# load id-&gt;word mapping (the dictionary), one of the results of step 2 above
id2word = 'enams4nieuw.dict'
# load corpus iterator
mm = gensim.corpora.MmCorpus('enams4nieuw.mm')

print(mm)

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
</code></pre>

<p>When I try to run this script, I receive the following log with error message:</p>

<pre><code>MmCorpus(40152 documents, 13061 features, 384671 non-zero entries)
2015-03-31 16:52:50,246 : INFO : loaded corpus index from enams4nieuw.mm.index
2015-03-31 16:52:50,246 : INFO : initializing corpus reader from enams4nieuw.mm
2015-03-31 16:52:50,246 : INFO : accepted corpus with 40152 documents, 13061 features, 384671 non-zero entries
Traceback (most recent call last):
  File ""C:/Users/gerbuiker/PycharmProjects/twitter-streaming.py/lda.py"", line 15, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
  File ""C:\Users\gerbuiker\AppData\Roaming\Python\Python27\site-packages\gensim\models\ldamodel.py"", line 244, in __init__
self.num_terms = 1 + max(self.id2word.keys())
AttributeError: 'str' object has no attribute 'keys'

Process finished with exit code 1
</code></pre>

<p>Anyone got a solution for this?</p>
",,2017-04-21 12:29:17,Error when running LDA on Tweets using gensim in Python,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4852,27177721,2014-11-27 19:37:11,,"<p>I have a tf-idf matrix already, with rows for terms and columns for documents. Now I want to train a LDA model with the given terms-documents matrix. The first step seems to be using <code>gensim.matutils.Dense2Corpus</code> to convert the matrix into the corpus format. But how to construct the <code>id2word</code> parameter? I have the list of the terms (#terms==#rows) but I don't know the format of the dictionary so I cannot construct the dictionary from functions like <code>gensim.corpora.Dictionary.load_from_text</code>. Any suggestions? Thank you.</p>
",,2014-12-09 12:15:19,Training a LDA model with gensim from some external tf-idf matrix and term list,<python-3.x><tf-idf><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4867,29369317,2015-03-31 13:01:20,,"<p>I've downloaded Tweets about Amsterdam, in UTF-8 using the Twitter API for python.
Now i'm trying to make a dictionary for LDA, using this code (just a part of the code, but this is the part that causes the error):</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file))
</code></pre>

<p>which always gives me an error, depending on which txt file I choose as input, either: </p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode byte 0xf1 in position 2: invalid continuation byte
</code></pre>

<p>or</p>

<pre><code> UnicodeDecodeError: 'utf8' codec can't decode byte xxxx in position 175-176: unexpected end of data
</code></pre>

<p>I expect the reason for this to be characters which are unknown in UTF-8 (some smilies used in Tweets maybe) and after Googling tried to replace the code by:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
TypeError: 'errors' is an invalid keyword argument for this function
</code></pre>

<p>or </p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
ValueError: mode string must begin with one of 'r', 'w', 'a' or 'U', not 'ignore'
</code></pre>

<p>Does anyone have a solution? Thanks</p>
",,2015-03-31 13:15:27,"Tweet analysis, Python error when making dictionary for LDA",<python><dictionary><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4945,29589795,2015-04-12 13:06:28,,"<p>I have 2 files, </p>

<p><strong>music.txt</strong> &amp; <strong>science.txt</strong></p>

<p>I'd lke to extract 2 topics from the above (<strong>Music</strong> , <strong>Science</strong>)</p>

<p>After creating the LDA model from these 2 files (setting num_topics=<strong>2</strong>)</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=my_corpus, id2word=corpus_dictionary, num_topics=2)

print(lda.print_topic(0))
print(lda.print_topic(1))
</code></pre>

<p>This is my output</p>

<pre><code>0.011*scientific + 0.010*musical + 0.007*music, + 0.006*music. + 0.006*study + 0.005*not + 0.005*research + 0.005*main

0.030*music + 0.013*science + 0.010*scientific + 0.009*musical + 0.006*not + 0.005*music. + 0.005*study + 0.005*music, + 0.005*their + 0.005*research
</code></pre>

<p>As you can see, <strong>both</strong> science and music are present in each of the 2 topics</p>

<p>I'd like to</p>

<ol>
<li>Use music.txt and create 1 topic <strong>Music</strong> LDA model</li>
<li>Use science.txt and create 1 topic <strong>Science</strong> LDA model</li>
<li>Combine the above 2 LDA models to give 1 LDA model with the above 2 topics</li>
</ol>

<p>is the above <strong>3rd</strong> step possible?  I'd like to have individual segregration of topics in my LDA model. If not, is there any alternative?</p>
",2015-04-12 17:11:30,2015-04-12 21:52:23,"Text Processing, How to assign 1 topic -> 1 document using LDA?",<machine-learning><nlp><topic-modeling><text-classification><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4949,29591581,2015-04-12 16:14:54,,"<p>I am loading pre-trained vectors from a binary file generated from the word2vec C code with something like:</p>

<pre><code>model_1 = Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am using those vectors to generate vector representations of sentences that contain words that may not have already existing vectors in <code>vectors.bin</code>. For example, if <code>vectors.bin</code> has no associated vector for the word ""yogurt"", and I try</p>

<pre><code>yogurt_vector = model_1['yogurt']
</code></pre>

<p>I get <code>KeyError: 'yogurt'</code>, which makes good sense. What I want is to be able to take the sentence words that do not have corresponding vectors and add representations for them to <code>model_1</code>. I am aware from <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">this post</a> that you cannot continue to train the C vectors. Is there then a way to train a new model, say <code>model_2</code>, for the words with no vectors and merge <code>model_2</code> with <code>model_1</code>?</p>

<p>Alternatively, is there a way to test if the model contains a word before I actually try to retrieve it, so that I can at least avoid the KeyError?</p>
",2017-05-23 12:17:44,2016-01-04 09:08:09,Gensim word2vec augment or merge pre-trained vectors,<python><gensim><keyerror><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
4971,29561063,2015-04-10 12:09:07,,"<p>I'm using LDA to categorize small documents, about 4-5 lines. </p>

<p>I'm categorizing them into topics such as Technology, Politics, Art, Music etc etc</p>

<p>I'm using wikipedia to download articles in each category (Technology, Politics, Art etc etc) and training LDA for each category</p>

<p>Wikipedia is huge (about 8GB compressed), and computations take hours! and uses a huge space in my hard drive</p>

<p>Is there any toolkit that already provides ""ready-made"" generic topics which i can directly use for categorization? </p>
",,2015-04-10 20:40:38,ready-made Topics in using LDA to categorize documents?,<python><nlp><text-processing><gensim>,2015-04-26 22:51:36,,CC BY-SA 3.0,False,False,True,False,False
4990,27220927,2014-12-01 02:40:15,,"<p>My term-document matrix is in a numpy matrix format, and I have a dictionary to represent the  of the term-document matrix.</p>

<p>Is there any way I can easily pass these two into Gensim's LDA model?</p>

<pre><code>tdMatrix = np.load('tdmatrix.npy')
dictionary = cPickle.load(open('dictionary.p', 'r')) # stores term represented by each column
</code></pre>

<p>Can I pass this somewhow to gensim.models.ldamodel.LDA?</p>
",,2014-12-09 12:12:32,Passing Term-Document Matrix to Gensim LDA Model,<python><numpy><machine-learning><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
4996,29602038,2015-04-13 09:31:22,,"<p>Are there any packages or methods for computing similarity scores for a pair of sentences based on <strong>WordNet</strong>?</p>

<p>I am doing some research about automatic summarization based on WordNet, and need to compute similarity scores between two sentences. </p>

<p>I was using <strong>NLTK</strong> and computing the score for every pair of words iterated over the two sentences, which take a LOT of time and inefficient.</p>

<p>I'm looking for projects/libraries/methods that already implement intelligently to compute similarity scores between two sentences based on WordNet.</p>

<p>In addition, I have found the similar question 
<a href=""https://stackoverflow.com/questions/17022691/python-semantic-similarity-score-for-strings"">Python:Semantic similarity score for String</a>, but the package named <strong>Gensim</strong> doesn't look like what I want. So could you help me? Thank you!</p>
",2017-05-23 11:58:14,2015-04-13 09:31:22,Python:How to calculate similarity score for two sentences based on WordNet,<python><nltk><wordnet>,,,CC BY-SA 3.0,True,False,True,False,False
4998,26145937,2014-10-01 16:20:48,,"<p>I am trying to follow the tutorial on topic modelling / Latent Dirichlet Allocation (LDA) in the book Building Machine Learning Systems"" with Python.</p>

<p>I have not gone too far in this book, and the the first part of topic modelling returns errors for me:</p>

<pre><code>from gensim import corpora, models, similarities
corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt')
</code></pre>

<p>Error:</p>

<pre><code>     63 
     64         self.fname = fname
---&gt; 65         with utils.smart_open(fname_vocab) as fin:
     66             words = [utils.to_unicode(word).rstrip() for word in fin]
     67         self.id2word = dict(enumerate(words))

/Users/user/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/utils.pyc in smart_open(fname, mode)
    659         from gzip import GzipFile
    660         return make_closing(GzipFile)(fname, mode)
--&gt; 661     return open(fname, mode)
    662 
    663 

IOError: [Errno 2] No such file or directory: './data/ap/vocab.txt'
</code></pre>

<p>The vocab.txt file does not exists, but switching to the directory where it is supposed to be, I find the following:</p>

<blockquote>
  <p>$ ls
  download_ap.sh        download_wp.sh      preprocess-wikidata.sh</p>
</blockquote>

<p>It looks like the ap data needs to be downloaded separately (not mentioned in the book), so by doing this:</p>

<pre><code>sh download_ap.sh
</code></pre>

<p>I get this:</p>

<pre><code>download_ap.sh: line 2: wget: command not found
tar: Error opening archive: Failed to open 'ap.tgz'
</code></pre>

<p>Does anybody knows how to solve this issue?</p>

<p>Thank you </p>
",2014-10-01 16:42:36,2014-10-01 19:03:55,BleiCorpus and Associated Press dataset in Gensim: IO Error,<python><enthought><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5058,29676413,2015-04-16 13:35:30,,"<p>I am using the Doc2vec class from the gensim framework to compute the vectorial representation of each document in a corpus.</p>

<p>The corpus contains very short sentences, they can have even one word. I observed that for many sentences, especially the short ones, Doc2vec does not provide any representations. Could someone explain the reasons for this?</p>
",2015-04-20 19:49:30,2015-04-20 19:49:30,Missing sentences from the Doc2vec representation,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5107,29751328,2015-04-20 14:55:22,,"<p>i am trying to implement Topic Tiling algorithm on my trained lda model.
For the algorithm I need all of the IDs that are assigned to a single word in an unseen document. I will then calculate the most frequent topic id for the given word and assign it as the mode of that word.</p>

<p>I am using the gensim lib so it is very easy to get topic->word dist, where the words are given with their probabilities. However how do I get ""what topic(s) are/were assigned to a single world"", meaning word->topic dists.</p>

<pre><code>Example:
s = ""Banks are closed on Sunday""

Topic -&gt; Word Dist from Gensim:
TopicTag -&gt; Prob*Word
Topic 0 -&gt; 0,3*Bank, 0,2*are
Topic 1 -&gt; 0,2*closed, 0,1*Sunday
Topic 2 -&gt; 0,4*Sunday, 0,3*on

What I want:
word -&gt; TopicTag(Frequency that given word was assigned with the specified topic tag)
Banks -&gt; Topic1(2), Topic2(2)
Closed -&gt; Topic0(1),Topic1 (4)
</code></pre>

<p>Please also note that I am not interested in parsing the Topic -> Word Dist results from Gensim, I am interested in finding an accurate way that my model assigns (numerous) topic(s) to each individual word that will come in an unseen document.</p>

<p>Thanks in advance.</p>
",,2016-05-16 23:53:36,LDA Gensim Word -> Topic Ids Distribution instead of Topic -> Word Distribution,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5108,29837563,2015-04-24 02:13:02,,"<p>What is the difference between 
<code>gensim.models.ldamodel.LdaModel(...)</code> and <code>gensim.models.LdaModel(...)</code>?</p>

<p>The <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">docs</a> use <code>gensim.models.ldamodel.LdaModel(...)</code>.</p>

<p>However, <a href=""https://www.google.com/search?q=%22models.LdaModel%22&amp;oq=%22models.LdaModel%22&amp;aqs=chrome..69i57j0.9506j0j7&amp;sourceid=chrome&amp;es_sm=119&amp;ie=UTF-8#q=%22models.LdaModel(corpus%22&amp;tbas=0"" rel=""nofollow"">many people</a> seem to use <code>gensim.models.LdaModel(...)</code>.</p>
",2015-04-27 03:41:47,2015-04-27 03:41:47,What is the difference between models.ldamodel.LdaModel and models.LdaModel?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5142,26202978,2014-10-05 13:27:25,,"<p>Based on my previous question <a href=""https://stackoverflow.com/questions/26177307/spark-and-python-use-custom-file-format-generator-as-input-for-rdd?noredirect=1#comment41082418_26177307"">Spark and Python use custom file format/generator as input for RDD</a> I think that I should be able to parse basically any input by sc.textFile() and then using my or from some library custom functions.</p>

<p>Now I am particularly trying to parse the wikipedia dump using gensim framework. I have already installed gensim on my master node and all my worker nodes and now I would like to use gensim build in function for parsing wikipedia pages inspired by this question <a href=""https://stackoverflow.com/questions/21096432/list-or-iterator-of-tuples-returned-by-map-pyspark"">List (or iterator) of tuples returned by MAP (PySpark)</a>.</p>

<p>My code is following:</p>

<pre><code>import sys
import gensim
from pyspark import SparkContext


if __name__ == ""__main__"":
    if len(sys.argv) != 2:
        print &gt;&gt; sys.stderr, ""Usage: wordcount &lt;file&gt;""
        exit(-1)

    sc = SparkContext(appName=""Process wiki - distributed RDD"")

    distData = sc.textFile(sys.argv[1])
    #take 10 only to see how the output would look like
    processed_data = distData.flatMap(gensim.corpora.wikicorpus.extract_pages).take(10)

    print processed_data
    sc.stop()
</code></pre>

<p>The source code of extract_pages can be found at <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""nofollow noreferrer"">https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py</a> and based on my going through it seems that it should work with Spark. </p>

<p>But unfortunately when I run the code I'm getting following error log:</p>

<pre><code>14/10/05 13:21:11 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, &lt;ip address&gt;.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
File ""/root/spark/python/pyspark/worker.py"", line 79, in main
serializer.dump_stream(func(split_index, iterator), outfile)
File ""/root/spark/python/pyspark/serializers.py"", line 196, in dump_stream
self.serializer.dump_stream(self._batched(iterator), stream)
File ""/root/spark/python/pyspark/serializers.py"", line 127, in dump_stream
for obj in iterator:
File ""/root/spark/python/pyspark/serializers.py"", line 185, in _batched
for item in iterator:
File ""/root/spark/python/pyspark/rdd.py"", line 1148, in takeUpToNumLeft
yield next(iterator)
File ""/usr/lib64/python2.6/site-packages/gensim/corpora/wikicorpus.py"", line 190, in extract_pages
elems = (elem for _, elem in iterparse(f, events=(""end"",)))
File ""&lt;string&gt;"", line 52, in __init__
IOError: [Errno 2] No such file or directory: u'&lt;mediawiki xmlns=""http://www.mediawiki.org/xml/export-0.9/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.mediawiki.org/xml/export-0.9/ http://www.mediawiki.org/xml/export-0.9.xsd"" version=""0.9"" xml:lang=""en""&gt;'
    org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)
    org.apache.spark.api.python.PythonRDD$$anon$1.&lt;init&gt;(PythonRDD.scala:154)
    org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
    org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
    org.apache.spark.scheduler.Task.run(Task.scala:54)
    org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>And then some probably Spark log:</p>

<pre><code>14/10/05 13:21:12 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
14/10/05 13:21:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
14/10/05 13:21:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/10/05 13:21:12 INFO scheduler.DAGScheduler: Failed to run runJob at PythonRDD.scala:296
</code></pre>

<p>and</p>

<pre><code>at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
</code></pre>

<p>I've tried this without Spark successfully, so the problem should be somewhere in combination of Spark and gensim, but I don't much understand the error that I'm getting. I don't see any file reading in the line 190 of gensim wikicorpus.py. </p>

<p><strong>EDIT:</strong></p>

<p>Added some more logs from Spark:</p>

<p><strong>EDIT2:</strong></p>

<p>gensim uses from <code>xml.etree.cElementTree import iterparse</code>, documentation <a href=""https://docs.python.org/2/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse"" rel=""nofollow noreferrer"">here</a>, which might cause the problem. It actually expects file name or file containing the xml data. Can be RDD considered as file containing the xml data?</p>
",2017-05-23 11:44:36,2015-04-06 18:51:43,Spark and Python trying to parse wikipedia using gensim,<python><apache-spark><gensim><wikimedia-dumps>,,,CC BY-SA 3.0,False,False,True,False,False
5163,29932784,2015-04-29 01:23:12,,"<p>I'm new to the <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">gensim</a> package and vector space models in general, and I'm unsure of <em>what exactly I should do with my LSA output.</em> </p>

<p>To give a brief overview of my goal, I'd like to enhance Naive Bayes Classifier using topic modeling to improve classification of reviews (positive or negative). Here's a <a href=""http://www.aclweb.org/anthology/I13-1158"" rel=""nofollow"">great paper</a> I've been reading that has shaped my ideas but left me still somewhat confused about implementation..</p>

<p>I've already got working code for Naive Bayes--currently, I'm just using unigram bag of words as my features and labels are either positive or negative.</p>

<p>Here's my gensim code</p>

<pre><code>from pprint import pprint # pretty printer
import gensim as gs

# tutorial sample documents
docs = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]


# stoplist removal, tokenization
stoplist = set('for a of the and to in'.split())
# for each document: lowercase document, split by whitespace, and add all its words not in stoplist to texts
texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in docs]


# create dict
dict = gs.corpora.Dictionary(texts)
# create corpus
corpus = [dict.doc2bow(text) for text in texts]

# tf-idf
tfidf = gs.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

# latent semantic indexing with 10 topics
lsi = gs.models.LsiModel(corpus_tfidf, id2word=dict, num_topics =10)

for i in lsi.print_topics():
    print i
</code></pre>

<p>Here's output</p>

<pre><code>0.400*""system"" + 0.318*""survey"" + 0.290*""user"" + 0.274*""eps"" + 0.236*""management"" + 0.236*""opinion"" + 0.235*""response"" + 0.235*""time"" + 0.224*""interface"" + 0.224*""computer""
0.421*""minors"" + 0.420*""graph"" + 0.293*""survey"" + 0.239*""trees"" + 0.226*""paths"" + 0.226*""intersection"" + -0.204*""system"" + -0.196*""eps"" + 0.189*""widths"" + 0.189*""quasi""
-0.318*""time"" + -0.318*""response"" + -0.261*""error"" + -0.261*""measurement"" + -0.261*""perceived"" + -0.261*""relation"" + 0.248*""eps"" + -0.203*""opinion"" + 0.195*""human"" + 0.190*""testing""
0.416*""random"" + 0.416*""binary"" + 0.416*""generation"" + 0.416*""unordered"" + 0.256*""trees"" + -0.225*""minors"" + -0.177*""survey"" + 0.161*""paths"" + 0.161*""intersection"" + 0.119*""error""
-0.398*""abc"" + -0.398*""lab"" + -0.398*""machine"" + -0.398*""applications"" + -0.301*""computer"" + 0.242*""system"" + 0.237*""eps"" + 0.180*""testing"" + 0.180*""engineering"" + 0.166*""management""
</code></pre>

<p>Any suggestions or general comments would be appreciated.</p>
",,2016-11-01 19:55:35,combining LSA/LSI with Naive Bayes for document classification,<document-classification><gensim><naivebayes><latent-semantic-indexing><latent-semantic-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
5166,29939984,2015-04-29 09:44:22,,"<p>Gensim is an optimized python port of Word2Vec (see <a href=""http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/</a>)</p>

<p>I am currently using these vectors: <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow"">http://clic.cimec.unitn.it/composes/semantic-vectors.html</a></p>

<p>I am going to rerun the model training with gensim because there was some noisy tokens in their models. So i would like to find out what are some equivalent parameters for <code>word2vec</code> in <code>gensim</code></p>

<p>And the parameters they used from <code>word2vec</code> are:</p>

<ul>
<li>2-word context window, PMI weighting, no compression, 300K dimensions</li>
</ul>

<p>What is the gensim equivalence when i train a Word2Vec model?</p>

<p>Is it:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)
</code></pre>

<p><strong>Is there a PMI weight option in gensim?</strong></p>

<p><strong>What is the default min_count used in word2vec?</strong></p>

<p>There's another set of parameters from word2vec as such:</p>

<ul>
<li>5-word context window, 10 negative samples, subsampling, 400 dimensions.</li>
</ul>

<p><strong>Is there a negative samples parameter in gensim?</strong></p>

<p><strong>What is the parameter equivalence of subsampling in gensim?</strong></p>
",2015-04-29 11:03:36,2015-05-11 05:20:29,Word2Vec and Gensim parameters equivalence,<python><nlp><neural-network><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5180,27291145,2014-12-04 10:05:22,,"<p>I can't install gensim successfully through many ways.For I'm  a freshman in coding,it's difficult for me to understand the following information.</p>

<hr>

<pre><code>**C:\Python27\Lib\site-packages\gensim-0.10.3&gt;python setup.py install
Traceback (most recent call last):
  File ""setup.py"", line 22, in &lt;module&gt;
    from setuptools import setup, find_packages, Extension
  File ""C:\Python27\lib\site-packages\setuptools\__init__.py"", line 12, in &lt;modu
le&gt;
    from setuptools.extension import Extension
  File ""C:\Python27\lib\site-packages\setuptools\extension.py"", line 7, in &lt;modu
le&gt;
    from setuptools.dist import _get_unpatched
  File ""C:\Python27\lib\site-packages\setuptools\dist.py"", line 16, in &lt;module&gt;
    from setuptools.compat import numeric_types, basestring
  File ""C:\Python27\lib\site-packages\setuptools\compat.py"", line 19, in &lt;module
&gt;
    from SimpleHTTPServer import SimpleHTTPRequestHandler
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 27, in &lt;module&gt;
    class SimpleHTTPRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 208, in SimpleHTTPRequestHand
ler
    mimetypes.init() # try to read system mime.types
  File ""C:\Python27\lib\mimetypes.py"", line 358, in init
    db.read_windows_registry()
  File ""C:\Python27\lib\mimetypes.py"", line 258, in read_windows_registry
    for subkeyname in enum_types(hkcr):
  File ""C:\Python27\lib\mimetypes.py"", line 249, in enum_types
    ctype = ctype.encode(default_encoding) # omit in 3.x!
UnicodeDecodeError: 'ascii' codec can't decode byte 0xb6 in position 6: ordinal
not in range(128)**
</code></pre>

<hr>

<p>Thanks for help!</p>
",,2014-12-05 16:39:46,A mistake in installing gensim,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5232,27308118,2014-12-05 03:10:43,,"<p>I have a list of bag of words for two classes. Say <strong><em>n</em></strong> items in class <strong><em>A</em></strong> and <strong><em>m</em></strong> items in class <strong><em>B</em></strong>. I want to use the topic modeling with gensim package (for LDA) in python in order to train a model for class A vs class B. Meanwhile I am new to both <strong>Topic Modeling</strong> and <strong>Python</strong>. Does anyone know how should I do this? I mean, should I merge all the bags for each class and the use gensim or should I use bag for each item seperately? Thanks!</p>
",,2014-12-05 17:06:21,Topic Modeling Using Gensim in Python,<python><machine-learning><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5266,26251674,2014-10-08 07:45:48,,"<p>I am trying to install gensim python library. However I am facing some dependencies errors. I ve isntalled schipy and numpy throught canopy. Next step to use pip install gensim in order to get gensim package. However I am getting error messages. I have installed python 2.7.4. I ve got visual studio 2010 installed on my machine.</p>

<p><img src=""https://i.stack.imgur.com/2dPne.jpg"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/7R2g3.jpg"" alt=""enter image description here""></p>
",2014-10-08 07:56:49,2014-10-08 08:22:32,Installing gensim in windows 7,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5277,27324292,2014-12-05 20:39:00,,"<p>From the <a href=""https://code.google.com/p/word2vec/"">word2vec</a> site I can download GoogleNews-vectors-negative300.bin.gz.  The .bin file (about 3.4GB) is a binary format not useful to me.  Tomas Mikolov <a href=""https://groups.google.com/d/msg/word2vec-toolkit/lxbl_MB29Ic/g4uEz5rNV08J"">assures us</a> that ""It should be fairly straightforward to convert the binary format to text format (though that will take more disk space). Check the code in the distance tool, it's rather trivial to read the binary file.""  Unfortunately, I don't know enough C to understand <a href=""http://word2vec.googlecode.com/svn/trunk/distance.c"">http://word2vec.googlecode.com/svn/trunk/distance.c</a>.</p>

<p>Supposedly <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial/"">gensim</a> can do this also, but all the tutorials I've found seem to be about converting <em>from</em> text, not the other way.</p>

<p>Can someone suggest modifications to the C code or instructions for gensim to emit text?</p>
",2014-12-05 20:54:12,2017-05-04 08:30:48,Convert word2vec bin file to text,<python><c><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5310,30142345,2015-05-09 16:23:20,,"<p>I'm using word2vec to represent a small phrase (3 to 4 words) as a unique vector, either by adding each individual word embedding or by calculating the average of word embeddings.</p>

<p>From the experiments I've done I always get the same cosine similarity. I suspect it has to do with the word vectors generated by word2vec being normed to unit length (Euclidean norm) after training? or either I have a BUG in the code, or I'm missing something.</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from nltk import PunktWordTokenizer
from gensim.models import Word2Vec
from numpy.linalg import norm
from scipy.spatial.distance import cosine

def pattern2vector(tokens, word2vec, AVG=False):
    pattern_vector = np.zeros(word2vec.layer1_size)
    n_words = 0
    if len(tokens) &gt; 1:
        for t in tokens:
            try:
                vector = word2vec[t.strip()]
                pattern_vector = np.add(pattern_vector,vector)
                n_words += 1
            except KeyError, e:
                continue
        if AVG is True:
            pattern_vector = np.divide(pattern_vector,n_words)
    elif len(tokens) == 1:
        try:
            pattern_vector = word2vec[tokens[0].strip()]
        except KeyError:
            pass
    return pattern_vector


def main():
    print ""Loading word2vec model ...\n""
    word2vecmodelpath = ""/data/word2vec/vectors_200.bin""
    word2vec = Word2Vec.load_word2vec_format(word2vecmodelpath, binary=True)
    pattern_1 = 'founder and ceo'
    pattern_2 = 'co-founder and former chairman'

    tokens_1 = PunktWordTokenizer().tokenize(pattern_1)
    tokens_2 = PunktWordTokenizer().tokenize(pattern_2)
    print ""vec1"", tokens_1
    print ""vec2"", tokens_2

    p1 = pattern2vector(tokens_1, word2vec, False)
    p2 = pattern2vector(tokens_2, word2vec, False)
    print ""\nSUM""
    print ""dot(vec1,vec2)"", np.dot(p1,p2)
    print ""norm(p1)"", norm(p1)
    print ""norm(p2)"", norm(p2)
    print ""dot((norm)vec1,norm(vec2))"", np.dot(norm(p1),norm(p2))
    print ""cosine(vec1,vec2)"",     np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2)))
    print ""\n""
    print ""AVG""
    p1 = pattern2vector(tokens_1, word2vec, True)
    p2 = pattern2vector(tokens_2, word2vec, True)
    print ""dot(vec1,vec2)"", np.dot(p1,p2)
    print ""norm(p1)"", norm(p1)
    print ""norm(p2)"", norm(p2)
    print ""dot(norm(vec1),norm(vec2))"", np.dot(norm(p1),norm(p2))
    print ""cosine(vec1,vec2)"",     np.divide(np.dot(p1,p2),np.dot(norm(p1),norm(p2)))


if __name__ == ""__main__"":
    main()
</code></pre>

<p>and here is the output:</p>

<pre><code>Loading word2vec model ...

Dimensions 200
vec1 ['founder', 'and', 'ceo']
vec2 ['co-founder', 'and', 'former', 'chairman']

SUM
dot(vec1,vec2) 5.4008677771
norm(p1) 2.19382594282
norm(p2) 2.87226958166
dot((norm)vec1,norm(vec2)) 6.30125952303
cosine(vec1,vec2) 0.857109242583


AVG
dot(vec1,vec2) 0.450072314758
norm(p1) 0.731275314273
norm(p2) 0.718067395416
dot(norm(vec1),norm(vec2)) 0.525104960252
cosine(vec1,vec2) 0.857109242583
</code></pre>

<p>I'm using the cosine similarity as defined here <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">Cosine Similarity (Wikipedia)</a>. The values for the norms and dot products are indeed different.</p>

<p>Can anyone explain why the cosine is the same? </p>

<p>Thank you,
David</p>
",,2015-05-10 11:12:38,"word2vec, sum or average word embeddings?",<cosine-similarity><word2vec><sentence-similarity>,,,CC BY-SA 3.0,True,False,True,False,False
5328,30155506,2015-05-10 19:04:33,,"<p>Gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">official tutorial</a> explicitly states that it is possible to continue training a (loaded) model. I'm aware that according to the documentation it is not possible to continue training a model that was loaded from the <code>word2vec</code> format. But even when one generates a model from scratch and then tries to call the <code>train</code> method, it is not possible to access the newly created labels for the <code>LabeledSentence</code> instances supplied to <code>train</code>.</p>

<pre><code>&gt;&gt;&gt; sentences = [LabeledSentence(['first', 'sentence'], ['SENT_0']), LabeledSentence(['second', 'sentence'], ['SENT_1'])]
&gt;&gt;&gt; model = Doc2Vec(sentences, min_count=1)
&gt;&gt;&gt; print(model.vocab.keys())
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
&gt;&gt;&gt; sentence = LabeledSentence(['third', 'sentence'], ['SENT_2'])
&gt;&gt;&gt; model.train([sentence])
&gt;&gt;&gt; print(model.vocab.keys())

# At this point I would expect the key 'SENT_2' to be present in the vocabulary, but it isn't
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
</code></pre>

<p>Is it at all possible to continue the training of a Doc2Vec model in Gensim with new sentences? If so, how can this be achieved?</p>
",,2018-03-18 21:56:13,Continue training a Doc2Vec model,<neural-network><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5379,26286206,2014-10-09 19:12:16,,"<p>I am using gensim package for topic modelling in python.</p>

<p>I am trying to train the topic model using gensim. Below is the train.py module:</p>

<pre><code>class Corpus(object):
    def __init__(self, cursor, reviews_dictionary, corpus_path):
        self.cursor = cursor
        self.reviews_dictionary = reviews_dictionary
        self.corpus_path = corpus_path

    def __iter__(self):
        self.cursor.rewind()
        for review in self.cursor:
            yield self.reviews_dictionary.doc2bow(review[""words""])

    def serialize(self):
        BleiCorpus.serialize(self.corpus_path, self, id2word=self.reviews_dictionary)

        return self


class Dictionary(object):
    def __init__(self, cursor, dictionary_path):
        self.cursor = cursor
        self.dictionary_path = dictionary_path

    def build(self):
        self.cursor.rewind()
        dictionary = corpora.Dictionary(review[""words""] for review in self.cursor)
        dictionary.filter_extremes(keep_n=10000)
        dictionary.compactify()
        corpora.Dictionary.save(dictionary, self.dictionary_path)

        return dictionary


class Train:
    def __init__(self):
        pass

    @staticmethod
    def run(lda_model_path, corpus_path, num_topics, id2word):
        corpus = corpora.BleiCorpus(corpus_path)
        lda = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=id2word)
        lda.save(lda_model_path)

        return lda
</code></pre>

<p>I am getting the below error when I run this module:</p>

<pre><code>&gt; Traceback (most recent call last):


    File ""train.py"", line 74, in &lt;module&gt;
    main()

    File ""train.py"", line 68, in main
    dictionary = Dictionary(reviews_cursor, dictionary_path).build()
    File ""train.py"", line 38, in build
    corpora.Dictionary.save(dictionary, self.dictionary_path)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 288, in save
    pickle(self, fname)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 666, in pickle
    with smart_open(fname, 'wb') as fout: # 'b' for binary, needed on Windows
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 661, in smart_open
    return open(fname, mode)
    IOError: [Errno 2] No such file or directory: 'models/dictionary.dict'
</code></pre>

<p>Could anyone please help me figure out the issue?</p>
",2014-10-09 19:18:07,2014-10-09 19:22:09,Python:: IOError: [Errno 2] No such file or directory: 'models/dictionary.dict',<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5393,27354912,2014-12-08 09:26:26,,"<p>last parts of the code:</p>

<pre><code>lda = models.LdaModel(corpus_tfidf, id2word = dic, num_topics = 64)
corpus_lda = lda[corpus_tfidf]
</code></pre>

<p>I am wondering how to save corpus_lda for further use?</p>
",,2014-12-08 14:04:59,How to save trainset's distribution on a trained LDA models by gensim?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5408,30323899,2015-05-19 11:07:34,,"<p>I want to use the gensim library in a python code and need to execute that code from Java.</p>

<p>Following is the python code, sent2vec.py:</p>

<pre><code>import gensim
sentences = gensim.models.doc2vec.LabeledLineSentence('/tmp/sentence.tmp')
model = gensim.models.doc2vec.Doc2Vec(sentences, size=10, window=5, min_count=2, workers=4)
model.save_word2vec_format('/tmp/sentenceVectors.txt')
</code></pre>

<p>The code is running fine in a standalone mode, i.e. when executed by:</p>

<pre><code>python sent2vec.py
</code></pre>

<p>But I need to call this function from a java code. I tried using Jython but having errors. My java code looks like the following:</p>

<pre><code>StringWriter writer = new StringWriter(); //output will be stored here
ScriptEngineManager manager = new ScriptEngineManager();
ScriptContext context = new SimpleScriptContext();

context.setWriter(writer); //configures output redirection
ScriptEngine engine = manager.getEngineByName(""python"");
engine.eval(new FileReader(""sent2vec.py""), context);
System.out.println(writer.toString());
</code></pre>

<p>I am having this error:</p>

<pre><code>Exception in thread ""main"" javax.script.ScriptException: ImportError: No module named gensim in &lt;script&gt; at line number 1
at org.python.jsr223.PyScriptEngine.scriptException(PyScriptEngine.java:202)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:42)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:47)
Caused by: Traceback (most recent call last):
File ""&lt;script&gt;"", line 1, in &lt;module&gt;
ImportError: No module named gensim

at org.python.core.Py.ImportError(Py.java:328)
at org.python.core.imp.import_first(imp.java:877)
at org.python.core.imp.import_module_level(imp.java:972)
at org.python.core.imp.importName(imp.java:1062)
at org.python.core.ImportFunction.__call__(__builtin__.java:1280)
at org.python.core.PyObject.__call__(PyObject.java:431)
at org.python.core.__builtin__.__import__(__builtin__.java:1232)
at org.python.core.imp.importOne(imp.java:1081)
at org.python.pycode._pyx0.f$0(&lt;script&gt;:5)
at org.python.pycode._pyx0.call_function(&lt;script&gt;)
at org.python.core.PyTableCode.call(PyTableCode.java:167)
at org.python.core.PyCode.call(PyCode.java:18)
at org.python.core.Py.runCode(Py.java:1386)
at org.python.core.__builtin__.eval(__builtin__.java:497)
at org.python.core.__builtin__.eval(__builtin__.java:501)
at org.python.util.PythonInterpreter.eval(PythonInterpreter.java:259)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:40)
... 3 more
</code></pre>

<p>Any help will be appreciated.
Thanks.</p>
",,2020-05-28 20:05:25,Using gensim python from Java with jython,<java><python><jython><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5417,30301922,2015-05-18 11:24:45,,"<p>I have trained a word2vec model using a corpus of documents with Gensim. Once the model is training, I am writing the following piece of code to get the raw feature vector of a word say ""view"".</p>

<pre><code>myModel[""view""]
</code></pre>

<p>However, I get a KeyError for the word which is probably because this doesn't exist as a key in the list of keys indexed by word2vec. How can I check if a key exits in the index before trying to get the raw feature vector?</p>
",,2018-07-21 10:21:50,How to check if a key exists in a word2vec trained model or not,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5438,26309463,2014-10-10 23:31:39,,"<p>I'm working on a mini project classification texts with Python.<br>
The idea is simple: we have a corpus of sentences respectively belonging to J. Chirac and Mitterrand (2 ex presidents of French republic (with associated labels).<br>
The goal is to build a model that predicts that belong to different sentences. for classes (labels) it has ""M"" for Mitterand and ""C"" for Chirac, correctly in my program I considered that <code>M == &gt; -1</code>, and <code>C ==&gt; 1</code>.<br>
Finally, I applied a clustering algorithm on my dataset that is called Naive Bayes, and I made a prediction on new data (Test).<br>
The problem here is that after the evaluation of performance of my system, I got a very low score, although I have used several methods to increase (stopwords, bigrams, smoothing ..)</p>

<p>If someone with another idea or suggestion for me to improve the performance of my system, I will be very satisfied.</p>

<p>I will attach below some of my code.</p>

<p>In the following code I chose my stopliste and I deleted the words that are not very important and splitters to produce my corpus, and I use bigrams:</p>

<pre><code>stoplist = set('le la les de des  un une en au ne ce d l c s je tu il que qui mais quand'.split())
stoplist.add('')
splitters = u'; |, |\*|\. | |\'|'
liste = (re.split(splitters, doc.lower()) for doc in alltxts) # generator = pas de place en memoire
dictionary = corpora.Dictionary([u""{0}_{1}"".format(l[i],l[i+1]) for i in xrange(len(l)-1)] for l in liste) # bigrams
print len(dictionary)
stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]
once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq &lt; 10 ]
dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once
dictionary.compactify() # remove gaps in id sequence after words that were removed
print len(dictionary)
liste = (re.split(splitters, doc.lower()) for doc in alltxts) # ATTENTION: quand le gnrator a dj servi, il ne se remet pas au dbut =&gt; le re-crer pour plus de scurit 
alltxtsBig = ([u""{0}_{1}"".format(l[i],l[i+1]) for i in xrange(len(l)-1)] for l in liste)
corpusBig = [dictionary.doc2bow(text) for text in alltxtsBig]
</code></pre>

<p>and here, I produce a corpus for my test dataset:</p>

<pre><code>liste_test = (re.split(splitters, doc.lower()) for doc in alltxts_test)
alltxtsBig_test = ([u""{0}_{1}"".format(l[i],l[i+1]) for i in xrange(len(l)-1)] for l in liste_test)
corpusBig_test = [dictionary.doc2bow(text) for text in alltxtsBig_test]
and here I am doing the processing of these data has a numpy matrix, and I apply the algorithm on data, and I make the prediction on test data:


dataSparse = gensim.matutils.corpus2csc(corpusBig)
dataSparse_test = gensim.matutils.corpus2csc(corpusBig_test)
import sklearn.feature_extraction.text as txtTools #.TfidfTransformer
t = txtTools.TfidfTransformer()
t.fit(dataSparse.T)
data2 = t.transform(dataSparse.T)
data_test = t.transform(dataSparse_test.T)
nb_classifier = MultinomialNB().fit(data2, labs)
y_nb_predicted = nb_classifier.predict(data_test)
</code></pre>

<p><strong>Edit:</strong><br>
The performance of my system gives a value of 0.28. Normally if the system is effective it will give more than 0.6.<br>
I work on a file Millers sentences, and I declared gensim, I did not paste all the code here because it's very long, my question is just if there are other methods of improve system performance, I have used bigrams, smoothing .. that's all.</p>
",2015-06-12 15:36:31,2015-06-12 15:36:31,machine learning text classification,<python><algorithm><machine-learning><text-classification>,,,CC BY-SA 3.0,False,False,True,False,True
5456,30446268,2015-05-25 21:30:00,,"<p>I have many independent tasks that read <em>but not write to</em> the same gensim model which is about 3.6GB in size. (Gensim is a topic modelling library built upon numpy.) So I decide to parallelize them by first loading the gensim model from a file:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
from multiprocessing import Pool
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>And then pass the <code>model</code> as a parameter to a pool of processes to run <code>doWork</code>:</p>

<pre><code>def doWork(experiment, doc):
  print ""Begin working""
  # do some work here; access model by experiment.model

class Experiment(object):
   def __init__(self, model, docs):
     self.model = model
     self.docs = docs
   def run(self):
     pool = Pool(processes = 4)
     print ""Done preparing""
     results = pool.map(doWork, [(self, doc) for doc in self.docs])
     return results

experiment = Experiment(model, ['doc1.txt', 'doc2.txt'])
experiment.run()
</code></pre>

<p>When I ran this script (the two segments I show here are a runnable script; please copy), it got stuck on the <code>pool.map</code> line and a <code>SystemError</code> occurred. The output was:</p>

<pre><code>Done preparing
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 763, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.py"", line 342, in _handle_tasks
    put(task)
SystemError: NULL result without error in PyObject_Call
</code></pre>

<p>The error had never occurred before I introduced gensim to my program. (Gensim without multiprocessing also works for me.) I think it may be related to the interoperation with C code underlying gensim and numpy (BLAS). <strong>I wanted to know the reason of this error and how to fix it.</strong> If I can't use gensim with subprocessing, what are the alternatives?</p>

<p>I don't think the <code>model</code> would be copied, because my OS (Mac OS X) should be using the copy-on-write strategy. I don't think it is related to memory synchronization either, because not a line of ""Begin working"" is printed, i.e. the <code>model</code> has not been accessed by my code. The error is in passing the <code>model</code> to the sub-processes.</p>
",2015-05-25 21:45:47,2015-05-25 21:45:47,SystemError when sharing a gensim (numpy) model in multiprocessing,<python><numpy><python-multiprocessing><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5489,30488695,2015-05-27 16:53:24,,"<p>I am using the doc2vec model from teh gensim framework to represent a corpus of 15 500 000  short documents (up to 300 words): </p>

<pre><code>gensim.models.Doc2Vec(sentences, size=400, window=10, min_count=1, workers=8 )
</code></pre>

<p>After creating the vectors there are  more than  18 000 000 vectors representing words and documents. </p>

<p>I want to find the most similar items (words or documents) for a given item: </p>

<pre><code> similarities = model.most_similar(uid_10693076)
</code></pre>

<p>but I get a MemoryError when the similarities are computed:</p>

<pre><code>Traceback (most recent call last):

   File ""article/test_vectors.py"", line 31, in &lt;module&gt; 
    similarities = model.most_similar(item) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 639, in most_similar 
    self.init_sims() 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 827, in init_sims 
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL) 
</code></pre>

<p>I have a Ubuntu machine  with 60GB Ram and 70GB swap . I checked the memory allocation (in htop) and I observed that never the memory was completely used. I also set to unlimited the the maximum address space which may be locked in memory in python:</p>

<pre><code>resource.getrlimit(resource.RLIMIT_MEMLOCK)
</code></pre>

<p>Could someone explain the reason for this MemoryError? In my opinion the available memory should be enough for doing this computations. Could be some memory limits in python or OS?</p>

<p>Thanks in advance!</p>
",,2015-06-17 00:47:03,Doc2vec MemoryError,<python><memory><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5509,30480027,2015-05-27 10:36:34,,"<p>I am trying to use the freebase word embeddings released by Google, but I have a hard time getting the words from the freebase name. </p>

<pre><code>model = gensim.models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000.bin',binary=True)
model.vocab.keys()[:10]

Out[22]:
[u'/m/026tg5z',
 u'/m/018jz8',
 u'/m/04klsk',
 u'/m/08gd39',
 u'/m/0kt94',
 u'/m/05mtf0t',
 u'/m/05tjjb',
 u'/m/01m3vn',
 u'/m/0h7p35',
 u'/m/03ggvg3']
</code></pre>

<p>Does anyone know if it exist some kind of table to map the freebase representations into the words they represent ?</p>

<p>Regards,</p>

<p>Hedi</p>
",,2016-08-10 21:27:07,Using freebase vectors with gensim,<python><freebase><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5546,30563361,2015-05-31 22:25:55,,"<p>I have two questions related to the usage of <strong>gensim</strong> for LDA. </p>

<p>1) How can I create a model using one corpus, save it and perhaps extend it later on another corpus by training the model on it ? Is it possible ?</p>

<p>2) Can LDA be used to classify an unseen document, or the model needs to be created again by including it in the corpus ? Is there an online way to do it and see the changes on the fly ?</p>

<p>I have a fairly basic understanding of LDA and have used it for Topic modeling on simple corpus using <strong>lda</strong> and <strong>gensim</strong> libraries.  Please point out any conceptual inconsistencies in the question. Thanks !</p>
",,2017-03-20 07:40:50,Latent Dirichlet Allocation using Gensim on more than one corpus,<python><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5551,30573873,2015-06-01 12:46:10,,"<p>I am thinking of training word2vec on huge large scale data of more than 10 TB+ in size on web crawl dump. </p>

<p>I personally trained c implementation GoogleNews-2012 dump (1.5gb) on my iMac took about 3 hours to train and generate vectors (impressed with speed). I did not try python implementation though :( I read somewhere that generating vectors on wiki dump (11gb) of 300 vector length takes about 9 days to generate. </p>

<ol>
<li><p>How to speed up word2vec? Do i need to use distributed models or what type of hardware i need to do it within 2-3 days? i have iMac with 8gb ram.</p></li>
<li><p>Which one is faster? Gensim python or C implemention?</p></li>
</ol>

<p>I see that word2vec implementation does not support GPU training.</p>
",2015-06-02 07:40:20,2018-01-22 03:59:22,How to train Word2vec on very large datasets?,<python><c><machine-learning><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5558,30628566,2015-06-03 19:07:00,,"<p>I have trained few million words in <code>Word2Vec</code> of <code>Gensim</code> of Python. I want to update this trained model with new data. 
But from your previous posts and other sources around the web I came to know this is not possible.
So I am trying to create multiple models and dump them. Now I want to merge the models I am dumping. I want to use these dumped results. I got a previous post <a href=""https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec"">Merging pretrained models in Word2Vec?</a>
but I am not getting how to do it. I came to know there is a library named deepdist, I am trying to see some experiments around:</p>

<pre><code>model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)
</code></pre>

<ol>
<li>Is there a possible solution?</li>
<li>If any, one may kindly suggest how to do it?</li>
</ol>

<p>I am using Python2.7 on Windows 7 Professional. </p>
",2017-05-23 11:46:18,2015-06-03 19:54:42,Averaging Multiple Models Word2vec Gensim,<python><python-2.7><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5569,30633871,2015-06-04 02:18:16,,"<p>I wish to test the scalability of two implementations of an algorithm (Latent Dirichlet Allocation) in Python - <strong>gensim</strong> and <strong>lda</strong> . Most of the google search results talk about scalability of websites and web-based applications. </p>

<p>How do I test the scalability of a simple algorithm (Not an entire system)? What are the best practices to kept in mind ?</p>
",2015-06-04 02:27:31,2015-06-04 03:01:21,Scalability of simple algorithms,<python><algorithm><scalability><lda>,,,CC BY-SA 3.0,False,False,True,False,False
5577,30583166,2015-06-01 20:58:47,,"<p>How can I find the N-nearest words given a word using gensim's word2vec implementation. What is the API for that? I am referring to skip grams here. Maybe I missed something, I read all about finding similar words, finding the odd one out and so on...</p>

<p>In DL4j I have this method called <code>wordsNearest(String A, int n) which gives me the n-nearest words to A</code>. What is the equivalent of this in Gensim?</p>
",,2016-12-02 16:22:05,Gensim word2vec finding nearest words given a word,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5586,30654526,2015-06-04 21:30:42,,"<p>so I am relatively new working with gensim and LDA, started about two weeks ago and I am having trouble trusting these results. The following are the topics produced by using 11 1-paragraph documents. </p>

<p>topic #0 (0.500): 0.059*island + 0.059*world + 0.057*computers + 0.056*presidential + 0.053*post + 0.047*posts + 0.046*tijuana + 0.045*vice + 0.045*tweets + 0.045*president</p>

<p>2015-06-04 16:22:07,891 : INFO : topic #1 (0.500): 0.093*computers + 0.064*world + 0.060*posts + 0.053*eurozone + 0.052*months + 0.049*tijuana + 0.048*island + 0.046*raise + 0.044*rates + 0.042*year</p>

<p>These topics just don't quite seem right. In fact they seem almost non-sensical. How exactly should I read these results? Also, is it normal that the topic distributions are exactly the same for both topics? </p>
",,2015-06-05 01:10:59,LDA generated topics,<python><machine-learning><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5615,30663755,2015-06-05 10:02:59,,"<p>I am new to stackoverflow. Please forgive my bad English.</p>

<p>I am using <strong>word2vec</strong> for a school project. I want to work with a domain specific corpus (like Physics Textbook) for creating the word vectors using <strong>Word2Vec</strong>. This standalone does not provide good results due to lesser size of the corpus. This especially hurts as we want to evaluate on words that may very well be outside the vocabulary of the text book.</p>

<p>We want the textbook to encode the domain specific relationships and semantic ""nearness"". ""Quantum"" and ""Heisenberg"" are especially close in this textbook for eg. which may not hold true for background corpus. To handle the generic words (like ""any"") we need the basic background model(like the one provided by Google on word2vec site).  </p>

<p>Is there any way that we can supplant to the background model using our newer corpus. Just training on the corpus etc. doesnot work well.</p>

<p>Are there any attempts to combine vector representations from two corpus- general and specific. I could not find any in my searches. </p>
",2015-06-05 10:54:52,2015-06-05 14:53:48,Biasing word2vec towards special corpus,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5641,30718471,2015-06-08 20:27:31,,"<p>How to save the output? I am using the following code:    </p>

<pre><code>%time lda1 = models.LdaModel(corpus1, num_topics=20, id2word=dictionary1, update_every=5, chunksize=10000, passes=100)
</code></pre>
",2016-01-07 17:15:14,2018-06-14 22:25:29,How to save gensim LDA topics output to csv along with the scores?,<python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5657,30742973,2015-06-09 21:15:31,,"<p>So I have a general bow corpus that I have created that yields documents per the format that <code>gensim</code> requires (<a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">see here</a>.)</p>

<p>However those documents have a lot of words that are used extremely often.  So I wanted to use a <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tfidf</a> to balance that out.</p>

<p>So I do something like</p>

<pre><code>tfidf_model = TfidfModel(corpus)
new_corpus = tfidf_model[corpus]
</code></pre>

<p>Now I want to train my LDA</p>

<pre><code>lda = LdaModel(corpus=new_corpus, num_topics=16)
</code></pre>

<p>And it trains and converges fine...great.  Now I have a new unseen document that I want to project onto my lda topics.  Do I always need to project this new doc with the <code>tfidf_model</code> first?  i.e.</p>

<pre><code>transformed_doc = tfidf_model[unseen_doc]
projections = lda[transformed_doc]
</code></pre>

<p>Or can <code>gensim</code> take the original and know to apply the <code>tfidf</code> first then project onto the <code>lda</code>.  </p>

<pre><code>projections = lda[unseen_doc]
</code></pre>

<p>The <code>gensim</code> docs are a little unclear on whether or not the model knows any other previous transformations were applied to a corpus.</p>
",,2016-11-22 02:02:56,Do I need to transform unseen documents before projecting them onto model topics?,<python><tf-idf><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5658,30745184,2015-06-10 00:37:51,,"<p>I wish to know the default number of iterations in <strong>gensim</strong>'s LDA (Latent Dirichlet Allocation) algorithm. I don't think the documentation talks about this. (Number of iterations is denoted by the parameter <strong>iterations</strong> while initializing the <strong>LdaModel</strong> ). Thanks !</p>
",2015-06-10 01:09:05,2017-12-12 00:14:42,Gensim LDA - Default number of iterations,<python><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5673,30770919,2015-06-11 03:13:23,,"<p>I am running (from Gensim)</p>

<pre><code>top_topics(corpus, num_topics=5, num_words=20)
</code></pre>

<p>I get the error:</p>

<pre><code>for topic in lda.top_topics(corpus=corpus, num_topics=5, num_words=20):
File ""/Library/Python/2.7/site-packages/gensim/models/ldamodel.py"", line 760, in top_topics
bestn = np.argsort(topic)[::-1][:num_words]
NameError: global name 'np' is not defined
</code></pre>

<p>I thought this was odd and I can see the file (indeed it has <code>import numpy</code> but not <code>import numpy as np</code>).</p>

<p>After adding <code>import numpy as np</code> and trying to run it again:</p>

<pre><code>  File ""/Library/Python/2.7/site-packages/gensim/models/ldamodel.py"", line 772, in top_topics
    if len(list(ifilter(lambda x: x[0] == id,corpus[document]))) &gt; 0:
</code></pre>

<p>NameError: global name 'ifilter' is not defined</p>

<p>Which was fixed by changing <code>from itertools import chain</code> to <code>from itertools import chain, ifilter</code>. The module then works perfectly. </p>

<p>So, I guess my question is whether this was an error specific to my system (is there some kind of 'import all' python trick that doesn't work for me?).</p>
",2015-06-11 03:42:18,2018-01-05 07:10:22,top_topics Gensim NameError: global name 'np' is not defined,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5682,30804073,2015-06-12 13:15:54,,"<p>I'm trying to fit an LDA to a corpus in LDA-C format.  I've got it working for a HDP model but I can't seem to make it work for LDA in gensim.  I'm looking to get the topic probability vector for each document as well as the probability distribution over words for each topic.</p>

<p>Here is the HDP model which works fine</p>

<p>.dat file has the corpus in LDA-C format and .vocab file has unique words</p>

<pre><code>corpus = gensim.corpora.belicorpus.BeliCorpus('ap.dat','ap.vocab')  
d = gensim.corpora.Dictionary()
d.token2id = dict(enumerate(l[:-1] for l in open('ap.vocab')))
hdp = gensim.models.HdpModel(corpus,d.token2id)
alpha, beta = hdp.hdp_to_lda()

# save topic prior
numpy.savetxt(corpus_name+'.alpha',alpha)

# save word distribution for each topic
numpy.savetxt(corpus_name+'.beta',beta)

# save topic distribution for each document in market matrix format
doc_hdp = hdp[corpus]
gensim.corpora.MmCorpus.save_corpus(corpus_name+'.mm',doc_hdp)
</code></pre>

<p>Here is the LDA implementation, I get the proper vectors but I can't seem to find a function that will give me the priors or word distribution/topic:</p>

<pre><code>corpus=gensim.corpora.bleicorpus.BleiCorpus('ap.dat','ap.vocab') 
d = gensim.corpora.Dictionary()
d.token2id = dict(enumerate(l[:-1] for l in open('ap.vocab')))
lda = gensim.models.LdaModel(corpus,num_topics=10,id2word=d.token2id)
</code></pre>
",2015-06-12 18:59:23,2016-03-03 13:19:01,Fitting LDA to corpus in LDA-C format in gensim,<lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5705,27470670,2014-12-14 15:13:43,,"<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>

<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>

<p>Thanks.</p>
",2020-04-05 17:56:23,2020-04-05 17:56:23,How to use Gensim doc2vec with pre-trained word vectors?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5731,30851588,2015-06-15 17:42:10,,"<p>So, I am relatively new using Gensim and LDA in general. The problem right now is that when I run LDA on my corpus, the topics' tokens' weights are all 0:</p>

<p>2015-06-15 12:21:12,439 : INFO : topic diff=0.082235, rho=0.250000</p>

<p>2015-06-15 12:21:12,454 : <strong>INFO : topic #0</strong> (0.100): 0.000*sundayes + 0.000*nowe + 0.000*easter + 0.000*iniunctions + 0.000*eyther + 0.000*christ, + 0.000*authoritie + 0.000*sir + 0.000*saint + 0.000*thinge</p>

<p>2015-06-15 12:21:12,468 : <strong>INFO : topic #1</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*iosephs + 0.000*lo! + 0.000*egypts + 0.000*iacob + 0.000*ioseph, + 0.000*beniamin</p>

<p>2015-06-15 12:21:12,482 : <strong>INFO : topic #2</strong> (0.100): 0.000*agreeable + 0.000*creede, + 0.000*fourme + 0.000*conteined + 0.000*apostolike, + 0.000*vicars, + 0.000*sacrament + 0.000*contrarywise + 0.000*parsons, + 0.000*propitiatorie</p>

<p>2015-06-15 12:21:12,495 : <strong>INFO : topic #3</strong> (0.100): 0.000*yf + 0.000*suche + 0.000*lyke + 0.000*shoulde + 0.000*moste + 0.000*youre + 0.000*oure + 0.000*lyfe, + 0.000*anye + 0.000*thinges</p>

<p>2015-06-15 12:21:12,507 : <strong>INFO : topic #4</strong> (0.100): 0.000*heau'nly + 0.000*eu'n + 0.000*heau'n + 0.000*sweet + 0.000*peace + 0.000*eu'ry + 0.000*constance + 0.000*constant + 0.000*doth + 0.000*oh</p>

<p>2015-06-15 12:21:12,521 : <strong>INFO : topic #5</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*vel + 0.000*iosephs + 0.000*heau'n + 0.000*lo! + 0.000*ac + 0.000*seu'n</p>

<p>2015-06-15 12:21:12,534 : <strong>INFO : topic #6</strong> (0.100): 0.000*thou + 0.000*would + 0.000*love + 0.000*king + 0.000*sir, + 0.000*doe + 0.000*thee + 0.000*1. + 0.000*never + 0.000*2.</p>

<p>2015-06-15 12:21:12,546 : <strong>INFO : topic #7</strong> (0.100): 0.000*quae + 0.000*vt + 0.000*qui + 0.000*ij + 0.000*non + 0.000*ad + 0.000*si + 0.000*vel + 0.000*atque + 0.000*cum</p>

<p>2015-06-15 12:21:12,558 : <strong>INFO : topic #8</strong> (0.100): 0.000*suspected + 0.000*supersticious + 0.000*squire + 0.000*parsons + 0.000*ordinarie + 0.000*vsed, + 0.000*english, + 0.000*fortnight + 0.000*squire, + 0.000*offenders</p>

<p>2015-06-15 12:21:12,572 : <strong>INFO : topic #9</strong> (0.100): 0.001*/ + 0.001*ile + 0.000*y^e + 0.000*che + 0.000*much + 0.000*tis + 0.000*could + 0.000*oh + 0.000*neuer + 0.000*heart</p>

<p>I have 307 documents and I'm running my LDA with the following code after removing the stopwords:</p>

<p>texts = [[token for token in text if frequency[token] > 3 ] for text in texts]</p>

<p>dictionary = corpora.Dictionary(texts)</p>

<p>corpus = [dictionary.doc2bow(text) for text in texts]</p>

<p>tfidf = models.TfidfModel(corpus)
tfidf_corpus = tfidf[corpus]</p>

<p>lda = models.LdaModel(tfidf_corpus, id2word = dictionary, update_every=1, chunksize= 20, num_topics = 10, passes = 1)</p>

<p>lda[tfidf_corpus]</p>

<p>lda.print_topics(10)</p>

<p>I am not sure what is wrong but everytime I run this, the token weights are 0. What might be causing this and how could I correct this? </p>
",2015-06-15 22:00:02,2015-06-15 22:00:02,LDA Results Errors,<machine-learning><nlp><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5738,27477084,2014-12-15 03:51:47,,"<p>I am trying to run the GENSIM Topic modeling example in Canopy Express and get the following error on Sum() line.</p>

<pre><code>from gensim import corpora, models, similarities
from itertools import chain

"""""" DEMO """"""
documents = [""Human machine interface for lab abc computer applications"",
         ""A survey of user opinion of computer system response time"",
         ""The EPS user interface management system"",
         ""System and human system engineering testing of EPS"",
         ""Relation of user perceived response time to error measurement"",
         ""The generation of random binary unordered trees"",
         ""The intersection graph of paths in trees"",
         ""Graph minors IV Widths of trees and well quasi ordering"",
         ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
     for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] for text in texts]
</code></pre>

<p>The error I get is TypeError: an integer is required.  It seems to be ok in regular Python but Canopy has an issue.  It seems it is how Canopy treats the sum statement but I'm not sure how to work around it.  Any ideas as I'm just getting started with Python and text analysis.</p>
",,2014-12-15 04:47:30,GENSIM Error in Canopy Express,<python-2.7><canopy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5787,30971935,2015-06-22 03:55:48,,"<p>I seem to be one of the many people struggling to install gensim on windows. I have trawled through countless forums but the errors poster there never appear to match my errors. So hopefully someone can point me in the right direction! </p>

<p>I am running Windows Server 2012 R2 Standard 64-bit. I have installed MinGW &amp; Anaconda 2.2.0 (64-bit), which comes with Python 2.7.9. </p>

<p>I have added a file distutils.cfg into C:\Users\Sam\Anaconda\Lib\distutils  with the contents:</p>

<pre><code>[build]
compiler=mingw32
</code></pre>

<p>I have added C:\MinGW\bin to my Environment variables. </p>

<p>If I install gensim using pip I do not get any errors, until I try to run Word2Vec when I get the error:</p>

<pre><code>C:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\gensim\models\word2vec.py:459: UserWarning: C extension com
pilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.
</code></pre>

<p>So I have uninstalled gensim and tried to re-install using the mingw32 compiler, but this gives me this error:</p>

<pre><code>python setup.py build --compiler=mingw32
c:\users\sam.passmore\appdata\local\continuum\anaconda\lib\site-packages\setuptools-14.3-py2.7.egg\setuptools\dist.py:282: UserWarni
ng: Normalizing '0.11.1-1' to '0.11.1.post1'
running build
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o
gcc: error: ./gensim/models/word2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.word2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed."")
building 'gensim.models.doc2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/doc2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\doc2vec_inner.o
gcc: error: ./gensim/models/doc2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.doc2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed.""
</code></pre>

<p>I have exhausted all options I can think of or find, so if anyone could give some advice it would be much appreciated. </p>
",2015-06-22 04:01:21,2015-07-22 00:24:30,Gensim with MinGW,<python><windows><python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5805,30973503,2015-06-22 06:41:44,,"<p>I am trying to perform tfidf on a matrix. I would like to use gensim, but <code>models.TfidfModel()</code> only works on a corpus and therefore returns a list of lists of varying lengths (I want a matrix).</p>

<p>The options are to somehow fill in the missing values of the list of lists, or just convert the corpus to a matrix </p>

<pre><code>numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)
</code></pre>

<p>Choosing the latter, I then try to convert this count matrix to a tf-idf weighted matrix:</p>

<pre><code>def TFIDF(m):
    #import numpy
    WordsPerDoc = numpy.sum(m, axis=0)
    DocsPerWord = numpy.sum(numpy.asarray(m &gt; 0, 'i'), axis=1)
    rows, cols = m.shape
    for i in range(rows):
        for j in range(cols):
            amatrix[i,j] = (amatrix[i,j] / WordsPerDoc[j]) * log(float(cols) /     DocsPerWord[i])
</code></pre>

<p>But, I get the error <code>AttributeError: 'numpy.ndarray' object has no attribute 'A'</code></p>

<p>I copied the function above from another script. It was:</p>

<pre><code>def TFIDF(self):
    WordsPerDoc = sum(self.A, axis=0)        
    DocsPerWord = sum(asarray(self.A &gt; 0, 'i'), axis=1)
    rows, cols = self.A.shape
    for i in range(rows):
       for j in range(cols):
          self.A[i,j] = (self.A[i,j] / WordsPerDoc[j]) * log(float(cols) / DocsPerWord[i])
</code></pre>

<p>Which I believe is where it's getting the <code>A</code> from. However, I re-imported the function. </p>

<p>Why is this happening?</p>
",2015-06-22 06:46:47,2015-06-22 16:47:29,AttributeError: 'numpy.ndarray' object has no attribute 'A',<python><numpy><matrix><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5826,31003021,2015-06-23 12:37:07,,"<p>I am trying to run exactly the same code, once at my macbook pro and once at Ubuntu machine at AWS.</p>

<p>My code looks just like this (It uses MultinomialNB() from scikit learn):</p>

<pre><code>clf = MultinomialNB()
clf.fit(vectorized_data, labels)
</code></pre>

<p>On my macbook model training goes well, but on the Ubuntu machine I am getting:</p>

<pre><code>&lt;ipython-input-5-c52751e2119e&gt; in &lt;module&gt;()
----&gt; 1 m.train_models()

/home/ubuntu/topic_modeling/classification.pyc in train_models(self, minimal)
133                 continue
134             bm = BinaryModel(label)
--&gt; 135         bm.train_models(self.vectorizer, self.data)
136             self.models.append(bm)
137             logger.info(""Successfully trained model for the %s tag"", label)

/home/ubuntu/topic_modeling/classification.pyc in train_models(self, vectorizer, data)
 92             # TODO some more complex grid search should be here
 93             clf = MultinomialNB()
---&gt; 94         clf.fit(vectorized_data, labels)
 95             self.models.append(clf)
 96
/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/naive_bayes.pyc in fit(self, X, y, sample_weight)
472             Returns self.
473         """"""
--&gt; 474     X, y = check_X_y(X, y, 'csr')
475         _, n_features = X.shape
476

/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)
442     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
443                     ensure_2d, allow_nd, ensure_min_samples,
--&gt; 444                 ensure_min_features)
445     if multi_output:
446         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)
342             else:
343                 dtype = None
--&gt; 344     array = np.array(array, dtype=dtype, order=order, copy=copy)
345         # make sure we actually converted to numeric:
346         if dtype_numeric and array.dtype.kind == ""O"":

ValueError: setting an array element with a sequence.
</code></pre>

<p>I'd like to run the training on the Ubuntu machine to be able to run it in screen.</p>

<p>When I try <code>pip freeze</code> than on both machines it looks exactly the same. 
Does anyone has some idea what can be possibly wrong?</p>

<p><strong>EDIT</strong></p>

<p><code>labels</code> is just list of 0 and 1, eg. <code>[0, 1, 0, 0, 0, 1]</code></p>

<p><code>vectorized_data</code> is obtained using gensim framework. First tokenizing text, then converting it to bow by:</p>

<pre><code>bow_text = self.dictionary.doc2bow(tokenized_text)
self.tfidf = models.TfidfModel(dictionary=self.dictionary)
gensim.matutils.sparse2full(self.tfidf[bow_text], self.tfidf.num_nnz)
</code></pre>
",2015-06-24 08:01:12,2015-06-24 08:01:12,ValueError: setting an array element with a sequence. Scikit learn,<python><ubuntu><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
5838,31011061,2015-06-23 18:48:58,,"<p>I am working with the gensim dictionary. For example, you can print <code>print(dictionary.token2id)</code>, as shown here <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">https://radimrehurek.com/gensim/tut1.html</a>. I can also <code>print dictionary</code>:</p>

<p><code>Dictionary(7 unique tokens: [u'nra', u'canon', u'deuterium', u'ion', u'facebook']...)</code></p>

<p>How do I access the key value pairs of the dictionary object, however? </p>
",,2017-11-02 17:32:24,Access key value pairs in gensim dictionary,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
5873,27533977,2014-12-17 20:09:12,,"<p>For research purposes, I want a large (~100K) set of web pages, though I am only interested in their text. I plan to use them for gensim LDA topic model. CommonCrawler seems like a good place to start, but I am not sure how to do it.
Could someone point the way how to download 100K text files or how to access them (if it's easier than downloading them)?</p>
",,2014-12-17 21:42:53,How to download subset of Amazon CommonCrawel (only the text (WET files?) is needed),<download><lda><gensim><common-crawl>,,,CC BY-SA 3.0,False,False,True,False,False
5880,31145174,2015-06-30 17:58:38,,"<p>I am trying to use word2vec and using freebase skip gram model. But I'm unable to load the model due to memory error.</p>

<p>Here is the code snippet for the same:</p>

<pre><code>model = gensim.models.Word2Vec()
model = models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000.bin.gz', binary=True)
</code></pre>

<p>I'm getting following error:</p>

<pre><code>MemoryError                               Traceback (most recent call last)
&lt;ipython-input-40-a1cfacf48c94&gt; in &lt;module&gt;()
      1 model = gensim.models.Word2Vec()
----&gt; 2 model = models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000.bin.gz', binary=True)

/../../word2vec.pyc in load_word2vec_format(cls, fname, fvocab, binary, norm_only)
    583             vocab_size, layer1_size = map(int, header.split())  # throws for invalid file format
    584             result = Word2Vec(size=layer1_size)
--&gt; 585             result.syn0 = zeros((vocab_size, layer1_size), dtype=REAL)
    586             if binary:
    587                 binary_len = dtype(REAL).itemsize * layer1_size

MemoryError: 
</code></pre>

<p>But the same thing is working fine with google news using following code:</p>

<pre><code>model = gensim.models.Word2Vec()
model = models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>I am unable to understand why. Is it that freebase requires much more memory than google news? I feel that shouldn't be the case. Am i missing something here?</p>
",2015-06-30 18:31:58,2019-08-13 14:05:46,Memory error in Word2vec while loading freebase-skipgram model,<python-2.7><freebase><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5887,31062273,2015-06-25 23:06:57,,"<p>I'm trying to train a word2vec model using a file with about 170K lines, with one sentence per line.</p>

<p>I think I may represent a special use case because the ""sentences"" have arbitrary strings rather than dictionary words. Each sentence (line) has about 100 words and each ""word"" has about 20 characters, with characters like <code>""/""</code> and also numbers.</p>

<p>The training code is very simple:</p>

<pre><code># as shown in http://rare-technologies.com/word2vec-tutorial/
import gensim, logging, os

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

current_dir = os.path.dirname(os.path.realpath(__file__))

# each line represents a full chess match
input_dir = current_dir+""/../fen_output""
output_file = current_dir+""/../learned_vectors/output.model.bin""

sentences = MySentences(input_dir)

model = gensim.models.Word2Vec(sentences,workers=8)
</code></pre>

<p>Thing is, things work real quick up to 100K sentences (my RAM steadily going up) but then I run out of RAM and I can see my PC has started swapping, and training grinds to a halt. I don't have a lot of RAM available, only about 4GB and <code>word2vec</code> uses up all of it before starting to swap.</p>

<p>I think I have OpenBLAS correctly linked to numpy: this is what <code>numpy.show_config()</code> tells me:</p>

<pre><code>blas_info:
  libraries = ['blas']
  library_dirs = ['/usr/lib']
  language = f77
lapack_info:
  libraries = ['lapack']
  library_dirs = ['/usr/lib']
  language = f77
atlas_threads_info:
  NOT AVAILABLE
blas_opt_info:
  libraries = ['openblas']
  library_dirs = ['/usr/lib']
  language = f77
openblas_info:
  libraries = ['openblas']
  library_dirs = ['/usr/lib']
  language = f77
lapack_opt_info:
  libraries = ['lapack', 'blas']
  library_dirs = ['/usr/lib']
  language = f77
  define_macros = [('NO_ATLAS_INFO', 1)]
openblas_lapack_info:
  NOT AVAILABLE
lapack_mkl_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_info:
  NOT AVAILABLE
atlas_3_10_info:
  NOT AVAILABLE
blas_mkl_info:
  NOT AVAILABLE
mkl_info:
  NOT AVAILABLE
</code></pre>

<p>My question is: <strong>is this expected on a machine that hasn't got a lot of available RAM (like mine) and I should get more RAM or train the model in smaller pieces?</strong> or <strong>does it look like my setup isn't configured properly (or my code is inefficient)?</strong></p>

<p>Thank you in advance.</p>
",2015-06-25 23:17:58,2015-09-30 21:00:52,Word2vec training using gensim starts swapping after 100K sentences,<python><numpy><blas><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5889,31166218,2015-07-01 15:56:07,,"<p>I am trying to use freebase along with gensim's word2vec to find similarity score between vectors of two word using following code.</p>

<pre><code>model = gensim.models.Word2Vec()
model = models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000-en.bin.gz', binary=True)
</code></pre>

<p>after creating a model based on freebase my code is giving me key error for any word.</p>

<pre><code>model.similarity('microsoft', 'apple')
</code></pre>

<p>This is giving me <code>KeyError: 'microsoft'</code></p>

<p>But when I use googlenews instead of freebase it works fine. Any idea why?</p>
",,2015-07-01 19:33:54,Unable to find words when using freebase with word2vec,<python-2.7><freebase><gensim><google-news>,,,CC BY-SA 3.0,False,False,True,False,False
5956,31286574,2015-07-08 07:48:57,,"<p>I'm studying word2vec, but when I use word2vec to train text data, occur OverFlowError with Numpy.</p>

<p>the message is,</p>

<pre><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)]
Warning (from warnings module):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 636
    warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""
UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Python34\lib\threading.py"", line 920, in _bootstrap_inner
    self.run()
  File ""C:\Python34\lib\threading.py"", line 868, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 675, in worker_loop
    if not worker_one_job(job, init):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 666, in worker_one_job
    job_words = self._do_train_job(items, alpha, inits)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 623, in _do_train_job
    tally += train_sentence_sg(self, sentence, alpha, work)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 112, in train_sentence_sg
    word_vocabs = [model.vocab[w] for w in sentence if w in model.vocab and
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 113, in &lt;listcomp&gt;
    model.vocab[w].sample_int &gt; model.random.randint(2**32)]
  File ""mtrand.pyx"", line 935, in mtrand.RandomState.randint (numpy\random\mtrand\mtrand.c:9520)
OverflowError: Python int too large to convert to C long
</code></pre>

<p>Can you tell me the cases?</p>

<p>My machine is x64 and OS is windows 7, but python34 is 32bit. numpy and scipy are also 32bit.</p>
",2015-07-10 08:13:35,2015-07-28 20:00:23,Python34 word2vec.Word2Vec OverFlowError,<python-3.x><windows-7-x64><integer-overflow><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
5989,31350481,2015-07-10 21:21:53,,"<p>I'd like to use gensim's Python wrapper for Dynamic Topic Models. Essentially, it is a topic modeling approach that slices the corpus by date (i.e. years) and looks at how topics evolve over time. However, I am finding nothing online that specifies how <a href=""https://radimrehurek.com/gensim/models/dtmmodel.html#gensim.models.dtmmodel.DtmModel.ftimeslices"" rel=""nofollow"">my_timeslices</a> should be formatted. Does anyone have an example of a file and/or preparation?</p>
",,2020-07-27 06:50:07,Gensim - Timeslice Data Format?,<python><python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6015,31321209,2015-07-09 14:57:45,,"<p>How to get document vectors of two text documents using Doc2vec?
I am new to this, so it would be helpful if someone could point me in the right direction / help me with some tutorial</p>

<p>I am using gensim.</p>

<pre><code>doc1=[""This is a sentence"",""This is another sentence""]
documents1=[doc.strip().split("" "") for doc in doc1 ]
model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>I get </p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'words'</p>
</blockquote>

<p>whenever I run this.</p>
",2018-12-15 19:33:57,2019-06-04 10:17:50,Doc2vec: How to get document vectors,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
6024,31338082,2015-07-10 10:05:38,,"<p>I want to process the wikipedia using <code>gensim.corpora.wikicorpus</code>. My final objective is to train a <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">Word2Vec</a> Model from it.</p>

<p>I have it working but I have a problem with the accented vowels of Spanish: , , , , .</p>

<p>I want to normalize them to a, e, i, o, u.</p>

<p>I have seem that there is a <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.deaccent"" rel=""nofollow"">deaccent</a> function in gensim but I dwould like to apply it directly while I am building the corpus. Can this be done?</p>

<p>Here is a working example:</p>

<pre><code>from gensim.corpora import WikiCorpus
from gensim.models.word2vec import  Word2Vec
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
                level=logging.INFO)

# I would like to include here the normalization
corpus = WikiCorpus('/Users/jesusfbes/Desktop/eswiki-latest-pages-articles.xml.bz2', dictionary=False)


max_sentence = -1


def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence == -1:
            yield text
        else:
            break

model = Word2Vec(size=400, window=5, min_count=5)
model.build_vocab(generate_lines())
model.train(generate_lines(), chunksize=500)

model.save('mymodel')
</code></pre>
",,2017-05-10 08:50:04,Spanish Wikipedia processing using Gensim,<python><wikipedia><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6031,31384947,2015-07-13 13:38:52,,"<p>I am trying to install <code>gensim</code> lib on Ubuntu using:</p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>However, I got an error like this:</p>

<pre><code>Requirement already up-to-date: gensim in /usr/local/lib/python3.4/dist-packages/gensim-0.12.0-py3.4-linux-x86_64.egg
Collecting numpy&gt;=1.3 (from gensim)
Downloading numpy-1.9.2.tar.gz (4.0MB)
100% |################################| 4.0MB 146kB/s 
Collecting scipy&gt;=0.7.0 (from gensim)
Downloading scipy-0.15.1.tar.gz (11.4MB)
100% |################################| 11.4MB 55kB/s
Requirement already up-to-date: six&gt;=1.2.0 in /usr/lib/python3/dist-packages (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
Downloading smart_open-1.2.1.tar.gz
Complete output from command python setup.py egg_info:
File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""/usr/lib/python3.4/encodings/ascii.py"", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4344: ordinal not in range(128)
----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-_nbem_oq/smart-open
</code></pre>

<p>Does anyone know how to fix it? </p>
",,2015-07-14 08:08:47,Getting UnicodeDecodeError when installing gensim on Ubuntu,<ubuntu><pip><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6061,31512853,2015-07-20 09:21:39,,"<p>I am trying to emulate streaming for some documents and update the LSI on additional documents streamed-in. I find this error:</p>

<pre><code>Traceback (most recent call last):
  File ""gensimStreamGen_tutorial5.py"", line 57, in &lt;module&gt;
    for vector in corpus_memory_friendly: # load one vector into memory at a time
  File ""gensimStreamGen_tutorial5.py"", line 44, in __iter__
    lsi = models.LsiModel(corpus, num_topics=10) # initialize an LSI transformation
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 331, in __init__
    self.add_documents(corpus)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 388, in add_documents
    update = Projection(self.num_terms, self.num_topics, job, extra_dims=self.extra_samples, power_iters=self.power_iters)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 126, in __init__
    extra_dims=self.extra_dims)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 677, in stochastic_svd
    q, _ = matutils.qr_destroy(y) # orthonormalize the range
  File ""/Users/Desktop/gensim-0.12.0/gensim/matutils.py"", line 398, in qr_destroy
    qr, tau, work, info = geqrf(a, lwork=-1, overwrite_a=True)
ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)
</code></pre>

<p>The code for streaming documents and updating LSI model:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
        for document in documents:
            # Stream-in documents and build TF-IDF model to construct new_vec
            yield new_vec
            corpus.append(new_vec)
            tfidf = models.TfidfModel(corpus)
            corpus_tfidf = tfidf[corpus]
            lsi = models.LsiModel(corpus_tfidf,  num_topics=2)
            corpus_lsi = lsi[corpus_tfidf]
            lsi.print_topics(2)
            for doc in corpus_lsi:
                print(doc)

corpus_memory_friendly = MyCorpus()
for vector in corpus_memory_friendly:
    print(vector)
</code></pre>

<p>The corpus gets a new new_vec every iteration. The new_vec on each yield for different iterations:</p>

<pre><code>[]
[(0, 1)]
[(1, 1), (2, 1), (3, 1)]
[(3, 2), (4, 1), (5, 1)]
[(2, 1), (6, 1), (7, 1)]
[]
[(8, 1)]
[(8, 1), (9, 1)]
[(9, 1), (10, 1), (11, 1)]
</code></pre>

<p>The error appears on the first iteration (first line in expected new_vec). The rest is the expected output from new_vec.</p>
",2015-07-20 19:58:18,2016-02-01 06:49:44,"Gensim: ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)",<python><gensim><latent-semantic-indexing>,,,CC BY-SA 3.0,False,False,True,False,False
6068,31425123,2015-07-15 08:22:03,,"<p>As far as I know, doc2vec computes both embeddings for documents and words. Can I use  a word vector and a document vector  to estimate the similarity of a word to a document or only documents against documents and words against words? Any remark would be helpful.</p>
",,2016-03-25 20:38:27,Comparing words with documents,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6090,31507399,2015-07-20 00:30:45,,"<p>I try to use word2vec, but it gives an error when trying to do anything with any word. It seems to be an encoding issue, here is what I did: </p>

<h3>Init word2vec:</h3>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

model = gensim.models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000/knowledge-vectors-skipgram1000.bin', binary=True)
model.init_sims(replace=True)
</code></pre>

<h3>Test it a bit:</h3>

<pre><code>print(model)  
# prints: Word2Vec(vocab=1422903, size=1000, alpha=0.025)

print(model.index2word[0])  
# prints: u'/m/0dgps15'
# I would expect a readable word, how to fix that?
</code></pre>

<h3>The error:</h3>

<pre><code>print(model.similarity('word', 'sound'))
# An error happen: KeyError: 'word'
</code></pre>

<p>I also tried to load the model with <code>binary=False</code>, but this makes an error while loading. </p>
",2015-07-20 00:46:56,2016-06-20 09:52:11,"Setting up word2vec - KeyError: ""word 'word' not in vocabulary""",<python><character-encoding><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6108,31543542,2015-07-21 15:34:47,,"<p>I am using the Gensim HDP module on a set of documents. </p>

<pre><code>&gt;&gt;&gt; hdp = models.HdpModel(corpusB, id2word=dictionaryB)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; hdp = models.HdpModel(corpusA, id2word=dictionaryA)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; len(corpusA)
1113
&gt;&gt;&gt; len(corpusB)
17
</code></pre>

<p>Why is the number of topics independent of corpus length?</p>
",,2020-10-17 18:56:09,Hierarchical Dirichlet Process Gensim topic number independent of corpus size,<python><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6115,31524433,2015-07-20 19:08:05,,"<p>I am not able to reproduce the word2vec results using Gensim, and some of the results do not make sense. Gensim is an open-source toolkit, is intended for handling large text collections using efficient online algorithms, including the <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow noreferrer"">python implementation of Google's word2vec algorithm</a>.</p>
<p>I am following an <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""nofollow noreferrer"">online tutorial</a> and am not able reproduece the results.  The most similar words for (positive=['woman', 'king'], negative=['man']) were supposed to to 'wenceslaus'and 'queen'.  In stead, I got 'u'eleonore' and 'iv'.  The most similar for 'fast' was slow and for 'quick' was 'mitsumi'.</p>
<p>Any insights?  Below are my codes and results:</p>
<blockquote>
<p>&gt;&gt;&gt; from gensim.models import word2vec</p>
<p>&gt;&gt;&gt; import logging</p>
<p>&gt;&gt;&gt; logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</p>
<p>&gt;&gt;&gt; sentences = word2vec.Text8Corpus('\tmp\text8')</p>
<p>&gt;&gt;&gt; model = word2vec.Word2Vec(sentences, size=200)</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)</p>
<p>out[63]: [(u'eleonore', 0.5138808...), (u'iv',0.510519325...)]</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['fast'])</p>
<p>Out[64]: [(u'slow', 0.48932...), (u'paced', 0.46925...)...]</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['quick'],topn=1)</p>
<p>out[65]: [(u'mitsumi', 0.48545..)]</p>
</blockquote>
",2020-06-20 09:12:55,2015-10-03 06:28:41,Why I cannot reproduce word2vec results using gensim,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6116,31524898,2015-07-20 19:35:07,,"<p>I wanted to know the difference between gensim word2vec's two similarity measures  : most_similar() and most_similar_cosmul(). I know that the first one works using cosine similarity of word vectors while other one uses using the multiplicative combination objective proposed by Omer Levy and Yoav Goldberg. I want to know how it affects the results? Which one gives semantic similarity ? etc.
Eg :</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
model.most_similar(positive=['woman', 'king'], negative=['man'])               
</code></pre>

<p>Result : [('queen', 0.50882536), ...]</p>

<pre><code>model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])
</code></pre>

<p>Result : [(u'iraq', 0.8488819003105164), ...]</p>
",,2015-07-30 10:11:30,Gensim Word2vec : Semantic Similarity,<python><semantics><similarity><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6159,31740413,2015-07-31 07:25:08,,"<p>I am getting the following error when i execute my code</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 21, in &lt;module&gt;
    print model.most_similar(positive=[''])
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 660, in most_similar
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word '\xe7\x94\xb7\xe4\xba\xba' not in vocabulary""
</code></pre>

<p>My code here</p>

<pre><code> # -*- coding: utf8 -*    
    from gensim.models import word2vec
    import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
        sentences = word2vec.Text8Corpus('/tmp/text8')
        model = word2vec.
    Word2Vec(sentences, size=200)
        model.most_similar([''])
</code></pre>
",,2015-08-03 07:02:12,word2vec getting encode error,<python><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6163,31742630,2015-07-31 09:24:14,,"<p>I post my question here because there are already some answers on how to use scikit methods with gensim like <a href=""https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda"">scikit vectorizers with gensim</a> or <a href=""https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix?rq=1"">this</a> but I haven't seen the whole pipeline to be used for text classification. I will try to explain a little bit my situation</p>

<p>I want to use gensim LDA implemented methods in order to proceed further to text classification. I have one dataset which is consisted from three parts(train(25K), test(25K) and unlabeled data(50K)). What I am trying to do is to learn the latent topics space using the unlabeled data and then transform the train and test set into this learned latent topic space. I am currently using the Scikit Learn implemented methods in order to extract the BoW representation. Later, I am transforming to the required inputs for the LDA implementation and at the end I am transforming the train and test set into the extracted latent topic space. Finally, I am going back to csr matrices in order to fit a classifier and to obtain the accuracy. <strong>Although, it seems to me that everything is fine, the performance of the classifier is almost 0%. I am attaching part of the code in order to get some additional help or if there is something obvious that I am currently missing.</strong></p>

<pre><code>#bow representations for the three sets unlabelled, train and test
vectorizer = CountVectorizer(max_features=3000,stop_words='english')


corpus_tfidf_unsuper = vectorizer.fit_transform(train_data_unsupervised[:,2])
corpus_tfidf_train = vectorizer.transform(train_ds[:,2])
corpus_tfidf_test= vectorizer.transform(test_ds[:,2])

#transform to gensim acceptable objects
vocab = vectorizer.get_feature_names()
id2word_unsuper=dict([(i, s) for i, s in enumerate(vocab)])
corpus_vect_gensim_unsuper = matutils.Sparse2Corpus(corpus_tfidf_unsuper.T)
corpus_vect_gensim_train = matutils.Sparse2Corpus(corpus_tfidf_train.T)
corpus_vect_gensim_test = matutils.Sparse2Corpus(corpus_tfidf_test.T)

#fit the model to the unlabelled data
lda = models.LdaModel(corpus_vect_gensim_unsuper, 
                  id2word = id2word_unsuper, 
                  num_topics = 10,
                  passes=1)
#transform the train and test set to the latent topic space
docTopicProbMat_train = lda[corpus_vect_gensim_train]
docTopicProbMat_test = lda[corpus_vect_gensim_test]
#transform to csr matrices
train_lda=matutils.corpus2csc(docTopicProbMat_train)
test_lda=matutils.corpus2csc(docTopicProbMat_test)
#fit the classifier and print the accuracy
clf =LogisticRegression()    
clf.fit(train_lda.transpose(), np.array(train_ds[:,0]).astype(int))     
ypred = clf.predict(test_lda.transpose())
print accuracy_score(test_ds[:,0].astype(int), ypred)
</code></pre>

<p>This is my first post, so if there are potential remarks, please do not hesitate to inform me.</p>
",2017-05-23 11:45:52,2017-01-27 12:29:19,Gensim LDA for text classification,<python><scikit-learn><lda><topic-modeling><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
6179,31685048,2015-07-28 19:05:06,,"<p>I am new to Web2Py and Python stack. I need to use a module in my Web2Py application which uses ""gensim"" and ""nltk"" libraries. I tried installing these into my Python 2.7 on a Windows 7 environment but came across several errors due to some issues with ""numpy"" and ""scipy"" installations on Windows 7. Then I ended up resolving those errors by uninstalling Python 2.7 and instead installing Anaconda Python which successfully installed the required ""gensim"" and ""nltk"" libraries. </p>

<p>So, at this stage I am able to see all these ""gensim"" and ""nltk"" libraries resolving properly without any error in ""Spyder"" and ""PyCharm"". However, when I run my application in Web2Py, it still complains about ""gensim"" and gives this error: <code>&lt;type 'exceptions.ImportError'&gt; No module named gensim</code></p>

<p>My guess is if I can configure Web2Py to use the Anaconda Python then this issue would be resolved. </p>

<p>I need to know if it's possible to configure Web2Py to use Anaconda Python and if it is then how do I do that? </p>

<p>Otherwise, if someone knows of some other way resolve that ""gensim"" error in Web2Py kindly share your thoughts.</p>

<p>All your help would be highly appreciated.</p>
",,2015-07-28 21:52:08,Configure Web2Py to use Anaconda Python,<python-2.7><web2py><anaconda><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
6181,31687263,2015-07-28 21:17:27,,"<p>I've got BOW vectors and I'm wondering if there's a supervised dimensionality reduction algorithm in sklearn or gensim capable of taking high-dimensional, supervised data and projecting it into a lower dimensional space which preserves the variance between these classes.</p>

<p>Actually I'm trying to find a proper metric for the classification/regression, and I believe using dimensionality can help me. I know there's unsupervised methods, but I want to keep the label information along the way.</p>
",,2016-08-31 08:43:46,supervised dimensionality redunction/topic model using sklearn or gensim,<python><machine-learning><gensim><dimensionality-reduction>,,,CC BY-SA 3.0,False,False,True,False,True
6203,31728460,2015-07-30 15:48:51,,"<p>I have a <code>Doc2Vec's</code> model and I want to create <code>Word2vec's</code> model with different dimension. How can I use Doc2Vec's model <code>vocab</code> for fast training? Or is it <code>feasible</code> to train like this? Does <code>vocab building</code> has any effect on <code>train</code>?</p>
",2015-07-30 16:00:45,2015-07-31 05:08:51,How to use Word2Vec's vocab of one model into another?,<python><deep-learning><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6211,31800674,2015-08-04 04:23:15,,"<p>It is my first time to use gensim package to run LDA model, and the problem happened as followed:</p>

<p>I had train and save lda model to the local file 'lda.model' last night,   and when I try to usinfer topic distributions on new, unseen documents, with run() funciton:</p>

<pre><code>self.lda = models.LdaMulticore.load('./lda_model/lda.model')
print self.lda[self.corpus_tfidf]
</code></pre>

<p>then Error happend!</p>

<pre><code>AttributeError: 'LdaMulticore' object has no attribute 'minimum_probability'
</code></pre>

<p>The following is my source code,Could you help me?</p>

<pre><code># -*- coding: utf-8 -*-
#@author: chenbjin
#@time:  2015-08-3
import jieba, os, logging
from base_func import *
from gensim import corpora, models, similarities

class LDAModel(object):
    """"""docstring for LDAModel""""""
    def __init__(self, train_data='./data/train_set.txt'):
        super(LDAModel, self).__init__()
        self.dic = None
        self.corpus = None
        self.tfidf_model = None
        self.corpus_tfidf = None
        self.lda_model = None
        self.train_set = load_train_set(train_data)
        self.test_set = None

    def train(self):
        self.dic = corpora.Dictionary(self.train_set)
        self.corpus = [ self.dic.doc2bow(text) for text in self.train_set ]
        self.tfidf_model = models.TfidfModel.load('./tfidf_model/tfidf.model')
        #self.tfidf_model = models.TfidfModel(self.corpus)
        #self.tfidf_model.save('./tfidf_model/tfidf.model')
        self.corpus_tfidf = self.tfidf_model[self.corpus]

        self.lda_model = models.LdaMulticore(self.corpus_tfidf, id2word = self.dic, num_topics = 50)
        self.lda_model.save('./lda_model/lda.model')

    def run(self, test_set='./data/test_set.txt'):
        self.test_set = load_train_set(test_set)
        self.dic = corpora.Dictionary(self.test_set)
        self.corpus = [ self.dic.doc2bow(text) for text in self.test_set ]

        self.tfidf_model = models.TfidfModel.load('./tfidf_model/tfidf.model')
        self.corpus_tfidf = self.tfidf_model[self.corpus]

        self.lda = models.LdaMulticore.load('./lda_model/lda.model')
        print self.lda[self.corpus_tfidf]


def main():
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    lda = LDAModel(train_data=None)
    lda.run()

if __name__ == '__main__':
    main()
</code></pre>
",,2015-08-04 04:23:15,'LdaMulticore' object has no attribute 'minimum_probability'?,<lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6223,31821821,2015-08-05 01:06:27,,"<p><strong>Background</strong></p>

<p>I am trying to judge whether a phrase is semantically related to other words found in a corpus using Gensim.  For example, here is the corpus document pre-tokenized:</p>

<pre><code> **Corpus**
 Car Insurance
 Car Insurance Coverage
 Auto Insurance
 Best Insurance
 How much is car insurance
 Best auto coverage
 Auto policy
 Car Policy Insurance
</code></pre>

<p>My code (based on <a href=""https://radimrehurek.com/gensim/tut3.html#similarity-interface"" rel=""nofollow"">this gensim tutorial</a>) judges the semantic relatendness of a phrase using cosine similarity against all strings in corpus.  </p>

<p><strong>Problem</strong></p>

<p>It seems that if a query contains ANY of the terms found within my dictionary, that phrase is judged as being semantically similar to the corpus (e.g.  **Giraffe Poop Car Murderer has a cosine similarity of 1 but SHOULD be semantically unrelated).  I am not sure how to solve for this issue.</p>

<p><strong>Code</strong></p>

<pre><code>#Tokenize Corpus and filter out anything that is a stop word or has a frequency &lt;1
texts = [[word for word in document if word not in stoplist]
        for document in documents]
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
texts = [[token for token in text if frequency[token] &gt; 1]
        for text in texts]
dictionary = corpora.Dictionary(texts)

# doc2bow counts the number of occurences of each distinct word, converts the word
# to its integer word id and returns the result as a sparse vector

corpus = [dictionary.doc2bow(text) for text in texts]  
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
doc = ""giraffe poop car murderer""
vec_bow = dictionary.doc2bow(doc.lower().split())

#convert the query to LSI space
vec_lsi = lsi[vec_bow]              
index = similarities.MatrixSimilarity(lsi[corpus])

# perform a similarity query against the corpus
sims = index[vec_lsi]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
</code></pre>
",2015-08-06 23:40:39,2015-08-14 14:18:21,Semantic Similarity between Phrases Using GenSim,<python-3.x><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
6254,31827623,2015-08-05 08:47:26,,"<p>In Gensim's Doc2Vec, how do you combine sentence vectors to make a single vector for a paragraph?  I realise you can train on the entire paragraph, but it would obviously be better to train on individual sentences, for context, etc. (I think...?)</p>

<p>Any advice or normal use case?</p>

<p>Also, how would I retrieve sentence/paragraph vectors from the model?</p>
",,2015-08-11 08:09:34,Combining Doc2Vec sentences into paragraph vectors,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6265,31814825,2015-08-04 16:42:17,,"<p>I am currently working on word2vec model using gensim in Python, and want to write a function that can help me find the antonyms and synonyms of a given word.
For example:
antonym(""sad"")=""happy""
synonym(""upset"")=""enraged""</p>

<p>Is there a way to do that in word2vec?</p>
",,2019-08-28 14:29:27,How to obtain antonyms through word2vec?,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6283,31870995,2015-08-07 06:23:30,,"<p>Sorry that I don't have enough reputation to post images.</p>

<p>The main problem is that it tells me that I need to install a C compiler and reinstall gensim or the train will be slow, and in fact it is really slow.</p>

<p>I have installed mingw32, Visual Studio 2008, and have added the mingw32 environment variable to my path.</p>

<p>Any ideas on how to solve it?</p>
",2015-08-07 07:27:13,2020-04-22 06:46:57,Gensim needs a C compiler?,<python><compilation><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6318,31996843,2015-08-13 19:28:39,,"<p>I have Windows 7 and WinPython 3.4.3.2; trying to install Gensim from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim</a></p>

<p>I'm getting this error:</p>

<pre><code>C:\Program Files (x86)\PowerCmd&gt;pip install e:\Python\gensim-0.12.1-cp34-none-win_amd64.whl
You are using pip version 6.0.8, however version 7.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Processing e:\python\gensim-0.12.1-cp34-none-win_amd64.whl
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.2.0 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Collecting smart-open&gt;=1.2.1 (from gensim==0.12.1)
  Using cached smart_open-1.2.1.tar.gz
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 20, in &lt;module&gt;
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 20, in &lt;module&gt;
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
        long_description = read('README.rst'),
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
        return open(os.path.join(os.path.dirname(__file__), fname)).read()
      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
        return codecs.charmap_decode(input,self.errors,decoding_table)[0]
    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

      File ""&lt;string&gt;"", line 20, in &lt;module&gt;

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;

        long_description = read('README.rst'),

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read

        return open(os.path.join(os.path.dirname(__file__), fname)).read()

      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode

        return codecs.charmap_decode(input,self.errors,decoding_table)[0]

    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;

    ----------------------------------------
    Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open
</code></pre>
",2015-08-13 21:29:00,2015-08-15 18:12:00,How to install Gensim on Windows 7,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6332,31975754,2015-08-12 21:30:08,,"<p>I'm using the LDA algorithm from the gensim package to find topics in a given text.</p>

<p>I've been asked that the resulting topics will include different words for each topic, E.G If topic A has the word 'monkey' in it then no other topic should include the word 'monkey' in its list.</p>

<p><strong>My thoughts so far:</strong> run it multiple times and each time add the previous words to the stop words list.</p>

<p>Since:
A) I'm not even sure of algorithmically/logically it's the right thing to do.
B) I hope there's a built in way to do it that i'm not aware of.
C) This is a large database, and it takes about 20 minutes to run the LDA
each time (using the multi-core version).</p>

<p><strong>Question:</strong> Is there a better way to do it?</p>

<p>Hope to get some help,</p>

<p>Thanks.</p>
",,2016-10-07 13:41:34,gensim LDA: How can i generate topics with different words for each topic?,<python><algorithm><api><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6343,32056080,2015-08-17 17:11:15,,"<p>I'm trying to use Doc2Vec to read in a file that is a list of sentences like this:</p>

<pre><code>The elephant flaps its large ears to cool the blood in them and its body.

A house is a permanent building or structure for people or families to live in.

...
</code></pre>

<p>What I want to do is generate two files, one with unique words from these sentences and another that has one corresponding vector per line (if there's no vector output I want to output a vector of 0's)</p>

<p>I'm getting the vocab fine with my code but I can't seem to figure out how to print out the individual sentence vectors. I have looked through the documentation and haven't found much help.  Here is what my code looks like so far. </p>

<pre><code>sentences = []
for uid, line in enumerate(open(filename)):
    sentences.append(LabeledSentence(words=line.split(), labels=['SENT_%s' %       uid]))

model = Doc2Vec(alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
for epoch in range(10):
    model.train(sentences)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
sent_reg = r'[SENT].*'
for item in model.vocab.keys():
    sent = re.search(sent_reg, item)
    if sent:
        continue
    else:
        print item

###I'm not sure how to produce the vectors from here and this doesn't work##   
sent_id = 0
for item in model:
    print model[""SENT_""+str(sent_id)]
    sent_id += 1
</code></pre>
",2015-08-31 23:06:17,2015-08-31 23:06:17,Using gensim's Doc2Vec to produce sentence vectors,<python><vector><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6365,32101795,2015-08-19 17:17:41,,"<p>I'm getting an <code>AttributeError</code> while loading the gensim model available at word2vec repository:</p>

<pre><code>from gensim import models
w = models.Word2Vec()
w.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-8219e36ba1f6&gt; in &lt;module&gt;()
----&gt; 1 w[""queen""]

C:\Anaconda64\lib\site-packages\gensim\models\word2vec.pyc in __getitem__(self, word)
    761 
    762         """"""
--&gt; 763         return self.syn0[self.vocab[word].index]
    764 
    765 

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Is this a known issue ?</p>
",,2019-06-18 10:09:56,Error while loading Word2Vec model in gensim,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6406,32201795,2015-08-25 10:38:47,,"<p>how to remove these numbers from output of LDA while using Gensim package?</p>

<p>2015-08-25 15:26:20,439 : INFO : topic #8 (0.100): 0.038*watch + 0.020*water + 0.014*strap + 0.011*analog + 0.011*resistance + 0.010*atm + 0.010*coloured + 0.010*timepiece + 0.010*5 + 0.009*classy</p>

<p>so that output will be  watch,water,strap...etc</p>
",,2015-08-26 17:11:39,how to remove numbers and symbols from output of LDA while using Gensim package?,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6412,32170261,2015-08-23 18:52:04,,"<p>I'm new to the world of word2vec and I just start to use gensim's implementation for word2vec.</p>

<p>I use two naive sentences as my first document set, </p>

<pre><code>[['first', 'sentence'], ['second', 'sentence']]
</code></pre>

<p>The vectors I get are like this:</p>

<pre><code>'first', -0.07386458, -0.17405555
'second', 0.0761444 , -0.21217766
'sentence', 0.0545655 , -0.07535963
</code></pre>

<p>However, when I type in another toy document sets:</p>

<pre><code>[['a', 'c'], ['b', 'c']]
</code></pre>

<p>I get the following result:</p>

<pre><code>'a', 0.02936198, -0.05837455
'b', -0.05362414, -0.06813956
'c', 0.11918657, -0.10411404
</code></pre>

<p>Again, I'm new to word2vec but according to my understanding,
my two document sets are structurally identical, so the results of the corresponding word should be the same.
But why I'm getting different results?
Is the algorithm always giving probalistic output or the document sets too small?</p>

<p>The function I used is as the following:</p>

<pre><code>model = word2vec.Word2Vec(sentences, size=2, min_count=1, window=2)
</code></pre>
",2015-08-23 18:57:54,2015-08-24 12:18:38,word2vec's probalistic output,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6424,32150468,2015-08-21 23:01:24,,"<p>I'm learning a topic model from a set of documents and that's working well. But I'm wondering if any existing system will actually generate <strong>new</strong> documents from the topics and words in the model. </p>

<p>Ie. say I want a new document of topic 0, will any of Gensim/MALLET/other tools actually produce a new document given some input of my topic choice (or choices)? Or is this a roll-your-own kind of problem? </p>

<p>Say I have two topics:</p>

<pre><code>topic #0: 0.009*river + 0.008*lake + 0.006*island + 0.005*mountain + 0.004*area + 0.004*park + 0.004*antarctic + 0.004*south + 0.004*mountains + 0.004*dam
topic #1: 0.026*relay + 0.026*athletics + 0.025*metres + 0.023*freestyle + 0.022*hurdles + 0.020*ret + 0.017*diviso + 0.017*athletes + 0.016*bundesliga + 0.014*medals
</code></pre>

<p>Is there any tool that will take ""topic 0: .5, topic 1: .5, length: 7""
and nicely produce a document like:</p>

<pre><code>island freestyle river south medals mountains area
</code></pre>

<p>or something along those lines? I don't want to duplicate this if it already exists. </p>
",,2015-10-13 13:08:58,Generating documents from LDA topic model,<modeling><lda><documents><mallet><generative>,,,CC BY-SA 3.0,False,False,True,False,False
6433,32321375,2015-08-31 23:25:19,,"<p>I am getting segmentation fault when I multiply a scipy sparse matrix by its transpose. I've searched all over the Internet but could not find any answer. Any help is appreciated.</p>

<pre><code>&gt;&gt;&gt; import cPickle
&gt;&gt;&gt; fs = open('vec.pickle', 'rb')
&gt;&gt;&gt; vec = cPickle.load(fs)
&gt;&gt;&gt; vec
&lt;3020x512 sparse matrix of type '&lt;type 'numpy.float64'&gt;' with 26008 stored elements in Compressed Sparse Column format&gt;
&gt;&gt;&gt; vec.max()
10.0
&gt;&gt;&gt; vec.min()
0.0
&gt;&gt;&gt; vec * vec.T
Segmentation fault: 11
</code></pre>

<p>I do not think this is memory issue since the dimension is small. The vec object is created by gensim, if that information helps.</p>

<p>I also do not think this is overflow issues since the range of element is [0.0, 10.0]</p>

<p>The pickle object is here:
<a href=""https://drive.google.com/open?id=0B3DJbsn85XMvdmFYT0MzZVFjOVU"" rel=""nofollow"">https://drive.google.com/open?id=0B3DJbsn85XMvdmFYT0MzZVFjOVU</a></p>
",2015-08-31 23:37:02,2015-09-01 05:55:07,sparse matrix python segmentation fault,<segmentation-fault><scipy><sparse-matrix><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6475,32276734,2015-08-28 17:35:07,,"<p>I am trying to run word2vec (skip-gram model implemented in gensim with a default window size of 5) on a corpus of .txt files. The iterator that I use looks something like this: </p>

<pre><code>class Corpus(object):
    """"""Iterator for feeding sentences to word2vec""""""
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):

        word_tokenizer = TreebankWordTokenizer()
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        text = ''

        for root, dirs, files in os.walk(self.dirname):

            for file in files:

                if file.endswith("".txt""):

                    file_path = os.path.join(root, file)


                    with open(file_path, 'r') as f:

                         text = f.read().decode('utf-8')
                         sentences = sent_tokenizer.tokenize(text)

                         for sent in sentences:
                             yield word_tokenizer.tokenize(sent)
</code></pre>

<p>Here I use the punkt tokenizer (which uses an unsupervised algorithm for detecting sentence boundaries) in the nltk package for splitting the text into sentences. However, when I replace this with just a simple <code>line.split()</code> i.e just considering each sentence as one line and splitting the words, I get a time efficiency that is 1.5 times faster than using the nltk parser. The code inside the 'with open' looks something like this: </p>

<pre><code>                 with open(file_path, 'r') as f:
                    for line in f:
                    line.decode('utf-8')
                    yield line.split()
</code></pre>

<p>My question is how important is it for the word2vec algorithm to be fed sentences that are actual sentences (something that I attempt to do with punkt tokenizer)? Is it sufficient for each word in the algorithm to receive a context of the surrounding words that lie on one line  (these words may not necessarily be an actual sentence in the case of a sentence spanning several lines) as opposed to the context of words that the word may have in a sentence spanning several lines. Also, what sort of a part does window size play in this. When a window size is set to 5 for example, does the size of sentences yielded by the Sentences iterator ceases to play a part? Will only window size decide the context words then? In that case should I just use <code>line.split()</code> instead of trying to detect actual sentence boundaries using the punkt tokenizer? </p>

<p>I hope I have been able to describe the issue sufficiently, I would really appreciate any opinions or pointers or help regarding this.</p>
",,2017-01-31 10:01:11,relationship between window size and the actual sentence length in word2vec,<gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
6481,32313062,2015-08-31 13:58:08,,"<p>I am trying to obtain the optimal number of topics for an LDA-model within Gensim. One method I found is to calculate the log likelihood for each model and compare each against each other, e.g. at <a href=""https://stats.stackexchange.com/questions/25113/the-input-parameters-for-using-latent-dirichlet-allocation"">The input parameters for using latent Dirichlet allocation</a></p>

<p>Hence I looked into calculating the log likelihood of a LDA-model with Gensim and came across following post: <a href=""https://stats.stackexchange.com/questions/126268/how-do-you-estimate-alpha-parameter-of-a-latent-dirichlet-allocation-model"">How do you estimate  parameter of a latent dirichlet allocation model?</a></p>

<p>which basically states that the update_alpha() method implements the method decribed in <em>Huang, Jonathan. Maximum likelihood estimation of Dirichlet distribution parameters</em>. Still I don't know how to obtain this parameter using the libary without changing the code.</p>

<p>How can I obtain log likelihood from an LDA model with Gensim?</p>

<p>Is there a better way to obtain optimal number of topics with Gensim?</p>
",,2020-10-15 11:48:55,What is the best way to obtain the optimal number of topics for a LDA-Model using Gensim?,<python><text-mining><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
6487,32251047,2015-08-27 13:42:51,,"<p>Have a hell of a blocker trying to use Gensim's doc2vec.</p>

<p>I import gensim.models.doc2vec.Doc2Vec and successfully train it on a set of tweets. I am able to pull my document vectors fine, using model['DOC_[0123..]''. </p>

<p>My issue now is that I'm trying to get a vector representation for a <em>new, unseen document</em> so that I can feed that vector back into a classifier. As far as I know, the only method that exists to do this with doc2vec is <code>infer_vector()</code>. </p>

<p>HOWEVER, when I try to call this method, I get the following: </p>

<p><em>AttributeError: 'Doc2Vec' object has no attribute 'infer_vector'</em></p>

<p>I'm able to use all the other methods described in the doc2vec documentation: <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>

<p>I've tried using different versions of gensim including 0.10.3 (the version released with doc2vec || <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">http://rare-technologies.com/doc2vec-tutorial/</a>) and the 0.13.1 (latest version). </p>

<p>PLEASE HELP.</p>
",2015-11-05 19:59:39,2015-11-05 19:59:39,Gensim doc2vec infer_vector method missing,<python><machine-learning><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6534,32476336,2015-09-09 09:51:02,,"<h1>LDA Original Output</h1>

<ul>
<li><p>Uni-grams </p>

<ul>
<li><p>topic1 -scuba,water,vapor,diving</p></li>
<li><p>topic2 -dioxide,plants,green,carbon</p></li>
</ul></li>
</ul>

<h1>Required Output</h1>

<ul>
<li><p>Bi-gram topics</p>

<ul>
<li><p>topic1 -scuba diving,water vapor</p></li>
<li><p>topic2 -green plants,carbon dioxide</p></li>
</ul></li>
</ul>

<p>Any idea?</p>
",2015-09-10 04:45:01,2018-11-13 02:02:23,How to abstract bigram topics instead of unigrams using Latent Dirichlet Allocation (LDA) in python- gensim?,<nlp><text-mining><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6581,32543235,2015-09-12 20:09:35,,"<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim import corpora, models, similarities
from nltk.corpus import stopwords
import codecs

documents = []
with codecs.open(""Master_File_for_Docs.txt"", encoding = 'utf-8', mode= ""r"") as fid:
   for line in fid:
       documents.append(line)
stoplist = []
x = stopwords.words('english')
for word in x:
    stoplist.append(word)

#Removes Stopwords
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]


dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.LdaModel(corpus, id2word=dictionary, num_topics=100)
lda.print_topics(20)
#corpus_lda = lda[corpus]
#for doc in corpus_lda:
 #   print(doc)
</code></pre>

<p>I am running Gensim for topic modeling and trying to get the above code working. I know that this code works because my friend ran it from a mac computer and it worked successfully but when I run it from a windows computer the code gives me a </p>

<pre><code>MemoryError
</code></pre>

<p>Also the logging that I set on the second line also doesn't appear on my windows computer. Is there something in Windows that I need to fix in order for gensim to work?</p>
",,2018-07-24 09:04:32,Python: Gensim Memory Error,<python><windows><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
6650,32744732,2015-09-23 16:23:21,,"<p>I am learning about Doc2Vec and the gensim library. I have been able to train my model by creating a corpus of documents such as</p>

<pre><code>LabeledSentence(['what', 'happens', 'when', 'an', 'army', 'of', 'wetbacks', 'towelheads', 'and', 'godless', 'eastern', 'european', 'commies', 'gather', 'their', 'forces', 'south', 'of', 'the', 'border', 'gary', 'busey', 'kicks', 'their', 'butts', 'of', 'course', 'another', 'laughable', 'example', 'of', 'reagan-era', 'cultural', 'fallout', 'bulletproof', 'wastes', 'a', 'decent', 'supporting', 'cast', 'headed', 'by', 'l', 'q', 'jones', 'and', 'thalmus', 'rasulala'], ['LABELED_10', '0'])`
</code></pre>

<p>note that this particular document has two tags, namely 'LABELED_10' and  '0'.</p>

<p>Now after i load my model and perform </p>

<pre><code>print(model.docvecs.most_similar(""LABELED_10""))
</code></pre>

<p>i get </p>

<pre><code>[('LABELED_107', 0.48432376980781555), ('LABELED_110', 0.4827481508255005), ('LABELED_214', 0.48039984703063965), ('LABELED_207', 0.479473352432251), ('LABELED_315', 0.47931796312332153), ('LABELED_307', 0.47898322343826294), ('LABELED_124', 0.4776897132396698), ('LABELED_222', 0.4768940210342407), ('LABELED_413', 0.47479286789894104), ('LABELED_735', 0.47462597489356995)]
</code></pre>

<p>which is perfect ! as i get all the tags most similar to LABELED_10. </p>

<p>Now i would like to have a feedback loop while training my model. So if i give my model a new document, i would like to know how good or bad the model's classification is before tagging and adding that document to my corpus. How would i do that using Doc2Vec? So how do i know whether the documents for LABELED_107 and LABELED_10 are actually similar or not. Here is one approach that i have in mind. Here is the code for my random forest classifier</p>

<pre><code>result = cfun.rfClassifer(n_estimators, trainingDataFV, train[""sentiment""],testDataFV)
</code></pre>

<p>and here is the function</p>

<pre><code>def rfClassifer(n_estimators, trainingSet, label, testSet):

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    forest = RandomForestClassifier(n_estimators)
    forest = forest.fit(trainingSet, label)
    result = forest.predict(testSet)

    return result
</code></pre>

<p>and finally i can do</p>

<pre><code>output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})

output.to_csv(""../../submits/Doc2Vec_AvgVecPredict.csv"", index=False, quoting=3)
</code></pre>

<p>Feedback process</p>

<ol>
<li><p>Keep a validation set which is tagged correctly.</p></li>
<li><p>Feed the tagged validation set to the classifier after removing the tags and save the result in a csv.</p></li>
<li><p>Compare the result with another csv that has the correct tags.</p></li>
<li><p>For every mismatch, add those documents to the labeled training set and train the model again.</p></li>
<li><p>Repeat for more validation sets.</p></li>
</ol>

<p>Is this approach correct? Also, can i incrementally train the doc2vec model? Lets say that initially i trained my doc2vec model with 100k tagged docs. Now after the validation step, i need my model to be trained on a further 10k documents. Will i have to train my model from the very beginning ? Meaning will i need to train my model on the initial 100k tagged docs again?</p>

<p>I would really appreciate your insights.</p>

<p>Thanks </p>
",2015-09-23 19:44:26,2016-03-28 16:15:32,Doc2Vec: Best practice for feedback loop for training the model,<nlp><python-3.4><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6689,32794087,2015-09-26 05:48:31,,"<p>Gensim has this document similarity feature which when inputted a query document, it outputs the similarity of that particular document with all the documents it has in its index</p>

<ol>
<li><p>Can this be used like an ""approximate"" version of supervised classification?</p></li>
<li><p>I know gensim's word2vec uses Deep Learning, is this involved during the above step?</p></li>
</ol>
",,2015-09-26 05:48:31,Gensim's Document similarity can be used as supervised classification?,<machine-learning><nlp><text-processing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6702,32826927,2015-09-28 15:46:29,,"<p>I have word2vec model and pyspark job in which I am summing up individual word vectors for each document. </p>

<pre><code>model = gensim.models.Word2Vec.load('w2v.mod')
model2 = sc.broadcast(model)

def getVector(article):
    vec = numpy.ndarray(100)
    for word in article:
        if word in model2.value:
            vec += model2.value[word]
    return vec /len(article)

data = sc.textFile('documents.txt').map(lambda doc:doc.split())
vectors=  data.map(lambda doc:(doc,getVector(doc)))
</code></pre>

<p>I am getting strange discrepancy between pyspark results and normal results. </p>

<pre><code>vectors.take(1)
</code></pre>

<p><strong>Results with Spark</strong></p>

<p><a href=""https://i.stack.imgur.com/zgPjC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zgPjC.png"" alt=""Results with spark""></a></p>

<p><strong>Results without spark</strong>
<a href=""https://i.stack.imgur.com/TSx9D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TSx9D.png"" alt=""Results without spark""></a></p>

<p>It just gives me arrays with intensity in dimension of 15 order of magnitude(look at the 7th element above) while computing the same vector locally gives me normal vectors. What is going wrong with using Spark here? Does it mess up answers in communicating the results to Master?</p>
",,2015-09-28 15:46:29,Discrepancy in results from Spark using Broadcasted varaibles,<python><apache-spark><pyspark><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6705,32812159,2015-09-27 19:48:10,,"<p>After I installed the Theano and genesim by pip install xxx --user on server side, I tried to import them in python code, but I had this error: </p>

<p>Exception: Compilation failed (return status=1): /usr/bin/ld: cannot find -lInclude. collect2: ld returned 1 exit status. </p>

<p>In addition, I have no root permission, how can I make it?</p>
",,2015-09-27 19:48:10,Cannot find -lInclude on server side,<python><compilation><server><theano><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6737,32796485,2015-09-26 11:05:54,,"<p>I have a <code>doc2vec</code> model build from my data, now I have a new sentence in run time which does not belong to the trained data set.</p>

<p>How can I build or predict a vector for this sentence from my model?</p>

<p>How should I handle unknown words in this sentence?</p>
",2015-09-26 11:41:38,2015-09-27 03:59:29,Building Vector for a sentence in doc2vec from an untrained data set,<python><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6771,32978429,2015-10-06 19:47:47,,"<p>I have a data pipeline with <code>luigi</code> that works perfectly fine if I put 1 worker to the task. However, if I put > 1 workers, then it dies (unexpectedly with exit code -11) in a stage with 2 dependencies. The code is rather complex, so a minimum example would be difficult to give. The gist of the matter is that I am doing the following things with <code>gensim</code>:</p>

<ol>
<li>Building a dictionary from some texts.</li>
<li>Building a corpus from said texts and the dictionary (requires (1)).</li>
<li>Training an LDA model from the corpus and dictionary (requires (1) and (2)).</li>
</ol>

<p>For some reason, step (3) crashes every time I put more than one worker, even if (1) and (2) are already completed...</p>

<p>Any help would be greatly appreciated!</p>

<p><strong>EDIT:</strong> Here is an example of the logging info. TrainLDA is task (3). There are still two tasks after that that require TrainLDA. All earlier tasks finished correctly. I substituted TrainLDA's arguments for <code>...</code> so that the output would be more readable. The additional info are just <code>print</code> statements we put to help us know what is happening.</p>

<p>DEB</p>

<pre><code>UG: Pending tasks: 3
DEBUG: Asking scheduler for work...
INFO: [pid 28851] Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) running   TrainLDA(...)
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
==============================
Corriendo LDA de spanish con nivel de limpieza stopwords
==============================
Nmero de tpicos: 40
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
INFO: Worker task TrainLDA(...) died unexpectedly with exit code -11
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: There are 2 pending tasks possibly being run by other workers
INFO: There are 2 pending tasks unique to this worker
INFO: Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) was stopped. Shutting down Keep-Alive thread
</code></pre>
",2015-10-07 23:03:14,2019-05-13 21:00:35,python luigi died unexpectedly with exit code -11,<python><text-mining><gensim><luigi>,,,CC BY-SA 3.0,False,False,True,False,False
6788,33070598,2015-10-11 22:03:37,,"<p>I am getting this error when I am trying to run word2vec from gensim library of python. I am using python 3.4 and OS is windows 7. I have also attached complete stacktrace as well.
I read online and it says that this is an issue with python 2.x, but I am getting in python 3.4</p>

<pre><code>model = word2vec.Word2Vec(sentences, workers=num_workers, \
        size=num_features, min_count = min_word_count, \
        window = context, sample = downsampling)

Traceback (most recent call last):
  File ""&lt;pyshell#137&gt;"", line 3, in &lt;module&gt;
    window = context, sample = downsampling)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 417, in __init__
    self.build_vocab(sentences)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 483, in build_vocab
    self.finalize_vocab()  # build tables &amp; arrays
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 611, in finalize_vocab
    self.reset_weights()
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 888, in reset_weights
    self.syn0[i] = self.seeded_vector(self.index2word[i] + str(self.seed))
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 900, in seeded_vector
    once = random.RandomState(uint32(self.hashfxn(seed_string)))
OverflowError: Python int too large to convert to C long
</code></pre>
",,2015-10-13 15:32:40,Python int too large to convert to C long in python 3.4,<python-2.7><python-3.x><machine-learning><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6835,33059671,2015-10-10 22:35:24,,"<p>I just started to experiment with word2vec form gensim using tutorial provide in <a href=""http://radek&#39;%20s%20tutorial"" rel=""nofollow"">http://rare-technologies.com/word2vec-tutorial/</a>. If we need need the raw output vectors, we write:</p>

<pre><code>model['computer']  
</code></pre>

<p>And the result is:</p>

<pre><code>array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>How can I get the word having the array? So if I write:</p>

<pre><code>f=model['computer']
</code></pre>

<p>how can I get the word 'computer' using f?</p>
",,2015-10-11 07:20:19,Get word from array in word2vec in gensim,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
6878,33200360,2015-10-18 16:31:42,,"<p>I just got acquainted with gensim and I tried to install it. I performed  any steps is written in page <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">https://radimrehurek.com/gensim/install.html</a> but I could not install it. I have installed python 2.7, scipy, numpy successfully on windows 8.1 64bit, but when I run setup.py in gensim it doesn't run.</p>

<p>Please help me I need to gensim Immediately and tell me installation steps with More details and other software that needs to be installed before it.
thanks</p>
",2015-10-18 16:43:35,2015-11-13 07:45:53,how to install gensim on windows 8.1,<python><numpy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
6919,33229360,2015-10-20 06:20:23,,"<p>I am starting with some python task, I am facing a problem while using gensim. I am trying to load files from my disk and process them (split them and lowercase() them)</p>

<p>The code I have is below:</p>

<pre><code>dictionary_arr=[]
for file_path in glob.glob(os.path.join(path, '*.txt')):
    with open (file_path, ""r"") as myfile:
        text=myfile.read()
        for words in text.lower().split():
            dictionary_arr.append(words)
dictionary = corpora.Dictionary(dictionary_arr)
</code></pre>

<p>The list (dictionary_arr) contains the list of all words across all the file, I then use gensim corpora.Dictionary to process the list. However I face a error.</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>I cant understand whats a problem, A little guidance would be appreciated.</p>
",,2020-02-26 18:17:20,"Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string",<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7022,33596082,2015-11-08 16:20:09,,"<p>I have a gensim Word2Vec model computed in Python 2 like that:</p>

<pre><code>from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

model = Word2Vec(LineSentence('enwiki.txt'), size=100, 
                 window=5, min_count=5, workers=15)
model.save('w2v.model')
</code></pre>

<p>However, I need to use it in Python 3. If I try to load it, </p>

<pre><code>import gensim
from gensim.models import Word2Vec
model = Word2Vec.load('w2v.model')
</code></pre>

<p>it results in an error:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xf9 in position 0: ordinal not in range(128)
</code></pre>

<p>I suppose the problem is in differences in encoding between Python2 and Python3. Also it seems like gensim is using pickle to save/load models.</p>

<p>Is there a way to set encoding/pickle options so that the model loads properly? Or maybe use some external tool to convert the model file?</p>

<p>Recomputing it in Python 3 is not an option: it takes way too much time.</p>
",,2016-04-21 18:25:56,"Load gensim Word2Vec computed in Python 2, in Python 3",<python><python-3.x><encoding><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7034,33525173,2015-11-04 15:05:25,,"<p><strong>Problem:</strong> I want to convert a list of list into a dataframe.</p>

<p><strong>Setup:</strong> I have the following list:</p>

<pre><code>data = [[(1,0.8),(2,0.2)],
       [(0,0.1),(1,0.3),(2,0.6)],
       [(0,0.05),(1,0.05),(2,0.3),(3,0.4),(4,0.2)]]
</code></pre>

<p>This is an LDA Document-Topic Probability List from <code>gensim</code> in which each list is a document and each tuple is one of five topic probabilities. (See an earlier question I posted on Stack Overflow <a href=""https://stackoverflow.com/questions/33002480/convert-get-documents-topics-to-data-frame"">here</a>). The first element in the tuple represents the topic number, the second element is the probability that the topic probability for the document.</p>

<p>Note that while some documents (like the 3rd list) can have up to five tuples (topic probabilities), gensim LDA does not output probabilities for topics with less 0.01 probabilities. Therefore, examples like document 1 and document 2 have less than five tuples.</p>

<p><strong>Goal:</strong> Use for loops to create a Document-Topic Probability matrix such that:</p>

<pre><code>ProbMatrix = [(0,0.8,0.2,0,0),
        (0.1,0.3,0.6,0,0),
        (0.05,0.05,0.3,0.4,0.2)]
</code></pre>

<p>As noted above, for ""missing"" tuples (topics), zero's need to be plugged in. Once I get this list, I figure I can use pandas dataframe function to produce my final output (df) such that</p>

<pre><code>df = pd.DataFrame(ProbMatrix)
</code></pre>

<p><strong>My (Failed) Attempt:</strong></p>

<pre><code>ProbMatrix = []
for i in data:      #each document i
    for j in i:     #each topic j
        if j[0] == 0:
            ProbMatrix[i,0].append(j[1])
        elif j[0]  == 1:
            ProbMatrix[i,1].append(j[1])
        elif j[0]  == 2:
            ProbMatrix[i,2].append(j[1])   
        elif j[0]  == 3:
            ProbMatrix[i,3].append(j[1])   
        elif j[0]  == 4:
            ProbMatrix[i,4].append(j[1])  
</code></pre>

<p>The problem is how I'm referencing ProbMatrix because I'm receiving the following error:</p>

<pre><code>TypeError: list indices must be integers, not tuple
</code></pre>

<p>Thank you for your help!</p>

<p><strong>Bonus (that is, it'd be even better if you can help):</strong></p>

<p>One problem I've found with gensim LDA is that, as mentioned, it does not output probabilities less than 0.01, even if <code>minimum_probability = None</code>. For example, see this earlier <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda/32651946#32651946"">post</a>. The example above is illustrative in that the topic probabilities sum to 1 for each document. However, in reality the output looks more like this:</p>

<pre><code>data = [[(1,0.79),(2,0.2)],  # topic 1 probability 0.79 from 0.8
       [(0,0.09),(1,0.3),(2,0.6)], # topic 0 probability 0.09 from 0.1
       [(0,0.05),(1,0.05),(2,0.3),(3,0.4),(4,0.2)]]
</code></pre>

<p>What I'm looking for is instead of putting zero into unknown topic probabilities, instead make the remaining missing topics an even probability such that topic probabilities for each document equal 1. For example, this would result in a ProbMatrix:</p>

<pre><code>ProbMatrix = [(0.0033,0.79,0.2,0.0033,0.0033),
        (0.09,0.3,0.6,0.005,0.005),
        (0.05,0.05,0.3,0.4,0.2)]
</code></pre>
",2017-05-23 11:50:42,2015-11-05 15:37:54,python - convert list of list to dataframe,<python><list><pandas><dataframe><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7041,33615029,2015-11-09 17:46:45,,"<p>Thanks for reading and taking the time to think about and respond to this.</p>

<p>I am using Gensim's wrapper for Mallet (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/wrappers/ldamallet.py"" rel=""nofollow"">ldamallet.py</a>), and it works like a charm.  I need to get the topic proportions for my corpus (over all my documents) and I do not know how to do that.  model.alpha is not it as it is not normalized to 1.  Plus, alpha contains my Dirichlet parameters, and not the topic proportions.  Am I correct?</p>

<p>Any help is much appreciated.</p>
",,2015-11-10 08:44:01,topic proportions in my corpus?,<lda><gensim><topic-modeling><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
7056,33638915,2015-11-10 20:24:43,,"<p>I am struggling to create an iterator from a query from sqlalchemy. </p>

<p>Here is what I tried so far </p>

<p><strong>create a table</strong> </p>

<pre><code>from sqlalchemy import create_engine, Column, MetaData, Table , Integer, String
engine = create_engine('sqlite:///test90.db')
conn = engine.connect()
metadata = MetaData()
myTable = Table('myTable', metadata,
     Column('Doc_id', Integer, primary_key=True),
     Column('Doc_Text', String))
metadata.create_all(engine)

conn.execute(myTable.insert(), [{'Doc_id': 1, 'Doc_Text' : 'first sentence'},
          {'Doc_id': 2, 'Doc_Text' : 'second sentence'},
          {'Doc_id': 3, 'Doc_Text' : 'third sentence'},
          {'Doc_id': 4, 'Doc_Text' : 'fourth sentence'}
          ])
</code></pre>

<p>I read everything I could on iterator but do not get it. 
Here the class I created to get an iterator but it does not work 
(it overflows although I specify a break) </p>

<pre><code>from sqlalchemy import create_engine

class RecordsIterator:
def __init__(self, xDB, xSQL):
    self.engine = create_engine(xDB)
    self.conn = self.engine.connect()
    self.xResultCollection = self.conn.execute(xSQL)
def __iter__(self):
    return self 
def next (self):
    while self.xResultCollection.closed is False:
        xText = (self.xResultCollection.fetchone())[1]
        xText = xText.encode('utf-8')
        yield xText.split()
        if not self.xResultCollection:
            break


x1 = RecordsIterator(xDB = 'sqlite:///test91.db', xSQL = 'select * from myTable')
</code></pre>

<p>In case you are wondering why I am not just using a <strong>generator</strong> . 
I need to feed the iterator in gensim.Word2Vec and unfortunately, it does not take a generator </p>

<pre><code>   import gensim
   gensim.models.Word2Vec(x1)
</code></pre>

<p>Thanks in advance  </p>
",2015-11-10 21:09:22,2015-11-11 10:52:51,results from an sqlachemy query as iterator,<python><iterator><sqlalchemy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7093,33702450,2015-11-13 21:59:05,,"<p>I'm running Anaconda Python 2.7 on Windows. I've installed gensim and pyLDAvis to do some topic modeling. (Note installing pyLDAvis on python 2.7 in windows is a little tricky as you have to make sure you are not using scikit-bio which doesn't appear to compile on Windows 2.7... I think I have a workaround for this, but I can't try it because of reasons to be outlined below!)</p>

<p>So I got pyLDAvis to install. However when running, it seems to have a problem with an import statement. </p>

<p>pyLDAvis is installed in this folder....</p>

<pre><code>C:\Anaconda2\Lib\site-packages\pyLDAvis-1.3.2-py2.7.egg\pyLDAvis
</code></pre>

<p><code>sys.path</code> returns this:</p>

<pre><code>['',
'C:\\Anaconda2\\lib\\site-packages\\pyldavis-1.3.2-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages\\joblib-0.9.3-py2.7.egg',
'C:\\Anaconda2\\python27.zip',
'C:\\Anaconda2\\DLLs',
'C:\\Anaconda2\\lib',
'C:\\Anaconda2\\lib\\plat-win',
'C:\\Anaconda2\\lib\\lib-tk',
'C:\\Anaconda2',
'C:\\Anaconda2\\Library\\bin',
'c:\\anaconda2\\lib\\site-packages\\sphinx-1.3.1-py2.7.egg',
'c:\\anaconda2\\lib\\site-packages\\setuptools-18.4-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages',
'C:\\Anaconda2\\lib\\site-packages\\cryptography-1.0.2-py2.7-win-amd64.egg',
'C:\\Anaconda2\\lib\\site-packages\\win32',
'C:\\Anaconda2\\lib\\site-packages\\win32\\lib',
'C:\\Anaconda2\\lib\\site-packages\\Pythonwin',
'C:\\Anaconda2\\lib\\site-packages\\IPython\\extensions']
</code></pre>

<p>What is happening is that when I try to run <code>pyLDAvis</code>, the library calls <code>import gensim</code>. However, <code>gensim</code> is both a folder in the <code>site-packages</code> and a file (<code>gensim.py</code>) inside <code>pyLDAvis</code>. </p>

<p>So when python tries to <code>import gensim</code> inside the <code>pyLDAvis</code> module, it imports the <code>gensim.py</code> file within the <code>pyLDAvis</code> module, not the ``gensim<code>folder inside</code>site-packages`. </p>

<p>How do I go about fixing this? </p>

<p>Thanks. </p>
",,2015-11-13 22:22:51,"import gensim imports a file in an active module, not the root site-packages folder",<python><python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7162,33828304,2015-11-20 13:47:58,,"<p>I'm applying TFIDF on text documents where I get varied length n dimensional vectors each corresponding to a document. </p>

<pre><code>    texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=dictionary)
    tfidf = models.TfidfModel(corpus)   
    corpus_tfidf = tfidf[corpus]
    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)
    corpus_lsi = lsi[corpus_tfidf]
    corpus_lda=lda[corpus]
    print ""TFIDF:""
    print corpus_tfidf[1]
    print ""__________________________________________""
    print corpus_tfidf[2]
</code></pre>

<p>The output to this is:</p>

<pre><code>TFIDF:
Vec1:    [(19, 0.06602704727889631), (32, 0.360417819987515), (33, 0.3078487494326974), (34, 0.360417819987515), (35, 0.2458968255872351), (36, 0.23680107692707422), (37, 0.29225639811281434), (38, 0.31741275088103), (39, 0.28571949457481044), (40, 0.32872456368129543), (41, 0.3855741727557306)]
    __________________________________________
Vec2:    [(5, 0.05617283528623041), (6, 0.10499864499395724), (8, 0.11265354901199849), (16, 0.028248249837939252), (19, 0.03948130674177094), (29, 0.07013501129200184), (33, 0.18408018239985235), (42, 0.14904146984986072), (43, 0.20484144632880313), (44, 0.215514203535732), (45, 0.15836501876891904), (46, 0.08505477582234795), (47, 0.07138425858136686), (48, 0.127695955436003), (49, 0.18408018239985235), (50, 0.2305566099597365), (51, 0.20484144632880313), (52, 0.2305566099597365), (53, 0.2305566099597365), (54, 0.053099690797234665), (55, 0.2305566099597365), (56, 0.2305566099597365), (57, 0.2305566099597365), (58, 0.0881162347543671), (59, 0.20484144632880313), (60, 0.16408387627386525), (61, 0.08256873616398946), (62, 0.215514203535732), (63, 0.2305566099597365), (64, 0.16731192344738707), (65, 0.2305566099597365), (66, 0.2305566099597365), (67, 0.07320703902661252), (68, 0.17912628269786976), (69, 0.12332630621892736)]
</code></pre>

<p>The vector points not represented are 0. Which means say (18, ....) does not exist in the vector, then it is 0. </p>

<p>I want to apply K means clustering on these vectors (Vec1 and Vec2)</p>

<p>Scikit's K means clustering needs vectors in equal dimension and in matrix format. What should be done about this? </p>
",,2015-11-27 17:09:34,K means Clustering on n dimensional vectors.,<python><scikit-learn><k-means><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
7171,33789541,2015-11-18 20:16:25,,"<p>I have been experimenting with the doc2vec module for sometime now. I can train my model and have the trained model output similar documents for a given document as follows :</p>

<pre><code>import re
modelloaded=Doc2Vec.load(""model_all_doc_dm_1"")

st = 'long description of a document as string'
doc = re.sub('[^a-zA-Z]', ' ', st).lower().split() 

new_doc_vec = modelloaded.infer_vector(doc)

modelloaded.docvecs.most_similar([new_doc_vec])
</code></pre>

<p>This works well, and gives me 10 results. Is there a way to get more than 10 results or is that the limit?</p>
",,2015-11-19 18:59:15,Is there a limit in Gensim's Doc2Vec most_similar documents result set?,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7186,33808746,2015-11-19 16:02:16,,"<p>I have a gensim LDA model that I am working on and I would like to fit into the sciKit Naive Bayes classifier, similar to sciKit's TfidfTransformer():</p>

<pre><code> lda = ldamodel.LdaModel(corpus=self.corpus, num_topics=num_topics)
</code></pre>

<p>The shape of the TfidfTransformer() is a sparse numpy matrix: </p>

<pre><code>&lt;2014x4604 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with 117869 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>There have been a few posts on how to take the sciKit Vectorizer and feed it into a gensim model but I want to do the opposite, something like:</p>

<pre><code> classifier = MultinomialNB()
 classifier.fit(lda_model, train_class)
</code></pre>

<p>How would I go about doing this?  I assume that the matrix from TfidfTransformer.fit_transform() is a matrix of tfidf scores for each word in an array of documents like:</p>

<pre><code>  [[.1234, .234, .345], # document 1
  [.098, .987, .876],   # document 2
 [.555, .666, .777]]    # document 3
</code></pre>

<p>If that's the case, I can just replace every word with it's score and feed that matrix into the classifier. But I can't really tell because there's a lot of stuff going on under the hood that I can't see.</p>

<p>Edit:  I was able to print the first entry of the numpy array, which is itself a numpy array, so I printed the first entry of that and got: </p>

<pre><code> (0, 3911)     0.22756829025
 (0, 3826)     0.161385996776
 (0, 3815)     0.100930582918
 (0, 3627)     0.158255401295
 (0, 3621)     0.194939740341
 (0, 3620)     0.100757250634
 (0, 3527)     0.0734744409855
 (0, 3246)     0.109635312627
</code></pre>

<p>The column on the left must be tfidf scores but I'm not sure what the right is.  I assume wordids and the document id?</p>
",2015-11-28 08:14:59,2015-11-28 08:14:59,Fit Gensim LDA into SciKit Naive Bayes classifier,<python><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
7192,34021351,2015-12-01 13:15:05,,"<p>I'm having a problem when trying to import gensim in python. When typing:</p>

<blockquote>
  <p>import gensim</p>
</blockquote>

<p>I got the following error:</p>

<p>Traceback (most recent call last):
  File """", line 1, in 
  File ""/Library/Python/2.7/site-packages/gensim/<strong>init</strong>.py"", line 6, in 
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
ImportError: cannot import name parsing</p>

<p>Also, when I view ""<strong>init</strong>.py"" it contains only the following lines:</p>

<blockquote>
  <h1>bring model classes directly into package namespace, to save some typing</h1>
  
  <p>from .summarizer import summarize, summarize_corpus</p>
  
  <p>from .keywords import keywords</p>
</blockquote>

<p>Any idea on how to solve this problem is highly appreciated.</p>

<p>I'm using:
MAC 10.10.5 and Python 2.7</p>

<p>Thank you</p>
",,2016-06-30 16:24:07,importing gensim in mac,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7197,33929680,2015-11-26 02:31:03,,"<p>When I run gensim's <code>LdaMulticore</code> model on a machine with 12 cores, using:</p>

<pre><code>lda = LdaMulticore(corpus, num_topics=64, workers=10)
</code></pre>

<p>I get a logging message that says </p>

<pre><code>using serial LDA version on this node  
</code></pre>

<p>A few lines later, I see another loging message that says </p>

<pre><code>training LDA model using 10 processes
</code></pre>

<p>When I run top, I see 11 python processes have been spawned, but 9 are sleeping, I.e. only one worker is active.  The machine has 24 cores, and is not overwhelmed by any means.  Why isn't LdaMulticore running in parallel mode?</p>
",2015-12-11 23:45:39,2016-08-25 12:07:26,gensim LdaMulticore not multiprocessing?,<python><multiprocessing><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7198,25272500,2014-08-12 19:26:56,,"<p>If I try to move my gensim database though windows explorer and access it from the new location I get an error because it says there is a sharding error?</p>

<p>Also I know the underlying SQLite is in gensim can I port that to the SQLite in a django model?</p>
",,2014-08-12 19:26:56,Gensim sharding in python when moving database,<python><django><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7205,33989826,2015-11-30 00:30:37,,"<p>I know that this question has been asked already, but I was still not able to find a solution for it. </p>

<p>I would like to use gensim's <code>word2vec</code> on a custom data set, but now I'm still figuring out in what format the dataset has to be. I had a look at <a href=""http://streamhacker.com/2014/12/29/word2vec-nltk/"">this post</a> where the input is basically a list of lists (one big list containing other lists that are tokenized sentences from the NLTK Brown corpus). So I thought that this is the input format I have to use for the command <code>word2vec.Word2Vec()</code>. However, it won't work with my little test set and I don't understand why.</p>

<p>What I have tried:</p>

<p><strong>This worked</strong>:</p>

<pre><code>from gensim.models import word2vec
from nltk.corpus import brown
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

brown_vecs = word2vec.Word2Vec(brown.sents())
</code></pre>

<p><strong>This didn't work</strong>:</p>

<pre><code>sentences = [ ""the quick brown fox jumps over the lazy dogs"",""yoyoyo you go home now to sleep""]
vocab = [s.encode('utf-8').split() for s in sentences]
voc_vec = word2vec.Word2Vec(vocab)
</code></pre>

<p>I don't understand why it doesn't work with the ""mock"" data, even though it has the same data structure as the sentences from the Brown corpus:</p>

<p><strong>vocab</strong>:</p>

<pre><code>[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]
</code></pre>

<p><strong>brown.sents()</strong>: (the beginning of it)</p>

<pre><code>[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', ""Atlanta's"", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', ""''"", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', ""''"", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]
</code></pre>

<p>Can anyone please tell me what I'm doing wrong? </p>
",,2018-03-07 06:14:11,Python: gensim: RuntimeError: you must first build vocabulary before training the model,<python><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
7215,33976953,2015-11-28 22:05:32,,"<p>I am using gensim word2vec library in python and using pre-trained GoogleNews-vectors-negative300.bin model. But,</p>
<blockquote>
<p>I have words in my corpus for which i don't have word vectors and am
getting keyError for that how do i solve this problem?</p>
</blockquote>
<h1>Here is what i have tried so far,</h1>
<h1>1: Loading <code>GoogleNews-vectors-negative300.bin</code> per-trained model:</h1>
<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print &quot;model loaded...&quot;
</code></pre>
<h1>2: Build word vector for training set by using the average value of all word vectors in the tweet, then scale</h1>
<pre><code>def buildWordVector(text, size):
vec = np.zeros(size).reshape((1, size))
count = 0.
for word in text:
    try:
        vec += model[word].reshape((1, size))
        count += 1.
        #print &quot;found! &quot;,  word
    except KeyError:
        print &quot;not found! &quot;,  word #missing words
        continue
if count != 0:
    vec /= count
return vec

trained_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
</code></pre>
<p>Please tell how it is possible to add new words in pre-trained Word2vec model?</p>
",2020-06-20 09:12:55,2020-06-17 08:32:24,How to add missing words vectors in GoogleNews-vectors-negative300.bin pre-trained model?,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
7239,34166369,2015-12-08 21:28:53,,"<p>I have an generator (a function that yields stuff), but when trying to pass it to <code>gensim.Word2Vec</code> I get the following error:</p>

<blockquote>
  <p>TypeError: You can't pass a generator as the sentences argument. Try an iterator.</p>
</blockquote>

<p>Isn't a generator a kind of iterator? If not, how do I make an iterator from it?</p>

<p>Looking at the library code, it seems to simply iterate over sentences like <code>for x in enumerate(sentences)</code>, which works just fine with my generator. What is causing the error then?</p>
",2015-12-08 21:32:25,2019-08-23 20:38:28,Generator is not an iterator?,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7255,34075899,2015-12-03 20:52:29,,"<p>I am trying to analyze Wikipedia dump file. I am using gensim.scripts, a Python library, and running this command in Windows 10 cmd.exe:</p>

<pre><code>python -m gensim.scripts.make_wiki enwiki-latest-pages-articles.xml.bz2 wiki_en_output
</code></pre>

<p>This gives me the error:Microsoft Windows [Version 10.0.10586]
(c) 2015 Microsoft Corporation. All rights reserved.</p>

<pre><code>2015-12-03 15:47:20,459 : INFO : running C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\scripts\make_wiki.py enwiki-latest-pages-articles.xml.bz2 wiki_en_output
Traceback (most recent call last):
  File ""C:\Python27\lib\runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""C:\Python27\lib\runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\scripts\make_wiki.py"", line 84, in &lt;module&gt;
    wiki = WikiCorpus(inp, lemmatize=lemmatize) # takes about 9h on a macbook pro, for 3.5m articles (june 2011)
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\wikicorpus.py"", line 270, in __init__
    self.dictionary = Dictionary(self.get_texts())
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\dictionary.py"", line 58, in __init__
    self.add_documents(documents, prune_at=prune_at)
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\dictionary.py"", line 119, in add_documents
    for docno, document in enumerate(documents):
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\wikicorpus.py"", line 290, in get_texts
    texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
IOError: [Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml.bz2'
</code></pre>

<p>Thoughts on what I should do to fix this?</p>

<p>On Windows 10. gensim.scripts has been installed.</p>
",2015-12-03 21:13:13,2015-12-03 23:04:41,"gensim.scripts ""No such file or directory""",<python><cmd><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7261,34057374,2015-12-03 03:28:22,,"<p>I'm trying to install gensim on Windows 7, with Python 3.4. According to <a href=""http://radimrehurek.com/gensim/install.html"" rel=""nofollow"">gensim official installation tutorial</a>, gensim depends on NumPy and SciPy, so I went to <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">here</a> to download .whl files for NumPy and SciPy installation. But when I used pip to install them, it gave me these errors:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#2&gt;"", line 1, in &lt;module&gt;
import gensim
File ""C:\Python34\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python34\lib\site-packages\gensim\matutils.py"", line 21, in &lt;module&gt;
import scipy.linalg
File ""C:\Python34\lib\site-packages\scipy\linalg\__init__.py"", line 172, in &lt;module&gt;
from .misc import *
File ""C:\Python34\lib\site-packages\scipy\linalg\misc.py"", line 5, in &lt;module&gt;
from .blas import get_blas_funcs
File ""C:\Python34\lib\site-packages\scipy\linalg\blas.py"", line 155, in &lt;module&gt;
from scipy.linalg import _fblas
ImportError: DLL load failed: 
</code></pre>

<p>""""means ""Cannot find the designated module"".
How can I resolve this?</p>
",,2019-07-05 02:20:10,How to resolve error when installing gensim?,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7265,34153708,2015-12-08 10:39:55,,"<p>I use the next code to train the model:</p>

<pre><code>norms_train = [ [''], [ u'word', u'to', u'learn', ... ], ...]
model = word2vec.Word2Vec(norms_train, size=100, window=10)
</code></pre>

<p>With procedure to check the results:</p>

<pre><code>i, j = 0, 0
for text in norms_train:
    j += len(text)
    for word in text:
        if word not in model.vocab:
            i += 1
print i, '/', j
</code></pre>

<p>13129 / 185379</p>
",,2015-12-08 11:00:02,"All of the words, those I use to train the word2vec model, must be in model.vocab, aren't they?",<python><gensim><training-data><word2vec>,2015-12-08 13:09:54,,CC BY-SA 3.0,False,False,True,False,False
7293,34249586,2015-12-13 09:15:04,,"<p>I'm currently use gensim to reproduce the result of example of Google provide. <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">here</a></p>

<p>The problem is the accuracy test of gensim doesn't match with Google's result.</p>

<p>For example, the accuracy of capital-common-countries of Google is 82.02%, the best result of gensim of different parameter sets is 64.4%. There is a big gap here.</p>

<p>Here is the code snippet of train word2vec and accuracy by using gensim</p>

<pre><code>sentences = word2vec.Text8Corpus('./text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5, sg=0, window=8, iter=15, sample=1e-4, negative=25)
model.accuracy(""./questions-words.txt"")[enter image description here][2]
</code></pre>

<p>Code snippet of Google's demo without changes any parameters</p>

<pre><code> ./demo-word-accuracy.sh
</code></pre>

<p><a href=""http://i.stack.imgur.com/2dhUK.png"" rel=""nofollow"">Accuracy comparison detail</a></p>

<p>Does anyone could help on this?</p>
",2015-12-13 10:04:51,2015-12-14 21:47:22,The accuracy test of word2vec in gensim,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7320,34207097,2015-12-10 16:35:27,,"<p>I am trying to learn topic modelling using Gensim python library.
I have tried so many different tutorials including official one.</p>

<p>Question:
How do I get document wise topic distribution using Gensim.</p>

<p>My current output is list of topics with its keywords and probability as shown below.</p>

<pre><code>(0, u'0.086*good + 0.086*brocolli + 0.086*health + 0.061*eat)
(1, u'0.068*mother + 0.068*brother + 0.068*drive + 0.041*pressur)
</code></pre>

<p>I would like to know if it is possible to have a list of say each document and top topics for that particular document?</p>

<p>My code is as following:</p>

<pre><code>tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# create sample documents
doc_a = ""Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.""
doc_b = ""My mother spends a lot of time driving my brother around to baseball practice.""
doc_c = ""Some health experts suggest that driving may cause increased tension and blood pressure.""
doc_d = ""I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.""
doc_e = ""Health professionals say that brocolli is good for your health."" 

# compile sample documents into a list
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]
num_topics=2;

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    #print(stopped_tokens)

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    #print(""printing stemmed tokens"")
    #print(stemmed_tokens)

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)
print(""printing each token/words along with unique integer id.."")
print(dictionary.token2id)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
print(""printing sample bag of words"")
print(corpus[0])

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
#print(ldamodel.print_topics(num_topics=3, num_words=3))
#print ldamodel.top_topics(corpus,2)
print(ldamodel.show_topics(num_topics=2, num_words=3, log=False, formatted=True))
print(ldamodel.show_topics())
print(""from for loop."")
for i in ldamodel.show_topics(len(dictionary)):
    print i
</code></pre>
",,2017-04-27 14:12:17,Print document wise topics in Gensim,<python><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
7361,34396300,2015-12-21 12:59:25,,"<p>I've installed gensim on my MacBookPro (Yosemite 10.10.5 ) and I'm using anconda. The installation with <code>pip install --upgrade gensim</code> was working without error message. 
When I tried to run the code of the tutorials, there appears an error when calling serialization:
<code>corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)</code></p>

<p>Complete error message:</p>

<p><code>File ""/Users/sage/Desktop/gensim/test_gensim.py"", line 39, in &lt;module&gt;
    corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/indexedcorpus.py"", line 94, in serialize
    offsets = serializer.save_corpus(fname, corpus, id2word, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/mmcorpus.py"", line 49, in save_corpus
    return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 486, in write_corpus
    mw = MmWriter(fname)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 436, in __init__
    self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
  File ""/System/Library/anaconda/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 111, in smart_open
    raise NotImplementedError('unknown file mode %s' % mode)
NotImplementedError: unknown file mode wb+</code></p>

<p>When I downloaded the tar files and performed <code>python setup.py test</code>,
the error <code>NotImplementedError: unknown file mode wb+</code> occurred too. </p>

<p>How can I fix this?</p>
",,2016-01-03 21:48:17,gensim installation on yosemite using anaconda,<python><python-2.7><anaconda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7381,34309428,2015-12-16 10:23:10,,"<p>I have created LDA model using gensim. Now, I wanted to visualise it using pyLDAvis library but getting :</p>

<pre><code>ImportError: cannot import name PCoA 
</code></pre>

<p>Can anyone help me with this or suggest some alternatives.</p>

<p>Thanks in advance.</p>
",,2015-12-17 13:04:08,Cannot run pyLDAvis. Getting Error : ImportError: cannot import name PCoA,<python><scikit-learn><gensim><skbio>,,,CC BY-SA 3.0,False,False,True,False,True
7414,34427678,2015-12-23 02:24:53,,"<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>

<p>I installed <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">gensim</a> and tries to load the module, but following error occurred: </p>

<pre><code>In [1]: import gensim  

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
",2015-12-30 10:59:30,2017-04-27 09:55:13,'utf-8' decode error when loading a word2vec module,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7433,34540518,2015-12-31 03:14:20,,"<p>Suppose I have a (possibly) large corpus, about 2.5M of them with 500 features (after running LSI on the original data with gensim). I need the corpus to train my classifiers using scikit-learn. However, I need to first convert the corpus into a numpy array. The corpus creation and classifier trainer are done in two different scripts.</p>

<p>So the problem is that, my collection size is expected to grow, and at this stage I already don't have enough memory (32GB on the machine) to convert all at once (with <code>gensim.matutils.corpus2dense</code>). In order to work around the problem I am converting one vector after another at a time, but it is very slow.</p>

<p>I have considered dumping the corpus into svmlight format, and have scikit-learn to load it with <code>sklearn.datasets.load_svmlight_file</code>. But then it would probably mean I will need to load everything into memory at once?</p>

<p>Is there anyway I can efficiently convert from gensim corpus to numpy array (or scipy sparse matrix)?</p>
",,2015-12-31 04:00:02,How to convert Gensim corpus to numpy array (or scipy sparse matrix) efficiently?,<python><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
7440,34384186,2015-12-20 18:14:30,,"<p>I am trying to use gensim for some topic modelling. When I print the similarity queries, I only get it the id's and values instead of the actual strings. How do i print the string of my similarity queries ? 
below is the code i am using.</p>

<pre><code>index = similarities.MatrixSimilarity(lsi[corpus]) 
sims =index[vec_lsi] # perform a similarity query against the corpus
print(list(enumerate(sims))) # print (document_number, document_similarity) 
</code></pre>
",,2015-12-20 18:14:30,Gensim similarity query-returns ids and values of vectors-no string,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7511,34721984,2016-01-11 12:49:10,,"<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>

<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>

<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>
",2016-01-12 06:25:02,2019-01-06 08:08:43,stopword removing when using the word2vec,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7514,34634367,2016-01-06 13:30:21,,"<p>FYI, when I try to import gensim module in Django views.py, my server just got stuck, all my restful apis don't not working, not 5**, not 4**, not 3**, not 2**, just no response.</p>

<p>But if I just comment <code>import gensim</code> in views.py, it just back to the normal.</p>

<p>So where is the problem, and how to debug Django in this situation?</p>

<p>Django version: 1.8   Gensim version: 0.12.3</p>

<h2>UPDATE</h2>

<p>I can import gensim in Django shell
<a href=""https://i.stack.imgur.com/OzDnj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OzDnj.jpg"" alt=""enter image description here""></a></p>

<p>And same issue on Django 1.9</p>
",2016-01-06 15:42:22,2017-06-15 02:43:23,Get stuck when importing gensim in Django views.py,<python><django><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7517,34637242,2016-01-06 15:52:28,,"<p>I was reading the <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow"">Experiments on the English Wikipedia</a> tutorial and noticed that many of the topics generated by LSA and LDA contained multi-word terms that had clearly been concatenated e.g. <em>northamerica</em>, <em>hockeyarchives</em></p>

<p>Could someone indicate where this takes place. I have looked at <em>gensim.scripts.make_wiki</em>, <em>gensim.corpora.wikicorpus</em> and <em>genesis.utils</em>.</p>
",,2016-01-06 15:52:28,Does Gensim handle multi-word terms when processing Wikipedia corpus?,<nlp><lda><gensim><topic-modeling><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
7541,34754547,2016-01-12 21:58:53,,"<p>I am using <code>gensim</code>, but when I try to save to a <code>s3</code> location with <code>Mmcorpus.serialize</code> it sends an error:</p>

<pre><code>corpora.MmCorpus.serialize('s3://my_bucket/corpus.mm', corpus)                                                                                           
2016-01-12 15:55:41,957 : INFO : storing corpus in Matrix Market format to s3://my_bucket/corpus.mm
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-33-513a98b2dfd4&gt; in &lt;module&gt;()
----&gt; 1 corpora.MmCorpus.serialize('s3://my_bucket/corpus.mm', corpus)

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/corpora/indexedcorpus.py in serialize(serializer, fname, corpus, id2word, index_fname, progress_cnt, labels, metadata)
     92                 offsets = serializer.save_corpus(fname, corpus, id2word, labels=labels, metadata=metadata)
     93             else:
---&gt; 94                 offsets = serializer.save_corpus(fname, corpus, id2word, metadata=metadata)
     95 
     96         if offsets is None:

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/corpora/mmcorpus.py in save_corpus(fname, corpus, id2word, progress_cnt, metadata)
     47         logger.info(""storing corpus in Matrix Market format to %s"" % fname)
     48         num_terms = len(id2word) if id2word is not None else None
---&gt; 49         return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
     50 
     51 # endclass MmCorpus

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/matutils.py in write_corpus(fname, corpus, progress_cnt, index, num_terms, metadata)
    484         is allowed to be larger than the available RAM.
    485         """"""
--&gt; 486         mw = MmWriter(fname)
    487 
    488         # write empty headers to the file (with enough space to be overwritten later)

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/matutils.py in __init__(self, fname)
    434         if fname.endswith("".gz"") or fname.endswith('.bz2'):
    435             raise NotImplementedError(""compressed output not supported with MmWriter"")
--&gt; 436         self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
    437         self.headers_written = False
    438 

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    132                 return S3OpenWrite(key, **kw)
    133             else:
--&gt; 134                 raise NotImplementedError(""file mode %s not supported for %r scheme"", mode, parsed_uri.scheme)
    135 
    136         elif parsed_uri.scheme in (""hdfs"", ):

NotImplementedError: ('file mode %s not supported for %r scheme', 'wb+', 's3')
</code></pre>

<p><em>NOTE</em>: <code>s3://my_bucket</code> exists (with another name), and <code>corpus</code> is the same from the tutorial of <code>gensim</code>.</p>

<p>Which is the correct way of do it? I want to achive the following: store the corpus (or a model, like LDA) in S3 and getting it from S3 and run it again.</p>
",2016-01-15 06:07:39,2018-10-12 09:32:29,Gensim saving corpus to S3,<amazon-s3><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7549,34831551,2016-01-16 20:05:51,,"<p>In <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus"">LDA model generates different topics everytime i train on the same corpus</a> , by setting the <code>np.random.seed(0)</code>, the LDA model will always be initialized and trained in exactly the same way. </p>

<p><strong>Is it the same for the Word2Vec models from <code>gensim</code>? By setting the random seed to a constant, would the different run on the same dataset produce the same model?</strong></p>

<p>But strangely, it's already giving me the same vector at different instances. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; exit()
alvas@ubi:~$ python
Python 2.7.11 (default, Dec 15 2015, 16:46:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; word0 = sentences[0][0]
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
</code></pre>

<p><strong>Is it true then that the default random seed is fixed?</strong> If so, what is the default random seed number? Or is it because I'm testing on a small dataset? </p>

<p>If it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.  </p>
",2017-05-23 12:18:32,2020-08-07 12:22:37,Ensure the gensim generate the same Word2Vec model for different runs on the same data,<python><random><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,True,False,True,False,False
7551,34765622,2016-01-13 11:34:03,,"<p>when I try and load the model and do anything with it I always get : ""<code>AttributeError: 'Projection' object has no attribute 'u'</code>""</p>

<p>I can't figure out what I am doing wrong here. Not sure if I am saving the model incorrectly or loading it incorrectly.</p>

<pre><code>index = similarities.MatrixSimilarity(lsiModel[tfIdfModel[myCorpus]])
File ""/home/analytics/.local/lib/python2.7/site-packages/gensim/models /lsimodel.py"", line 405, 
in __getitem__ 
assert self.projection.u is not None, ""decomposition not initialized yet""   
AttributeError: 'Projection' object has no attribute 'u'
</code></pre>
",2016-01-13 12:08:47,2017-01-01 21:27:27,AttributeError: 'Projection' object has no attribute 'u' gensim python lsi,<python-2.7><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7577,34866830,2016-01-19 00:59:56,,"<p>I'm using the <code>most_similar()</code> method as below to get all the words similar to a given word:</p>

<pre><code>word,score= model.most_similar('apple',topn=sizeofdict)
</code></pre>

<p>AFAIK, what this does is, calculate the cosine similarity between the given word and all the other words in the dictionary. When i'm inspecting the words and scores, I can see there are words with negative score down the list. What does this mean? are them the words that has opposite meaning to the given word?</p>

<p>Also if it's using cosine similarity, how does it get a negative value? cosine similarity varies between 0-1 for two documents.</p>
",,2016-01-19 11:31:46,about word2vec most_similar() function,<text-mining><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7597,34898059,2016-01-20 10:55:02,,"<p>I have upgraded gensim from 0.12.2 to 0.12.3 and I am facing an issue while inferencing in doc2vec. This is the code for performing inference:</p>

<pre><code>doc = query.lower().split()
inf_vec = formmodel.infer_vector(doc)
similarF = formmodel.docvecs.most_similar([inf_vec])
</code></pre>

<p>When doc2vec model training and inference were done using version 0.12.3 this code gives results as shown below:</p>

<pre><code>[(644539, 0.55715829133987427), (647249, 0.55713766813278198),...]
</code></pre>

<p>When doc2vec model training and inference were done using version 0.12.2 the same code gave results like so:</p>

<pre><code>[(docId1, 0.55715829133987427), (docId2, 0.55713766813278198),....]
</code></pre>

<p>How do I get document labels instead of numbers in 0.12.3?</p>
",2016-01-20 11:05:00,2016-01-28 21:42:29,Doc2vec inference in gensim 0.12.3,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7604,34948650,2016-01-22 14:07:52,,"<p>I am using <code>Doc2Vec</code> function of <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">gensim</a> in Python to convert a document to a vector.</p>

<p>An example of usage</p>

<p><code>model = Doc2Vec(documents, size=100, window=8, min_count=5, workers=4)</code></p>

<p>How should I interpret the <code>size</code> parameter. I know that if I set <code>size = 100</code>, the length of output vector will be 100, but what does it mean? For instance, if I increase <code>size</code> to 200, what is the difference?</p>
",,2016-01-28 08:41:05,"How should I interpret ""size"" parameter in Doc2Vec function of gensim?",<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7641,34923628,2016-01-21 12:18:15,,"<p>I have a twitter corpus which I am using to build sentiment analysis application. The corpus has 5k tweets which have been hand labelled as - negative, neutral or positive</p>

<p>To represent the text - I am using gensim word2vec pretrained vectors. Each word is mapped to 300 dimensions. For a tweet, I add all the word vectors to get a single 300 dim vectors. Thus every tweet is mapped to a single vector of 300 dimension. </p>

<p>I am visualizing my data using t-SNE (tsne python package). See attached image <a href=""https://i.stack.imgur.com/3E0Qf.jpg"" rel=""nofollow noreferrer"">1</a> - Red points = negative tweets, Blue points = neutral tweets and Green points = Positive tweets</p>

<p><a href=""https://i.stack.imgur.com/3E0Qf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3E0Qf.jpg"" alt=""tweets represented using word2vec""></a>   </p>

<p><strong>Question:</strong>
In the plot there no clear separation (boundary) among the data points. Can I assume this will also be the case with the original points in 300 Dimensions ?</p>

<p>i.e if points overlap in t-SNE graph then they also overlap in original space and vice-versa ?</p>
",2016-01-21 12:34:10,2016-01-21 13:49:47,t-SNE High Dimension Data Visualisation,<python><machine-learning><nlp><scikit-learn><data-analysis>,,,CC BY-SA 3.0,False,False,True,False,True
7673,35117491,2016-01-31 18:17:52,,"<p>I am using pre-trained Google news dataset for getting word vectors by using Gensim library in python</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>After loading the model I am converting training reviews sentence words into vectors</p>

<pre><code>#reading all sentences from training file
with open('restaurantSentences', 'r') as infile:
x_train = infile.readlines()
#cleaning sentences
x_train = [review_to_wordlist(review,remove_stopwords=True) for review in x_train]
train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
</code></pre>

<p>During word2Vec process i get a lot of errors for the words in my corpus, that are not in the model. Problem is how can i retrain already pre-trained model (e.g GoogleNews-vectors-negative300.bin'), in order to get word vectors for those missing words.</p>

<p>Following is what I have tried:
Trained a new model from training sentences that I had</p>

<pre><code># Set values for various parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 10   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window    size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
# Initialize and train the model (this will take some time)
print ""Training model...""
model = gensim.models.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count, 
                      window = context, sample = downsampling)


model.build_vocab(sentences)
model.train(sentences)
model.n_similarity([""food""], [""rice""])
</code></pre>

<p>It worked! but the problem is that I have a really small dataset and less resources to train a large model.</p>

<p>Second way that I am looking at is to extend the already trained model such as GoogleNews-vectors-negative300.bin.</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
model.train(sentences)
</code></pre>

<p>Is it possible and is that a good way to use, please help me out</p>
",2016-03-14 13:10:58,2016-10-24 11:03:12,Is it possible to re-train a word2vec model (e.g. GoogleNews-vectors-negative300.bin) from a corpus of sentences in python?,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7705,35121779,2016-02-01 01:33:53,,"<p>I was going through this website yesterday (<a href=""http://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow"">http://rutumulkar.com/blog/2015/word2vec/</a>)  and the author made use of the file <code>text8-queen</code>. In his script I noticed that she did not specify the location of the file and I was wondering how was he able to run it? I am unable to run it? Is there a way to run this file? Thank you.</p>

<p>The script is as follows:</p>

<pre><code>import gensim.models
import time
time1 = time.time()

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


modelbase = gensim.models.Word2Vec()
sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
modelbase.build_vocab(sentences2)
modelbase.train(sentences2)
modelbase.save_word2vec_format(""wordvectors/model-text8-queen-only"")
modelbase.accuracy(""questions-words.txt"")

model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-rest"")
model.build_vocab(sentences)
model.train(sentences)
model.save_word2vec_format(""model-text8-rest"")
model.accuracy(""questions-words.txt"")

sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
model.update_vocab(sentences2)
model.train(sentences2)
model.save_word2vec_format(""wordvectors/model-text8-queen"")
model.accuracy(""questions-words.txt"")

model1 = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-all"")
model1.build_vocab(sentences)
model1.train(sentences)
model1.save_word2vec_format(""wordvectors/model-text8-all"")
model1.accuracy(""questions-words.txt"")
print (""total time: %s"" % (time.time() - time1))
</code></pre>

<p>My question is in the in the line:  </p>

<pre><code>sentences = gensim.models.word2vec.Sentences(""text8-rest"")
</code></pre>

<p>how did the author call <code>text8-rest</code> and <code>text8-queen</code>? where should I put these text file (<code>text8-rest</code>, <code>text8-queen</code>) ? Do I have to specify the location of the text file or is python able to detect it?</p>
",2016-05-17 17:25:49,2016-05-17 17:25:49,How can I run this gensim code? Do I need some text files?,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7754,35252743,2016-02-07 11:13:24,,"<p>I am trying to clean some text. I am keeping alphabets and numbers only. However, my text still contains other characters. </p>

<p>This is my function:</p>

<pre><code>def review_to_wordlist(review, remove_stopwords=False, remove_numbers = False ):
# Function to convert a document to a sequence of words,
# optionally removing stop words and numbers.  Returns a list of words.
#
# 1. Remove HTML
review_text = BeautifulSoup(review).get_text()
#
# 2. Remove non-letters
if True:
    review_text = re.sub(""[^a-zA-Z0-9]"","" "", review_text)
#
# 3. Convert words to lower case and split them
words = review_text.lower().split()
#
# 4. Optionally remove stop words (false by default)
if remove_stopwords:
    stops = set(stopwords.words(""english""))
    words = [w for w in words if not w in stops]
#
# 5. Return a list of words
return(words)
</code></pre>

<p>and this is one result that I get:</p>

<blockquote>
  <p>NuTone Central Vacuum System 45 Ell Ohio Steel Tandem Natural and
  Synthetic Turf Sweeping System Unique Home Designs 36 in. x 80 in. Su
  Casa Black Surface Mount Outswing Steel Security Door with Expanded
  Metal Screen Unique Home Designs 36 in. x 80 in. Su Casa Black Surface
  Mount Outswing Steel Security Door with Expanded Metal Screen Unique
  Home Designs 36 in. x 80 in. Su Casa Black Surface Mount Outswing
  Steel Security Door with Expanded Metal Screen MP Global Best 400 in.
  x 36 in. x 1/8 in. Acoustical Recycled Fiber Underlayment with Film
  for Laminate Wood MP Global Best 400 in. x 36 in. x 1/8 in. Acoustical
  Recycled Fiber Underlayment with Film for Laminate Wood Grip-Rite
  #10-1/4 in. x 2-1/2 in. 8 Bright Steel Ring-Shank Common Nails (1 lb.-Pack)</p>
</blockquote>

<p>the error that I get is:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 5-6: unexpected end of data


676
Husky Pneumatic 3-1/2 in. 21 Full-Head Strip Framing Nailer
5157
RIDGID 3-1/2 in. 21 Round-Head Nailer
5158
RIDGID 3-1/2 in. 21 Round-Head Nailer
</code></pre>
",2016-02-07 12:16:23,2019-12-12 17:25:38,UnicodeDecodeError when cleaning text data,<python><regex><beautifulsoup><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7775,35276944,2016-02-08 18:43:09,,"<p>I am building an RNN model in Keras for sentences with word embeddings from gensim. I am initializing the embedding layer with GloVe vectors. Since this is a sequential model and sentences have variable lengths, vectors are zero-padded. e.g.</p>

<pre><code>[0, 0, 0, 6, 2, 4]
</code></pre>

<p>Let's say the GloVe vectors have dimensions <code>[NUM_VOCAB, EMBEDDING_SIZE]</code>. The zero index is masked (ignored) so to get the proper indexing of words, do we add an extra column to the GloVe matrix so the dimensions are: <code>[NUM_VOCAB+1, EMBEDDING_SIZE]</code>?</p>

<p>Seems like there is an unnecessary vector that the model will estimate unless there is a more elegant way.</p>

<pre><code>glove = Word2Vec.load_word2vec_format(filename)
embedding_matrix = np.vstack([np.zeros(EMBEDDING_SIZE), glove.syn0])

model = Sequential()

# -- this uses Glove as inits
model.add(Embedding(NUM_VOCAB, EMBEDDING_SIZE, input_length=maxlen, mask_zero=True,
                           weights=[embedding_matrix]))

# -- sequence layer
model.add(LSTM(32, return_sequences=False, init='orthogonal'))
model.add(Activation('tanh'))

...
</code></pre>

<p>Thanks</p>
",,2016-03-12 18:49:05,Index of Embedding layer with zero padding in Keras,<deep-learning><gensim><keras>,,,CC BY-SA 3.0,False,False,True,False,False
7812,35372917,2016-02-12 22:01:00,,"<p>I am using Gensim's LDAMulticore to perform LDA. I have around 28M small documents (around 100 characters each). </p>

<p>I have given workers argument to be 20 but the top shows it using only 4 processes. There are some discussions around it that it might be slow in reading corpus like:
<a href=""https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing"">gensim LdaMulticore not multiprocessing?</a> 
<a href=""https://github.com/piskvorky/gensim/issues/288"" rel=""nofollow noreferrer"">https://github.com/piskvorky/gensim/issues/288</a></p>

<p>But both of them uses MmCorpus . Although my corpus is completely in memory. I have machine with very large RAM (250 GB) and loading the corpus in memory takes around 40 GB. But even after that LDAMulticore is using just 4 processes. I created the corpus as:</p>

<p><code>corpus = [dictionary.doc2bow(text) for text in texts]</code></p>

<p>I am not able to understand what can be the limiting factor here?</p>
",2017-05-23 12:25:52,2019-11-15 10:52:37,Gensim LdaMulticore is not multiprocessing properly (using just 4 workers),<python><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
7835,35398153,2016-02-14 21:23:16,,"<p>Hello stack overflow community,</p>

<p>I have the following problem: I am currently mining a database of support tickets and would like to use e.g. Doc2Vec to check for similarities between tickets. However, the text contains huge strings, produced by the OS or compiler commands. So it would not be clever to use those strings as single words in the model. What is a good practice here? Does have anyone experience with something like that?</p>

<p>Best
Thorsten </p>
",,2016-02-14 21:23:16,NLP on compiler output and other system error messages,<python><nlp><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
7846,35470805,2016-02-18 00:56:37,,"<p>I am looking to calculate the similarity between documents using gensim on Python.</p>

<p>I want a way to be able to restrict the calculations to only a subset of the corpus. Specifically, my documents have an associated year, and i want a way of only computing similarities between the search document and other document which have the same value for that variable. </p>

<p>I can not see any instructions on e.g. <a href=""http://radimrehurek.com/gensim/simserver.html"" rel=""nofollow"">http://radimrehurek.com/gensim/simserver.html</a> on how to associate additional variables with each document, and in turn how to restrict the similarities to only those documents - and indeed what i am trying to do may not be feasible. My question is thus, is this it is possible, or is the only way to achieve this to  use multiple corpuses.</p>
",,2017-04-07 20:09:15,Restrict gensim similarity calculations to a subset of a corpus,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7881,35502363,2016-02-19 09:57:58,,"<p>I am new to the LDA and I have three questions. I would like to classify my text (tags) with the LDA. First I filter the words, which have been used only by one user, machine tags, tags containing only digits and tags with the frequency less than 3.
Then, I calculate the amount of topics with the Elbow method and there I get the memory error (this will be the third question). So the amount of topics suggested by the Elbow method is 8 (I have filtered some more tags to overcome the memory issue but I would need to apply it to bigger datasets in the future).</p>

<ol>
<li><p>Should I use tf-idf as a preprocessing step for the LDA? Or if I filter the ""useless"" tags before it doesn't make sense? I think I don't understand what is going on exactly in the LDA.</p>

<pre><code>dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
lda = ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, alpha = 0.1, num_topics=8)
corpus_lda = lda[corpus_tfidf]
</code></pre></li>
<li><p>Does it make sense to validate the topics quality with the LSI? As I understand the LSI is a method for dimensionality reduction, so I use it to apply K-Means and to see if the 8 clusters of the topics actually look like clusters. But to be honest I don't really understand what exactly I am visualising.</p>

<pre><code>lsi = models.LsiModel(corpus_lda, id2word=dictionary, num_topics=2)
lsi_coord = ""file.csv""
fcoords = codecs.open(lsi_coord,'w','utf-8')
for vector in lsi[corpus_lda]:
    if len(vector) != 2:
    continue
    fcoords.writelines(""%6.12f\t%6.12f\n"" % (vector[0][1],vector[1][1]))
fcoords.close()
num_topics = 8
X = np.loadtxt(lsi_coord, delimiter=""\t"")
my_kmeans = KMeans(num_topics).fit(X)
k_means_labels = my_kmeans.labels_
k_means_cluster_centers = my_kmeans.cluster_centers_
colors = ['b','g','r','c','m','y','k','greenyellow']
for k, col in zip(range(num_topics), colors):
my_members = k_means_labels == k
plt.scatter(X[my_members, 0], X[my_members, 1], s=30, c=colors[k], zorder=10)
cluster_center = k_means_cluster_centers[k]
plt.scatter(cluster_center[0], cluster_center[1], marker='x', s=30, linewidths=3, color='r', zorder=10)
plt.title('K-means clustering')
plt.show()
</code></pre></li>
<li><p>Memory issues. I am trying to create a matrix which has values for every unique term. So if the term is not in the document it gets zero. So it is a sparse matrix, because I have around 1300 unique terms and every document has about 5. And the memory issue arise at the converting to np.array. I guess I have to optimize the matrix somehow.</p>

<pre><code> # creating term-by-document matrix
Y = []
for z in corpus_lda:
    Y1=[]
    temp_dict={}
    for g in z:
        temp_dict.update({g[0]:g[1]})
     counter=0
     while counter &lt; len(dictionary.keys()):
        if counter in temp_dict.keys():
            Y1.append(temp_dict[counter])
        else:
            Y1.append(0)
        counter+=1
    Y.append(Y1)
Y = np.array(Y)
</code></pre></li>
</ol>

<p>The following code I took from here : <a href=""https://stackoverflow.com/questions/6645895/calculating-the-percentage-of-variance-measure-for-k-means"">Calculating the percentage of variance measure for k-means?</a></p>

<pre><code>    K = range(1,30) # amount of clusters 
    KM = [kmeans(Y,k) for k in K] 
    KM = []
    for k in K:
        KM_result = kmeans(Y,k)
        KM.append(KM_result)

    centroids = [cent for (cent,var) in KM]

    scipy.spatial.distance import cdist
    D_k = [cdist(Y, cent, 'euclidean') for cent in centroids] 
    cIdx = [np.argmin(D,axis=1) for D in D_k]
    dist = [np.min(D,axis=1) for D in D_k]
    avgWithinSS = [sum(d)/Y.shape[0] for d in dist]  
    kIdx = 8

    # elbow curve
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(K, avgWithinSS, 'b*-')
    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')
    plt.grid(True)
    plt.xlabel('Number of clusters')
    plt.ylabel('Average within-cluster sum of squares')
    plt.title('Elbow for KMeans clustering')
</code></pre>

<p>Any ideas for any of the questions are highly appreciated!</p>
",2017-05-23 10:29:31,2016-02-19 09:57:58,is it necessary/appropriate to calculate tf-idf as a preprocessing for LDA (Gensim)?,<python><tf-idf><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7885,35609171,2016-02-24 17:36:39,,"<p>Today I just started writing an script which trains LDA models on large corpora (minimum 30M sentences) using gensim library.
Here is the current code that I am using:</p>

<pre><code>from gensim import corpora, models, similarities, matutils

def train_model(fname):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    dictionary = corpora.Dictionary(line.lower().split() for line in open(fname))
    print ""DOC2BOW""
    corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]

    print ""running LDA""
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>running this script on a small corpus (2M sentences) I realized that it needs about 7GB of RAM.
And when I try to run it on the larger corpora, it fails because of the memory issue.
The problem is obviously due to the fact that I am loading the corpus using this command:</p>

<pre><code>corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]
</code></pre>

<p>But, I think there is no other way because I would need it for calling the LdaModel() method:</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>I searched for a solution to this problem but I could not find anything helpful.
I would imagine that it should be a common problem since we mostly train the models on very large corpora (usually wikipedia documents). So, it should be already a solution for it.</p>

<p>Any ideas about this issue and the solution for it?</p>
",2016-02-24 18:53:38,2019-12-31 20:10:36,Memory efficient LDA training using gensim library,<lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
7902,35591567,2016-02-24 01:22:29,,"<p>I believe my question is easy, but I'm very new to python and I think that is blinding me a bit.</p>

<p>I've downloaded a Wikipedia dump as explained under ""Preparing the Corpus"" here: <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">https://radimrehurek.com/gensim/wiki.html</a>.  Then I ran the following lines of code:</p>

<pre><code>import gensim

# these next two lines take around 16 hours
wikiDocs = gensim.corpora.wikicorpus.WikiCorpus('enwiki-latest-pages-articles.xml.bz2')
gensim.corpora.MmCorpus.serialize('wiki_en_vocab200k', wikiDocs)
</code></pre>

<p>These lines of code are taken from the link above.  Now, in a separate script I've done some text analysis.  The result of that text analysis is a number representing the index of a particular article in the wikiDocs corpus.  The problem, I don't know how to print out the text of that article.  The obvious thing to try is:</p>

<pre><code>wikiDocs[index_of_article]
</code></pre>

<p>but that returns the error</p>

<pre><code>TypeError: 'WikiCorpus' object does not support indexing
</code></pre>

<p>I've tried a few other things but I'm stuck.  Thanks for any help.</p>
",2016-02-24 01:37:59,2016-02-24 02:35:06,Print Wikipedia Article Title from Gensim WikiCorpus,<python><nlp><wikipedia><gensim><text-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
7904,35596031,2016-02-24 07:39:48,,"<p>After training a word2vec model using python <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>, how do you find the number of words in the model's vocabulary?</p>
",2019-02-26 18:28:30,2019-02-26 18:28:30,gensim word2vec: Find number of words in vocabulary,<python><neural-network><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7921,35681403,2016-02-28 10:35:06,,"<p>I am trying to find out new concepts in a Corpus from Konkani language.
I had trained two models on 1) a domain specific corpus 2) on newspaper corpus.</p>

<p>I have used Gensim word2vec to train the model however I am unable to get the terms of similar meaning on close proximity in vector space.</p>

<p>The closes words show no relation of being synonym with each other. Their similarity is as good as just some random words.</p>

<p>What am i doing wrong?</p>
",2017-03-21 21:25:35,2017-03-21 21:25:35,Finding concepts from a large corpus using Word embeddings,<gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
7937,35716121,2016-03-01 06:30:21,,"<p>For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using <strong>Phrases</strong> model in gensim, I tried below code but it's not giving me desired output.</p>

<p><strong>My code</strong></p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram[sent])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
</code></pre>

<p><strong>But it should come as</strong> </p>

<pre><code>[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?</p>

<pre><code>print bigram.vocab

defaultdict(&lt;type 'int'&gt;, {'useful': 1, 'was_there': 1, 'learning_can': 1, 'learning': 1, 'of_new': 1, 'can_be': 1, 'mayor': 1, 'there': 1, 'machine': 1, 'new': 1, 'was': 1, 'useful_sometimes': 1, 'be': 1, 'mayor_of': 1, 'york_was': 1, 'york': 1, 'machine_learning': 1, 'the_mayor': 1, 'new_york': 1, 'of': 1, 'sometimes': 1, 'can': 1, 'be_useful': 1, 'the': 1}) 
</code></pre>
",,2016-03-02 13:39:57,How to extract phrases from corpus using gensim,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7954,35738391,2016-03-02 04:15:29,,"<p>I am using HDP (Hierarchical Dirichilet Process) package from gensim topic modelling software. Gensim HDP implementation expects user to provide number of topics (T) in advance.</p>

<p><code>hdpmodel.HdpModel(self, corpus, id2word,T=150)
</code></p>

<p>The documentation defines T as top level truncation level. </p>

<p>Can HDP determine number of topics on its own?
Is there an implementation of HDP which can detect number of topics? Any help is appreciated.</p>
",2016-03-02 04:28:02,2016-03-02 13:17:04,Can HDP (Hierarchical Dirichilet Process) detect the number of topics from the data?,<machine-learning><data-mining><gensim><topic-modeling><unsupervised-learning>,,,CC BY-SA 3.0,False,False,True,False,False
7956,35616088,2016-02-25 00:40:27,,"<p>I am looking at the <code>DeepDist</code> (<a href=""http://deepdist.com"" rel=""nofollow noreferrer"">link</a>) module and thinking to combine it with <code>Gensim</code>'s <code>Doc2Vec</code> API to train paragraph vectors on <code>PySpark</code>. The link actually provides with the following clean example for how to do it for <code>Gensim</code>'s <code>Word2Vec</code> model:</p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext

sc = SparkContext()
corpus = sc.textFile('enwiki').map(lambda s: s.split())

def gradient(model, sentences):  # executes on workers
    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn0, 'syn1': model.syn1 - syn1}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.syn1 += update['syn1']

with DeepDist(Word2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>To my understanding, <code>DeepDist</code> is distributing the work of gradient descent into workers in batches, and the recombining them and updating at master. If I replace <code>Word2Vec</code> with <code>Doc2Vec</code>, there should be the document vectors that are being trained with the word vectors.</p>

<p>So I looked into the source code of <code>gensim.models.doc2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py"" rel=""nofollow noreferrer"">link</a>). There are the following fields in the <code>Doc2Vec</code> model instance:</p>

<ol>
<li><code>model.syn0</code></li>
<li><code>model.syn0_lockf</code></li>
<li><code>model.docvecs.doctag_syn0</code></li>
<li><code>model.docvecs.doctag_syn0_lockf</code></li>
</ol>

<p>Comparing with the source code of <code>gensim.models.word2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">link</a>), the following fields went missing in <code>Doc2Vec</code> model:</p>

<ol start=""3"">
<li><code>model.syn1</code></li>
<li><code>model.syn1neg</code></li>
</ol>

<p>I think I do not touch the <code>lockf</code> vectors because they seem to be used after the training is done when new data points come in. Therefore my code should be something like </p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.doc2vec import Doc2Vec, LabeledSentence
from pyspark import SparkContext

sc = SparkContext()

# assume my dataset is in format 10-char-id followed by doc content
# 1 line per doc
corpus = sc.textFile('data_set').map(
    lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10])
)

def gradient(model, sentence):  # executes on workers
    syn0, doctag_syn0 = model.syn0.copy(), model.docvecs.doctag_syn0.copy()   # previous weights
    model.train(sentence)
    return {'syn0': model.syn0 - syn0, 'doctag_syn0': model.docvecs.doctag_syn0 - doctag_syn0}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.docvecs.doctag_syn0 += update['doctag_syn0']

with DeepDist(Doc2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>Am I missing anything important here? For example:</p>

<ol>
<li>Should I care about <code>model.syn1</code> at all? What do they mean after all?</li>
<li>Am I right that <code>model.*_lockf</code> is the locked matrices after training?</li>
<li>Is it ok that I use <code>lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10]</code> to parse my dataset, assuming I have each document in one line, prefixed by a 0-padded 10-digit id?</li>
</ol>

<p>Any suggestion/contribution are very appreciated. I will write up a blog post to summarize the result, mentioning contributors here, potentially to help others train Doc2Vec models on scaled distributed systems without spending much dev time trying to solve what I am solving now.</p>

<p>Thanks</p>

<hr>

<p>Update 06/13/2018</p>

<p>My apologies as I did not get to implement this. But there are better options nowaday, and <code>DeepDist</code> haven't been maintained for awhile now. Please read comment below.</p>

<p>If you insist on trying out my idea at the moment, be reminded you are proceeding with your own risk. Also, if someone knows that <code>DeepDist</code> still works, please report back in comments. It would help other readers. </p>
",2018-06-13 07:40:52,2019-03-03 20:34:51,Doc2Vec and PySpark: Gensim Doc2vec over DeepDist,<apache-spark><pyspark><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
7967,35778075,2016-03-03 16:46:19,,"<p>I use the Gensim package for topic modelling. The idea is to understand what are the topics in the flickr tags.
Till now I am using this code (document are tags):</p>

<pre><code>    texts = [[word for word in document.split("";"") if word not in stoplist] for document in documents]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, alpha = 0.1, num_topics=10)
    topic = []
    for f in lda.print_topics(num_topics=4, num_words=10):
        topic_number = f[0]
        keywords = f[1]
        keywords = keywords.split("" + "")
        keywords_update = {}
        for ii in keywords:
            ii = str(ii)
            keyword = ii[6:]
            probab = ii[0:5]
            probab = float(probab)
            if probab &gt; 0.02:
                keywords_update.update({keyword:probab})
        topic.append(keywords_update)
    print topic
</code></pre>

<p>So basically I train the LDA on all my documents and then print the 10 most probable words for every topic. Is it correct? Or do I have to train the data on some part of the documents and then use corpus_lda = lda[corpus] in order to apply the trained model on the unseen documents?
If the results are different every time I run the model, does it mean that the amount of the topics is not correct? What is the best way to evaluate the results?</p>
",,2017-05-29 18:11:11,LDA for tags (gensim),<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
7972,35721503,2016-03-01 11:20:22,,"<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an ""unknown"" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>

<p><strong>dict.txt</strong></p>

<pre><code>you
love
this
code
</code></pre>

<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>

<pre><code>[[0, 1, 2, 3, 6],
 [3, 5, 5, 1, 6],
 [0, 1, 3, 6, 6]]
</code></pre>

<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>

<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>
",2016-03-12 19:02:16,2016-11-25 15:52:52,Gensim word2vec on predefined dictionary and word-indices data,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
7990,35727272,2016-03-01 15:46:52,,"<p>I tried to install gensim for word2vec project and it stuck at the point like this:</p>

<pre><code>----------------------------------------
  Rolling back uninstall of scipy
Command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c ""import setuptools;__file__='/var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-build/scipy/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-IDV4D9-record/install-record.txt --single-version-externally-managed failed with error code 1 in /var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-build/scipy
Storing complete log in /Users/Myname/.pip/pip.log
</code></pre>

<p>And I did some research I realize it's sth wrong with the scipy. I tried to upgrade it (since I've installed long time ago) and it repetitively show this error. I tried ""sudo"" and install libblas-dev but still show this error.</p>

<pre><code>   ----------------------------------------
  Rolling back uninstall of scipy
Command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c ""import setuptools;__file__='/tmp/pip-build/scipy/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-7Z45Gz-record/install-record.txt --single-version-externally-managed failed with error code 1 in /tmp/pip-build/scipy
Storing complete log in /Users/MyName/.pip/pip.log
</code></pre>

<p>I guess it's the same core problem essentially and I stuck here for long time.</p>

<p>Could someone help me please?</p>
",,2016-03-01 15:46:52,"When upgrading scipy, I came across Storing complete log in pip.log",<python><scipy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8053,35900953,2016-03-09 19:33:30,,"<p>I'm trying to run the distributed LDA example as described here:</p>

<p><a href=""https://radimrehurek.com/gensim/dist_lda.html"" rel=""nofollow"">https://radimrehurek.com/gensim/dist_lda.html</a></p>

<p>I created the set of documents by following the tutorial here:</p>

<p><a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow"">https://radimrehurek.com/gensim/dist_lsi.html</a></p>

<p>by ""inflat[ing] the corpus to 1M documents, by repeating its documents over&amp;over"" as it suggests</p>

<p>I'm using python 3.3 and numpy 1.9.2
I keep getting the following error:</p>

<pre><code>Exception in thread oneway-call:
Traceback (most recent call last):
  File ""/usr/lib64/python3.3/threading.py"", line 901, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.3/site-packages/Pyro4/core.py"", line 1484, in run
    super(_OnewayCallThread, self).run()
  File ""/usr/lib64/python3.3/threading.py"", line 858, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/lda_worker.py"", line 71, in requestjob
    self.processjob(job)
  File ""/usr/lib64/python3.3/site-packages/gensim/utils.py"", line 98, in _synchronizer
    result = func(self, *args, **kwargs)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/lda_worker.py"", line 80, in processjob
    self.model.do_estep(job)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py"", line 480, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py"", line 423, in inference
    if doc and not isinstance(doc[0][0], six.integer_types):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>

<p>I ran the distributed lsi example and it ran fine, but for some reason I couldn't seem to get the lda to work. </p>

<p>I tried changing line 423 in /usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py to:</p>

<pre><code>if doc is not None and not isinstance(doc[0][0], six.integer_types):
</code></pre>

<p>The error went away, but I got a warning that </p>

<pre><code>FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.
</code></pre>

<p>Could someone explain what I've done wrong? Is my change to this file correct? Or should I be running LDA differently?</p>
",,2018-04-11 09:04:08,Trouble running Gensim LDA,<python><numpy><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8072,35914287,2016-03-10 10:48:09,,"<p>I use ANN to predict words from words. The input and output are all words vectors. I do not know how to get words from the output of ANN. By the way,  it's gensim I am using</p>
",2016-03-10 11:12:33,2016-08-01 09:48:57,word2vec how to get words from vectors?,<machine-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8074,35985851,2016-03-14 11:07:17,,"<p>I installed <a href=""http://radimrehurek.com/gensim"" rel=""nofollow"">gensim</a>, Python library.
I executed the command </p>

<pre><code>Import gensim
</code></pre>

<p>It executed without any error. 
Then I tried to import test from gensim using the command </p>

<pre><code>from gensim import test
</code></pre>

<p>and it showed the following error</p>

<blockquote>
  <p>Traceback (most recent call last):
    File """", line 1, in 
      from gensim import test
  ImportError: cannot import name 'test'</p>
</blockquote>

<p>Python site-packages had gensim folder in that. 
Any help would be highly appreciated. </p>
",2020-01-30 00:04:19,2020-01-30 00:04:19,'from gensim import test' is not importing successfully,<python><python-3.4><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
8075,35985951,2016-03-14 11:12:35,,"<p>I am using Gensim Library in python for using and training word2vector model. Recently, I was looking at initializing my model weights with some pre-trained word2vec model such as (GoogleNewDataset pretrained model). I have been struggling with it couple of weeks. Now, I just searched out that in gesim there is a function that can help me to initialize the weights of my model with pre-trained model weights. That is mentioned below:</p>

<pre><code>reset_from(other_model)

    Borrow shareable pre-built structures (like vocab) from the other_model. Useful if testing multiple models in parallel on the same corpus.
</code></pre>

<p>I don't know this function can do the same thing or not. Please help!!!</p>
",,2018-02-15 15:48:25,How to initialize a new word2vec model with pre-trained model weights?,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
8089,35917500,2016-03-10 13:10:30,,"<p>I'm playing around with Linan Qiu's <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">example</a> of a word2vec implementation (<a href=""https://github.com/linanqiu/word2vec-sentiments"" rel=""nofollow"">Github</a>), with the final goal to analyse a bunch of tweets. </p>

<p>The problem I'm facing is that I have no idea on how to extract positive/negative/polarity percentages from this implementation of word2vec. The code delivers an accuracy rate so I presume it must check the predicted value (POS/NEG) against the known value (in this case an entire .txt is filled with either POS or NEG). So my approach would be to get the predicted POS/NEG rating per document(in this case per review), then of course simply add those up (the number of ratings, I mean) and divide POS and NEG from it to get a percentage. This percentage would then cover all documents in that file. From this, the polarity could perhaps also be calculated but I'm trying to figure POS/NEG out first.</p>

<p>Would anyone have any idea on how to get those predicted ratings? Below is the post-vectorisation code, but it's rather similar (cough) to the standard used.</p>

<p>Thank you so much!</p>

<pre><code># gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

# numpy
import numpy

# shuffle
from random import shuffle

# logging
import logging
import os.path
import sys
import cPickle as pickle

# logres
from sklearn.linear_model import LogisticRegression

#commit

model = Doc2Vec.load('./imdb.d2v')

train_arrays = numpy.zeros((25000, 100))
train_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]
    train_labels[i] = 1
    train_labels[12500 + i] = 0



test_arrays = numpy.zeros((25000, 100))
test_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_test_pos = 'TEST_POS_' + str(i)
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]
    test_labels[i] = 1
    test_labels[12500 + i] = 0


classifier = LogisticRegression()
classifier.fit(train_arrays, train_labels)

print classifier.score(test_arrays, test_labels)
</code></pre>
",2016-03-10 13:51:35,2016-04-21 04:39:32,Getting positive/negative percentages from word2vec,<python><sentiment-analysis><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
8099,35981178,2016-03-14 06:49:59,,"<p>I know that word2vec in gensim can compute similarity between words. But now I want to compute word similarity using TF-IDF or LSA with <strong>gensim</strong>. How to do it? </p>

<p>note: Computing document similarity using LSA with gensim is easy: <a href=""http://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">http://radimrehurek.com/gensim/wiki.html</a></p>
",2016-03-14 10:31:31,2016-03-14 10:31:31,How to compute word similarity using TF-IDF or LSA with gensim?,<python><nlp><tf-idf><gensim><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
8111,36013137,2016-03-15 13:47:16,,"<p>My objective is to find a vector representation of phrases. Below is the code I have, that works partially for bigrams using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">Word2Vec</a> model provided by the <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow"">GenSim</a> library.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, window=4, sg=1, hs=1, negative=0, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>

<p>The problem is that the Word2Vec model is seemingly doing automatic pruning of some of the bigrams, i.e. <code>len(model.vocab.keys()) != len(bigrams.vocab.keys())</code>. I've tried adjusting various parameters such as <code>trim_rule</code>, <code>min_count</code>, but they don't seem to affect the pruning.</p>

<p>PS - I am aware that bigrams to look up need to be represented using underscore instead of space, i.e. proper way to call my function would be <code>bigram2vec(unigrams, 'this_report')</code></p>
",,2016-03-16 11:49:08,GenSim Word2Vec unexpectedly pruning,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8117,36013211,2016-03-15 13:50:36,,"<p>I read the Kaggles word2vec example from <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors"" rel=""nofollow"">https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors</a>
and I cant understand how come the models vocabulary length is different from the word vector length.</p>

<p>Doesnt every cell at a word vector represent the relation with other word from the vocabulary, so each word has a relation to each other word?
If not, so what does each cell at the word vector represent?</p>

<p>Really appreciate any help.</p>
",,2016-03-18 04:49:48,Why does word2vec vocabulary length is different from the word vector length,<machine-learning><text-classification><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8119,36014333,2016-03-15 14:37:41,,"<p>As I was trying to use Gensim to do some plain-text extraction from PDFs. However I encountered problems with using this library. </p>

<p>I followed the instructions on the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">website</a> and it seemed to work properly. </p>

<p>I also have downloaded a Python IDE called Pycharm. But then when I am trying to do the ""quick example"" from <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">here</a>. I got some errors in my Pycharm. The logging activation has no errors, but the line <code>from gensim import corpora, models, similarities</code> isn't working. The IDE hints me that the word <code>gensim</code> (and the other three words) is <code>unresolved reference</code>.</p>

<p>So I think that maybe I need to do something to try to link Gensim as a reference library. But I am a totally newbie to python so I hope that someone can tell me how to do it. Or someone has worked with Gensim may also help me with this problem. Any ideas? </p>

<p>By the way, I am using python3 for the project as well. </p>
",,2017-07-26 00:52:50,how to include gensim into pycharm,<python><python-3.x><ubuntu><pycharm><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8122,36113487,2016-03-20 12:09:37,,"<p>I have PC with NVIDIA gpu. I have installed OpenBLAS. I am trying to train word vectors using gensim's word2vec implementation. I have set number of workers =4. But when I run top command to see CPU usage. It is showing only 100%. Does it mean only one core is utilised? And my program does not show any speed-up.</p>

<p>My code snippet is:</p>

<pre><code>import gensim
import time
import numpy
class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
    #called when Word2Vec is called
    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()


sentences=MySentences(""/home/lalchand/NewdatasetforAssgn2/tfidf/spam"")


start = time.time()

model = gensim.models.Word2Vec(sentences, min_count=1,iter=5,workers=4)
print(model.syn0.shape)
</code></pre>
",2016-12-02 16:18:15,2016-12-02 16:18:28,word vectors using gensim's word2vec implementation and GPU does not show any speed-up,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8127,36032410,2016-03-16 10:04:50,,"<p>When I use PyCharm to install gensim package, the installment failed.
The error messages were listed below:</p>

<p><a href=""https://i.stack.imgur.com/CEaTN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CEaTN.png"" alt=""enter image description here""></a></p>

<pre><code>Collecting gensim
  Using cached gensim-0.12.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Requirement already satisfied (use --upgrade to upgrade): scipy&gt;=0.7.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
  Using cached smart_open-1.3.2-py2-none-any.whl
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.5.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from gensim)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from gensim)
Requirement already satisfied (use --upgrade to upgrade): boto&gt;=2.32 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from smart-open&gt;=1.2.1-&gt;gensim)
Requirement already satisfied (use --upgrade to upgrade): bz2file in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from smart-open&gt;=1.2.1-&gt;gensim)
Collecting requests==2.8.1 (from smart-open&gt;=1.2.1-&gt;gensim)
  Using cached requests-2.8.1-py2.py3-none-any.whl
Collecting httpretty==0.8.10 (from smart-open&gt;=1.2.1-&gt;gensim)
  Using cached httpretty-0.8.10-py2-none-any.whl
Installing collected packages: requests, httpretty, smart-open, gensim
  Found existing installation: requests 2.7.0
    Uninstalling requests-2.7.0:

You are using pip version 7.1.0, however version 8.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/basecommand.py"", line 223, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/commands/install.py"", line 299, in run
    root=options.root_path,
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_set.py"", line 640, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_install.py"", line 726, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_uninstall.py"", line 125, in remove
    renames(path, new_path)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/utils/__init__.py"", line 314, in renames
    shutil.move(old, new)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 303, in move
    os.unlink(src)
OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/requests-2.7.0.dist-info/DESCRIPTION.rst'
</code></pre>

<hr>

<p>And I also use pip to install, but also failed....
The error messages were listed below:</p>

<pre><code>chenguanyingdeMacBook-Pro:~ ChenGuanYing$ sudo pip install gensim
Password:
The directory '/Users/ChenGuanYing/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
You are using pip version 7.1.0, however version 8.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
The directory '/Users/ChenGuanYing/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting gensim
  Downloading gensim-0.12.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.5MB)
    100% || 2.5MB 178kB/s 
Requirement already satisfied (use --upgrade to upgrade): scipy&gt;=0.7.0 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
  Downloading smart_open-1.3.2-py2-none-any.whl
Collecting six&gt;=1.5.0 (from gensim)
  Downloading six-1.10.0-py2.py3-none-any.whl
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from gensim)
Collecting boto&gt;=2.32 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading boto-2.39.0-py2.py3-none-any.whl (1.3MB)
    100% || 1.3MB 312kB/s 
Collecting bz2file (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading bz2file-0.98.tar.gz
Collecting requests==2.8.1 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading requests-2.8.1-py2.py3-none-any.whl (497kB)
    100% || 499kB 948kB/s 
Collecting httpretty==0.8.10 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading httpretty-0.8.10-py2-none-any.whl
Installing collected packages: boto, bz2file, requests, httpretty, smart-open, six, gensim
  Running setup.py install for bz2file
  Found existing installation: requests 2.7.0
    Uninstalling requests-2.7.0:
      Successfully uninstalled requests-2.7.0
  Found existing installation: six 1.4.1
    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.
    Uninstalling six-1.4.1:
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/basecommand.py"", line 223, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/commands/install.py"", line 299, in run
    root=options.root_path,
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_set.py"", line 640, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_install.py"", line 726, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_uninstall.py"", line 125, in remove
    renames(path, new_path)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/utils/__init__.py"", line 314, in renames
    shutil.move(old, new)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 302, in move
    copy2(src, real_dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 131, in copy2
    copystat(src, dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 103, in copystat
    os.chflags(dst, st.st_flags)
OSError: [Errno 1] Operation not permitted: '/tmp/pip-vV05uu-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'
</code></pre>

<hr>

<p>Thank for your help, and sorry for that my English is very pool.</p>
",,2016-05-10 02:47:20,python - Can't install gensim in Mac OS,<python><macos><python-2.7><pycharm><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8131,36034454,2016-03-16 11:31:27,,"<p>I am using Word2vec through <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
&gt;&gt;&gt; king_vector = w2v['king']
&gt;&gt;&gt; numpy.linalg.norm(king_vector)
2.9022589
</code></pre>

<p>However, in the <a href=""https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213"" rel=""noreferrer""><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>

<pre><code>&gt;&gt;&gt; w2v.init_sims()
&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]
&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)
0.99999994
</code></pre>

<p>The larger vector is just a scaled up version of the unit vector:</p>

<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector
array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        ... (some lines omitted) ...
        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)
</code></pre>

<p>Given that word similarity comparisons in Word2Vec are done by <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>

<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>
",,2018-05-27 11:38:54,What meaning does the length of a Word2vec vector have?,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8141,36001230,2016-03-15 01:42:14,,"<p>I know that there exists already an implementation of PV-DBOW (paragraph vector) in python (gensim).
But I'm interested in knowing how to implement it myself.
The explanation from the <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">official paper</a> for PV-DBOW is as follows:</p>

<blockquote>
  <p>Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.</p>
</blockquote>

<p>According to the paper the word vectors are not stored
and PV-DBOW is said to work similar to skip gram in word2vec.</p>

<p>Skip-gram is explained in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a>.
In the skip gram model the word vectors are mapped to the hidden layer.
The matrix that performs this mapping is updated during the training.
In PV-DBOW the dimension of the hidden layer should be the dimension of one paragraph vector. When I want to multiply the word vector of a sampled example with the paragraph vector they should have the same size.
The original representation of a word is of size (vocabulary size x 1). Which mapping is performed to get the right size (paragraph dimension x 1)
in the hidden layer. And how is this mapping performed when the word vectors are not stored?
I assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a></p>
",2020-01-21 18:34:01,2020-01-21 18:34:01,doc2vec: How is PV-DBOW implemented,<machine-learning><nlp><neural-network><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
8143,36132650,2016-03-21 13:42:51,,"<p>I have a large corpus of texts represented as list of texts:</p>

<p>[text1, text2, ..., textn]</p>

<p>I also have a list of names of these texts:</p>

<p>[text1_name, text2_name, ..., textn_name]</p>

<p>How do I turn such data into Mallet Corpus?
So that it could be possible to use LDA from gensim?</p>

<pre><code>from gensim.models.ldamulticore import LdaMulticore
corpus = gensim.corpora.MalletCorpus( **what_should_stay_here??** )
lda = LdaMulticore(corpus, workers = -1)
</code></pre>
",2016-03-21 14:26:16,2016-03-21 14:26:16,How to transform list of texts into Mallet Corpus?,<python><nlp><lda><gensim><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
8170,36019864,2016-03-15 19:00:34,,"<p>sometimes half hour of running the following script I get a Segmentantion Fault error:
2016-02-09 21:01:21,256 : INFO : PROGRESS: at sentence #9130000, processed 201000982 words, keeping 85047862 word types
Segmentation fault</p>

<p>Im using Mint on a a VM (VMware workstation 12.0.1) using the word2vec version of gensim-0.12.3-py2.7-linux-x86_64.egg  (Python 2.7.6)</p>

<h1>coding: utf-8</h1>

<h1>In[1]:</h1>

<p>import os, nltk
import io
import gensim, logging
import nltk</p>

<p>logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</p>

<h1>In[2]:</h1>

<p>class MySentences(object):</p>

<pre><code>def __init__(self, dirname, encoding='iso-8859-1'):
    self.dirname = dirname
    self.encoding = encoding #added encoding parameter for non utf8 texts

def __iter__(self):
    for fname in os.listdir(self.dirname):
        sub_dir = os.path.join(self.dirname, fname)
        for fname in os.listdir(sub_dir):
            text_file = os.path.join(sub_dir, fname)
            for line in io.open(text_file, encoding=self.encoding):
                yield nltk.word_tokenize(line, language='english') #you can change tokenizer
</code></pre>

<h1>In[3]:</h1>

<p>sentences = MySentences('/home/arie/extracted')</p>

<h1>In[ ]:</h1>

<p>model = gensim.models.Word2Vec(sentences)</p>

<p>I just saw the memory monitor and it looks it will crash again any time: </p>

<p>Every 5,0s: free -m                                     Tue Mar 15 19:55:36 2016</p>

<pre><code>         total       used       free     shared    buffers     cached
</code></pre>

<p>Mem:          9837       7735       2102         15        141       1232
-/+ buffers/cache:       6360       3476
Swap:         2044          0       2044</p>

<p>Every 5,0s: free -m                                     Tue Mar 15 19:59:06 2016</p>

<pre><code>         total       used       free     shared    buffers     cached
</code></pre>

<p>Mem:          9837       8563       1274         14          1        108
-/+ buffers/cache:       8453       1384
Swap:         2044         12       2032</p>
",,2016-03-15 19:00:34,Segmentation Fault in Pythons Gensim,<python><linux><segmentation-fault><virtual-machine><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
8198,36160229,2016-03-22 16:39:52,,"<p>I have been trying to run the file blei_lda.py from chapter 4 in the book Building Machine Learning Systems with Python with no success. I am using Python 2.7 with Enthought Canopy GUI. Below is the actual file provided from the creators, but there are also multiple copies up on github.</p>

<p><a href=""https://github.com/luispedro/BuildingMachineLearningSystemsWithPython/blob/master/ch04/blei_lda.py"" rel=""nofollow"">github repository</a></p>

<p>The problem is I'm continually receiving this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
c:\users\matt\desktop\pythonprojects\pml\ch04\blei_lda.py in &lt;module&gt;()
    for ti in range(model.num_topics):
        words = model.show_topic(ti, 64)
 ------&gt;tf = sum(f for f, w in words)
        with open('topics.txt', 'w') as output:
        output.write('\n'.join('{}:{}'.format(w, int(1000. * f / tf)) for f, w in words))
        output.write(""\n\n\n"")

TypeError: unsupported operand type(s) for +: 'int' and 'unicode' 
</code></pre>

<p>I've tried to create a work around, but wasn't able to find anything that worked completely.</p>

<p>I've also searched all over the web and stack overflow for a solution, but it seems like I'm the only person who is having trouble running this file.</p>

<pre><code># This code is supporting material for the book
# Building Machine Learning Systems with Python
# by Willi Richert and Luis Pedro Coelho
# published by PACKT Publishing
#
# It is made available under the MIT License

from __future__ import print_function
from wordcloud import create_cloud
try:
    from gensim import corpora, models, matutils
except:
    print(""import gensim failed."")
    print()
    print(""Please install it"")
    raise

import matplotlib.pyplot as plt
import numpy as np
from os import path

NUM_TOPICS = 100

# Check that data exists
if not path.exists('./data/ap/ap.dat'):
    print('Error: Expected data to be present at data/ap/')
    print('Please cd into ./data &amp; run ./download_ap.sh')

# Load the data
corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt')

# Build the topic model
model = models.ldamodel.LdaModel(
    corpus, num_topics=NUM_TOPICS, id2word=corpus.id2word, alpha=None)

# Iterate over all the topics in the model
for ti in range(model.num_topics):
    words = model.show_topic(ti, 64)
    tf = sum(f for f, w in words)
    with open('topics.txt', 'w') as output:
        output.write('\n'.join('{}:{}'.format(w, int(1000. * f / tf)) for f, w in words))
        output.write(""\n\n\n"")

# We first identify the most discussed topic, i.e., the one with the
# highest total weight

topics = matutils.corpus2dense(model[corpus], num_terms=model.num_topics)
weight = topics.sum(1)
max_topic = weight.argmax()


# Get the top 64 words for this topic
# Without the argument, show_topic would return only 10 words
words = model.show_topic(max_topic, 64)

# This function will actually check for the presence of pytagcloud and is otherwise a no-op
create_cloud('cloud_blei_lda.png', words)

num_topics_used = [len(model[doc]) for doc in corpus]
fig,ax = plt.subplots()
ax.hist(num_topics_used, np.arange(42))
ax.set_ylabel('Nr of documents')
ax.set_xlabel('Nr of topics')
fig.tight_layout()
fig.savefig('Figure_04_01.png')


# Now, repeat the same exercise using alpha=1.0
# You can edit the constant below to play around with this parameter
ALPHA = 1.0

model1 = models.ldamodel.LdaModel(
    corpus, num_topics=NUM_TOPICS, id2word=corpus.id2word, alpha=ALPHA)
num_topics_used1 = [len(model1[doc]) for doc in corpus]

fig,ax = plt.subplots()
ax.hist([num_topics_used, num_topics_used1], np.arange(42))
ax.set_ylabel('Nr of documents')
ax.set_xlabel('Nr of topics')

# The coordinates below were fit by trial and error to look good
ax.text(9, 223, r'default alpha')
ax.text(26, 156, 'alpha=1.0')
fig.tight_layout()
fig.savefig('Figure_04_02.png')
</code></pre>
",2016-03-22 16:58:02,2016-12-27 17:31:23,Can someone explain the unsupported operand error I'm getting while running the file blei_lda.py from Building Machine Learning Systems with Python?,<python-2.7><machine-learning><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
8199,36160322,2016-03-22 16:43:32,,"<p>I am implementing the tutorial of gensim <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/</a> that includes the line 
    sentences = word2vec.Text8Corpus('/tmp/text8')
however when I run the program I get the error that text8 does not exist. Looking through the code I see that Text8Corpus is a method that accepts argument type object. The instructions indicate that it should be passed</p>

<p><a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow"">http://mattmahoney.net/dc/text8.zip</a></p>

<p>When I manually download this file and attempt to pass the resulting imbd uncompressed data set I am told that permissions denied. Does anyone have any insight into this problem ? Am I suppose to have downloaded the imdb dataset myself or was there suppose to be some pointers in the code that do it automatically ? </p>
",,2016-03-22 16:53:18,what is ('/tmp/text8') gensim,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8208,36256584,2016-03-28 05:35:38,,"<p>I am unable to install any python package through </p>

<pre><code>pip install &lt;name&gt;
</code></pre>

<p>it always throws the error </p>

<pre><code>Command python setup.py egg_info failed with error code 1 in /tmp/pip-build-V2srLa/Gensim
Storing debug log for failure in /home/&lt;username&gt;/.pip/pip.log
</code></pre>

<p>for other packages using <code>sudo apt-get install python-&lt;name&gt;</code> used to work but not for gensim</p>

<p>I have tried to insttall/unzip the package and go <code>python setup.py test
python setup.py install</code> it throws <code>pkg_resources.DistributionNotFound: The 'ruamel.yaml&gt;=0.10.7' distribution was not found and is required by the application
</code></p>

<p>I have tried to install 'ruamel.yaml>=0.10.7' and it still doesn't work </p>

<p>Any help why this is failing ? and how can I solve it ? </p>

<p><strong>Update:</strong> moving to root (sudo su) worked for me  </p>
",2016-03-30 09:10:58,2019-08-10 20:22:11,Can't install python package gensim ubuntu,<python><pip><gensim><ruamel.yaml>,,,CC BY-SA 3.0,False,False,True,False,False
8211,36263594,2016-03-28 13:41:49,,"<p>i am using gensim in ubuntu. version is 0.12.4. my word2vec model is not consistent. every time i build the model based on the same exact sentences and same parameter it still have different presentations of the words.</p>

<p>here is the code (that i stole from the initial post)</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04913874,  0.04574081, -0.07402877, -0.03270053,  0.06598952,
        0.04157289,  0.05075986,  0.01770534, -0.03796235,  0.04594197], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04907205,  0.04569579, -0.07379777, -0.03273782,  0.06579078,
        0.04167712,  0.05083019,  0.01780009, -0.0378389 ,  0.04578455], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04906179,  0.04569826, -0.07382379, -0.03274316,  0.06583244,
        0.04166647,  0.0508585 ,  0.01777468, -0.03784611,  0.04578935], dtype=float32)
</code></pre>

<p>I have also tried to set seed to some fixed int but this didnt seem to help. i also tried to reinstall gensim which also didnt help.</p>

<p>Any idea how to stabilize my model??</p>
",2016-03-28 15:17:46,2017-03-22 10:38:11,gensim word2vec giving inconsistent results,<python><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
8222,36223864,2016-03-25 16:25:17,,"<p>I'm using doc2vec with a corpus of about 1 million titles. To train the corpus, I'm using the following code: </p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=10, size=300, workers=4)
model.build_vocab(corpus)
for epoch in range(10):
    model.train(corpus)
</code></pre>

<p>Everything seems to train properly and I am able to infer a vector using titles.most_similar. </p>

<p>I encounter a problem, however, when I try to use the vectors. It seems as though some documents are missing from the final model! I.e.: </p>

<pre><code>model.docvecs['SENT_157000']
</code></pre>

<blockquote>
  <p>KeyError: 'SENT_157000'</p>
</blockquote>

<p>I checked the gensim forum and stackoverflow and the only suggestion I could find was to ensure that the min_count = 1. I did that but I'm still having this issue.</p>
",2016-03-25 16:26:08,2018-03-23 14:40:42,KeyError in Doc2Vec model even when min_count set to 1 during training,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8246,36250297,2016-03-27 17:20:41,,"<p>Given such a data frame, including the item and corresponding review texts:</p>

<pre><code>item_id          review_text
B2JLCNJF16       i was attracted to this...
B0009VEM4U       great snippers...
</code></pre>

<p>I want to map the top <code>5000</code> most frequent word in <code>review_text</code>, so the resulting data frame should be like:</p>

<pre><code>item_id            review_text
B2JLCNJF16         1  2  3  4  5...
B0009VEM4U         6... #as the word ""snippers""  is out of the top 5000 most frequent word
</code></pre>

<p>Or, a bag-of-word vector is highly preferred:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [1,1,1,1,1....]
B0009VEM4U         [0,0,0,0,0,1....] 
</code></pre>

<p>How can I do that? Thanks a lot!</p>

<p>EDIT:
I have tried @ayhan 's answer. Now I have successfully changed the review text to a <code>doc2bow</code> form:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [(123,2),(130,3),(159,1)...]
B0009VEM4U         [(3,2),(110,2),(121,5)...]
</code></pre>

<p>It denotes the word of ID <code>123</code> has occurred <code>2</code> times in that document. Now I'd like to  transfer it to a vector like:</p>

<pre><code>[0,0,0,.....,2,0,0,0,....,3,0,0,0,......1...]
        #123rd         130th        159th
</code></pre>

<p>Do you how to do that? Thank you in advance!</p>
",2016-04-03 16:26:24,2016-04-03 16:48:13,How to map the word in data frame to integer ID with python-pandas and gensim?,<python><pandas><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8250,36192132,2016-03-24 02:14:38,,"<p>I am working on a project where I need to apply topic modelling to a set of documents and I need to create a matrix :</p>

<p>DT , a D  T matrix, where D is the number of documents and T is the number of topics. DT(ij) contains the number of times a word in document Di  has been assigned to topic Tj.</p>

<p>So far I have followed this tut: <a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html</a></p>

<p>I am new to gensim  and so far I have
1. created a document list
2. preprocessed and tokenized the documents.
3. Used corpora.Dictionary() to create id-> term dictionary (id2word)
4. convert tokenized documents into a document-term matrix </p>

<p>generated an LDA model.
So now I get the topics.</p>

<p>How can I now get the matrix that I mentioned before.
I will be using this matrix to calculate similarity between 2 documents on topic t as :</p>

<p>sim(a,b) = 1- |DT(a,t) - DT(b, t)|</p>
",,2017-06-17 01:00:29,Gensim - LDA create a document- topic matrix,<python><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
8261,36360367,2016-04-01 15:39:02,,"<p>I am new to Doc2vec use. In case I could get some advice before I start on it, it will save a LOT of time.
My data is an stream of text data (such as tweets) continuously coming in time. For clustering these tweets, I was thinking of using doc2vec to reduce the text content into a fixed size vector and use that to compare between documents. 
So in this case, the text data is getting accumulated over time, can this be still used with Doc2Vec, I may have to learn the model again and again (may be!) or could I use some large corpus such as Wikipedia or a large newscorpus to train the Doc2Vec model.</p>

<p>Any suggestions will help!</p>

<p>Thanks in Advance.</p>
",,2016-04-05 00:37:56,Can doc2vec be used if my text data is incrementally increasing?,<twitter><text><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8290,36324016,2016-03-31 04:13:04,,"<p>I'm totally new in Python and gensim. I'm trying to use word2vec from gensim in Python 3.4 on windows7 (64).</p>

<pre><code>import csv
with open('Data.csv', 'r') as csvfile:
Word2VecTextTrain = csv.reader(csvfile, delimiter=' ')
   from gensim.models import Word2Vec
   model = Word2Vec( Word2VecTextTrain, size=100, window=3, min_count=5, workers=4)
</code></pre>

<p>""Data.csv"" contains 30k rows of texts. These texts are either a complete or incomplete sentences including up to 20 words. Some of them may contain ""/"" or numbers.</p>

<p>I'm facing this error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Home/PycharmProjects/Word2Vec Project/Word2Vec_2016_03_23"", line 26, in &lt;module&gt;
     model = Word2Vec( Word2VecTextTrain, size=100, window=5, min_count=5, workers=4)
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 431, in __init__
     self.build_vocab(sentences, trim_rule=trim_rule)
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 497, in build_vocab
     self.finalize_vocab()  # build tables &amp; arrays
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 625, in finalize_vocab
     self.reset_weights()
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 932, in reset_weights
     self.syn0[i] = self.seeded_vector(self.index2word[i] + str(self.seed))
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 946, in seeded_vector
     once = random.RandomState(uint32(self.hashfxn(seed_string)))
OverflowError: Python int too large to convert to C long

Process finished with exit code 1
</code></pre>

<p>I have no idea for the reason of this error. Any help is truly appreciated.</p>
",,2016-04-01 04:51:41,Error in performing Word2Vec in Python,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8293,36328261,2016-03-31 08:39:37,,"<p>I am getting the following error and am just not able to figure out why gensim cant be imported. I tried reimporting gensim again by creating virtual environment but that didnt work as well. 
I am new to python, please be generous.</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Tejasvi\workspace\major project\Tag Recommendation\test.py"", line 6, in &lt;module&gt;
import gensim
File ""C:\Python27\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python27\lib\site-packages\gensim\models\__init__.py"", line 18, in &lt;module&gt;
from . import wrappers
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\__init__.py"", line 5, in &lt;module&gt;
from .ldamallet import LdaMallet
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 40, in &lt;module&gt;
from smart_open import smart_open
File ""C:\Python27\lib\site-packages\smart_open\__init__.py"", line 1, in &lt;module&gt;
from .smart_open_lib import *
File ""C:\Python27\lib\site-packages\smart_open\smart_open_lib.py"", line 34, in &lt;module&gt;
from boto.compat import BytesIO, urlsplit, six
ImportError: cannot import name BytesIO
</code></pre>

<p>This is my code:</p>

<pre><code>import string
import re
import gensim
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
stop_words.update(['.', ',', '""', ""'"", '?', '!', ':', ';', '(', ')', '[',    ']',
'{', '}', 'lt', 'gt','xA', '/', 'lt'])

outFile = open('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'w')

with open('C:/Users/Tejasvi/Desktop/major/3/c#.txt') as f:
    for line in f:
        new_str = re.sub('[^a-zA-Z0-9\n\.]', ' ', line)
        new_string = ' '.join([w for w in new_str.split() if len(w)&gt;2])
        for c in string.punctuation:
            new_string=new_string.replace(c,"""")
            for w in new_string.split():
                if w.lower() not in stop_words:
                    outFile.write(w)
                    outFile.write("" "")
outFile.close()

from gensim import corpora
with open ('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'r') as f:
    for line in f:
        dictionary = corpora.Dictionary(line.strip().split())
</code></pre>

<p>here is the output of pip freeze</p>

<pre><code>-curses==2.2
alabaster==0.7.6
alembic==0.7.6
astroid==1.3.6
astropy==1.0.3
Babel==1.3
backports.datetime-timestamp==1.0.2.dev0
backports.functools-lru-cache==1.0.2.dev0
backports.method-request==1.0.1.dev0
backports.shutil-get-terminal-size==1.0.0
backports.ssl-match-hostname==3.4.0.2
Beaker==1.7.0
BeautifulSoup==3.2.1
beautifulsoup4==4.3.2
blinker==1.4.dev0
blosc==1.2.7
bloscpack==0.7.2
boto==2.24.0
Bottleneck==1.0.0
bz2file==0.98
CacheControl==0.11.5
cchardet==0.3.5
cdecimal==2.3
certifi==2015.4.28
cffi==1.1.2
chardet==2.3.0
colorama==0.3.3
configobj==5.0.6+xy.1
configparser==3.5.0b2
cov-core==1.15.0
coverage==3.7.1
cryptography==1.0.dev1
cssselect==0.9.1
cx-Freeze==4.3.4
cyordereddict==0.2.3.dev7
datrie==0.7.1.dev37
decorator==3.4.0
distlib==0.2.0
docutils==0.12
ecdsa==0.13.1.dev0
ed25519ll==0.6
enum34==1.0.4
faulthandler==2.4
formlayout==1.0.15
funcsigs==0.4
futures==3.0.3
gensim==0.12.4
gevent==1.0.2
gevent-websocket==0.9.5
GraphLab-Create==0.9.1
greenlet==0.4.7
grin==1.2.1+xy1
guidata==1.6.1
guiqwt==2.3.2
h5py==2.5.0
html5lib==0.99999
httpretty==0.8.10
idna==2.1.dev1
ipaddress==1.0.7
ipdb==0.8.1
ipdbplugin==1.4.2
ipython==2.4.1
jaraco.apt==1.0
jaraco.classes==1.2
jaraco.collections==1.1
jaraco.context==1.3
jaraco.functools==1.3
jaraco.structures==1.0
jaraco.text==1.4
jaraco.ui==1.3.1
jaraco.windows==3.4
jedi==0.9.0
Jinja2==2.7.3
keyring==5.3
lda==1.0.3
libnacl==1.4.3
librato-metrics==0.4.9
linecache2==1.0.0
lockfile==0.10.2.post7
logilab-common==0.63.2
lxml==3.4.4
mahotas==1.3.0
Mako==1.0.1
MarkupSafe==0.23
matplotlib==1.4.3
mixpanel-py==3.1.1
mock==1.0.1
modernize==0.4
more-itertools==2.3.dev0
ndg-httpsclient==0.4.0
netifaces==0.10.4
nltk==3.1
nose==1.3.7
nose-cov==1.6
nose-fixes==1.3
numexpr==2.4.3
numpy==1.10.4
numpydoc==0.6.dev0
oauthlib==0.7.3.dev0
objgraph==2.0.1.dev0
packaging==15.2
pandas==0.16.2
paramiko==1.15.2
pathlib==1.0.1
patsy==0.3.0
pbr==1.8.1
pep8==1.6.2
Pillow==2.8.2
ply==3.6
prettytable==0.7.2
psutil==1.1.3
psycopg2==2.6.1
py==1.4.30
py2exe==0.6.9
pyasn1==0.1.8
pyasn1-modules==0.0.6
PyAudio==0.2.8
pycparser==2.14
pycrypto==2.6.1
pyemf==2.0.0
pyflakes==0.9.2
Pygments==2.0.2
PyICU==1.9.2+xy.1
PyJWT==1.3.1.dev2
pylint==1.4.3
pyMinuit==1.2.1
PyOpenGL==3.1.0
PyOpenGL-accelerate==3.1.0
pyOpenSSL==0.15.1
pyparsing==2.0.3
PyQt4==4.11.3
pyreadline==2.0.6+xy.1
PyStemmer==1.3.0
python-dateutil==2.4.2
pytz==2015.4
pywin==0.3.1
pywin32==219
PyYAML==3.11
pyzmq==14.7.0
reportlab==3.2.0
requests==2.8.1
requests-oauthlib==0.5.0
rope==0.10.2
sampy==1.2.1
scandir==1.1.1.dev7
scikit-learn==0.17
scipy==0.15.1 
scp==0.10.2
singledispatch==3.4.0.3
six==1.9.0
smart-open==1.3.2
snowballstemmer==1.2.1.dev1
Sphinx==1.3.2
sphinx-rtd-theme==0.1.8
sphinxcontrib-plantuml==0.6
spyder==2.3.5.2
SQLAlchemy==1.0.6
statsmodels==0.6.1
stop-words==2015.2.23.1
tables==3.2.0
Tempita==0.5.2
textmining==1.0
tornado==3.2.1
traceback2==1.4.0
ujson==1.33
unittest2==1.0.1
urllib3==1.10.4
veusz==1.23.1
virtualenv==13.0.3
virtualenvwrapper-win==1.2.1
ViTables==2.2a1
wheel==0.24.0
Whoosh==2.7.0
wincertstore==0.2
wsaccel==0.6.2
wxPython==2.8.12.1
wxPython-common==2.8.12.1
yappi==0.94
yg.lockfile==2.0
zc.lockfile==1.1.0
</code></pre>

<p>I also checked if I had my own version of io.py but it doesn't exist.</p>
",2016-03-31 09:13:59,2016-03-31 17:12:22,ImportError: cannot import name BytesIO on eclipse,<python><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,True
8296,36371591,2016-04-02 09:37:58,,"<p>I'm using Window 10.1, python3.4. I installed the nltk, numpy, scipy and gensim using wheel files (url:<a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a>)</p>

<p><a href=""http://i.stack.imgur.com/fPNxd.jpg"" rel=""nofollow"">Screenshot of installed modules</a></p>

<p>when I'm running this statement</p>

<pre><code>from gensim import corpora, models, similarities
</code></pre>

<p>I'm getting this Import error:</p>

<p><code>Traceback (most recent call last):
  File ""C:\Python34\sample.py"", line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
  File ""C:\Python34\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File ""C:\Python34\lib\site-packages\gensim\matutils.py"", line 21, in &lt;module&gt;
    import scipy.linalg
  File ""C:\Python34\lib\site-packages\scipy\linalg\__init__.py"", line 174, in &lt;module&gt;
    from .misc import *
  File ""C:\Python34\lib\site-packages\scipy\linalg\misc.py"", line 5, in &lt;module&gt;
    from .blas import get_blas_funcs
  File ""C:\Python34\lib\site-packages\scipy\linalg\blas.py"", line 155, in &lt;module&gt;
    from scipy.linalg import _fblas
ImportError: DLL load failed: The specified module could not be found.</code></p>

<p>Please help, i really need this.</p>
",2016-04-02 17:08:09,2016-04-02 17:08:09,NLTK installation. ImportError: DLL load failed: The specified module could not be found,<python><installation><scipy><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
8305,36315770,2016-03-30 17:31:01,,"<p>May we use Word2Vec for implementing Parts of Speech(PoS) Tagging, or similar other problems of label sequencing. 
I feel we may address this problem from clustering, so I was curious. 
Generally any good lead or discussion would be nice, but a Python example preferably with Gensim learn may be great, as it may have a Word2Vec implementation.  </p>
",,2016-03-30 17:31:01,Word2Vec for PoS Tagging,<machine-learning><nlp><nltk><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
8315,36397798,2016-04-04 08:29:35,,"<p>I have some training sentences generally of warning nature.  Now my goal is to predict weather incoming sentence is a warning message or not. I have gone through <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">Sentiment Analysis Using Doc2Vec</a> but according to my understanding it have not considered newly arriving sentence to predict if its positive or negative. </p>

<p>According to my experience I found that the output vector in <code>gensim.doc2vec</code> for each sentence is dependent on other sentences as well, which means we can not directly use the model to generate vector for newly arriving sentence. Please anyone help me with this. Thanks.</p>
",,2016-04-05 21:33:51,Is it possible to use gensim doc2vec for classification,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8336,36509957,2016-04-08 21:55:54,,"<p>I am trying to experiment gensim doc2vec, by using following code. As far as  I understand from tutorials, it should work. However it gives <strong>AttributeError: 'list' object has no attribute 'words'.</strong></p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, Doc2Vec
document = LabeledSentence(words=['some', 'words', 'here'], tags=['SENT_1']) 
model = Doc2Vec(document, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>So what did I do wrong? Any help please. Thank you. I am using python 3.5 and gensim 0.12.4</p>
",2016-04-11 11:04:55,2016-04-14 08:23:41,Why Gensim doc2vec give AttributeError: 'list' object has no attribute 'words'?,<python-3.x><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8341,36535206,2016-04-10 20:30:12,,"<p>I was trying to build an entity resolution system, where my entities are,</p>

<pre><code>(i) General named entities, that is organization, person, location,date, time, money, and percent.
(ii) Some other entities like, product, title of person like president,ceo, etc. 
(iii) Corefererred entities like, pronoun, determiner phrase,synonym, string match, demonstrative noun phrase, alias, apposition. 
</code></pre>

<p>From various literature and other references, I have defined its scope as I would not consider the ambiguity of each of the entity beyond its entity category. That is, I am taking Oxford of Oxford University
as different from Oxford as place, as the previous one is the first word of an organization entity and second one is the entity of location. </p>

<p>My task is to construct one resolution algorithm, where I would extract 
and resolve the entities. </p>

<p>So, I am working out an entity extractor in the first place. 
In the second place, if I try to relate the coreferences as I found from 
various literatures like this <a href=""http://anthology.aclweb.org/J/J01/J01-4004.pdf"" rel=""nofollow"">seminal work</a>, they are trying to work out 
a decision tree based algorithm, with some features like, distance,
i-pronoun, j-pronoun, string match, definite noun
phrase, demonstrative noun phrase, number agreement feature,
semantic class agreement, gender agreement, both proper names, alias, apposition
etc. </p>

<p>The algorithm seems a nice one where enities are extracted with Hidden Markov Model(HMM).</p>

<p>I could work out one entity recognition system with HMM.
Now I am trying to work out a coreference as well as an entity
resolution system. I was trying to feel instead of using so many
features if I use an annotated corpus and train it directly with 
HMM based tagger, with a view to solve a relationship extraction like,</p>

<pre><code>*""Obama/PERS is/NA delivering/NA a/NA lecture/NA in/NA Washington/LOC, he/PPERS knew/NA it/NA was/NA going/NA to/NA be/NA
small/NA as/NA it/NA may/NA not/NA be/NA his/PoPERS speech/NA as/NA Mr. President/APPERS""

where, PERS-&gt; PERSON
       PPERS-&gt;PERSONAL PRONOUN TO PERSON
       PoPERS-&gt; POSSESSIVE PRONOUN TO PERSON
       APPERS-&gt; APPOSITIVE TO PERSON
       LOC-&gt; LOCATION
       NA-&gt; NOT AVAILABLE*
</code></pre>

<p>would I be wrong? I made an experiment with around 10,000 words. Early results seem
encouraging. With a support from one of my colleague I am trying to insert some
semantic information like,
PERSUSPOL, LOCCITUS, PoPERSM, etc. for PERSON OF US IN POLITICS, LOCATION CITY US, POSSESSIVE PERSON MALE, in the tagset to incorporate entity disambiguation at one go. My feeling relationship extraction would be much better now. 
Please see this new thought too. 
I got some good results with Naive Bayes classifier also where sentences
having predominately one set of keywords are marked as one class. </p>

<p>If any one may suggest any different approach, please feel free to suggest so.</p>

<p>I use Python2.x on MS-Windows and try to use libraries like NLTK, Scikit-learn, Gensim,
pandas, Numpy, Scipy etc. </p>

<p>Thanks in Advance.  </p>
",2016-04-12 17:47:28,2016-04-12 21:58:08,Name Entity Resolution Algorithm,<python><algorithm><machine-learning><nlp>,,,CC BY-SA 3.0,True,False,True,False,True
8361,36374414,2016-04-02 14:18:38,,"<p>Let me make my question clearer:</p>

<p>I am using python <code>gensim.models.Word2Vec</code> to train a word embedding model. Based on my understanding, the model training is in essence a machine learning issue---to train a neural network via a prediction task. For example, if I select parameters to train a skip-gram model, then the model is trained by predicting context words from target word. Once the model is well-trained, word vectors are just obtained from the model.</p>

<p>If my understanding is correct, so since in fact it is a machine learning process and the training goal is to perform well in the prediction task, there should be a loss function during training and the model is supposed to make the loss as low as possible. So, how to know the <strong>model loss value</strong> for a given set of parameters? Or is there <strong>any other metrics</strong> that we can know to <strong>understand the model itself</strong>?</p>

<p>Hope I have made my question clear. In a word, I <strong>don't</strong> want to evaluate the model by its outputs as in the Google test set <a href=""http://word2vec.googlecode.com/svn/trunk/questions-words.txt"" rel=""nofollow"">http://word2vec.googlecode.com/svn/trunk/questions-words.txt</a>, but I want to understand the model itself as a simple machine learning problem during its training process. Would this be possible?    </p>
",2016-04-03 15:27:59,2016-04-03 15:27:59,Python Word2Vec: understand the trained model itself in detail,<python><model><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8362,36491071,2016-04-08 03:35:44,,"<p>What is a text mining tool with some easy tutorials and active community? I found some popular but not sure which one to start with.  </p>
",,2016-04-08 09:19:31,"An easy tutorial for a tool that supports text classification, clustering and topic modeling",<weka><text-mining><gensim><topic-modeling><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
8377,36462394,2016-04-06 21:08:55,,"<p>I want to use gensim to convert Wikipedia dump to plain text using <code>python -m gensim.scripts.make_wiki</code> script.</p>

<p>I use it as :</p>

<pre><code>python -m gensim.scripts.make_wiki ./enwiki-latest-pages-articles.xml.bz2 ./results
</code></pre>

<p>gives me an error at the end:</p>

<pre><code>2016-04-06 20:43:46,471 : INFO : storing corpus in Matrix Market format to ./results/_bow.mm
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/scripts/make_wiki.py"", line 88, in &lt;module&gt;
    MmCorpus.serialize(outp + '_bow.mm', wiki, progress_cnt=10000) # another ~9h
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/corpora/indexedcorpus.py"", line 89, in serialize
    offsets = serializer.save_corpus(fname, corpus, id2word, progress_cnt=progress_cnt, metadata=metadata)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/corpora/mmcorpus.py"", line 49, in save_corpus
    return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/matutils.py"", line 486, in write_corpus
    mw = MmWriter(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/matutils.py"", line 436, in __init__
    self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 111, in smart_open
NotImplementedError: unknown file mode wb+
</code></pre>

<p>Does anybody know what is going on?</p>
",2016-04-07 01:48:40,2017-05-26 12:09:03,Convert Wikipedia dump to text using python -m gensim.scripts.make_wiki,<python><wikipedia><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8421,36578341,2016-04-12 15:54:13,,"<p>How to use similarities.Similarity in gensim</p>

<p>Because if I use similarities.MatrixSimilarity:</p>

<p><code>index = similarities.MatrixSimilarity(tfidf[corpus])
</code>
It just told me:</p>

<pre><code>C:\Users\Administrator\AppData\Local\Enthought\Canopy\User\lib\site- packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\similarities\docsim.pyc in __init__(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)
513                 raise ValueError(""cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)"")
514             logger.info(""creating matrix with %i documents and %i features"", corpus_len, num_features)
515             self.index = numpy.empty(shape=(corpus_len, num_features), dtype=dtype)
516             # iterate over corpus, populating the numpy index matrix with (normalized)
517             # document vectors
</code></pre>

<p>MyProgram:</p>

<p>It works okay when the input content is less than 20,000 lines, but when the lines go more than 20,000, it just cannot build an index for 'corpus_tfidf'.</p>

<pre><code>    # -*- coding: utf-8 -*-
    import logging,time
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    start=time.clock()

    def mmin(a,b):
        if a&gt;b:
            return b
        else:
            return a
    from gensim import corpora, models, similarities

    fsource_01='E:\\cm_test\\ptfidf_test\\dim_items_terms_pre_pre.csv'
    fsource_02=fsource_01

    fcontent='E:\\cm_test\\ptfidf_test\\'

    f0=open(fsource_01)
    lines=f0.readlines()
    terms_list=[]
    for line in lines:
        line=line.strip('\n') #
        terms_sline=line.split(',') # es:['48909,53517,116593,55095']-&gt;['48909','53517','116593','55095']
        terms_list.append(terms_sline) 
    f0.close()

    from collections import defaultdict
    frequency = defaultdict(int)
    for text in terms_list:
            cnt_single=defaultdict(int)
            for token in text:
                frequency[token] += 1

    terms_list = [[token for token in text if frequency[token] &gt; 1] for text in terms_list]

    terms_list_qc=[]
    for ttext in terms_list:
        cnt_single=defaultdict(int)
        terms_list_qc_item=[]
        for token in ttext:
            cnt_single[token]+=1
            if(cnt_single[token]&lt;=1):
                terms_list_qc_item.append(str(token))
        terms_list_qc.append(terms_list_qc_item)

    dictionary = corpora.Dictionary(terms_list)
    #dictionary.save(fcontent+'dim_items_terms.dict')

    corpus = [dictionary.doc2bow(text) for text in terms_list]
    #corpora.MmCorpus.serialize(fcontent+'dim_items_terms.mm', corpus)  
    end1=time.clock()   
    print ""01.  Time Cost for trim_items_terms_to_sparse_matrix: %f s"" % (end1-start)

    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    end2=time.clock()   
    print ""2.   Time Cost for bagofwords_to_tfidf: %f s"" % (end2-end1)

    index = similarities.MatrixSimilarity(tfidf[corpus_tfidf])
    #index.save(fcontent+'dim_items_terms_tfidf.index')
    f0=open(fsource_02)
    lines=f0.readlines()
    f1=open(fcontent+'out_recordid_tfidf.txt',""w"")
    f2=open(fcontent+'out_cosine_tfidf.txt',""w"")
    for line in lines:
        line=line.strip('\n') 
        doc = line
        vec_bow = dictionary.doc2bow(doc.split(','))
        vec_lsi = tfidf[vec_bow]
        sims = index[vec_lsi]

        sims = sorted(enumerate(sims), key=lambda item: -item[1])
        osize=mmin(len(sims),400)
        for i in range(osize):
            f1.write(str(sims[i][0]+1)+',')
            f2.write(str(""%.2f""%sims[i][1])+',')
        f1.write('\n')
        f2.write('\n')
    f0.close()
    f1.close()
    f2.close()

    end3=time.clock()   
    print ""3.   Time Cost for get_sim_itemsid_top_fh: %f s"" % (end3-end2)
</code></pre>
",2018-06-05 12:18:54,2018-06-05 12:18:54,How to use similarities.Similarity in gensim?,<python><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
8428,36653882,2016-04-15 17:56:37,,"<p>I am trying to find the most important words in a corpus based on their TF-IDF scores.</p>

<p>Been following along the example at <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/tut2.html</a>. Based on</p>

<pre><code>&gt;&gt;&gt; for doc in corpus_tfidf:
...     print(doc)
</code></pre>

<p>the TF-IDF score is getting updated in each iteration. For example,</p>

<ul>
<li>Word 0 (""<em>computer</em>"" based on <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/tut1.html</a>), has a TF-IDF score of 0.5773 (Doc #1), 0.4442 (Doc #2).</li>
<li>Word 10 (""<em>graph</em>"") has a TF-IDF score of 0.7071 (Doc #7), 0.5080 (Doc #8), 0.4588 (Doc #9)</li>
</ul>

<p>So here's how I am currently getting the final TF-IDF score for each word,</p>

<pre><code>tfidf = gensim.models.tfidfmodel.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
d = {}
for doc in corpus_tfidf:
    for id, value in doc:
        word = dictionary.get(id)
        d[word] = value
</code></pre>

<p>Is there a better way?</p>

<p>Thanks in advance.</p>
",,2016-05-03 05:52:54,Getting TF-IDF Scores Of Words Using Gensim,<python><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8486,36673316,2016-04-17 06:18:13,,"<p>I have been generating topics with <a href=""https://www.yelp.com/dataset_challenge"" rel=""nofollow noreferrer"">yelp data</a> set of customer reviews by using Latent Dirichlet allocation(LDA) in python(gensim package). While generating tokens, I am selecting only the words having length >= 3 from the reviews( By using <code>RegexpTokenizer</code>):</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w{3,}')
tokens = tokenizer.tokenize(review)
</code></pre>

<p>This will allow us to filter out the noisy words of length less than 3, while creating the corpus document. <br/></p>

<p>How will filtering out these words effect performance with the LDA algorithm?</p>
",2017-02-04 19:22:46,2019-04-12 23:37:18,Latent Dirichlet allocation(LDA) performance by limiting word size for Corpus Documents,<python><tokenize><lda><gensim><corpus>,,,CC BY-SA 3.0,True,False,True,False,False
8490,36763582,2016-04-21 08:10:31,,"<p>I'm looking for an efficient way of creating a similarity vector of a single sentence against a list of sentences.</p>

<p>The trivial way of doing that is by iterating over the list of sentences and detect similarity between the single sentence and each one of the sentences in the list. This solution is too slow and I'm looking for a faster way of doing that. </p>

<p>My final goal is to detect if there is a really similar sentence in the list of sentences to the one I'm checking, if so I'll go to next sentence.</p>

<p>My solution right now is:</p>

<pre><code>for single_sentence in list_of_sentences:
    similarity_score = word2vec.sentences_similarity(sentence2test, single_sentence)
    if similarity_score &gt;= similarity_th:
       ignore_sent_flag = True
       break 
list_of_sentences.append(sentence2test)
</code></pre>

<p>Iv'e tried to put 'list_of_sentences' in a dictionary/set but the improvement in terms of time is minor.</p>

<p>I came across <a href=""https://github.com/piskvorky/gensim/pull/617"" rel=""nofollow"">this</a> solution but it is based on a Linux only package so no relevant for me. </p>
",2016-04-21 08:29:00,2016-04-25 07:52:27,Find similarity between a sentence to a list of sentences,<python><nlp><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8499,36764191,2016-04-21 08:39:49,,"<p>In <code>pandas</code> or <code>numpy</code>, I can do the following to get one-hot vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; x = [0,2,1,4,3]
&gt;&gt;&gt; pd.get_dummies(x).values
array([[ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  1.,  0.]])

&gt;&gt;&gt; np.eye(len(set(x)))[x]
array([[ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  1.,  0.]])
</code></pre>

<p>From text, with <code>gensim</code>, I can do:</p>

<pre><code>&gt;&gt;&gt; from gensim.corpora import Dictionary
&gt;&gt;&gt; sent1 = 'this is a foo bar sentence .'.split()
&gt;&gt;&gt; sent2 = 'this is another foo bar sentence .'.split()
&gt;&gt;&gt; texts = [sent1, sent2]
&gt;&gt;&gt; vocab = Dictionary(texts)
&gt;&gt;&gt; [[vocab.token2id[word] for word in sent] for sent in texts]
[[3, 4, 0, 6, 1, 2, 5], [3, 4, 7, 6, 1, 2, 5]]
</code></pre>

<p>Then I'll have to do the same <code>pd.get_dummies</code> or <code>np.eyes</code> to get the one-hot vector but I get an error where there's one dimension missing from my one-hot vector I have 8 unique words but the one-hot vector lengths are only 7:</p>

<pre><code>&gt;&gt;&gt; [pd.get_dummies(sent).values for sent in texts_idx]
[array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.]]), array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.]])]
</code></pre>

<p>It seems like it's doing one-hot vector individually as it iterates through each sentence, instead of using the global vocabulary.</p>

<p>Using <code>np.eye</code>, I do get the right vectors:</p>

<pre><code>&gt;&gt;&gt; [np.eye(len(vocab))[sent] for sent in texts_idx]
[array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]), array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])]
</code></pre>

<p>Also, currently, I have to do several things from using <code>gensim.corpora.Dictionary</code> to converting the words to their ids then getting the one-hot vector.</p>

<p><strong>Are there other ways to achieve the same one-hot vector from texts?</strong></p>
",2016-04-21 08:46:40,2020-09-21 03:47:28,Extracting one-hot vector from text,<python><numpy><pandas><vector><nlp>,,,CC BY-SA 3.0,False,False,True,False,False
8501,36780138,2016-04-21 20:47:16,,"<p>So,I'm trying to learn and understand Doc2Vec. 
I'm following this <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">tutorial</a>. My input is a list of documents i.e list of lists of words. This is what my code looks like:  </p>

<pre><code>    input = [[""word1"",""word2"",...""wordn""],[""word1"",""word2"",...""wordn""],...] 

    documents = TaggedLineDocument(input)

    model = doc2vec.Doc2Vec(documents,size = 50, window = 10, min_count = 2, workers=2) 
</code></pre>

<p>But I am getting some unicode error(tried googling this error, but no good ):</p>

<pre><code>   TypeError('don\'t know how to handle uri %s' % repr(uri))
</code></pre>

<p>Can somebody please help me understand where i am going wrong ? Thank you ! </p>
",,2017-09-22 08:06:08,Doc2vec : TaggedLineDocument(),<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8515,36885223,2016-04-27 09:07:25,,"<p>I am trying to use gensim for topic classification. I already have all feature words from multiple documents in the following form:</p>

<pre><code>corpus = [['word1','word2',..],['A','B',...]] (python list of lists)
</code></pre>

<p>and also a term-frequency matrix in sparse form and a dict. </p>

<p>I was trying to train gensim LDA on this:</p>

<pre><code> lda_model = gensim.models.LdaModel(term_freq_matrix, num_topics=10, id2word=feature_names_dict, passes=4)
</code></pre>

<p>But I get the following error: </p>

<pre><code>  File ""/home/oliver/Environments/cmpdp/local/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 523, in &lt;genexpr&gt;
    corpus_words = sum(cnt for document in chunk for _, cnt in document)
ValueError: need more than 1 value to unpack
</code></pre>

<p>From this tutorial <a href=""http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow"">http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html</a>
it seems like it all looks okay, only the sparse matrix form looks a bit different:</p>

<p>My corpus:</p>

<pre><code> print(next(iter(term_freq_matrix)))
  (0, 12036)    1
  (0, 12406)    2
...
  (0, 3916) 1
  (0, 3157) 1
</code></pre>

<p>Tutorial corpus:
print(next(iter(mm_corpus)))</p>

<pre><code>[(24, 1.0), (38, 1.0), (53, 1.0), (103, 1.0), (111, 1.0), (213, 3.0), (237, 1.0), (242, 2.0)]
</code></pre>

<p>What do you think?</p>
",,2016-04-27 09:07:25,Create a gensim corpus from term-frequency matrix or from a collection of strings,<nlp><gensim><corpus><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8529,36757409,2016-04-21 00:13:12,,"<p>I'm doing a small experiment where I have 2000 tweets as my input document. I train word2vec on this input tweets and then find the top 10 most similar words to a particular word - <code>w1</code>.</p>

<p>My concern is if I run word2vec 10 times (with same parameters) and inspect the top 10 most similar words to <code>w1</code>, gives me the same set of words (weights are also the same). </p>

<p>Now AFAIK word2vec initializes random weights at the beginning so why it's giving me the same output at different runs?</p>
",,2016-04-21 06:24:59,"Word2Vec, most_similar(word1) returns same output on different runs",<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8540,36815038,2016-04-23 18:52:33,,"<p>I am having a ready to go word2vec model that I already trained. I have serialized it as a CSV file:</p>

<pre><code>word,  v0,     v1,     ..., vN
house, 0.1234, 0.4567, ..., 0.3461
car,   0.456,  0.677,  ..., 0.3461
</code></pre>

<p>What I'd like to know is how I can load that word vector model in <code>gensim</code> and use that to train a paragraph or doc2vec model.</p>

<p>This <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec tutorial</a> says I can load a model in form of a ""<code># C text format</code>"" but I have no idea what that actually means. What is ""C text format"" in the first place but more important: </p>

<ul>
<li>How can I load my word2vec model and use it for doc2vec training?</li>
</ul>

<p>How do I build the vocabulary from my word2vec model?</p>
",2016-04-23 19:54:15,2016-07-29 02:38:08,How to load pre-trained model with in gensim and train doc2vec with it?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8548,36790867,2016-04-22 10:08:03,,"<p>In the gensim's documentation <code>window</code> size is defined as,</p>

<blockquote>
  <p><em>window</em> is the maximum distance between the current and predicted word within a sentence.</p>
</blockquote>

<p>which should mean when looking at context it doesn't go beyond the sentence boundary. right?</p>

<p>What i did was i created a document with several thousand tweets and selected a word (<code>q1</code>) and then selected most similar words to <code>q1</code> (using <code>model.most_similar('q1')</code>). But then, if I randomly shuffle the tweets in the input document and then did the same experiment (without changing word2vec parameters) I got a different set most_similar words to <code>q1</code>.</p>

<p>Can't really understand why that happens if only it's gonna look at is sentence level information? can anyone explain this?</p>

<p><b> EDIT: added model parameters and a graph </b> </p>

<p>used model parameters:</p>

<pre><code>model1 = word2vec.Word2Vec(sents1 , size=100, window=5, min_count=5, iter=n_iter, sg=0)
</code></pre>

<p><strong>Graph</strong>: 
To draw the graph what i did was I ran word2vec with above parameters for the original document (D) and the shuffled document (D') and took the top 10 or 20 (two bars) <code>most_similar('q')</code> words to a specific query word <code>q</code>, and calculated the jaccard similarity score between the two sets of words when iter=1,10,100. </p>

<p>It seems as the no of iterations increase, lesser and lesser similar words between the two sets of words got from running word2vec on D and D'.</p>

<p>can't really understand why this is happening or what's going on?</p>

<p><a href=""https://i.stack.imgur.com/MOl6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MOl6R.png"" alt=""enter image description here""></a></p>
",2016-04-27 10:47:17,2016-04-27 10:47:17,Gensim Word2Vec changing the input sentence order?,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8558,36909819,2016-04-28 08:55:36,,"<p>I am using sklearn.cluster ""KMean"" with python. I am trying to cluster data and plot graph for vectors(size =109977)
Here is my code:</p>

<pre><code>model = gensim.models.Word2Vec.load(""../wordvectors_300dimeansion.model"")

vectors = model.syn0 #size 109977

n_clusters_kmeans = 20 # more for visualization 100 better for clustering
#kmeans = KMeans(init='k-means++', n_clusters=n_clusters_kmeans, n_init=10)
min_kmeans = MiniBatchKMeans(init='k-means++',    n_clusters=n_clusters_kmeans, n_init=10)

min_kmeans.fit(vectors)

X_reduced = TruncatedSVD(n_components=50,   random_state=0).fit_transform(vectors)
X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)


target = min_kmeans.labels_
fig = plt.figure(figsize=(10, 10))
ax = plt.axes(frameon=False)
plt.setp(ax, xticks=(), yticks=())
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9, wspace=0.0, hspace=0.0)
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=target, marker=""x"")
plt.show()
</code></pre>

<p>kMean computing pairwise distance means to create a matrix with pairwise distances of 109977X109977</p>

<p>And I think due to this I got ""out of memory"" error.</p>

<p>pl. suggest me some scalable k-means clustering algorithm for large dataset. </p>

<p>Is there any tool available for this purpose? which I can import in my program directly as I did for sklearn.cluster ""KMean"" and ""MiniBatchKMean"", so that I can directly import it in my program.</p>

<p>Thanks.</p>
",,2016-04-28 08:55:36,k-means and MiniBatchKMean in python is out of memory,<python><memory-management><cluster-analysis><k-means><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
8566,36913218,2016-04-28 11:26:14,,"<p>We're running LDA using gensim and we're getting some strange results for perplexity. We're finding that perplexity (and topic diff) both increase as the number of topics increases - we were expecting it to decline. We've tried lots of different number of topics 1,2,3,4,5,6,7,8,9,10,20,50,100. We've also played around with alpha (symmetric and auto) and keep getting the same results. </p>

<p>Our documents have 20+ words but most of them are 20-30. Are these documents too small for LDA to work?</p>

<p>Should we try to increase the amount of training data (we are running on 100k)? Or increase number of passes (but it looks like it has converged)?</p>

<p>Thanks</p>
",,2016-04-28 11:26:14,LDA with gensim - strange values for perplexity,<lda>,,,CC BY-SA 3.0,False,False,True,False,False
8598,36940334,2016-04-29 13:59:32,,"<p>I have installed the 2 python libraries:</p>

<ol>
<li><p>NumPy - 1.10.4</p></li>
<li><p>Scipy - 0.17.0</p></li>
</ol>

<p>which are required for the successful installation of gensim as stated in: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">https://radimrehurek.com/gensim/install.html</a>. I have used the wheel file from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a> for installation on a windows 7 64 bit machine with python 3.4. I am running into certain compatibility issues.</p>

<pre><code>   C:\Users\888537&gt;py -m pip install gensim-0.12.4-cp34-none-win_amd64.whl         
   Processing c:\users\888537\gensim-0.12.4-cp34-none-win_amd64.whl  
   Collecting numpy&gt;=1.3 (from gensim==0.12.4)                                         
   Using cached numpy-1.11.0-cp34-none-win_amd64.whl                                
   Collecting smart-open&gt;=1.2.1 (from gensim==0.12.4)                                
   Using cached smart_open-1.3.2.tar.gz                                              
   Complete output from command python setup.py egg_info:                          
   D:\Program Files\Python\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'install_requires'                                                   warnings.warn(msg)                                                            
   D:\Program Files\Python\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'test_suite'                                                         
   warnings.warn(msg)                                                            
   usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]                    
   or: -c --help [cmd1 cmd2 ...]                                                    
   or: -c --help-commands                                                          
   or: -c cmd --help                                                                                                                                            
   error: invalid command 'egg_info'                                                                                                                               ---------------------------------------- 
</code></pre>

<p>Error:Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\888537\AppData\Local\Temp\pip-build-7zxq63k_\smart-open\ </p>

<p>The same occurs during a pip installation:</p>

<pre><code>Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\888537\A
ppData\Local\Temp\pip-build-4o3kecba\gensim\
</code></pre>

<p>Is there a way out of this other than installation from the git repo? I was unsuccessful trying to import from the git repo too. :P</p>

<pre><code>C:\Users\888537&gt;py -m pip install -e git+ssh://git@github.com/piskvorky/gensim.g
it
--editable=git+ssh://git@github.com/piskvorky/gensim.git is not the right format
; it must have #egg=Package
</code></pre>
",2016-04-29 14:05:08,2016-05-02 13:32:27,Installation problems with Gensim library Python 3.4 : http://www.lfd.uci.edu/~gohlke/pythonlibs/,<python><git><numpy><scipy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8600,36958388,2016-04-30 18:05:48,,"<p>I would like to use genism doc2vec model for a classification task.
However, It seems like the gensim implementation of doc2vec requires to see all documents (train and test) to build the vocabulary before training the model. Otherwise, you get keyerror if you want to get document vector of a document that was not present when building the vocabulary. I wonder if my understanding is correct! In practice, one does not have access to the test data at the time of training.</p>

<p>Is there any way to update the vocabulary at the test time to be able to get document representation of test documents?</p>
",,2016-05-28 19:27:46,getting paragraph representation for unseen paragraphs in doc2vec,<classification><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8623,36900356,2016-04-27 20:35:46,,"<p>I thought this may have been discussed before, but somehow I couldn't find answers, so here it is.</p>

<p>Below are the topics generated using gensim lsi from some customer survey. My questions are:</p>

<ol>
<li>what does the minus and plus signs in front of the words mean?</li>
<li>here I generated 5 topics and I could have generated more. how do I determine what might be the optimal number of topics? for example, maybe statistically after the third topic everything else will just be trivial.</li>
</ol>

<p>Any suggestions are appreciated.</p>

<p>0.527*""interest"" + 0.475*""lower"" + 0.376*""rates"" + 0.338*""rate"" + 0.324*""good"" + 0.257*""service""
0.671*""good"" + 0.586*""service"" + -0.254*""interest"" + -0.251*""lower"" + -0.159*""rate"" + -0.150*""rates""
0.600*""great"" + 0.351*""easy"" + 0.337*""rewards"" + 0.242*""use"" + -0.167*""service"" + 0.160*""like""
-0.503*""rates"" + 0.499*""rate"" + -0.39*""great"" + 0.364*""high"" + -0.289*""lower"" + 0.167*""easy""
-0.608*""great"" + 0.362*""easy"" + -0.303*""rate"" + 0.275*""rates"" + 0.244*""use"" + -0.227*""high""</p>
",,2016-10-14 00:55:22,How to interpret gensim topics properly?,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8633,37049640,2016-05-05 11:38:23,,"<p>i want to plot my gensim-word2vec model in kind of a ""word-galaxy""(like here: <a href=""http://www.anthonygarvan.com/wordgalaxy/"" rel=""nofollow"">http://www.anthonygarvan.com/wordgalaxy/</a>) and flashing out a single dot by entering it's name in a search field and pressing a submit-button. I'm fairly new to all this python-stuff and so i actually don't understand the curdoc documentation or the example here: <a href=""https://github.com/bokeh/bokeh/tree/master/examples/app/movies"" rel=""nofollow"">https://github.com/bokeh/bokeh/tree/master/examples/app/movies</a>. This is my code:</p>

<pre><code>from bokeh.plotting import figure, output_file, show, ColumnDataSource
from bokeh.models.widgets import TextInput
from bokeh.models import HoverTool
from gensim.models import word2vec
from sklearn.manifold import TSNE

model = word2vec.Word2Vec.load_word2vec_format('GoT.model.vector', binary=True) #load the trained model. (Game of Thrones script)

ts = TSNE(2)
vectors, words, x, y = []
form word in model.vocab:
    vectors.append(model[word]) #append my vector to ""word""
    words.append(word) #append my word
reduced_vecs = ts.fit_transform(vectors)
for vec in reduced_vecs:
    x.append(vec[0])
    y.append(vec[1])

search_word=TextInput(title=""Search"")
source = ColumnDataSource(data = dict(x=x,y=y,words=words))
hover=HoverTool(tooltips=[(""word"", ""@words"")]

p = figure(plot_height=600, plot_width=800, title=""word2vec"", tools=[hover], logo=None)
p.circle('x','y', radius=0.1, source=source, line_color=None)

show(p)
output_file('plot.html', mode=""cdn"")
</code></pre>

<p>Can you help me with this? Thank you,
FFoDWindow</p>
",,2016-05-05 11:38:23,Pointing out a single dot with text input,<python><plot><bokeh><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
8636,37053011,2016-05-05 14:17:52,,"<p>In gensim word2vec, the input can be a list of sentences. However, in tensorflow word2vec, the input is a list of words (concatenate sentences together). Is there a way to separate the sentences when constructing {target word, context word} pairs? 
I am using the following code:
<a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/models/embedding/word2vec.py"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/models/embedding/word2vec.py</a></p>
",,2017-07-09 18:12:28,"In tensorflow, how to separate by sentences when running word2vec model?",<tensorflow><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8659,36989108,2016-05-02 18:25:11,,"<p>i've been experimenting with word2vec and gensim as its python implementation. Now i have to make my model accesible on a Website - so i need Flask. I defined a Form in forms.py like that:</p>

<pre><code>from wtforms Import Form, StringField, SubmitField, validators
class msForm(Form):
    ms_1 = StringField(label='Eingabe_1', default = 'king', validators=[validators.input_required()])
    ms_2 = StringField(label='Eingabe_2', default = 'man', validators=[validators.input_required()])
    ms_3 = StringField(label='Eingabe_3', default = 'queen', validators=[validators.input_required()])
    submit=SubmitField()
</code></pre>

<p>now my views.py looks like:</p>

<pre><code>from app import app
from .forms import msForm
from flask import render_template, flash, request
from gensim.models import word2vec

global model
model = word2vec.Word2Vec.load_word2vec_format('./app/static/GoT.model.vector', binary=True)
global form
form = msForm()

@app.route('/')
def index():
return render_template('my-form.html', form=form)

@app.route('/', methods=['POST'])
def msForm_post():
    text1 = form.ms_1.data
    text2 = form.ms_2.data
    text3 = form.ms_3.data      
    processed_text = model.most_similar(positive=[text3, text2], negative = [text1])        
    return processed_text[0][0]
</code></pre>

<p>When i execute my run.py, go to <a href=""http://localhost:5000/"" rel=""nofollow"">http://localhost:5000/</a>, change my Input and click the 'Submit' button, i only get the answer on my default-input. Why doesnt he send my Input?</p>

<p>Thanks for your help and sorry for my english,
FFoDWindow</p>
",,2018-08-29 19:22:13,Getting Data from wtforms,<python><input><flask><wtforms><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8677,37101671,2016-05-08 15:42:48,,"<p>I have this piece of code:</p>

<pre><code>import gensim
import random


file = open('../../../dataset/output/interaction_jobroles_titles_tags.txt')

read_data = file.read()

data = read_data.split('\n')

sentences = [line.split() for line in data]
print(len(sentences))
print(sentences[1])

model = gensim.models.Word2Vec(min_count=1, window=10, size=300, negative=5)
model.build_vocab(sentences)

for epoch in range(5):
    shuffled_sentences = random.shuffle(sentences)
    model.train(shuffled_sentences)
    print(epoch)
    print(model)

model.save(""../../../dataset/output/wordvectors_jobroles_titles_300d_10w_wordshuffling"" + '.model')
</code></pre>

<p>If I print a single sentence, then it output is something like this:</p>

<pre><code>['JO_3787672', 'JO_272304', 'JO_2027410', 'TI_2969041', 'TI_2509936', 'TA_954638', 'TA_4321623', 'TA_339347', 'TA_272304', 'TA_3017535', 'TA_494116', 'TA_798840']
</code></pre>

<p>What I need is to shuffle the words before training and then save the model. </p>

<p>I am not sure whether I am coding it in a right way. I end up with exception:</p>

<pre><code>Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 747, in job_producer
    for sent_idx, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/site-packages/gensim/utils.py"", line 668, in __iter__
    for document in self.corpus:
TypeError: 'NoneType' object is not iterable
</code></pre>

<p>I would like to ask you how can I shuffle words.</p>
",2016-05-08 16:51:29,2016-05-09 08:34:03,How to shuffle words in word2vec,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8679,37196520,2016-05-12 20:10:03,,"<p>I have some sample sentences that I want to run through a Doc2Vec model. My end goal is a matrix of size (num_sentences, num_features). </p>

<p>I'm using the Gensim package.</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
# warning: long sample of data. It's just 40 sentences really though.
labeled_sents = [TaggedDocument(words=['u0644', 'u0646', 'u062f', 'u0646', 'u060c', 'u0628', 'u0631', 'u0637', 'u0627', 'u0646', 'u06cc', 'u06c1', 'u06a9', 'u0627'], tags='400'), TaggedDocument(words=['do', 'pan', 'en', '1713', 'o', 'soar', 'onde', 'se', 'sit', 'xfaa'], tags='401'), TaggedDocument(words=['u0420', 'u044c', 'u043e', 'u043d', 'u0442', 'u0433', 'u0435', 'u043d', '1901', 'xa0', 'u2022', 'u041b', 'u043e', 'u0440', 'u0435', 'u043d', 'u0446', 'xa0', 'u0417', 'u0435', 'u0435', 'u043c', 'u0430', 'u043d', '1902', 'xa0', 'u2022', 'u0411', 'u0435', 'u043a', 'u0435', 'u0440', 'u0435', 'u043b', 'xa0', 'u041f', 'u0438', 'u0435', 'u0440', 'u041a', 'u044e', 'u0440', 'u0438', 'xa0', 'u041c', 'u0430', 'u0440', 'u0438', 'u044f', 'u041a', 'u044e', 'u0440', 'u0438', '1903', 'xa0', 'u2022', 'u0420', 'u0435', 'u043b', 'u0435', 'u0439', '1904', 'xa0', 'u2022', 'u041b', 'u0435', 'u043d', 'u0430', 'u0440', 'u0434', '1905', 'xa0', 'u2022', 'u0414', 'u0436', 'u0414', 'u0436', 'u0422', 'u043e', 'u043c', 'u0441', 'u044a', 'u043d', '1906', 'xa0', 'u2022', 'u041c', 'u0430', 'u0439', 'u043a', 'u0435', 'u043b', 'u0441', 'u044a', 'u043d', '1907', 'xa0', 'u2022', 'u041b', 'u0438', 'u043f', 'u043c', 'u0430', 'u043d', '1908', 'xa0', 'u2022', 'u041c', 'u0430', 'u0440', 'u043a', 'u043e', 'u043d', 'u0438', 'xa0', 'u0411', 'u0440', 'u0430', 'u0443', 'u043d', '1909', 'xa0', 'u2022', 'u0412', 'u0430', 'u043d', 'xa0', 'u0434', 'u0435', 'u0440', 'xa0', 'u0412', 'u0430', 'u0430', 'u043b', 'u0441', '1910', 'xa0', 'u2022', 'u0412', 'u0438', 'u043d', '1911', 'xa0', 'u2022', 'u0414', 'u0430', 'u043b', 'u0435', 'u043d', '1912', 'xa0', 'u2022', 'u041a', 'u0430', 'u043c', 'u0435', 'u0440', 'u043b', 'u0438', 'u043d', 'u0433', 'xa0', 'u041e', 'u043d', 'u0435', 'u0441', '1913', 'xa0', 'u2022', 'u0424', 'u043e', 'u043d', 'xa0', 'u041b', 'u0430', 'u0443', 'u0435', '1914', 'xa0', 'u2022', 'u0423', 'u0438', 'u043b', 'u044f', 'u043c', 'u041b', 'u0411', 'u0440', 'u0430', 'u0433', 'xa0', 'u0423', 'u0438', 'u043b', 'u044f', 'u043c', 'u0425', 'u0411', 'u0440', 'u0430', 'u0433', '1915', 'xa0', 'u2022', 'u0411', 'u0430', 'u0440', 'u043a', 'u043b', 'u0430', '1917', 'xa0', 'u2022', 'u041f', 'u043b', 'u0430', 'u043d', 'u043a', '1918', 'xa0', 'u2022', 'u0429', 'u0430', 'u0440', 'u043a', '1919'], tags='402'), TaggedDocument(words=['nagusia', 'da'], tags='403'), TaggedDocument(words=['sino', 'que', 'los', 'ciudadanos', 'pueden', 'elegir', 'detraer', 'un', 'porcentaje', 'de', 'sus', 'impuestos', 'para', 'esta', 'causa', '68', '69', 'un', 'sistema', 'similar', 'se', 'da', 'en', 'alemania', 'o', 'austria', 'aunque', 'all', 'xed', 'se', 'impone', 'un', 'impuesto', 'eclesi', 'xe1stico'], tags='404'), TaggedDocument(words=['1244', 'c', 'xfc'], tags='405'), TaggedDocument(words=['u062a', 'u063a', 'u064a', 'u064a', 'u0631', 'u0644', 'u0641', 'u0638', 'u0627', 'u0644', 'u0643', 'u0644', 'u0645', 'u0629', 'u060c', 'u0641', 'u0645', 'u062b', 'u0644', 'u0627', 'u064b', 'rat', 'u062a', 'u0644', 'u0641', 'u0638', 'u0631', 'u0627', 'u062a'], tags='406'), TaggedDocument(words=['d', 'xfcrziler'], tags='407'), TaggedDocument(words=['xung', 'quanh', 'u0111', 'xf3'], tags='408'), TaggedDocument(words=['oblika', 'u0161to'], tags='409'), TaggedDocument(words=['u0432', 'u0430', 'u043b', 'u044e', 'u0442', 'u043d', 'u043e', 'u0433', 'u043e', 'u0441', 'u043e', 'u044e', 'u0437', 'u0443'], tags='410'), TaggedDocument(words=['sacerdotal', 'es'], tags='411'), TaggedDocument(words=['natoque', 'nisi'], tags='412'), TaggedDocument(words=['u0631', 'u0627', 'u0645', 'u06cc', 'u200c', 'u062a', 'u0648', 'u0627', 'u0646', 'u062f', 'u0631', 'u0627', 'u06cc', 'u0627', 'u0644', 'u0627', 'u062a', 'u0645', 'u062a', 'u062d', 'u062f', 'u0647', 'u0622', 'u0645', 'u0631', 'u06cc', 'u06a9', 'u0627', 'u06a9', 'u0627', 'u0646', 'u0627', 'u062f', 'u0627', 'u0628', 'u0631', 'u0632', 'u06cc', 'u0644', 'u0648', 'u0622', 'u0631', 'u0698', 'u0627', 'u0646', 'u062a', 'u06cc', 'u0646'], tags='413'), TaggedDocument(words=['u0423', 'u0439', 'u0433', 'u0443', 'u0440', 'u0441', 'u044c', 'u043a', 'u0430', 'u043c', 'u043e', 'u0432', 'u0430'], tags='414'), TaggedDocument(words=['termin', 'poznat', 'kao'], tags='415'), TaggedDocument(words=['les', 'fr', 'xe8res', 'lumi', 'xe8re'], tags='416'), TaggedDocument(words=['26', 'u03c0', 'u03b5', 'u03c1', 'u03af', 'u03c0', 'u03bf', 'u03c5', 'u03b1', 'u03b9', 'u03ce', 'u03bd', 'u03b5', 'u03c2', 'u03b7', 'u03c0', 'u03cc', 'u03bb', 'u03b7', 'u03c4', 'u03b7', 'u03c2', 'u0391', 'u03c5', 'u03bb', 'u03ce', 'u03bd', 'u03b1', 'u03c2', 'u03b5', 'u03af', 'u03bd', 'u03b1', 'u03b9', 'u03c3', 'u03ae', 'u03bc', 'u03b5', 'u03c1', 'u03b1'], tags='417'), TaggedDocument(words=['xcen', '13'], tags='418'), TaggedDocument(words=['acts', 'of', 'civil', 'disobedience', 'forced', 'the', 'head', 'of', 'the', 'local'], tags='419'), TaggedDocument(words=['hugo', 'az', 'xe1llamcs', 'xedny'], tags='420'), TaggedDocument(words=['f', 'xf8rste', 'nu', 'uofficielle', 'vers', 'forbindes', 'ofte', 'med', 'nynazistiske', 'synspunkter'], tags='421'), TaggedDocument(words=['gisulti', 'kanila', 'sa', 'mga', 'langyaw', 'nagtuong', 'gipangutana', 'sila', 'kon'], tags='422'), TaggedDocument(words=['u043d', 'u0430', 'u0438', 'u0432', 'u0440', 'u0438', 'u0442'], tags='423'), TaggedDocument(words=['its', 'influence'], tags='424'), TaggedDocument(words=['a', 'b', 'azerbaijan', 'homeowners', 'evicted', 'for', 'city'], tags='425'), TaggedDocument(words=['dinast', 'xeda', 'lunar', 'de'], tags='426'), TaggedDocument(words=['2', 'wyznawa', 'u0142o', 'judaizmu', '5', 'ponad'], tags='427'), TaggedDocument(words=['quyosh', 'vaqt', 'degani'], tags='428'), TaggedDocument(words=['u306e', 'u884c', 'u4fe1', 'u30fb', 'u91cd', 'u5f18', 'u3001', 'u9678', 'u5965', 'u56fd', 'u306e', 'u821e', 'u8349', 'u6d3e', 'u3001', 'u51fa', 'u7fbd', 'u56fd', 'u306e', 'u6708', 'u5c71', 'u6d3e', 'u3001', 'u4f2f', 'u8006', 'u56fd', 'u306e', 'u5b89', 'u92fc', 'u6d3e', 'u3001', 'u5099', 'u4e2d', 'u56fd', 'u306e', 'u53e4', 'u9752', 'u6c5f', 'u6d3e', 'u306e', 'u5b88', 'u6b21', 'u30fb', 'u6052', 'u6b21', 'u30fb', 'u5eb7', 'u6b21', 'u30fb', 'u8c9e', 'u6b21', 'u30fb', 'u52a9', 'u6b21', 'u30fb', 'u5bb6', 'u6b21', 'u30fb', 'u6b63', 'u6052', 'u3001', 'u8c4a', 'u5f8c', 'u56fd', 'u306e', 'u5b9a', 'u79c0', 'u6d3e', 'u3001', 'u85a9', 'u6469', 'u56fd', 'u306e', 'u53e4', 'u6ce2', 'u5e73', 'u6d3e', 'u306e', 'u884c', 'u5b89', 'u306a', 'u3069', 'u304c', 'u5b58', 'u5728', 'u3059', 'u308b', '7', '8', '9'], tags='429'), TaggedDocument(words=['p', 'xe5', '4'], tags='430'), TaggedDocument(words=['editovat'], tags='431'), TaggedDocument(words=['u0437', 'u0437', 'u0430', 'u0431', 'u043e', 'u0439', 'u0441', 'u0442', 'u0432', 'u0430', 'u043c', 'u0443'], tags='432'), TaggedDocument(words=['10', 'u043b', 'u0438', 'u043f', 'u043d', 'u044f', '1943', 'u0440', 'u043e', 'u043a', 'u0443', 'u0441', 'u043e', 'u044e', 'u0437', 'u043d', 'u0438', 'u043a', 'u0438', 'u0432', 'u0438', 'u0441', 'u0430', 'u0434', 'u0438', 'u043b', 'u0438', 'u0441', 'u044f', 'u0432', 'u0421', 'u0438', 'u0446', 'u0438', 'u043b', 'u0456', 'u0457', 'u0406', 'u0442', 'u0430', 'u043b', 'u0456', 'u0439', 'u0441', 'u044c', 'u043a', 'u0456'], tags='433'), TaggedDocument(words=['136', 'selvom', 'det', 'egentligt', 'ligger', 'i', 'sundby', 'p', 'xe5', 'lollandssiden', 'af', 'guldborgsund', 'centret', 'blev', 'grundlagt', 'i', '1989', 'da', 'byen', 'fejrede', '700', 'xe5rs', 'jubil', 'xe6um', 'bymuseet', 'rekonstruerede', 'som', 'de', 'f', 'xf8rste', 'i', 'verden', 'en', 'middelalderlig', 'kastemaskine', 'kaldet', 'en', 'blide'], tags='434'), TaggedDocument(words=['latine', 'redditur'], tags='435'), TaggedDocument(words=['ljubljani', 'in', 'njeni'], tags='436'), TaggedDocument(words=['u0442', 'u0430', 'u043d', 'u044b', 'u043c', 'u0430', 'u043b', 'u049b', 'u043e', 'u043d', 'u0430', 'u049b', 'u04af', 'u0439', 'u043b', 'u0435', 'u0440'], tags='437'), TaggedDocument(words=['u2022', 'hassib', 'ben'], tags='438'), TaggedDocument(words=['kurtulmu', 'u015f', 'olan', 'u0130talya'], tags='439')]

model = Doc2Vec(documents=labeled_sents, size=10, alpha=.035, window=4, 
    sample=1e-5, workers=4, min_count=1)
</code></pre>

<p>Now, I thought that <code>model.docvecs</code> would give me a list of arrays, with the first array corresponding to the vector for sentence 1, the second array corresponding to the vector for sentence 2, etc. But instead, it's got length 10! </p>

<p>I get <code>model.docvecs[0] = array([ 0.02312995, -0.00339695, -0.01273827,  0.01944644, -0.03247212, -0.04663946,  0.01369059,  0.03289782,  0.03516903, -0.03435936], dtype=float32)</code></p>

<p>What are these <code>docvecs</code> then? How do I get the output desired, which is a matrix of dimensions (40, 10) in this example?</p>

<hr>

<p>I saw this <a href=""https://stackoverflow.com/questions/31321209/doc2vec-how-to-get-document-vectors"">here</a>, and the correct answer says at the bottom ""where 99 is the document id whose vector we want."" So this makes me even more confused, as he seems to say that <code>model.docvecs</code> SHOULD be indexing a matrix where each row is a document vector!</p>
",2017-05-23 11:53:08,2017-01-18 19:47:34,Understanding the output of Doc2Vec from Gensim package,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8704,37089933,2016-05-07 14:50:12,,"<p>Im trying to install gensim using pip but i'm getting:</p>

<pre><code>""Could not import setuptools which is required to install from a source distribution.
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip/req/req_install.py"", line 377, in setup_py
import setuptools  # noqa
  File ""/Library/Python/2.7/site-packages/setuptools/__init__.py"", line 11, in &lt;module&gt;
from setuptools.extern.six.moves import filterfalse, map
  File ""/Library/Python/2.7/site-packages/setuptools/extern/__init__.py"", line 1, in &lt;module&gt;
from pkg_resources.extern import VendorImporter
ImportError: No module named extern""
</code></pre>

<p>Other posts have suggested running </p>

<pre><code> pip install -U setuptools
</code></pre>

<p>which reports to have successfully installed the package.</p>

<pre><code>Installing collected packages: setuptools
Successfully installed setuptools-21.0.0
</code></pre>

<p>However, running the gensim pip install command:</p>

<pre><code>pip install gensim
</code></pre>

<p>just gives the first error again.</p>

<p>any ideas why this might be happening?</p>
",2016-05-07 16:38:54,2017-05-09 07:13:31,"Error installing Gensim: ""Could not import setuptools which is required to install from a source distribution.""",<python><pip><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8714,37147955,2016-05-10 20:04:39,,"<p>I just updated my version of Xcode to 7.3. When I run <code>pip install --upgrade gensim</code> the process completed without any issues. However, when I try import gensim within the python shell the terminal bars a bunch of C++ output with a block of execution errors that begins with:</p>

<p><code>Exception: Compilation failed (return status=1): clang: error: unsupported option '-b mi2'. clang: error: unsupported option '-b mi'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-sse4a'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-tbm'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'....</code></p>

<p>I think this has something to do with where gensim is looking for its header files, but I'm somewhat at a loss. Any help debugging would be greatly appreciated.</p>
",,2016-05-10 21:42:39,import gensim fails since updating to Xcode 7.3,<python><c++><xcode><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8723,37190989,2016-05-12 15:12:23,,"<p>I am using gensim word2vec package in python. I know how to get the vocabulary from the trained model. But how to get the word count for each word in vocabulary?</p>
",,2018-11-09 12:35:49,How to get vocabulary word count from gensim word2vec?,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8803,37405958,2016-05-24 06:31:07,,"<p>I would like to find or to design a parser that can find the wikipedia XML Dump but I am unable to find any nor do I have any idea on how to do it. </p>

<p>I have tried using wikiextractor but unfortunately it does not extract articles based on the user defined categories but instead it extract articles that contain the keyword.</p>

<p>For example in the xml file that I have downloaded, the wikipedia article ""Anarchism"".This article can be found in the following categories: </p>

<pre><code>[[Category:Anarchism| ]]
[[Category:Political culture]]
[[Category:Political ideologies]]
[[Category:Social theories]]
[[Category:Anti-fascism]]
[[Category:Anti-capitalism]]
[[Category:Far-left politics]]
</code></pre>

<p>For example part of the xml for Anarchism:</p>

<pre><code> &lt;page&gt;
    &lt;title&gt;Anarchism&lt;/title&gt;
    &lt;ns&gt;0&lt;/ns&gt;
    &lt;id&gt;12&lt;/id&gt;
    &lt;revision&gt;
      &lt;id&gt;716551092&lt;/id&gt;
      &lt;parentid&gt;714146352&lt;/parentid&gt;
      &lt;timestamp&gt;2016-04-22T10:19:33Z&lt;/timestamp&gt;
      &lt;contributor&gt;
        &lt;ip&gt;85.193.216.88&lt;/ip&gt;
      &lt;/contributor&gt;
      &lt;comment&gt;a better word;  use [[plain English]] -&amp;gt; [[WP:MOS]]&lt;/comment&gt;
      &lt;model&gt;wikitext&lt;/model&gt;
      &lt;format&gt;text/x-wiki&lt;/format&gt;
      &lt;text xml:space=""preserve""&gt;{{Redirect2|Anarchist|Anarchists|the fictional character|Anarchist (comics)|other uses|Anarchists (disambiguation)}}
{{pp-move-indef}}
{{Use British English|date=January 2014}}
{{Anarchism sidebar}}
'''Anarchism''' is a [[political philosophy]] that advocates [[self-governance|self-governed]] societies based on voluntary institutions. These are often described as [[stateless society|stateless societies]],&amp;lt;ref&amp;gt;&amp;quot;ANARCHISM, a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man's natural social tendencies.&amp;quot; George Woodcock. &amp;quot;Anarchism&amp;quot; at The Encyclopedia of Philosophy&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;In a society developed on these lines, the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions.&amp;quot; [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin___Anarchism__from_the_Encyclopaedia_Britannica.html Peter Kropotkin. &amp;quot;Anarchism&amp;quot; from the Encyclopdia Britannica]&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;Anarchism.&amp;quot; The Shorter Routledge Encyclopedia of Philosophy. 2005. p. 14 &amp;quot;Anarchism is the view that a society without the state, or government, is both possible and desirable.&amp;quot;&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;Sheehan, Sean. Anarchism, London: Reaktion Books Ltd., 2004. p. 85&amp;lt;/ref&amp;gt; although several authors have defined them more specifically as institutions based on non-[[Hierarchy|hierarchical]] [[Free association (communism and anarchism)|free associations]].&amp;lt;ref&amp;gt;&amp;quot;as many anarchists have stressed, it is not government as such that they find objectionable, but the hierarchical forms of government associated with the nation state.&amp;quot; Judith Suissa. ''Anarchism and Education: a Philosophical Perspective''. Routledge. New York. 2006. p. 7&amp;lt;/ref&amp;gt;&amp;lt;ref name=&amp;quot;iaf-ifa.org&amp;quot;/&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;That is why Anarchy, when it works to destroy authority in all its aspects, when it demands the abrogation of laws and the abolition of the mechanism that serves to impose them, when it refuses all hierarchical organisation and preaches free agreement  at the same time strives to maintain and enlarge the precious kernel of social customs without which no human or animal society can exist.&amp;quot; [[Peter Kropotkin]]. [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin__Anarchism__its_philosophy_and_ideal.html Anarchism: its philosophy and ideal]&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;anarchists are opposed to irrational (e.g., illegitimate) authority, in other words, hierarchy  hierarchy being the institutionalisation of authority within a society.&amp;quot; [http://www.theanarchistlibrary.org/HTML/The_Anarchist_FAQ_Editorial_Collective__An_Anarchist_FAQ__03_17_.html#toc2 &amp;quot;B.1 Why are anarchists against authority and hierarchy?&amp;quot;] in [[An Anarchist FAQ]]&amp;lt;/ref&amp;gt; Anarchism considers the [[state (polity)|state]] to be undesirable, unnecessary, and harmful.&amp;lt;ref name=&amp;quot;definition&amp;quot;&amp;gt;
</code></pre>

<p>I would like to search the Wikipedia XML dump for all articles that is contained in a particular category like for example, <code>[[Category:Anti-fascism]]</code> and to generate the XML file. I will then clean the XML file so that I can train it using gensim word2vec model.</p>

<p>Please advice me on how to do it, I have only basic programming experience and I need to do this in Python. </p>

<p>Thank you</p>
",2016-05-24 08:00:02,2016-05-24 08:00:02,Searching for articles in Wikipedia XML Dump based on its Categories,<python><wikipedia><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8812,37350767,2016-05-20 15:49:17,,"<p>I am training my own word2vec model on Gensim in python, on a relatively small dataset. The data consist of about 3000 short-text entries from different people, most of which are two or three sentences. I know this is small for a word2vec dataset, but I've seen similar ones work in the past.</p>

<p>For some reason, when I train my model all of the features are impractically close to one another. For instance:</p>

<pre><code>model.most_similar('jesus/NN')
[(u'person/NN', 0.9999418258666992),
 (u'used/VB', 0.9998890161514282),
 (u'so/RB', 0.9998359680175781),
 (u'question/NN', 0.9997845888137817),
 (u'just/RB', 0.9996646642684937),
 (u'other/NN', 0.9995589256286621),
 (u'allow/VB', 0.9995476603507996),
 (u'feel/VB', 0.9995381236076355),
 (u'attend/VB', 0.9995047450065613),
 (u'make/VB', 0.9994802474975586)]
</code></pre>

<p>The parts of speech are included because I lemmatize the data.</p>

<p>Here is my training code:</p>

<pre><code>cleanedResponses = []
#For every response
for rawValue in df['QB17_W6'].values:
    lemValue = lemmatize(rawValue)
    cleanedResponses.append(lemValue)

df['cleaned_responses'] = cleanedResponses
bigram_transformer = Phrases(df.cleaned_responses.values)

model = Word2Vec(bigram_transformer[df.cleaned_responses.values], size=5)
</code></pre>

<p>This also happens when I train without the bigram transformer. Does anybody have an idea as to why the distances are so close?</p>
",,2016-05-20 15:49:17,Gensim Word2Vec distances are too close,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8819,37335842,2016-05-19 23:49:10,,"<p>I trained a gensim.models.doc2vec.Doc2Vec model<br>
d2v_model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)
and I can get document vectors by
docvec = d2v_model.docvecs[0]</p>

<p>How can I get word vectors from trained model ?</p>
",,2018-08-14 03:07:06,How to get word vectors from a gensim Doc2Vec?,<gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8852,37461117,2016-05-26 12:39:01,,"<p>I want to try to implement word2vec to Vietnamase language, but I'm confused about the pre-trained vectors, when I tried to use in the English language I use Google News-vectors-negative300.bin.gz (about 3.4GB) for pre-trained vectors and it works good. if i do with vietnam language should I make the data pre-trained vectors themselves ??
how to make a pre-trained vectors such as Google News-vectors-negative300.bin.gz, then I try to convert Google News-vectors-negative300.bin to text format the result as:</p>

<p>3000000 300</p>

<p> 0.001129 -0.000896 0.000319 0.001534 0.001106 -0.001404 -0.000031 -0.000420 -0.000576 0.001076 -0.001022 -0.000618 -0.000755 0.001404 -0.001640 -0.000633 0.001633 -0.001007 -0.001266 0.000652 -0.000416 -0.001076 0.001526 -0.000275 0.000140 0.001572 0.001358 -0.000832 -0.001404 0.001579 0.000254 -0.000732 -0.000105 -0.001167 0.001579</p>

<p>how to change a letter or word into the form above ??</p>
",,2016-08-15 10:03:26,how to make a pre-trained vectors for other language (word2vec)?,<c><python-2.7><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8889,37466345,2016-05-26 16:26:23,,"<p>I am trying to compute the probability of a document to belong to each topic found by the LDA model. I have succeded in producing the LDA but now I am stuck. My code goes as following:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

## Tokenizing
tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = stopwords.words('english')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

import json
import nltk
import re
import pandas

appended_data = []
for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)

appended_data = pandas.concat(appended_data)
doc_set = appended_data.body

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # add tokens to list
    texts.append(stopped_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=50)
</code></pre>

<p>I am trying to follow the method <a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi"">here</a> but I find it confusing. For example, when I try the following code:</p>

<pre><code># Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))

threshold = sum(scores)/len(scores)
print(threshold)

cluster1 = [j for i,j in zip(lda_corpus,doc_set) if i[0][1] &gt; threshold]

print(cluster1)
</code></pre>

<p>It seems it works, since it retrieves the the articles that belong to topic 1. Nevertheless, can someone explain what is the intuition behind and if there is other alternatives. For example, what is the intuition behind the threshold level here? Thanks</p>
",2017-05-23 12:17:08,2016-05-26 19:20:40,Assigning a topic to each document in a corpus (LDA),<python><pandas><lda>,,,CC BY-SA 3.0,True,False,True,False,False
8892,37593293,2016-06-02 13:28:33,,"<p>I want to calculate tf-idf from the documents below. I'm using python and pandas.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'docId': [1,2,3], 
               'sent': ['This is the first sentence','This is the second sentence', 'This is the third sentence']})
</code></pre>

<p>First, I thought I would need to get word_count for each row. So I wrote a simple function:</p>

<pre><code>def word_count(sent):
    word2cnt = dict()
    for word in sent.split():
        if word in word2cnt: word2cnt[word] += 1
        else: word2cnt[word] = 1
return word2cnt
</code></pre>

<p>And then, I applied it to each row.</p>

<pre><code>df['word_count'] = df['sent'].apply(word_count)
</code></pre>

<p>But now I'm lost. I know there's an easy method to calculate tf-idf if I use Graphlab, but I want to stick with an open source option. Both Sklearn and gensim look overwhelming. What's the simplest solution to get tf-idf?</p>
",2020-09-20 18:19:29,2020-09-20 18:19:29,How to get tfidf with pandas dataframe?,<python><pandas><scikit-learn><tf-idf><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
8896,37487504,2016-05-27 15:40:47,,"<p>I computed my LDA model, I retrieved my topics and now I am looking for the way to compute the weight/percentage of each topic on the corpus. Surprisingly I cannot find the way to do this, so far my code looks like:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

## Tokenizing
tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = stopwords.words('english')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

import json
import nltk
import re
import pandas

appended_data = []

#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # add tokens to list
    texts.append(stopped_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=50)
ldamodel.save(""model.lda0"")  
</code></pre>

<p>So far, what I have seen in other forums is to do the following:</p>

<pre><code>from itertools import chain
print(type(doc_set))
print(len(doc_set))

for top in ldamodel.print_topics():
  print(top)
print

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]
#print(lda_corpus)

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))
print(sum(scores))
print(len(scores))
threshold = sum(scores)/len(scores)
print(threshold)

cluster1 = [j for i,j in zip(lda_corpus,doc_set) if i[0][1] &gt; threshold]
cluster2 = [j for i,j in zip(lda_corpus,doc_set) if i[1][1] &gt; threshold]
cluster3 = [j for i,j in zip(lda_corpus,doc_set) if i[2][1] &gt; threshold] 
</code></pre>

<p>However I get the error in the cluster two: <code>IndexError: list index out of range</code>. Any idea why?</p>
",2019-01-21 22:16:17,2019-01-21 22:16:17,computing the weight of LDA topic for all the documents in the corpus,<python><lda><gensim><corpus>,,,CC BY-SA 4.0,True,False,True,False,False
8907,37539760,2016-05-31 07:43:15,,"<p>I was making a corpus using the command </p>

<pre><code>background_corpus = TextCorpus('wiki.en.text')
</code></pre>

<p>This is an over 10 GB file so while making this Corpus and it adding to a dictionary it gives this </p>

<pre><code>adding document #820000 to Dictionary(2000000 unique tokens: [u'tripolitan', u'ftdna', u'soestdijk', u'billycorgan', u'olmsville']...)

discarding 31072 tokens: [(u'vnsas', 1), (u'ezequeel', 1), (u'trapeztafel', 1), (u'pubsub', 1), (u'gyvenimas', 1), (u'gilibrand', 1), (u'catfaced', 1), (u'beuningan', 1), (u'moodadi', 1), (u'nocaster', 1)]...

keeping 2000000 tokens which were in no less than 0 and no more than 830000 (=100.0%) documents
</code></pre>

<p>Hence its discarding the new tokens as its maximum size is 2000000. Is there anyway I can not limit on the size of the dictionary?</p>
",,2017-05-10 11:44:27,How to increase Dictionary size in gensim while making Corpus?,<python><dictionary><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
8943,37570696,2016-06-01 13:50:52,,"<p>I can't seem to find it or probably my knowledge on statistics and its terms are the problem here but I want to achieve something similar to the graph found on the bottom page of the <a href=""http://pythonhosted.org/lda/getting_started.html"" rel=""noreferrer"" title=""LDA"">LDA lib from PyPI</a> and observe the uniformity/convergence of the lines. How can I achieve this with <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore"" rel=""noreferrer"" title=""Gensim LDA MultiCore"">Gensim LDA</a>?</p>
",,2017-08-19 20:48:27,How to monitor convergence of Gensim LDA model?,<python><lda><gensim><convergence>,,,CC BY-SA 3.0,False,False,True,False,False
8984,37745250,2016-06-10 09:55:13,,"<p>I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words).</p>

<p>For instance let us have:</p>

<pre><code>sentences=[['first','second','third','fourth']]*n
</code></pre>

<p>and I implement two identical models:</p>

<pre><code>model = gensim.models.Word2Vec(sententes, min_count=1,size=2)
model2=gensim.models.Word2Vec(sentences, min_count=1,size=2)
</code></pre>

<p>I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. </p>

<p>For instance, if n=100 I obtain</p>

<pre><code>print(model['first']==model2['first'])
True
</code></pre>

<p>while, for n=1000:</p>

<pre><code>print(model['first']==model2['first'])
False
</code></pre>

<p>How is it possible?</p>

<p>Thank you very much!</p>
",,2016-07-06 16:10:30,Different models with gensim Word2Vec on python,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8989,37763883,2016-06-11 12:45:03,,"<p>Is there a way to get the document vectors of unseen and seen documents from Doc2Vec in the gensim 0.11.1 version? </p>

<ul>
<li><p>For example, suppose I trained the model on 1000 thousand - Can I get
the doc vector for those 1000 docs?     </p></li>
<li><p>Is there a way to get document vectors of unseen documents composed<br>
from the same vocabulary?</p></li>
</ul>
",,2016-07-01 18:32:48,How to get the Document Vector from Doc2Vec in gensim 0.11.1?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
8997,37749777,2016-06-10 13:39:30,,"<p>I have anaconda and Pip installed. I tried doing</p>

<pre><code>conda install -c anaconda gensim-0.11.1
</code></pre>

<p>but it couldn't find the package and the following msg was thrown on the PowerShell.</p>

<pre><code>Using Anaconda Cloud api site https://api.anaconda.org
Fetching package metadata: ......
Solving package specifications: .
Error:  Package missing in current win-64 channels:
  - gensim-0.11.1

You can search for this package on anaconda.org with

    anaconda search -t conda gensim-0.11.1
</code></pre>

<p>Any help would be appreciated. Thanks!</p>

<p>--Conda works well with the machine but even help with Pip would be appreciated.</p>
",,2016-06-18 04:35:57,How to install Gensim version 0.11.1 on Windows 10 Machine?,<python><pip><conda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9009,37789724,2016-06-13 12:20:20,,"<p>Aim- User inputs a string. I need to compare this input with Sentence 1 and Sentence 2 and find the maximum similarity with either of these sentences.</p>

<p>Current Approach- I tokenize the input and both sentences, find synonym sets of each token and compare maximum similarity by adding similarity for each token using nltk .path_similarity(token1,token2).</p>

<p>Problem- If sentence 1 is short and sentence 2 is long with many tokens, since I sum up individual similarities, the similarity of sentence 2 with input is always more even if most of tokens of input match with sentence 1.</p>

<p>One solution- I can divide the similarity of each sentence with length of sentence and hence I get similarity per token of Sentence. But this approach is too aggressive. Is there an industry standard approach for this?</p>
",,2016-06-13 12:20:20,algorithm for sentence matching after calculating word similarity using nltk,<machine-learning><nlp><nltk><semantics><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
9010,37793118,2016-06-13 15:01:18,,"<p>I have downloaded pretrained glove vector file from the internet. It is a .txt file. I am unable to load and access it. It is easy to load and access a word vector binary file using gensim but I don't know how to do it when it is a text file format.</p>

<p>Thanks in advance</p>
",,2020-07-03 05:05:04,Load Pretrained glove vectors in python,<python-2.7><vector><nlp>,,,CC BY-SA 3.0,False,False,True,False,False
9018,37696459,2016-06-08 07:53:53,,"<p>I have 2 documents A-B (or 2 series of documents), and would like to get the 
a new document showing difference between the two document: A-B</p>

<p>By difference, there are several definitions, one is :
           List of words/""concept"" include in A but not in B.</p>

<p>I am thinking of using TF IDF for each sentence of A and B ,
such as :</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
d1 = [open(f1) for f1 in text_files]
tfidf = TfidfVectorizer().fit_transform(d1)
pairwise_similarity = tfidf * tfidf.T
</code></pre>

<p>I am not sure if this would be relevant to generate a new document C= ""A-B"",
especially am interested in ""semantic difference"" in the document C</p>
",2016-06-08 14:16:33,2016-06-08 14:16:33,NLP How to get the difference between 2 documents,<nlp><scikit-learn><stanford-nlp><gensim><spacy>,,,CC BY-SA 3.0,False,True,True,True,True
9056,37930925,2016-06-20 20:04:51,,"<p>I am using Gensim to train sentences with size 4 and I have 1192 unique words in training dataset. Number of words in model len(model.vocab) is 141 though that does not make sense. Is there any reason for seeing this? How I can change them model to have a key for every word in the training?
model = Word2Vec(windows,min_count=1)</p>
",,2017-05-20 21:23:56,number of vocabulary in gensim is much lower than the ones in training data,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9064,37935593,2016-06-21 04:26:33,,"<p>I am trying to implement LDA upon a set of tweets treated as a document. While preprocessing, in the stemming part it shows error as :
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)</p>

<p>My code is as shown below:</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim
import csv
import itertools

tokenizer = RegexpTokenizer(r'\w+')

en_stop = get_stop_words('en')

p_stemmer = PorterStemmer()

reader = csv.reader(open('/home/balki/Documents/Bangalore-13062016.csv', 'rU'), dialect=csv.excel_tab)

your_list = list(reader)
chain=itertools.chain(*your_list)
your_list2=list(chain)

texts = []

for i in your_list2:

    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    stopped_tokens = [i for i in tokens if not i in en_stop]


    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    print(stemmed_tokens)
</code></pre>

<p>Please suggest what should be done.</p>
",2016-06-21 05:05:32,2016-06-21 05:05:32,Stemming of Text using NLTK in Python,<python><json><lda>,,,CC BY-SA 3.0,True,False,True,False,False
9065,37935785,2016-06-21 04:47:58,,"<p>I am reading a paper about doc2vec. 
But I don't really get what is paragraph id and how it is trained...</p>

<p>I have tried to implement a sentiment analysis task with gensim package and succeeded, without knowing how exactly it works...</p>

<p>The paper said the Document vector is trained just like another word. But how is it processed? Is it trained at the same time with word2vec training? and how can it contain the message of the paragraph if it is treated as a word? 
And what is sentence label and one most confusing me is <strong>the matrix D</strong>...</p>

<p>Is there anybody can explain the process to me?
I got totally messed up... please help me...thx</p>
",2016-06-21 04:53:29,2016-06-21 04:53:29,"what is document vector, paraghaph id in Doc2Vec",<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9078,37894390,2016-06-18 07:19:39,,"<p>I'm using Spark's LDA implementation, as shown in example code <a href=""https://gist.github.com/jkbradley/ab8ae22a8282b2c8ce33"" rel=""nofollow noreferrer"">here</a>. I want to get consistent topics/topic distributions for my training data. I'm training on two machines and would like the output to be the same.</p>

<p>I understand that LDA uses a random component for training/inference, it's explained in this <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus/15069580#15069580"">SO post</a>. It looks like consistent results can be achieved in python gensim by setting the seed value manually. I've tried this in Spark but I am still getting slight variance in my outputted topic distributions.</p>

<pre><code>ldaParams: LDA = new LDA().setK(10).setMaxIterations(60).setSeed(10L)
val distributedLDAModel: DistributedLDAModel = ldaParams.run(corpusInfo.docTermVectors).asInstanceOf[DistributedLDAModel]
val topicDistributions: Map[Long,Vector] = distributedLDAModel.topicDistributions.collect.toMap //produces different results on each run
</code></pre>

<p>Is there a way I can get consistent topic distributions for my training set of data?</p>
",2017-05-23 10:29:01,2016-06-18 07:19:39,Spark LDA - Consistent Topic Distributions,<scala><apache-spark><apache-spark-mllib><lda>,,,CC BY-SA 3.0,False,False,True,False,False
9094,37861873,2016-06-16 14:18:52,,"<p>In <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow""><code>doc2vec</code> function</a>, there is a parameter called <code>size</code>.</p>

<p>I understand that, <code>size</code> is the dimension of output vector, and if <code>size=400</code> it will capture the content better than if <code>size=100</code>.</p>

<p>However, I do not understand, what does <code>size</code> stand for? Does it mean how far Doc2Vec will lookup from a word, to predict the next word? Or what does it mean?</p>

<p>Thanks a lot,</p>
",,2016-07-29 02:42:25,What does size parameter in gensim doc2vec represent,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9102,37818426,2016-06-14 17:22:31,,"<p>Using the <code>gensim.models.Word2Vec</code> library, you have the possibility to provide a model and a ""word"" for which you want to find the list of most similar words:</p>

<pre><code>model = gensim.models.Word2Vec.load_word2vec_format(model_file, binary=True)
model.most_similar(positive=[WORD], topn=N)
</code></pre>

<p>I wonder if there is a possibility to give the system as input the model and a ""vector"", and ask the system to return the top similar words (which their vectors is very close to the given vector). Something similar to:</p>

<pre><code>model.most_similar(positive=[VECTOR], topn=N)
</code></pre>

<p>I need this functionality for a bilingual setting, in which I have 2 models (English and German), as well as some English words for which I need to find their most similar German candidates.
What I want to do is to get the vector of each English word from the English model:</p>

<pre><code>model_EN = gensim.models.Word2Vec.load_word2vec_format(model_file_EN, binary=True)
vector_w_en=model_EN[WORD_EN]
</code></pre>

<p>and then query the German model with these vectors.</p>

<pre><code>model_DE = gensim.models.Word2Vec.load_word2vec_format(model_file_DE, binary=True)
model_DE.most_similar(positive=[vector_w_en], topn=N)
</code></pre>

<p>I have implemented this in C using the original distance function in the word2vec package. But, now I need it to be in python, in order to be able to integrate it with my other scripts.</p>

<p>Do you know if there is already a method in <code>gensim.models.Word2Vec</code> library or other similar libraries which does this? Do I need to implement it by myself?</p>
",2016-12-17 05:20:11,2018-12-19 15:58:57,"Get most similar words, given the vector of the word (not the word itself)",<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9133,38098824,2016-06-29 11:56:19,,"<p>I am taking different documents from a database and I check with LDA (gensim), what kind of latent topics are there in these documents. This works pretty well. What I would like to do is to save in the database for every document what is its most probable topic. And I am not sure what is the best solution for it. I could, for example, at the beginning extract a unique id of every document from the database together with the text_column and somehow process it that I know at the end which id belongs to which topic number. Or may be I should do it in the last part, where I print the documents and their topics. But I don't know how to connect it back to the database. By the comparison of the text_column with the document and assigning the corresponding topic number? Would be grateful for any comment.</p>

<pre><code>stop = stopwords.words('english')

sql = """"""SELECT text_column FROM table where NULLIF(text_column, '') IS NOT NULL;""""""
cur.execute(sql)
dbrows = cur.fetchall()
conn.commit()

documents = []
    for i in dbrows:
    documents = documents + list(i)

# remove all the words from the stoplist and tokenize
stoplist = stopwords.words('english')

additional_list = set(""``;''"".split("";""))

texts = [[word.lower() for word in document.split() if word.lower() not                 in stoplist and word not in string.punctuation and word.lower() not in additional_list] 
     for document in documents]

# remove words that appear less or equal of 2 times
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) &lt;= 2)
texts = [[word for word in text if word not in tokens_once]
     for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
my_num_topics = 10

# lda itself
lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=my_num_topics)
corpus_lda = lda[corpus]

# print the most contributing words for selected topics
for top in lda.show_topics(my_num_topics):
    print top

# print the most probable topic and the document
for l,t in izip(corpus_lda,documents):
    selected_topic = max(l,key=lambda item:item[1])
    if selected_topic[1] != 1/my_num_topics:
        selected_topic_number = selected_topic[0]
        print selected_topic
        print t
</code></pre>
",,2016-07-04 09:43:16,LDA gensim. How to update a Postgres database with the correct topic number for every document?,<python><postgresql><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9135,38054356,2016-06-27 12:45:42,,"<p>Using <code>gensim</code>, I want to calculate the similarity within a list of documents. This library is excellent at handling the amounts of data that I have got. The documents are all reduced to timestamps and I have got a function <code>time_similarity</code> to compare them. <code>gensim</code> however, uses the cosine similarity.</p>

<p>I am wondering if anyone has attemted this before or has a different solution.  </p>
",,2016-07-06 00:14:37,gensim: custom similarity measure,<python><time><similarity><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9157,38062337,2016-06-27 20:05:11,,"<p>I am working with the Doc2Vec and Word2Vec deep learning algorithms (<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec API  description from Gensim</a>). <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">More description here</a></p>

<p>Currently I am interested in using the <code>model.n_similarity(wordSet1, wordSet2)</code> method which basically computes the  cosine similarity between two sets of words.</p>

<p>I am interested in any ways of validating the models performance, not just on the <code>n_similiarity()</code> function, but overall how accurate or realistic results can the model provide. Since it performs deep learning, I do not know if there is any ways of knowing how well does it perform. </p>

<p>Are there any techniques that I should look up, then use or is there a data-set that has results and I should compare ?</p>

<p>Any suggestion is much appreciated. Thank you.</p>
",,2016-06-27 20:05:11,Is there any way to validate the performance of a Doc2Vec/ Word2Vec Deep Learning model?,<python><deep-learning><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9161,37818728,2016-06-14 17:39:48,,"<p>I've been working on latent semantic analysis (lsa) and applied this example: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow"">https://radimrehurek.com/gensim/tut2.html</a></p>

<p>It includes the terms clustering under topics but couldn't find anything how we can cluster documents under topics.</p>

<p>In that example, it says that 'It appears that according to LSI, trees, graph and minors are all related words (and contribute the most to the direction of the first topic), while the second topic practically concerns itself with all the other words. As expected, the first five documents are more strongly related to the second topic while the remaining four documents to the first topic'.</p>

<p>How can we relate those five documents with Python code to the related topic?</p>

<p>You can find my python code below. I would appreciate any help.</p>

<pre><code>from numpy import asarray
from gensim import corpora, models, similarities

#https://radimrehurek.com/gensim/tut2.html
documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)

texts = [[word for word in text if word not in tokens_once] for text in texts]

dictionary = corpora.Dictionary(texts)
corp = [dictionary.doc2bow(text) for text in texts]

tfidf = models.TfidfModel(corp) # step 1 -- initialize a model
corpus_tfidf = tfidf[corp]

# extract 400 LSI topics; use the default one-pass algorithm
lsi = models.lsimodel.LsiModel(corpus=corp, id2word=dictionary, num_topics=2)

corpus_lsi = lsi[corpus_tfidf]


#for i in range(0, lsi.num_topics-1):
for i in range(0, 3):
    print lsi.print_topics(i)

for doc in corpus_lsi: # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly
    print(doc)
</code></pre>
",,2017-04-21 07:00:20,How to cluster documents under topics using latent semantic analysis (lsa),<python><cluster-analysis><tf-idf><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
9162,37823014,2016-06-14 22:15:14,,"<p>I am trying to categorize the blog content using Topic Modeling. Using LDA transformation, I couldn't find the correlation b/w topics. Say, cricket is a sub topic of Sports topic. However, I come to know that it could be achieved using HLDA. Could some one help me how to implement the HLDA transformation in python gensim package?</p>
",,2016-07-26 18:43:49,How to implement hlda transformation to find correlation of topics in gensim?,<python><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
9182,38005590,2016-06-24 04:26:32,,"<p>I have installed Anacoda Python v2.7 and Gensim v 0.13.0</p>

<p>I am using Spyder as IDE</p>

<p>I have the following simple code:</p>

<pre><code> from gensim import corpora
</code></pre>

<hr>

<p>I got the following error:</p>

<pre><code>from gensim import corpora

  File ""gensim.py"", line 7, in &lt;module&gt;

ImportError: cannot import name corpora
</code></pre>

<p>I reinstalled:
- Gensim
- Scipy
- Numpy
but still have the same issue.</p>
",,2017-11-07 07:29:49,ImportError: cannot import name corpora with Gensim,<python-2.7><nltk><lda><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
9193,38088351,2016-06-29 00:23:06,,"<p>I want to do LDA topic analysis over a huge corpus. I have tried that in R and Python. Now I'd like to seed special words into the model. That means that I have some prior probability for some words associated with topics and I want seed these to the model. As I searched only, function LDA() in R and gensim.models.ldamodel.LdaModel() in python both have such paremters.</p>

<p>But I hardly saw any example codes using that thus I have no ideas about how to input my priors into the model. Can anyone provide some help? Thanks.</p>
",,2016-11-30 20:09:11,Seeding Words to LDA model in R/Python,<python><r><lda><seeding><text-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
9222,38200241,2016-07-05 09:50:01,,"<p>I already have a trained doc2vec model.</p>

<pre><code>model = Doc2Vec(size = 100, window = 10, min_count = 1, workers=4, hashfxn=hash32)
model.build_vocab(doc)
model.train(doc)
model.save(r'C:\Data\Model\hs8000_2016Q2.doc2vec') 
</code></pre>

<p>model only has n_similarity or most_similar as function to check similarity between sentences. However, I want to know top 50 Topics in the model. how do I extract Topics from the model?</p>

<p>Thank you!</p>
",2016-07-05 10:23:03,2016-07-05 10:23:03,Gensim: how to extract Topics from a trained Doc2Vec model in Gensim?,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9233,38208132,2016-07-05 16:15:05,,"<p>The above code is more or less what the Keras documentation gives us as a language model. The thing is that this language model predicts characters, not words. Strictly speaking, a language model is supposed to predict full words. </p>

<p>My question is, how do I change this in order to predict full words?</p>

<pre><code>from __future__ import print_function
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.utils.data_utils import get_file
import numpy as np
import random
import sys

path = ""C:/Users/Cedric     Oeldorf/Desktop/University/Research/Data/Gutenberg/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

chars = set(text)
print('total chars:', len(chars))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

# cut the text in semi-redundant sequences of maxlen characters
maxlen = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])
print('nb sequences:', len(sentences))

print('Vectorization...')
X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

from keras.callbacks import History
histLSTM = History()

# build the model: 2 stacked LSTM
print('Build model...')
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

model.fit(X, y, batch_size=128, nb_epoch=4, callbacks=[histLSTM])
</code></pre>

<p>My data preprocessing idea so far is:</p>

<pre><code>path = ""C:/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

#tokenize corpus and get list of unique words
tok = gensim.utils.simple_preprocess(text, deacc=False)
words = set(tok)
word_indices = dict((c, i) for i, c in enumerate(words))
indices_word = dict((i, c) for i, c in enumerate(words))

sentences1 = text.split('.')
SYMBOLS = '{}()[].,:;+-*/&amp;|&lt;&gt;=~$'
m = [item.translate(None, SYMBOLS).strip() for item in sentences1]
del text

maxlen = 60
step = 3
sentences = []
next_words = []
for i in range(0, len(tok) - maxlen, step):
    sentences.append(tok[i: i + maxlen])
    next_words.append(tok[i + maxlen])
print('nb sequences:', len(sentences))

X = np.zeros((len(sentences), maxlen), dtype=""int32"")
y = np.zeros((len(sentences),maxlen), dtype=""int32"")
</code></pre>

<p>This step isnt working out:</p>

<pre><code>#In X, change boolean to true for every listed character, same for y
for i, sentence in enumerate(sentences):
    for t, words in enumerate(sentence):
        X[i, t,] = word_indices[words]
    y[i, t] = word_indices[words]
</code></pre>

<p>And I don't know what input shape I should be using:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(len(sentences), maxlen)))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=True))
model.add(Dropout(0.2))
#model.add(Dense(len(chars)))
#Insert this instead:
model.add(TimeDistributedDense(len(words)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(X, y, batch_size=128, nb_epoch=2)
</code></pre>
",,2016-07-05 18:38:37,How do I change the Keras text generation example from being on character level to word level?,<python><nlp><keras><lstm><language-model>,,,CC BY-SA 3.0,False,False,True,False,False
9262,38245739,2016-07-07 12:41:04,,"<p>I used the <code>MySentences</code> class for extracting sentences from all files in a directory and use this sentences for train a <em>word2vec</em> model.
My dataset is unlabeled.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences('sentences')
model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>Now I want to use that class to make a <em>doc2vec</em> model. I read <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec</a> reference page. <code>Doc2Vec()</code> function gets sentences as parameter, but it doesn't accept above sentences variable and return error :</p>

<pre><code>AttributeError: 'list' object has no attribute 'words'
</code></pre>

<p>What is the problem? What is the correct type of that parameter?</p>

<p><strong>Update :</strong></p>

<p>I think, unlabeled data is the problem. It seems doc2vec needs labeled data.</p>
",2016-07-07 12:59:04,2018-10-16 18:48:56,Gensim Doc2Vec - Pass corpus sentences to Doc2Vec function,<python><text-mining><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9284,38324328,2016-07-12 09:01:53,,"<p>I have been reading more modern posts about sentiment classification (analysis) such as <a href=""http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis"" rel=""nofollow"">this</a>.</p>

<p>Taking the IMDB dataset as an example I find that I get a similar accuracy percentage using Doc2Vec (88%), <strong>however a far better result using a simple tfidf vectoriser with tri-grams for feature extraction (91%)</strong>. I think this is similar to Table 2 in <a href=""http://arxiv.org/pdf/1412.5335v7.pdf"" rel=""nofollow"">Mikolov's 2015 paper</a>.</p>

<p>I thought that by using a bigger data-set this would change. So I re-ran my experiment using a breakdown of 1mill training and 1 mill test from <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow"">here</a>. Unfortunately, in that case my tfidf vectoriser feature extraction method increased to 93% but doc2vec fell to 85%.</p>

<p><strong>I was wondering if this is to be expected and that others find tfidf to be superior to doc2vec even for a large corpus?</strong></p>

<p>My data-cleaning is simple:</p>

<pre><code>def clean_review(review):
    temp = BeautifulSoup(review, ""lxml"").get_text()
    punctuation = """""".,?!:;(){}[]""""""
    for char in punctuation
        temp = temp.replace(char, ' ' + char + ' ')
    words = "" "".join(temp.lower().split()) + ""\n""
    return words
</code></pre>

<p>And I have tried using 400 and 1200 features for the Doc2Vec model:</p>

<pre><code>model = Doc2Vec(min_count=2, window=10, size=model_feat_size, sample=1e-4, negative=5, workers=cores)
</code></pre>

<p>Whereas my tfidf vectoriser has 40,000 max features:</p>

<pre><code>vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)
</code></pre>

<p>For classification I experimented with a few linear methods, however found simple logistic regression to do OK...</p>
",,2016-07-29 02:33:21,Is Doc2Vec suited for Sentiment Analysis?,<machine-learning><sentiment-analysis><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9321,38315158,2016-07-11 19:56:06,,"<p>My goal is to convert a 6000-record CSV file into an array, clean and normalize it, so that I can read it into a corpus.Dictionary() to use in doc2bow in Gensim to perform a SparseMatrixSimiliarity query.  I was successful in reading in the CSV file at first, and it printed out an array I call ""definitions"" for each one of the 6000 sub-category record numbers.</p>

<pre><code> f = open('test.csv')
 csv_f = csv.reader(f)
 definitions = []

 for row in csv_f:
    definitions.append(row[2])

 print(definitions)
</code></pre>

<p>But then hit a wall with UTF-8 and ASCII errors.  Gensim has ""strict"" UTF-8 settings. </p>

<p>After several hours spent on Stack Overflow, researching, and <a href=""https://docs.python.org/2/library/csv.html"" rel=""nofollow"">trying to apply a few ""UTF-8"" encoders</a> per the Python CSV documentation, I read that since Python 2.7 doesn't have ""out of the box"" unicode-encoding using the import csv package, that I could use the codecs package. </p>

<p>I figured that instead of finding every line in my original ""definitions"" 6000-line array and decoding, that I could take an initial stab at decoding it right off the bat using codecs.  However, the below code fails to write anything to my definitions array. Being a newbie, I imagine that I may be using codecs the wrong way, and/or closing the wrong way.</p>

<pre><code> with codecs.open('test.csv', 'rb', encoding='utf-8') as f:    
     csv_f = csv.reader(f)
     definitions= []

     for row in csv_f:   
       definitions.append(np.array((array.float(i) for i in l)))

 f.close()        
 print(definitions)
</code></pre>

<p>I am a total newbie, apologies for any errors in my description.  Learning as I go, really appreciate any feedback and help. Perhaps I'm going about this the wrong way, and welcome any education.   Thank you again. </p>
",,2016-07-11 19:56:06,Empty array after writing to CSV file python,<python><arrays><encoding><utf-8><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9371,38442161,2016-07-18 16:53:23,,"<p>In the official explanation, there is no natural ordering between the topics in LDA.</p>

<p>As for the method show_topics(), if it returned num_topics &lt;= self.num_topics subset of all topics is therefore arbitrary and may change between two LDA training runs.</p>

<p>But I tends to find the top ten frequent topics of corpus. Is there any other ways to achieve this?</p>

<p>Many thanks.</p>
",,2018-07-23 11:23:30,How to print top ten topics using Gensim?,<python><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
9385,38556496,2016-07-24 20:31:38,,"<p>I need to do some experiments on text files using gensim on mac Yosemite.</p>

<p>I've already installed <code>numpy</code> and <code>scipy</code> but when I want to import <code>gensim</code>.</p>

<p>I'm facing this error:</p>

<pre><code>from six.moves.queue import Queue as _Queue
ImportError: No module named queue
</code></pre>

<p>I upgraded <code>numpy</code> and <code>scipy</code> to latest version and Python is 2.7.10.</p>

<p>I read that the problem may be solved by hacking the <code>gensim</code> code to <code>from Queue import Queue as _Queue</code> but I don't know how!</p>

<p>Is there any other way?</p>
",2016-07-24 20:34:23,2016-11-17 06:41:10,gensim can not be imported because ImportError: No module named queue?,<python-2.7><queue><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9387,38343475,2016-07-13 05:42:34,,"<pre><code>from deepdist import DeepDist

from gensim.models.word2vec import Word2Vec

from pyspark import SparkConf, SparkContext

conf = (SparkConf()
     .setAppName(""Work2Vec"")
)

sc = SparkContext(conf=conf)
corpus = sc.textFile('AllText.txt').map(lambda s: s.split())

def gradient(model, sentences):

    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn01, 'syn1': model.syn1 - syn1}


def descent(model, update):

    model.syn0 += update['syn0']

    model.syn1 += update['syn1']


with DeepDist(Word2Vec(corpus.collect())) as dd:

    dd.train(corpus, gradient, descent)

    dd.model.save(""Model"")
</code></pre>

<p>Please help me, I have a 56Gb text and want to build a word2Vec model but using only gensim is very slow, so i try deepdist and their example code on the web, so I just wondering have anyone seen this kind of error </p>

<p>The output when i run this script:</p>

<p><a href=""https://i.stack.imgur.com/L4SO8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L4SO8.png"" alt=""script output""></a></p>
",2016-07-13 06:23:01,2016-08-02 23:07:09,"Trying Deepdict, run gensim word2vec with pyspark",<python><pyspark><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9414,38484117,2016-07-20 14:31:00,,"<p>I trained a doc2vec model with Python2 and I would like to use it in Python3.</p>

<p>When I try to load it in Python 3, I get : </p>

<pre><code>Doc2Vec.load('my_doc2vec.pkl')

UnicodeDecodeError: 'ascii' codec can't decode byte 0xb0 in position 0: ordinal not in range(128)
</code></pre>

<p>It seems to be related to a pickle compatibility issue, which I tried to solve by doing :</p>

<pre><code>with open('my_doc2vec.pkl', 'rb') as inf:
    data = pickle.load(inf)
data.save('my_doc2vec_python3.pkl')
</code></pre>

<p>Gensim saved other files which I renamed as well so they can be found when calling</p>

<pre><code>de = Doc2Vec.load('my_doc2vec_python3.pkl')
</code></pre>

<p>The load() does not fail with UnicodeDecodeError but after the inference provides meaningless results.</p>

<p>I can't easily re-train it using Gensim in Python 3 as I used this model to create derived data from it, so I would have to re-run a long and complex pipeline.</p>

<p>How can I make the doc2vec model compatible with Python 3?</p>
",2016-07-22 06:59:00,2016-07-22 06:59:00,Doc2Vec model Python 3 compatibility,<python><python-3.x><pickle><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9463,38507935,2016-07-21 15:09:39,,"<p>I have trained a GloVe with ~5M <strong>spanish</strong> articles. I know how to load this GloVe in gensim and use it as if it was a word2vec model.
Now I am facing  the problem of topic modelling and keywords extraction from news articles (also in spanish) so I was wondering how could I use the trained model to do so.</p>

<p>How could I do it?</p>
",,2016-07-22 22:00:11,How can I use a trained GloVe/word2vec model to extract keywords from articles?,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9466,38665556,2016-07-29 18:40:54,,"<p>I have had the <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer"">gensim</a> <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">Word2Vec</a> implementation compute some word embeddings for me. Everything went quite fantastically as far as I can tell; now I am clustering the word vectors created, hoping to get some semantic groupings.</p>
<p>As a next step, I would like to look at the words (rather than the vectors) contained in each cluster. I.e. if I have the vector of embeddings <code>[x, y, z]</code>, I would like to find out which actual word this vector represents. I can get the words/Vocab items by calling <code>model.vocab</code> and the word vectors through <code>model.syn0</code>. But I could not find a location where these are explicitly matched.</p>
<p>This was more complicated than I expected and I feel I might be missing the obvious way of doing it. Any help is appreciated!</p>
<h3>Problem:</h3>
<p>Match words to embedding vectors created by <code>Word2Vec ()</code> -- how do I do it?</p>
<h3>My approach:</h3>
<p>After creating the model (code below*), I would now like to match the indexes assigned to each word (during the <code>build_vocab()</code> phase) to the vector matrix outputted as <code>model.syn0</code>.
Thus</p>
<pre><code>for i in range (0, newmod.syn0.shape[0]): #iterate over all words in model
    print i
    word= [k for k in newmod.vocab if newmod.vocab[k].__dict__['index']==i] #get the word out of the internal dicationary by its index
    wordvector= newmod.syn0[i] #get the vector with the corresponding index
    print wordvector == newmod[word] #testing: compare result of looking up the word in the model -- this prints True
</code></pre>
<ul>
<li><p>Is there a better way of doing this, e.g. by feeding the vector into the model to match the word?</p>
</li>
<li><p>Does this even get me correct results?</p>
</li>
</ul>
<p>*My code to create the word vectors:</p>
<pre><code>model = Word2Vec(size=1000, min_count=5, workers=4, sg=1)
        
model.build_vocab(sentencefeeder(folderlist)) #sentencefeeder puts out sentences as lists of strings

model.save(&quot;newmodel&quot;)
</code></pre>
<p>I found <a href=""https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors"">this question</a> which is similar but has not really been answered.</p>
",2020-06-20 09:12:55,2017-06-16 02:41:24,Matching words and vectors in gensim Word2Vec model,<python><vector><machine-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9467,38682784,2016-07-31 09:55:43,,"<p>Does anyone have any idea or can give me directions about how I can extract categories from an article?</p>

<p>What I have is a corpus of few thousands of articles (about sports, news, buisness etc.) I can work with.</p>

<p>For example, if theres an article about sports I would like my program to know if its soccer or basketball (or somthing else) so the output will be somthing like:</p>

<p>soccer 90% basketball 10%</p>
",,2016-07-31 12:29:54,NLP - extract categories/tags from text,<python><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
9490,38620124,2016-07-27 18:00:35,,"<p>I'm using Gensim for an NLP task and currently I have a corpus which includes empty documents.  I don't want to rerun my code, although that is an option, and would just like to remove the documents that don't have any content.  The documents are already saved as TF-IDF corpora and was wondering if there was a way to remove these documents that are empty.  I can figure out which documents are empty but the corpora file is an iterator and not any type of data structure ie list.  Thanks,</p>

<p>Cameron</p>
",,2016-07-27 19:44:51,Removing documents in Gensim,<python><python-2.7><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9537,38852074,2016-08-09 13:23:48,,"<p>Im pretty sure im using yield improperly:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
from gensim import corpora, models, similarities
from collections import defaultdict
from pprint import pprint  # pretty-printer
from six import iteritems
import openpyxl
import string
from operator import itemgetter

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

#Creating a stoplist from file
with open('stop-word-list.txt') as f:
    stoplist = [x.strip('\n') for x in f.readlines()]

corpusFileName = 'content_sample_en.xlsx'
corpusSheetName = 'content_sample_en'

class MyCorpus(object):
    def __iter__(self):
        wb = openpyxl.load_workbook(corpusFileName)
        sheet = wb.get_sheet_by_name(corpusSheetName)
        for i in range(1, (sheet.max_row+1)/2):
            title = str(sheet.cell(row = i, column = 4).value.encode('utf-8'))
            summary = str(sheet.cell(row = i, column = 5).value.encode('utf-8'))
            content = str(sheet.cell(row = i, column = 10).value.encode('utf-8'))
            yield reBuildDoc(""{} {} {}"".format(title, summary, content))


def removeUnwantedPunctuations(doc):
    ""change all (/, \, &lt;, &gt;) into ' ' ""
    newDoc = """"
    for l in doc:
        if  l == ""&lt;"" or l == ""&gt;"" or l == ""/"" or l == ""\\"":
            newDoc += "" ""
        else:
            newDoc += l
    return newDoc

def reBuildDoc(doc):
    """"""
    :param doc:
    :return: document after being dissected to our needs.
    """"""
    doc = removeUnwantedPunctuations(doc).lower().translate(None, string.punctuation)
    newDoc = [word for word in doc.split() if word not in stoplist]
    return newDoc

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus, normalize=True)
</code></pre>

<p>In the following example you can see me trying to create a corpus from an xlsx file. Im reading from the xlsx file 3 lines which are title summary and content and appending them into a big string. my <code>reBuildDoc()</code> and <code>removeUnwantedPunctuations()</code> functions then adjust the text to my needs and in the end returns a big list of words. (for ex: <code>[hello, piano, computer, etc... ]</code>) in the end I yield the result but I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Eran/PycharmProjects/tfidf/docproc.py"", line 101, in &lt;module&gt;
    tfidf = models.TfidfModel(corpus, normalize=True)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 96, in __init__
    self.initialize(corpus)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 119, in initialize
    for termid, _ in bow:
ValueError: too many values to unpack
</code></pre>

<p>I know the error is from the yield line because I had a different yield line that worked. It looked like this: </p>

<pre><code> yield [word for word in dictionary.doc2bow(""{} {} {}"".format(title, summary, content).lower().translate(None, string.punctuation).split()) if word not in stoplist]
</code></pre>

<p>It was abit messy and hard to put functionallity to it so I've changed it as you can see in the first example.</p>
",2016-08-09 13:37:02,2016-08-09 14:16:54,python - Yield improperly usage,<python><parsing><yield><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9539,38852506,2016-08-09 13:43:38,,"<p>I am trying to analyze news snippets in order to identify crisis periods. 
To do so, I have already downloaded news articles over the past 7 years and have those available. 
Now, I am applying a LDA (Latent Dirichlet Allocation) model on this dataset in order to identify those countries show signs of an economic crisis. </p>

<p>I am basing my code on a blog post by Jordan Barber (<a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html</a>)  here is my code so far:</p>

<pre><code>import os, csv

#create list with text blocks in rows, based on csv file
list=[]

with open('Testfile.csv', 'r') as csvfile:
    emails = csv.reader(csvfile)
    for row in emails:
         list.append(row)

#create doc_set
doc_set=[]

for row in list:
    doc_set.append(row[0])

#import plugins - need to install gensim and stop_words manually for fresh python install
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()


# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)


# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=10)

print(ldamodel.print_topics(num_topics=5, num_words=5))

# map topics to documents
doc_lda=ldamodel[corpus]

with open('doc_lda.csv', 'w') as outfile:
    writer = csv.writer(outfile)
    for row in doc_lda:
        writer.writerow(row)
</code></pre>

<p>Essentially, I identify a number of topics (5 in the code above  to be checked), and using the last line I assign each news article a score, which indicates the probability of an article being related to one of these topics. 
Now, I can only manually make a qualitative assessment of whether a given topic is related to a crisis, which is bit unfortunate.
What I would much rather do, is to tell the algorithm whether an article was published during a crisis and use this additional piece of information to identify both topics for my crisis years as well as for my non-crisis-years. Simply splitting my dataset to just consider topics for my bads (i.e. crisis years only) wont work in my opinion, as I would still need to manually select which topics would actually be related to a crisis, and which topics would show up anyways (sports news, ). </p>

<p>So, is there a way to adapt the code to a) incorporate the information of crisis vs non-crisis and b) to automatically chose the optimal number of topics / words to optimize the predictive power of the model?</p>

<p>Thanks a lot in advance!</p>
",,2016-08-10 10:13:05,"Stipulation of ""Good""/""Bad""-Cases in an LDA Model (Using gensim in Python)",<python><python-2.7><lda><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
9541,38630720,2016-07-28 08:16:09,,"<p>I have libbz2-dev installed however I am still getting the following  import error while importing gensim :</p>

<pre><code>&gt;&gt;&gt; import gensim
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/__init__.py"", line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/corpora/__init__.py"", line 14, in &lt;module&gt;
    from .wikicorpus import WikiCorpus
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/corpora/wikicorpus.py"", line 21, in &lt;module&gt;
    import bz2
ImportError: No module named bz2
</code></pre>
",,2017-07-17 16:48:05,Python import error no module named bz2,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9551,38772040,2016-08-04 15:50:35,,"<p>I use Gensim Doc2vec model to train document vectors.
I printed out representations for the word 'good', but I found every epoch, I found not updating! While I printed out representations for the document with id '3', every epoch different! </p>

<p>My codes are below, do not know what is happening. </p>

<pre><code>model = gensim.models.Doc2Vec(dm = 0, alpha=0.1, size= 20, min_alpha=0.025)

model.build_vocab(documents)

print ('Building model....',(time4-time3))
for epoch in range(10):
    model.train(documents)

    print('Now training epoch %s' % epoch)
    print(model['good'])
    print(model.docvecs[str(3)])
</code></pre>
",,2016-09-27 04:30:48,Doc2vec Gensim: the word embeddings not updating during each epoch,<nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9570,38739250,2016-08-03 09:11:31,,"<p>Not able to install gensim on windows.Please help me I need to gensim Immediately and tell me installation steps with More details and other software that needs to be installed before it. thanks</p>
",,2020-08-12 19:07:10,How to install gensim on windows,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9621,38985470,2016-08-16 22:27:47,,"<p>I am training on two identical sentences (documents) using from <code>gensim.models.doc2vec import Doc2Vec</code> and when checking out the vectors for each sentence they are completely different. Does the Neural Network have a different random initialisation per sentence?</p>

<pre><code># imports
from gensim.models.doc2vec import LabeledSentence
from gensim.models.doc2vec import Doc2Vec
from gensim import utils

# Document iteration class (turns many documents in to sentences
# each document being once sentence)
class LabeledDocs(object):
    def __init__(self, sources):
        self.sources = sources
        flipped = {}
        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                # print fin.read().strip(r""\n"")
                yield LabeledSentence(utils.to_unicode(fin.read()).split(),
                                      [prefix])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                #print fin, fin.read()
                self.sentences.append(
                    LabeledSentence(utils.to_unicode(fin.read()).split(),
                                    [prefix]))
        return self.sentences

# play and play3 are names of identical documents (diff gives nothing)
inp = LabeledDocs({""play"":""play"", ""play3"":""play3""})
model = Doc2Vec(size=20, window=8, min_count=2, workers=1, alpha=0.025,
                min_alpha=0.025, batch_words=1)
model.build_vocab(inp.to_array())
for epoch in range(10):
    model.train(inp)

# post to this model.docvecs[""play""] is very different from
# model.docvecs[""play3""]
</code></pre>

<p>Why is this ? Both <code>play</code> and <code>play3</code> contain :</p>

<pre class=""lang-none prettyprint-override""><code>foot ball is a sport
played with a ball where
teams of 11 each try to
score on different goals
and play with the ball
</code></pre>
",2016-08-17 00:02:24,2016-09-07 22:03:16,Why does gensim Doc2Vec give me different vectors for the same sentence?,<python><neural-network><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9622,38986235,2016-08-17 00:02:35,,"<p>So lately I've been playing around with a WikiDump.
I preprocessed it and trained it on Word2Vec + Gensim</p>

<p>Does anyone know if there is only one script within Spacy that would generate
tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once</p>

<p>I have not been able to find clear documentation
Thank you </p>
",2016-09-22 17:54:46,2019-05-03 11:08:30,Spacy Pipeline?,<python><nlp><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
9632,39006270,2016-08-17 21:05:21,,"<p>Given a matrix </p>

<p>----<code>d1 d2 d3
 a: v1  0  v2
 b: v1  v3  0</code></p>

<p>I want </p>

<p>----<code>v1 v2 v3
 a: 1  1   0
 b: 1  0   1</code></p>

<p>I remember vaguely that this can be done with <code>Gensim</code>...but there must also be some module in pandas? I have tried to do <code>for v in v: for el in [a,b]</code>(happy to post the code, but I think that the example is clear enough) but it is very slow, and I imagine this must have been solved before.</p>
",,2016-08-17 22:19:33,How to convert a set of features to a count matrix in pandas,<python><pandas><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9649,38968353,2016-08-16 06:53:20,,"<p>i want to have phrases in doc2vec and i use gensim.phrases. in doc2vec we need tagged document to train the model and i cannot tag the phrases. how i can do this?</p>

<p>here is my code</p>

<pre><code>text = phrases.Phrases(text)
for i in range(len(text)):
    string1 = ""SENT_"" + str(i)

    sentence = doc2vec.LabeledSentence(tags=string1, words=text[i])
    text[i]=sentence

print ""Training model...""
model = Doc2Vec(text, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)
</code></pre>
",,2016-08-16 23:24:42,How to use doc2vec with phrases?,<python><nlp><gensim><phrases><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9675,39144787,2016-08-25 12:02:18,,"<p>I have tried topic modelling in python . But its displaying wrong output.
I have provided sample example and codes below.</p>

<pre><code>## Documents

doc1 = ""Sugar is bad to consume. My sister likes to have sugar, but not my father.""
doc2 = ""My father spends a lot of time driving my sister around to dance practice.""
doc3 = ""Doctors suggest that driving may cause increased stress and blood pressure.""
doc4 = ""Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.""
doc5 = ""Health experts say that Sugar is not good for your lifestyle."" 

# compile documents
doc_complete = [doc1, doc2, doc3, doc4, doc5]

from nltk.corpus import stopwords 
from nltk.stem.wordnet import WordNetLemmatizer
import string
stop = set(stopwords.words('english'))
exclude = set(string.punctuation) 
lemma = WordNetLemmatizer()
def clean(doc):
stop_free = """".join([i for i in doc.lower() if i not in stop])
punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
return normalized

doc_clean = [clean(doc) for doc in doc_complete] 

#Preparing Document Term Matrix
import gensim 

dictionary = corpora.Dictionary([doc_clean])
corpus = [dictionary.doc2bow(doc) for doc in [doc_clean]]

#Running LDA Model

Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)


print(ldamodel.print_topics(num_topics=3, num_words=3))
</code></pre>

<p>I am getting an output like following :</p>

<pre><code>[(0, u'0.200*cr ugge h rvng cue ncree re n bl preure + 0.200*fher pen l f e rvng er run nce prcce + 0.200*ee feel preure perfr well chl bu fher never ee rve er beer'), (1, u'0.200*helh exper h ugr n g fr ur lfele + 0.200*cr ugge h rvng cue ncree re n bl preure + 0.200*ee feel preure perfr well chl bu fher never ee rve er beer'), (2, u'0.200*fher pen l f e rvng er run nce prcce + 0.200*ugr b cnue er lke hve ugr bu n fher + 0.200*ee feel preure perfr well chl bu fher never ee rve er beer')]
</code></pre>

<p>I am wondering , what I have missed. Thanks</p>
",,2016-08-25 13:04:31,Having a wrong Output in topic modelling,<python><nltk><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,False
9699,39130456,2016-08-24 18:20:19,,"<p>We're running into a lot of problems installing word2vec within python3. We just keep getting a standard error that python can't find the package it's looking for. </p>

<blockquote>
  <p>Competition-Repo/agents$ python3</p>
  
  <p>Python 3.4.3  [GCC 4.8.4] on linux</p>
  
  <blockquote>
    <blockquote>
      <blockquote>
        <p>import word2vec </p>
        
        <p>Traceback (most recent call last):<br>
        File """", line 1, in  ImportError: No module named 'word2vec'</p>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>

<p>Most everything I've found has either someone else's implementation of word2vec within python3, or is accessed through Gensim. The pip-install for python2 imports flawlessly, now we need to set up the same thing for python3. </p>

<p>Thanks!
-Ben</p>
",2016-08-24 18:58:08,2016-08-24 18:58:08,Implementing word2vec in python3 WITHOUT gensim,<python-3.x><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9713,39301199,2016-09-02 21:54:52,,"<p>I'm working using a cluster to generate word2vec models using gensim from sentences from medical journals that are stored in JSON files and I'm having trouble with memory usage being too large.</p>

<p>The task is to keep a cumulative list of all sentences up to a particular year, and then generate a word2vec model for that year. Then, add the sentences for the next year to the cumulative list and generate and save another model for that year based on all the sentences.</p>

<p>The I/O on this particular cluster is slow enough and the data large enough (reading 2/3 into memory takes about 3 days) that streaming each JSON from disk for each year's model would have taken forever, so the solution was to load all 90GB of JSON into memory in a python list. I have permission to use up to 256GB of memory for this, but could get more if necessary.</p>

<p>The trouble I'm having is that I'm running out of memory. I have read some other posts about the way Python implements free lists not returning memory to the OS and I think that may be part of the problem, but I am not sure.</p>

<p>Thinking that the free list might be the problem and that maybe a numpy would have a better implementation for a large number of elements, I changed from the cumulative list of sentences to a cumulative array of sentences (gensim requires that sentences be lists of words/strings). But I ran this on a small subset of the sentences and it used slightly more memory, so I am unsure of how to proceed. </p>

<p>If anyone has any experience with this I would be very happy to have your help. Also, if there is anything else that could be changed I would appreciate you telling me as well. The full code is below:</p>

<pre><code>import ujson as json
import os
import sys
import logging
from gensim.models import word2vec
import numpy as np

PARAMETERS = {
    'numfeatures': 250,
    'minwordcount': 10,
    'context': 7,
    'downsampling': 0.0001,
    'workers': 32
}

logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)


def generate_and_save_model(cumulative_sentences, models_dir, year):
    """"""
    Generates and saves the word2vec model for the given year
    :param cumulative_sentences: The list of all sentences up to the current year
    :param models_dir: The directory to save the models to
    :param year: The current year of interest
    :return: Nothing, only saves the model to disk
    """"""
    cumulative_model = word2vec.Word2Vec(
        sentences=cumulative_sentences,
        workers=PARAMETERS['workers'],
        size=PARAMETERS['numfeatures'],
        min_count=PARAMETERS['minwordcount'],
        window=PARAMETERS['context'],
        sample=PARAMETERS['downsampling']
    )
    cumulative_model.init_sims(replace=True)
    cumulative_model.save(models_dir + 'medline_abstract_word2vec_' + year)


def save_year_models(json_list, json_dir, models_dir, min_year, max_year):
    """"""
    :param json_list: The list of json year_sentences file names
    :param json_dir: The directory holding the the sentences json files
    :param models_dir: The directory to serialize the models to
    :param min_year: The minimum value of a year to generate a model for
    :param max_year: The maximum value of a year to generate a model for
    Goes year by year through each json of sentences, saving a cumulative word2vec
    model for each year
    """"""

    cumulative_sentences = np.array([])

    for json_file in json_list:
        year = json_file[16:20]

        # If this year is greater than the maximum, we're done creating models
        if int(year) &gt; max_year:
            break

        with open(json_dir + json_file, 'rb') as current_year_file:
            cumulative_sentences = np.concatenate(
                (np.array(json.load(current_year_file)['sentences']),
                 cumulative_sentences)
            )

        logger.info('COMPLETE: ' + year + ' sentences loaded')
        logger.info('Cumulative length: ' + str(len(cumulative_sentences)) + ' sentences loaded')
        sys.stdout.flush()

        # If this year is less than our minimum, add its sentences to the list and continue
        if int(year) &lt; min_year:
            continue

        generate_and_save_model(cumulative_sentences, models_dir, year)

        logger.info('COMPLETE: ' + year + ' model saved')
        sys.stdout.flush()


def main():
    json_dir = '/projects/chemotext/sentences_by_year/'
    models_dir = '/projects/chemotext/medline_year_models/'

    # By default, generate models for all years we have sentences for
    minimum_year = 0
    maximum_year = 9999

    # If one command line argument is used
    if len(sys.argv) == 2:
        # Generate the model for only that year
        minimum_year = int(sys.argv[1])
        maximum_year = int(sys.argv[1])

    # If two CL arguments are used
    if len(sys.argv) == 3:
        # Generate all models between the two year arguments, inclusive
        minimum_year = int(sys.argv[1])
        maximum_year = int(sys.argv[2])

    # Sorting the list of files so that earlier years are first in the list
    json_list = sorted(os.listdir(json_dir))

    save_year_models(json_list, json_dir, models_dir, minimum_year, maximum_year)

if __name__ == '__main__':
    main()
</code></pre>
",,2016-09-07 16:59:26,Excessive memory usage for very large Python lists loaded from 90GB of JSON for word2vec,<python><numpy><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9727,39247496,2016-08-31 10:45:53,,"<p>I wanted to output the log-probability during learning of the word and doc vectors in gensim. I have taken a look at the implementation of the score function in the ""slow plain numpy"" version.</p>

<pre class=""lang-py prettyprint-override""><code>def score_cbow_pair(model, word, word2_indices, l1):
    l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size
    sgn = (-1.0)**word.code  # ch function, 0-&gt; 1, 1 -&gt; -1
    lprob = -log(1.0 + exp(-sgn*dot(l1, l2a.T)))
    return sum(lprob)
</code></pre>

<p>The score function should make use of the parameters learned during hierarchical softmax training. But in the calculation of the log-probability there is supposed to be a sigmoid function( <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained equation (45)</a>).
So does gensim really calculate the log-probability in <code>lprob</code> or is it just a score for comparison purposes.</p>

<p>I would have calculated the log-probability as follows:
<code>-log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T))))</code></p>

<p>Is this equation not used because it explodes for values close to zero or is it in general wrong?</p>
",2016-08-31 11:03:21,2016-09-06 21:03:22,score_cbow_pair in word2vec (gensim),<python><numpy><probability><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9729,39252207,2016-08-31 14:21:26,,"<p>With Doc2Vec modelling, I have trained a model and saved following files:</p>

<pre><code>1. model
2. model.docvecs.doctag_syn0.npy
3. model.syn0.npy
4. model.syn1.npy
5. model.syn1neg.npy
</code></pre>

<p>However, I have a new way to label the documents and want to train the model again. since the word vectors already obtained from previous version. Is there any way to reuse that model (e.g., taking the previous w2v results as initial vectors for training)? Any one know how to do it? </p>
",2016-09-01 01:29:56,2016-09-07 07:42:51,Gensim: how to retrain doc2vec model using previous word2vec model,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9749,39252860,2016-08-31 14:51:26,,"<p>I'm trying to understand the PV-DM implementation with averaging in gensim.
In the function <code>train_document_dm</code> in <code>doc2vec.py</code> the return value (""errors"") of <code>train_cbow_pair</code> is in the case of averaging (<code>cbow_mean=1</code>) not divided by the number of input vectors (<code>count</code>).
According to this explanation there should be a division by the number of documents in the case of averaging the input vectors: <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained, equation (23)</a>.
Here is the code from <code>train_document_dm</code>:</p>

<pre class=""lang-py prettyprint-override""><code>l1 = np_sum(word_vectors[word2_indexes], axis=0)+np_sum(doctag_vectors[doctag_indexes], axis=0)  
count = len(word2_indexes) + len(doctag_indexes)  
if model.cbow_mean and count &gt; 1:  
    l1 /= count  
neu1e = train_cbow_pair(model, word, word2_indexes, l1, alpha,
                                learn_vectors=False,  learn_hidden=learn_hidden)  
if not model.cbow_mean and count &gt; 1:  
    neu1e /= count  
if learn_doctags:  
    for i in doctag_indexes:  
        doctag_vectors[i] += neu1e * doctag_locks[i]  
if learn_words:  
    for i in word2_indexes:  
        word_vectors[i] += neu1e * word_locks[i]  
</code></pre>
",,2017-01-19 02:39:59,updates of the document vectors in doc2vec (PV-DM) in gensim,<python><numpy><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9757,39294921,2016-09-02 14:33:13,,"<p>I'm using <code>gensim</code>'s <code>LdaModel</code>, which, according to the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">documentation</a>, has the parameter <code>random_state</code>. However, I'm getting an error that says:</p>

<pre><code> TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>Without the <code>random_state</code> parameter, the function works as expected. So, the workflow looks like this for those that want to know what else is happening...</p>

<pre><code>from gensim import corpora, models
import numpy as np

# pseudo code of text pre-processing all on ""comments"" variable
# stop words
# remove punctuation (optional)
# keep alpha only
# stemming
# get bigrams and integrate with corpus (gensim makes this very easy)


dictionary = corpora.Dictionary(comments)
corpus = [dictionary.doc2bow(comm) for comm in comments]
tfidf = models.TfidfModel(corpus) # change weights
corp_tfidf = tfidf[corpus] # apply them to corpus

# set random seed
random_seed = 135
state = np.random.RandomState(random_seed)

# train model
num_topics = 3
lda_mod = models.LdaModel(corp_tfidf, # corpus
                          num_topics=num_topics, # number of topics we want back
                          id2word=dictionary, # our id-word map
                          passes=10, # how many passes to take over the data
                          random_state=state) # reproduce the results
</code></pre>

<p>Which results in the error message above...</p>

<pre><code>TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>I'd like to be able to recreate my results, if possible.</p>
",2016-09-02 14:40:45,2016-09-02 15:01:40,LdaModel - random_state parameter not recognized - gensim,<python-3.x><numpy><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
9758,39296592,2016-09-02 16:02:43,,"<p>I will like to plot in a simple vector space graph the similarity between different words. I have calculated them using the model <code>word2vec</code> given by gensim but I cannot find any graphical examples in the literature. My code is as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

## Building the deep learning model
import itertools

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

from gensim.models import word2vec


result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]

model = gensim.models.Word2Vec(result)
</code></pre>

<p>In a simple vector space graph, I will like to place the following words: bank, finance, market, property, oil, energy, business and economy. I can easily calculate the similarity of these pairs of words with the function:</p>

<pre><code>model.similarity('bank', 'property')
0.25089364531360675
</code></pre>

<p>Thanks a lot</p>
",,2017-02-22 11:11:00,Graphical plot of words similarity given by Word2Vec,<python><graph><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
9759,39391753,2016-09-08 13:04:24,,"<p>I've patched the following code from examples I've found over the web:</p>

<pre><code># gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec
from sklearn.cluster import KMeans

# random
from random import shuffle

# classifier

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
        return self.sentences

    def sentences_perm(self):
        shuffle(self.sentences)
        return self.sentences

sources = {'test.txt' : 'DOCS'}
sentences = LabeledLineSentence(sources)

model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())

for epoch in range(10):
    model.train(sentences.sentences_perm())

print(model.docvecs)
</code></pre>

<p>my test.txt file contains a paragraph per line.</p>

<p>The code runs fine and generates DocvecsArray for each line of text</p>

<p>my goal is to have an output like so:</p>

<p>cluster 1: [DOC_5,DOC_100,...DOC_N]<br>
cluster 2: [DOC_0,DOC_1,...DOC_N]</p>

<p>I have found the <a href=""https://stackoverflow.com/questions/27889873/clustering-text-documents-using-scikit-learn-kmeans-in-python"">following Answer</a>, but the output is:</p>

<p>cluster 1: [word,word...word]<br>
cluster 2: [word,word...word]</p>

<p>How can I alter the code and get document clusters?</p>
",2017-05-23 12:17:30,2018-06-05 11:02:48,doc2vec How to cluster DocvecsArray,<python><machine-learning><k-means><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,True
9781,39275547,2016-09-01 15:27:12,,"<p>I will like to analyze my first deep learning model using Python and in order to do so I have to first split my corpus (8807 articles) into sentences. My corpus is built as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body
</code></pre>

<p>I am trying to use the function <code>Word2Vec.load_word2vec_format</code> from the library <code>gensim.models</code> but I have to first split my corpus (<code>doc_set</code>) into sentences.</p>

<pre><code>from gensim.models import word2vec
model = Word2Vec.load_word2vec_format(doc_set, binary=False)
</code></pre>

<p>Any recommendations? </p>

<p>cheers</p>
",,2016-09-01 17:29:38,Tokenizing a corpus composed of articles into sentences Python,<python><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
9798,39489933,2016-09-14 12:00:31,,"<p>I'm using Gensim Doc2Vec model, trying to cluster portions of a customer support conversations. My goal is to give the support team an auto response suggestions.</p>

<p><strong>Figure 1:</strong> shows a sample conversations where the user question is answered in the next conversation line, making it easy to extract the data:</p>

<p><a href=""https://i.stack.imgur.com/N4ri4.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N4ri4.gif"" alt=""Figure 1""></a> </p>

<p><sup>during the conversation <strong>""hello""</strong> and <strong>""Our offices are located in NYC""</strong> should be suggested</sup></p>

<hr>

<p><strong>Figure 2:</strong> describes a conversation where the questions and answers are not in sync</p>

<p><a href=""https://i.stack.imgur.com/oHUQu.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oHUQu.gif"" alt=""Figure 2""></a></p>

<p><sup>during the conversation <strong>""hello""</strong> and <strong>""Our offices are located in NYC""</strong> should be suggested</sup></p>

<hr>

<p><strong>Figure 3:</strong> describes a conversation where the context for the answer is built over time, and for classification purpose (I'm assuming) some of the lines are redundant.</p>

<p><a href=""https://i.stack.imgur.com/muf6Y.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/muf6Y.gif"" alt=""Figure 3""></a></p>

<p><sup>during the conversation <strong>""here is a link for the free trial account""</strong> should be suggested</sup></p>

<hr>

<p>I have the following data per conversation line (simplified):<br>
who wrote the line (user or agent), text, time stamp</p>

<p>I'm using the following code to train my model:</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedLineDocument
import datetime

print('Creating documents',datetime.datetime.now().time())
context = TaggedLineDocument('./test_data/context.csv')

print('Building model',datetime.datetime.now().time())

model = Doc2Vec(context,size = 200, window = 10, min_count = 10, workers=4)
print('Training...',datetime.datetime.now().time())

for epoch in range(10):
    print('Run number :',epoch)
    model.train(context)

model.save('./test_data/model')
</code></pre>

<p><strong>Q</strong>: How should I structure my training data and what heuristics could be applied in order to extract it from the raw data?</p>
",2016-09-15 20:44:37,2016-09-20 19:30:24,"How to break conversation data into pairs of (Context , Response)",<python><text-mining><doc2vec><gensym>,,,CC BY-SA 3.0,False,False,True,False,False
9826,39406092,2016-09-09 07:28:24,,"<pre><code>from gensim.models import word2vec

sentences = word2vec.Text8Corpus('TextFile')
model = word2vec.Word2Vec(sentences, size=200, min_count = 2, workers = 4)
print model['king']
</code></pre>

<p>Is the output vector the context vector of 'king' or the word embedding vector of 'king'? How can I get both context vector of 'king' and the word embedding vector of 'king'? Thanks!</p>
",,2017-04-02 17:06:35,How to get both the word embeddings vector and context vector of a given word by using word2vec?,<python><vector><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
9850,39615420,2016-09-21 11:33:05,,"<p>In gensim, when I give a string as input for training doc2vec model,  I get this error : </p>

<blockquote>
  <p>TypeError('don\'t know how to handle uri %s' % repr(uri))</p>
</blockquote>

<p>I referred to this question <a href=""https://stackoverflow.com/questions/36780138/doc2vec-taggedlinedocument"">Doc2vec : TaggedLineDocument()</a>
but still have a doubt about the input format. </p>

<p><code>documents = TaggedLineDocument('myfile.txt')</code></p>

<p>Should the myFile.txt have tokens as list of lists or separate list in each line for each document or a string? </p>

<p><code>For eg</code> - I have 2 documents.</p>

<p>Doc 1 : Machine learning is a subfield of computer science that evolved from the study of pattern recognition.</p>

<p>Doc 2 :  Arthur Samuel defined machine learning as a ""Field of study that gives computers the ability to learn"".</p>

<p>So, what should the <code>myFile.txt</code> look like?</p>

<p>Case 1 : simple text of each document in each line</p>

<p>Machine learning is a subfield of computer science that evolved from the study of pattern recognition</p>

<p>Arthur Samuel defined machine learning as a Field of study that gives computers the ability to learn</p>

<p>Case 2 : a list of lists having tokens of each document</p>

<p><code>[ [""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]</code>,</p>

<pre><code>[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""] ]
</code></pre>

<p>Case 3 : list of tokens of each document in a separate line</p>

<pre><code>[""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]

[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""]
</code></pre>

<p>And when I am running it on the test data, what should be the format of the sentence which i want to predict the doc vector for? Should it be like case 1 or case 2 below or something else?</p>

<p><code>model.infer_vector(testSentence, alpha=start_alpha, steps=infer_epoch)</code></p>

<p>Should the testSentence be :</p>

<p>Case 1 : string</p>

<pre><code>testSentence = ""Machine learning is an evolving field""
</code></pre>

<p>Case 2 : list of tokens</p>

<pre><code>testSentence = [""Machine"", ""learning"", ""is"", ""an"", ""evolving"", ""field""]
</code></pre>
",2017-12-20 11:55:30,2017-12-20 11:55:30,doc2vec - Input Format for doc2vec training and infer_vector() in python,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9851,39615436,2016-09-21 11:33:45,,"<p>When using LDA model, I get different topics each time and I want to replicate the same set. I have searched for the similar question in Google such as <a href=""https://groups.google.com/forum/#!topic/gensim/s1EiOUsqT8s"" rel=""nofollow"">this</a>.</p>

<p>I fix the seed as shown in the article by <code>num.random.seed(1000)</code> but it doesn't work. I read the <code>ldamodel.py</code> and find the code below:</p>

<pre><code>def get_random_state(seed):

    """"""
    Turn seed into a np.random.RandomState instance.
    Method originally from maciejkula/glove-python, and written by @joshloyal
    """"""
     if seed is None or seed is numpy.random:
         return numpy.random.mtrand._rand
     if isinstance(seed, (numbers.Integral, numpy.integer)):
         return numpy.random.RandomState(seed)
     if isinstance(seed, numpy.random.RandomState):
        return seed
     raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                      ' instance' % seed)
</code></pre>

<p>So I use the code:</p>

<pre><code>lda = models.LdaModel(
    corpus_tfidf,
    id2word=dic,
    num_topics=2,
    random_state=numpy.random.RandomState(10)
)
</code></pre>

<p>But it's still not working.</p>
",2016-09-21 11:55:49,2018-06-25 05:55:06,Fails to fix the seed value in LDA model in gensim,<python><numpy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9855,39549248,2016-09-17 16:40:54,,"<p>I want to use a pre-trained <code>word2vec</code> model, but I don't know how to load it in python.</p>

<p>This file is a MODEL file (703 MB).
It can be downloaded here:<br>
<a href=""http://devmount.github.io/GermanWordEmbeddings/"" rel=""noreferrer"">http://devmount.github.io/GermanWordEmbeddings/</a></p>
",2017-11-29 04:56:30,2017-11-29 04:56:30,How to load a pre-trained Word2vec MODEL File and reuse it?,<python><file><model><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9863,39657215,2016-09-23 09:24:33,,"<p>The Code is in python. I loaded up the binary model into gensim on python, &amp; used the ""init_sims"" option to make the execution faster. The OS is OS X.
It takes almost 50-60 seconds to load it up. And an equivalent time to find ""most_similar"". Is this normal? Before using the init_sims option, it took almost double the time! I have a feeling it might be an OS RAM allocation issue.</p>

<pre><code>model=Word2Vec.load_word2vec_format('GoogleNewsvectorsnegative300.bin',binary=True)
model.init_sims(replace=True)
model.save('SmallerFile')
#MODEL SAVED INTO SMALLERFILE &amp; NEXT LOAD FROM IT
model=Word2Vec.load('SmallerFile',mmap='r')
#GIVE RESULT SER!
print model.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>
",,2016-09-27 07:01:49,Word2Vec: Using Gensim and Google-News dataset- Very Slow Execution Time,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9865,39580232,2016-09-19 18:57:36,,"<p>I have trained paragraph vectors for around 2300 paragraphs(between 2000-12000 words each) each with vector size of 300. Now, I need to infer paragraph vectors of around 100,000 sentences which I have considered as paragraphs(each sentence is around 10-30 words each corresponding to the earlier 2300 paragraphs already trained).</p>

<p>So, am using </p>

<p><code>model.infer_vector(sentence)</code></p>

<p>But, the problem is it is taking too long, and it does not take any arguments such as ""<code>workers</code>"" .! Is there a way I can speed up the process by threading or some other way? I am using a machine with 8gb ram and when I checked the available cores using</p>

<pre><code>cores = multiprocessing.cpu_count()
</code></pre>

<p>it comes out to be 8.</p>

<p>I need this for answering multiple choice questions. Also, are there any other libraries/models such as <code>doc2vec</code> which can help in this task?</p>

<p>Thanks in advance for your time.</p>
",2017-12-20 11:56:10,2020-05-31 17:48:15,doc2vec - How to infer vectors of documents faster?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9872,39552088,2016-09-17 21:54:26,,"<p>Is it possible to choose between the <code>Skip-gram</code> and the <code>CBOW</code> model in <em>Gensim</em> when training a <em>Word2Vec</em> model?</p>
",2020-10-13 21:33:47,2020-10-13 21:33:47,Select between skip-gram and CBOW model for training word2Vec in gensim,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
9874,39558642,2016-09-18 14:20:48,,"<p>Using Word2vec and Doc2vec methods provided by Gensim, they have a distributed version which uses BLAS, ATLAS, etc to speedup (details <a href=""http://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""noreferrer"">here</a>). However, is it supporting GPU mode? Is it possible to get GPU working if using Gensim?</p>
",,2019-07-13 00:33:28,Does Gensim library support GPU acceleration?,<optimization><gpu><gensim><deeplearning4j>,,,CC BY-SA 3.0,False,False,True,False,False
9903,39570120,2016-09-19 09:49:46,,"<p>I'm dealing with topic-modelling of Twitter to define profiles of invidual Twitter users. I'm using Gensim module to generate a LDA model. My question is about choosing good input data. I'd like to generate topics which then I'd assign to specific users. Question is about input data. Now I'm using a supervised method of choosing users from different categories on my own (sports, IT, politics etc) and putting their tweets into the model but it's not very efficient and effective.</p>

<p>What would be a good method for generating meaningful topics of the whole Twitter?</p>
",,2017-05-17 19:38:06,Generating a good LDA model of Twitter in Python with correct input data,<python><twitter><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
9906,39738327,2016-09-28 04:54:24,,"<p>I am trying to make a Apple Siri-like application in python in which you give it vocal commands or questions through a microphone, it determines the text version of the inputted audio, and then determines the appropriate action to take based on the meaning of the command/question. I am going to be using the Speech Recognition library to accept microphone input and convert from speech to text (via the IBM Watson Speech to Text API).</p>

<p>The main problem I have with it right now is that when I define an action for the app to execute when the appropriate command is given/question is asked, I don't know how to determine if the said command/question is denoting that action. Let me clarify what I mean by that with an example:</p>

<p>Say we have a action called <code>hello</code>. There are multiple ways for somebody to say ""hello"" to another person (or in this case, my application), such as:</p>

<ul>
<li>""Hello""</li>
<li>""Hi""</li>
<li>""Howdy""</li>
<li>...Etcetera...</li>
</ul>

<p>Of course, I want all of these ways of saying ""hello"" to be classified under the action of <code>hello</code>. That is, when someone says ""hello"", ""hi"", or ""howdy"", the response for the action <code>hello</code> should be executed (most likely just the app saying ""hello"" back in this case).</p>

<p>My first thought on how to solve this was to supply the app with all of or the most common ways to say a certain command/question. So, if I follow the previous example, I would tell the computer that ""hello"", ""hi"", and ""howdy"" all meant the same thing: the <code>hello</code> action. However, this method has a couple flaws. First off, it simply wouldn't understand ways of saying ""hello"" that weren't hardcoded in, such as ""hey"". Second off, once the responses for new commands/questions start getting coded in, it would become very tedious entering all the ways to say a certain phrase.</p>

<p>So then, because of the aforementioned problems, I started looking into ways to calculate the similarities between a group of sentences, and a single query. I eventually came across the Gensim library for python. I looked into it and found some very promising information on complex processes such as latent semantic indexing/analysis (LSI/LSA) and Tf-idf. However, it seemed to me like these things were mainly for comparing documents with large word counts as they rely on the frequency of certain terms. Assuming this is true, these processes wouldn't really provide me with accurate results as the commands/questions given to my app will probably be about eight words on average. I could be completely wrong, after all I know very little about these processes.</p>

<p>I also discovered WordNet, and how to work with it in python using the Natural Language Toolkit (NLTK). It looks like it could be useful, but I'm not sure how.</p>

<p>So, finally, I guess my real question here is what would be the best solution to the problem I've mentioned? Should I use one of the methods I've mentioned? Or is there a better way to do what I want that I don't know about?</p>

<p>Any help at all would be greatly appreciated. Thanks in advance.</p>

<p>P.S. Sorry for the wordy explanation; I wanted to be sure I was clear :P</p>
",,2016-09-28 09:38:24,Siri-like app: calculating similarities between a query and a predefined set of control phrases,<python><nlp><nltk><wordnet><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
9917,39781812,2016-09-30 00:09:37,,"<p>I am trying to use Gensim's Word2Vec implementation. Gensim warns that if you don't have a C compiler, the training will be 70% slower.  Is there away to verify that Gensim is correctly using the C Compiler I have installed?</p>

<p>I am using Anaconda Python 3.5 on Windows 10.</p>
",2016-10-12 02:57:51,2016-10-12 02:57:51,How can I tell if Gensim Word2Vec is using the C compiler?,<python><compilation><installation><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9938,39843584,2016-10-04 03:13:27,,"<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>

<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>

<ol>
<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>
<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>
<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>
<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>
</ol>

<h3>Gensim</h3>

<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)
model.build_vocab(corpus)
epochs = 100
for i in range(epochs):
    model.train(corpus)
</code></pre>

<h3>TF</h3>

<pre><code>batch_size = 512
embedding_size = 100 # Dimension of the embedding vector.
num_sampled = 10 # Number of negative examples to sample.


graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])

    # The variables   
    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))
    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],
                             stddev=1.0 / np.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    ###########################
    # Model.
    ###########################
    # Look up embeddings for inputs and stack words side by side
    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),
                            shape=[int(batch_size/context_window),-1])
    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)
    embed = tf.concat(1,[embed_words, embed_docs])
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
</code></pre>

<h2>Update:</h2>

<p>Check out the jupyter notebook <a href=""https://github.com/sachinruk/doc2vec_tf"" rel=""noreferrer"">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>
",2017-09-17 04:12:13,2017-09-17 04:12:13,gensim Doc2Vec vs tensorflow Doc2Vec,<python><tensorflow><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
9946,39806859,2016-10-01 12:55:42,,"<p>I am attempting to build a model that will attempt to identify the interest category / topic of supplied text. For example:</p>

<blockquote>
  <p>Shop for Bridal Wedding Sarees from our exhausting variety of beautiful and designer sarees. Get great deals, quality stitching and
  Free International delivery.</p>
</blockquote>

<p>would resolve to a top level category like:</p>

<blockquote>
  <p>Fashion or Wedding Fashion</p>
</blockquote>

<p>To acheive this, I have used Latent Dirichlet allocation (LDA) which is a topic model that generates topics based on word frequency from a set of documents. </p>

<p>So I got topics of document as below but don't find way to map them to human understandable format</p>

<blockquote>
  <p>topic #0 (0.500): 0.100*sare + 0.060*intern + 0.060*get + 0.060*deal +
  0.060*exhaust + 0.060*design + 0.060*free + 0.060*qualiti + 0.060*shop + 0.060*great</p>
  
  <p>topic #1 (0.500): 0.063*sare + 0.063*beauti + 0.063*deliveri +
  0.063*stitch + 0.063*varieti + 0.063*wed + 0.062*bridal + 0.062*great + 0.062*shop + 0.062*qualiti</p>
</blockquote>

<p>I have used this <a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">script</a> to implement above things.</p>

<p>So the Question is How to map above identified topics to human readable category like Fashion?</p>
",2016-10-03 04:39:56,2018-02-01 20:44:56,Identifying interest / topic from text,<python><nltk><lda><gensim><nltk-trainer>,,,CC BY-SA 3.0,True,False,True,False,False
9947,39644667,2016-09-22 16:48:13,,"<p>I will like to know more about whether or not there are any rule to set the hyper-parameters alpha and theta in the LDA model. I run an LDA model given by the library <code>gensim</code>:</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=50, minimum_probability=0)
</code></pre>

<p>But I have my doubts on the specification of the hyper-parameters. From what I red in the library documentation, both hyper-parameters are set to 1/number of topics. Given that my model has 30 topics, both hyper-parameters are set to a common value 1/30. I am running the model in news-articles that describe the economic activity. For this reason, I expect that the document-topic distribution (theta) to be high (similar topics in documents),while the topic-word distribution (alpha) be high as well (topics sharing many words in common, or, words not being so exclusive for each topic). For this reason, and given that my understanding of the hyper-parameters is correct, is 1/30 a correct specification value?</p>
",,2018-05-11 07:54:50,Rules to set hyper-parameters alpha and theta in LDA model,<lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
9998,39969919,2016-10-11 03:07:18,,"<p>I am hoping to assign each document to one topic using LDA. Now I realise that what you get is a distribution over topics from LDA. However as you see from the last line below I assign it to the most probable topic.</p>

<p>My question is this. I have to run <code>lda[corpus]</code> for somewhat the second time in order to get these topics. Is there some other builtin gensim function that will give me this topic assignment vectors directly? Especially since the LDA algorithm has passed through the documents it might have saved these topic assignments?</p>

<pre class=""lang-py prettyprint-override""><code>    # Get the Dictionary and BoW of the corpus after some stemming/ cleansing
    texts = [[stem(word) for word in document.split() if word not in STOPWORDS] for document in cleanDF.text.values]
    dictionary = corpora.Dictionary(texts)
    dictionary.filter_extremes(no_below=5, no_above=0.9)
    corpus = [dictionary.doc2bow(text) for text in texts]

    # The actual LDA component
    lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=30, chunksize=10000, passes=10,workers=4) 

    # Assign each document to most prevalent topic
    lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda[corpus]]
</code></pre>
",2020-04-20 03:31:54,2020-04-20 03:31:54,Gensim LDA topic assignment,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
10019,39948442,2016-10-09 20:56:26,,"<p>I am trying to visualize LDA topics in Python using PyLDAVis but I can't seem to get it right. My model has a vocab size of 150K words and about 16 Million tokens were taken to train it.</p>

<p>I am doing it outside of an iPython notebook and this is the code that I wrote to do it.</p>

<pre><code>model_filename = ""150k_LdaModel_topics_""+ topics +""_passes_""+passes +"".model""

dictionary = gensim.corpora.Dictionary.load('LDADictSpecialRemoved150k.dict')
corpus = gensim.corpora.MmCorpus('LDACorpusSpecialRemoved150k.mm')
ldamodel = gensim.models.ldamodel.LdaModel.load(model_filename)

import pyLDAvis.gensim
vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
pyLDAvis.save_html(vis, ""topic_viz_""+topics+""_passes_""+passes+"".html"")
</code></pre>

<p>I get the following error after 2-3 hours of running code on a high speed server with >30GBs of RAM. Can someone help where I am going wrong?</p>

<pre><code>Traceback (most recent call last):
  File ""create_vis.py"", line 36, in &lt;module&gt;
    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/gensim.py"", line 110, in prepare
    return vis_prepare(**opts)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/_prepare.py"", line 398, in prepare
    token_table        = _token_table(topic_info, term_topic_freq, vocab, term_frequency)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/_prepare.py"", line 267, in _token_table
    term_ix.sort()
  File ""/local/lib/python2.7/site-packages/pandas/indexes/base.py"", line 1703, in sort
    raise TypeError(""cannot sort an Index object in-place, use ""
TypeError: cannot sort an Index object in-place, use sort_values instead
</code></pre>
",,2016-10-12 12:30:24,"PyLdaVis : TypeError: cannot sort an Index object in-place, use sort_values instead",<python><visualization><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
10021,39890544,2016-10-06 07:54:48,,"<p>The <code>gensim</code> <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow"">Dictionary</a> object keeps track of the vocabulary of the collection of documents (aka corpus). But to feed the data into the object, the data has to be fed into the memory, e.g.</p>

<pre><code>import io
from gensim.corpora import Dictionary

infile = '/path/to/data'

with io.open(infile, 'r', encoding='utf8') as fin:
    d = Dictionary(map(lambda x: x.split(), fin.readlines()))
    d.save('data.dict')
</code></pre>

<p><strong>Can I read a file object into a gensim Dictionary class?</strong></p>
",,2016-10-06 07:54:48,Can I read a file object into a gensim Dictionary class?,<python><dictionary><nlp><gensim><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
10028,39973361,2016-10-11 08:36:47,,"<p>I was wondering what changed in Gensim Word2Vec model between 0.12.3 and 0.13.2.</p>

<p>When I train a small sample of sentences on 0.12.3 (setting the size=2 for visualization). The distribution looks as follows:
<a href=""https://i.stack.imgur.com/zMOcc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zMOcc.png"" alt=""enter image description here""></a></p>

<p>When I do the same on 0.13.2 it looks like this:
<a href=""https://i.stack.imgur.com/NYUxP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NYUxP.png"" alt=""enter image description here""></a></p>

<p>Did the dimensions change to polar or something?</p>
",,2016-10-11 08:36:47,Gensim Word2Vec dimension change between 0.12.3 and 0.13.2,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10057,39944487,2016-10-09 14:07:01,,"<p>This is the structure I'm dealing with:</p>

<pre><code>src/
    processing/
        station_level/
            train_paragraph_vectors.py
    doc2vec_ext.py
    word_embeddings_station_level.py
</code></pre>

<p>I have trained and stored a model in <code>word_embeddings_station_level.py</code> like this:</p>

<pre><code>from src.doc2vec_ext import WeightedDoc2Vec

# ...

model = WeightedDoc2Vec(
    # ...
)

train(model, vocab, station_sentences, num_epochs)

# Saving the model -&gt; pickles it
model.save(open(model_file, ""w""))
</code></pre>

<p>This is working fine so far. However, I want to load that model in <code>train_paragraph_vectors.py</code> like this:</p>

<pre><code>import sys
from src import doc2vec_ext
sys.modules[""doc2vec_ext""] = doc2vec_ext

if __name__ == ""__main__"":
# ...
    model = doc2vec_ext.WeightedDoc2Vec.load(station_level_sentence_vectors)
</code></pre>

<p>but I'm getting:</p>

<pre><code>Traceback (most recent call last):
  File ""E:/python/kaggle/seizure_prediction/src/processing/station_level/train_paragraph_vectors.py"", line 57, in &lt;module&gt;
    model = doc2vec_ext.WeightedDoc2Vec.load(station_level_sentence_vectors)
  File ""C:\Python27\lib\site-packages\gensim\models\word2vec.py"", line 1684, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 248, in load
    obj = unpickle(fname)
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 911, in unpickle
    return _pickle.loads(f.read())
ImportError: No module named doc2vec_ext
</code></pre>

<p><strong>doc2vec_ext.py</strong></p>

<p>Here you can see, that I just inherit from the <code>gensim.models.Doc2Vec</code> class and do some stuff:</p>

<pre><code>class WeightedDoc2Vec(Doc2Vec):

    def __init__(self, dm=1,window=5, f_size=0, size=100, min_count=1, negative=0, dbow_words=1, alpha=0.015, workers=8, seed=42, dm_weighted=False, dm_stacked=False):
        Doc2Vec.__init__(self,
            # Constructor arguments ..
            )

        # ...
</code></pre>

<p>I don't know what's the problem here. I've tried to do the <code>sys.modules[]</code> but it's still not working properly.</p>

<p>How can I load my stored model?</p>

<hr>

<p><strong>Important:</strong></p>

<p>I noticed that I can't even load from the same module. If I try to load the model in the file where it was created (here <code>word_embeddings_station_level.py</code>) it's still not working giving me the same error.</p>
",2016-10-09 14:36:22,2016-10-09 18:17:26,Pickle load: ImportError: No module named doc2vec_ext,<python><pickle><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10101,40121204,2016-10-19 02:42:17,,"<p>I want to use Google word2vec (GoogleNews-vectors-negative300.bin) <br>
I downloaded it from <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow"">https://code.google.com/archive/p/word2vec/</a><br>
When I load it, the memory errors occured <br>
(Process finished with exit code 139 (interrupted by signal 11: SIGSEGV))</p>

<pre><code>from gensim.models.word2vec import Word2Vec
embedding_path = ""data/GoogleNews-vectors-negative300.bin""
word2vec = Word2Vec.load_word2vec_format(embedding_path, binary=True)
print word2vec
</code></pre>

<p>I use ubuntu 16.04 / GTX-1070(8gb) / Ram(16gb).
How can I fix it?!
 <a href=""https://i.stack.imgur.com/wreiV.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/wreiV.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/iaFpr.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/iaFpr.png"" alt=""enter image description here""></a></p>
",,2018-06-09 15:00:59,Google word2vec load error,<gpu><segmentation-fault><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10120,40158051,2016-10-20 15:01:36,,"<p>I have this sentences iterator that is using <code>yield</code> so it's formally a generator</p>

<pre><code>from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import json
import os

class LyricsCorpus(object):

    def __init__(self, corpus, tokenize=False, deaccent=False):
        self.corpus = corpus
        self.tokenize = tokenize
        self.deaccent = deaccent

    def __iter__(self):
        for index,fname in enumerate( os.listdir(self.corpus) ):
            with open( os.path.join(self.corpus, fname) ) as data_file:
                data = json.load(data_file)
                for item in data:
                    if ""lyrics"" in item:
                        if ""lyrics_body"" in item[""lyrics""]:
                            if self.tokenize:
                                yield self.tokens( item[""lyrics""][""lyrics_body""] )
                            else:
                                yield item[""lyrics""][""lyrics_body""].split()
    '''
        This lowercases, tokenizes, de-accents (optional).  the output are final tokens = unicode strings, that wont be processed any further.
    '''
    def tokens(self,text):
        return [token for token in simple_preprocess(text, deacc=self.deaccent, min_len=2, max_len=15) if token not in STOPWORDS]
</code></pre>

<p>when running into <code>Word2vec</code> like</p>

<pre><code>min_count = 1
size = 50
window = 4
model = Word2Vec(corpus_iterator, min_count=min_count, size=size, window=window)
</code></pre>

<p>the iterator does not stop, <em>looping through the files in the corpus folder indefinitively</em>. This does not happen in a ordinary iteration like</p>

<pre><code>from LyricsCorpus import *
it=LyricsCorpus('./corpus')
[item for k in it]
</code></pre>
",2019-05-18 20:25:12,2019-05-18 20:25:12,Gensim word2vec: iterator does not stop with yield,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
10133,40091380,2016-10-17 16:30:06,,"<p>I am using word vectors for text classification solution. I am using word vectors mainly to address the case of synonyms which are not there in the training set but will be present in the actual use-cases. By simply using word vectors, I am not getting a good enough accuracy in prediction. Can anyone please suggest some enhancements I can do over word vectors in order to improve accuracy?</p>
",,2016-10-18 01:48:37,Enhancements for text classification using word vectors,<machine-learning><scikit-learn><text-classification><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
10141,40160526,2016-10-20 17:11:33,,"<p>wiki= gensim.corpora.MmCorpus(r'C:\Users\Public\Documents\Python Scripts\wiki_en_vocab200k.mm')</p>

<p>tfidf= gensim.models.TfidfModel.load(r'C:\Users\tfidf_model')</p>

<p>corpus_tfidf = tfidf[wiki]</p>

<p>I have the above steps but now I want to be able to find individual word scores</p>

<p>Many thanks in advance</p>
",,2016-10-20 17:11:33,How do I get the TF-IDF score for a particular word in Gensim,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10150,40205725,2016-10-23 17:23:45,,"<p>So I am trying to use gensim to generate an LSI model along with corpus_lsi following <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow"">this</a> tutorial.</p>

<p>I start with a corpus and a dictionary that I generated myself.
The list of documents are too small (9 lines = 9 documents), which is the sample list provided in <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim</a> tutorials</p>

<p>However, pythos just crashes when it reaches the line for generating LSI_model.
You can see below my code along with the generated output</p>

<p><strong>Code</strong></p>

<pre><code>#!/usr/bin/env python
import os
from gensim import corpora, models, similarities
import logging

#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

if __name__ == '__main__':
    if (os.path.exists(""tmp\dictionary.dict"")):
        dictionary = corpora.Dictionary.load('tmp\dictionary.dict')
        corpus = corpora.MmCorpus('tmp\corpus.mm')
        print(""Used files generated Dataset Generator"")
    else:
        print(""Please run dataset generator"")

print (""generating tf-idf model ..."")
tfidf = models.TfidfModel(corpus)   # Generate tfidf matrix (tf-idf model)
print (""generating corpus_tf-idf model ..."")
corpus_tfidf = tfidf[corpus]    #use the model to transform vectors

print (""generating LSI model ..."")
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
print (""generating corpus_lsi model ..."")
corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi

lsi.print_topics(2)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>Used files generated Dataset Generator
generating tf-idf model ...
generating corpus_tf-idf model ...
generating LSI model ...
</code></pre>

<p>After printing ""generating LSI model"" it crashes</p>

<p>Any suggestions ?</p>

<p><strong>Other things I tried</strong></p>

<ul>
<li>Changing python version to python 2.6</li>
<li>Removing gensim and installing it again from github (instead of conda) </li>
</ul>
",2016-10-24 07:22:42,2016-11-04 16:05:48,"gensim Generating LSI model causes ""Python has stopped working""",<python><python-3.x><gensim><latent-semantic-indexing><latent-semantic-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
10158,40168109,2016-10-21 03:53:22,,"<pre><code>from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from gensim import corpora, models 
import gensim
import os
from os import path
from time import sleep

tokenizer = RegexpTokenizer(r'\w+')
en_stop = set(get_stop_words('en'))
with open(os.path.join('c:\users\kaila\jobdescription.txt')) as f:
    Reader = f.read()


texts = unicode(Reader, errors='replace')
tdm = []

#Converted all the text to lowercase for uniform analysis
raw = texts.lower()
#Tokenized the text to individual terms and created the stop list
tokens = tokenizer.tokenize(raw)
stopped_tokens = [i for i in tokens if not i in en_stop]
tdm.append(stopped_tokens)

dictionary = corpora.Dictionary(tdm)
corpus = [dictionary.doc2bow(i) for i in tdm]
sleep(3)
#Implemented the LdaModel
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary)
print(ldamodel.print_topics(num_topics=5, num_words=8))
</code></pre>

<p>The issue is my result has similar words to describe all the topics and the probabilities are way too low. Is there anything I am doing wrong? If anyone can assist me in yielding better results it will be great. </p>

<p>The following is my result:</p>

<p><strong>(0, u'0.019*will + 0.010*experience + 0.009*accounts + 0.009*finance + 0.008*accounting + 0.008*financial + 0.008*team + 0.007*reporting'), (4, u'0.016*will + 0.009*finance + 0.009*experience + 0.008*management + 0.008*accounting + 0.007*role + 0.007*financial + 0.007*work'), (7, u'0.017*will + 0.013*experience + 0.012*finance + 0.008*role + 0.008*financial + 0.007*accounting + 0.007*accounts + 0.007*years'), (2, u'0.019*will + 0.011*experience + 0.010*finance + 0.008*role + 0.007*business + 0.007*accounts + 0.007*reporting + 0.007*work'), (5, u'0.013*will + 0.011*finance + 0.011*experience + 0.009*financial + 0.008*management + 0.008*accounting + 0.008*role + 0.007*please')</strong></p>
",,2016-10-24 08:14:12,After implementing topic modelling of a text file I am getting similar words to describe all the topics and the results are inaccurate.,<python-2.7><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,False
10163,40247112,2016-10-25 18:27:34,,"<p>I am using a LDA model on a corpus to learn the topics covered in it. I am using the gensim package (e.g., gensim.models.ldamodel.LdaModel); can easily use other versions of LDA if necessary.</p>

<p>My question is what is the most efficient way to use the parameterized model and/or topic words or topic IDs to find and retrieve new documents that contain the topic?</p>

<p>Concretely, I want to scrape a media API to find new articles (out-of-sample documents) that relate to my topics contained in my original corpus. Because I am doing this 'blind search', running the LDA on each new document may be too cumbersome; most new documents will not contain the topic.</p>

<p>Can of course simply retrieve new documents that contain one to n most of the frequent words of the LDA-learned topics; and then apply LDA to the returned documents for further confidence.</p>

<p>I am wondering if there is a more sophisticated method that gives better confidence that the new out-of-sample articles actually contain the same topic; as opposed to coincidentally containing one or two of the topic words. </p>

<p>Am looking at Topic Tiling algorithms but not sure if they are applicable here.</p>
",,2016-10-27 12:34:02,"How to use Topic Model (LDA) output to match and retrieve new, same-topic documents",<text><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
10171,40296765,2016-10-28 01:49:44,,"<p>I am preparing a Doc2Vec model using tweets. Each tweet's word array is considered as a separate document and is labeled as ""SENT_1"", SENT_2"" etc.</p>

<pre>
taggeddocs = []
for index,i in enumerate(cleaned_tweets):
    if len(i) > 2: # Non empty tweets
        sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[u'SENT_{:d}'.format(index)])
        taggeddocs.append(sentence)

# build the model
model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)

for epoch in range(200):
    if epoch % 20 == 0:
        print('Now training epoch %s' % epoch)
    model.train(taggeddocs)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</pre>

<p>I wish to find tweets similar to a given tweet, say ""SENT_2"". How?</p>

<p>I get labels for similar tweets as:</p>

<pre>
sims = model.docvecs.most_similar('SENT_2')
for label, score in sims:
    print(label)
</pre>

<p>It prints as:</p>

<pre>
SENT_4372
SENT_1143
SENT_4024
SENT_4759
SENT_3497
SENT_5749
SENT_3189
SENT_1581
SENT_5127
SENT_3798
</pre>

<p>But given a label, how do I get original tweet words/sentence? E.g. what are the tweet words of, say, ""SENT_3497"". Can I query this to Doc2Vec model?</p>
",,2017-01-19 03:54:21,How to extract words used for Doc2Vec,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10180,40181943,2016-10-21 16:59:07,,"<p>I am trying to get the keys as well as the vectors in the vector <code>model.syn0</code> which gives vectors by <code>model.syn0[""word""]</code> which gives an n-dim vector. Is there a better way to create a list of all the words in the model in the same order as the the vectors of <code>syn0</code> than this? I have 350000 words and this would take too long.</p>

<pre><code>from gensim.models import word2vec as wv
model = wv.Word2Vec.load('model')
lab=[]
for i in model.syn0:
    lab.append(model.similar_by_vector(i)[0])

print(type(model.syn0))
    &lt;type 'numpy.ndarray'&gt;
</code></pre>
",2016-10-23 16:10:45,2017-04-15 09:13:11,How to get the key value pairs in numpy.ndarray? (Gensim Word2vec),<python><performance><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10186,40250057,2016-10-25 21:29:47,,"<p>I am wondering if it is possible to train spark word2vec in batch mode. Or in other words, if it is possible to update the vocabulary list of a spark word2vec model which is already trained. 
My application is:
my paragraphs are located in multiple files, and when I use gensim i can do</p>

<pre><code>class MySentences(object):
    def __init__(self, file_list, folder):
        self.file_list = file_list
        self.folder = folder

    def __iter__(self):
        for file in self.file_list:
            if 'walk_' in file:
                print file
                with open(self.folder + file, 'r') as f:
                    for line in f:
                        yield line.split()

model = Word2Vec(MySentences(files, fileFolder), size=32, window=5, min_count=5, workers=15)  
</code></pre>

<p>i can even do</p>

<pre><code>for epoch in range(10):
    model.train(MySentences(files, fileFolder))
</code></pre>

<p>I am wondering how I can do similar things in spark word2vec. </p>

<p>In spark, I found I can only do RDD union with multiple files as:</p>

<pre><code>from pyspark.mllib.feature import Word2Vec 
from pyspark.sql import SQLContext

inp1 = sc.textFile(""file1"").map(lambda row: row.split('\t'))
inp2 = sc.textFile(""file2"").map(lambda row: row.split('\t'))

inp = sc.union([inp1,inp2])
word2vec = Word2Vec().setVectorSize(4).setMinCount(1)
model = word2vec.fit(inp)
</code></pre>

<p>otherwise, if I train model with inp1, then inp2, the words from inp1 will be gone. </p>

<p>If i cannot do the training on batch mode, how can i update a trained model with new paragraphs in future? </p>
",2016-10-25 21:41:36,2017-02-28 16:38:46,Is it possible to train spark word2vec model in batch mode,<apache-spark><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10189,40149951,2016-10-20 09:03:16,,"<p>I follow <a href=""http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"" rel=""nofollow noreferrer"">this</a> tutorial and everything is fine, I preprocess and train my model But when I want to find similarity with following code:</p>

<pre><code>model = gensim.models.Word2Vec.load(""wiki.fa.word2vec.model"")
print model.most_similar(U'')
</code></pre>

<p>Vectors are not in proper format:</p>

<pre><code>[(u'\u0631\u0648\u0632', 0.6399222612380981), (u'\u0647\u0641\u062a\u0647', 0.5578583478927612), (u'\u0645\u0627\u0647\u0647\u0627\u06cc', 0.5577661991119385), (u'\u062f\u0631\u0645\u0627\u0647', 0.5260834097862244), (u'\u0634\u0627\u0645\u06af\u0627\u0647', 0.5142802596092224), (u'\u06cc\u06a9\u0645\u0627\u0647', 0.48211610317230225), (u'\u0642\u062f\u06cc\u0631\u0641', 0.4799095690250397), (u'\u06cc\u06a9\u0633\u0627\u0644', 0.47623544931411743), (u'\u0645\u0627\u0647\u0647', 0.46996498107910156), (u'\u062d\u0648\u062a', 0.4551585912704468)]
</code></pre>

<p>Does anyone knows how to fix this issue?</p>
",2019-03-18 19:57:59,2019-03-18 19:57:59,Arabic/Persian language isn't printed correctly to screen,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
10202,40230532,2016-10-25 02:41:08,,"<p>Here is my code for training my doc2vec model</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from FileDocIterator import FileDocIterator

doc_file_name = 'doc_6million.txt'
docs = FileDocIterator(doc_file_name)
print ""Fitting started""
model = Doc2Vec(docs, size=100, window=5, min_count=5, negative=20, workers=6, iter=4)
print ""Saving model""
model.save(""doc2vec_model"")
print ""model saved""
</code></pre>

<p>Now lets take a look at <code>FileDocIterator</code></p>

<pre><code>import json

from gensim.models.doc2vec import TaggedDocument
from gensim.models import Phrases

class FileDocIterator(object):
    def __init__(self, fileName):
        self.fileName = fileName
        self.phrase = Phrases.load(""phrases"")

    def __iter__(self):
        for line in open(self.fileName):
            jsData = json.loads(line)
            yield TaggedDocument(words=jsData[""data""], tags=jsData[""id""])
</code></pre>

<p>Now I do understand that phrases isn't being used in this implementation, but bear with me here, lets take a look at how the data looks like. Here is the first data point</p>

<pre><code>{""data"":[""strategic"",""and"",""analytical"",""technical"",""program"",""director"",""and"",""innovator"",""who"",""inspires"",""calculated"",""risk-taking"",""in"",""emerging"",""technologies"","","",""such"",""as"",""cyber"",""security"","","",""risk"","","",""analytics"","","",""big"",""data"","","",""cloud"","","",""mobility"",""and"",""3d"",""printing"",""."",""known"",""for"",""growing"",""company"",""profit"",""through"",""innovative"",""thinking"",""aimed"",""at"",""improving"",""employee"",""productivity"",""and"",""providing"",""solutions"",""to"",""private"",""industry"",""and"",""government"",""customers"",""."",""recognized"",""for"",""invigorating"",""creative"",""thinking"",""and"",""collaboration"",""within"",""large"",""companies"",""to"",""leverage"",""their"",""economies"",""of"",""scale"",""to"",""capture"",""market"",""share"",""."",""successful"",""in"",""managing"",""the"",""risk"",""and"",""uncertainty"",""throughout"",""the"",""innovation"",""lifecycle"",""by"",""leveraging"",""an"",""innovation"",""management"",""framework"",""to"",""overcome"",""barriers"",""."",""track"",""record"",""of"",""producing"",""results"",""in"",""competitive"","","",""rapidly"",""changing"",""environments"",""where"",""innovation"",""and"",""customer"",""satisfaction"",""is"",""the"",""business"",""."",""competencies"",""include"","":"",""innovation"",""management"",""cyber"","","",""risk"","","",""analytics"","","",""cloud"",""computing"",""and"",""mobility"",""technology"",""development"",""security"",""compliance"","":"",""dod/ic"",""("",""nispom"","","",""icd"",""503"","","",""fedramp"","")"",""commercial"",""("",""iso/iec"",""27002"","","",""pci"",""dss"","")"",""relationship"",""management"","":"",""dod"","","",""public"",""sector"",""and"",""intelligence"",""community"",""change"",""management"",""it"",""security"",""&amp;"",""risk"",""management"",""("",""cissp"","")"",""program"","","",""product"",""&amp;"",""portfolio"",""management"",""("",""pmp"","")"",""data"",""analytics"",""management"",""("",""cchd"","")"",""itil"",""service"",
""management"",""("",""itilv3-expert"","")""],
""id"":""55c37f730d03382935e12767""}
</code></pre>

<p>My understanding is that the id, <code>55c37f730d03382935e12767</code> should be the id of the document, so doing the following ought to give me back a docVector.</p>

<pre><code>model.docvecs[""55c37f730d03382935e12767""]
</code></pre>

<p>Instead, this is what is outputed. </p>

<pre><code>&gt;&gt;&gt; model.docvecs[""55c37f730d03382935e12767""]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 341, in __getitem__
    return self.doctag_syn0[self._int_index(index)]
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 315, in _int_index
    return self.max_rawint + 1 + self.doctags[index].offset
KeyError: '55c37f730d03382935e12767'
</code></pre>

<p>Trying to get most similar gives the following back</p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(""55c37f730d03382935e12767"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 450, in most_similar
    raise KeyError(""doc '%s' not in trained set"" % doc)
KeyError: ""doc '55c37f730d03382935e12767' not in trained set""
</code></pre>

<p>What I'm trying to understand is how are doc vectors saved and what id's are used. What part of my approach isn't working above? </p>

<p>Now here's something interesting, if I do the following I get back similar doc vectors but they have no meaning to me. </p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(str(1))
[(u'8', 0.9000369906425476), (u'3', 0.8878246545791626), (u'7', 0.886141836643219), (u'2', 0.8834314942359924), (u'e', 0.8812381029129028), (u'a', 0.8648831248283386), (u'd', 0.8587037920951843), (u'0', 0.8413013219833374), (u'4', 0.8385311365127563), (u'c', 0.8290119767189026)]
</code></pre>
",2018-12-15 19:48:33,2018-12-15 19:48:33,gensim doc2vec documents not found by id,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
10208,40315446,2016-10-29 02:21:12,,"<p>I am trying to implement doc2vec from gensim but having some errors and theres not enough documentation or help on the web.
Here is part of my working code:</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence

class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        with open(self.filename, 'r') as f:
            for uid, line in enumerate(f):
                print LabeledSentence(line.split(), tags=['TXT_%s' % uid])
                yield LabeledSentence(words=line.split(), tags=['TXT_%s' % uid])

sentences = LabeledLineSentence('myfile.txt')
</code></pre>

<p>what my txt file looks like:</p>

<pre><code>  1 hi how are you
  2 hi how are you
  3 hi how are you
  4 its such a great day
  5 its such a great day
  6 its such a great day
  7 i like dogs
  8 i like cats
  9 i like snakes
 10 the ice cream was yummy
 11 the cake was awesome  
</code></pre>

<h1>init the model</h1>

<pre><code>model = Doc2Vec(alpha=0.025, min_alpha=0.025, size=50, window=5, min_count=5,
                dm=1, workers=8, sample=1e-5)       
</code></pre>

<h1>example print output:</h1>

<pre><code>LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_0'])
LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_1'])
LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_2'])
LabeledSentence(['its', 'such', 'a', 'great', 'day'], ['TXT_3'])
LabeledSentence(['its', 'such', 'a', 'great', 'day'], ['TXT_4'])
</code></pre>

<p>This is where the error is:</p>

<pre><code>for epoch in range(500):
    try:
        print 'epoch %d' % (epoch)
        model.train(sentences)
        model.alpha *= 0.99
        model.min_alpha = model.alpha
    except (KeyboardInterrupt, SystemExit):
        break

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>Any idea why? </p>
",2016-10-29 07:49:11,2016-10-30 10:08:23,Python simple implementation of doc2vec?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10211,40356631,2016-11-01 08:44:08,,"<p>I have a set of documents and I want to know the topic distribution for each document (for different values of number of topics). I have taken a toy program from <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda"">this question</a>.
I have first used LDA provided by gensim and then I am again giving test data as my training data itself to get the topic distribution of each doc in training data . But I am getting uniform topic distribution always.</p>

<p>Here is the toy code I used</p>

<pre><code>import gensim
import logging
logging.basicConfig(filename=""logfile"",format='%(message)s', level=logging.INFO)


def get_doc_topics(lda, bow):
    gamma, _ = lda.inference([bow])
    topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution

documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
mm = [dictionary.doc2bow(text) for text in texts]
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=2, update_every=1, chunksize=10000, passes=1,minimum_probability=0.0)

newdocs=[""human system""]
print lda[dictionary.doc2bow(newdocs)]

newdocs=[""Human machine interface for lab abc computer applications""] #same as 1st doc in training
print lda[dictionary.doc2bow(newdocs)]
</code></pre>

<p>Here is the output:</p>

<pre><code>[(0, 0.5), (1, 0.5)]
[(0, 0.5), (1, 0.5)]
</code></pre>

<p>I have checked with some more examples but all ended up giving the same equiprobable result.</p>

<p>Here is the logfile generated(i.e output of logger)</p>

<pre><code>adding document #0 to Dictionary(0 unique tokens: [])
built Dictionary(42 unique tokens: [u'and', u'minors', u'generation', u'testing', u'iv']...) from 9 documents (total 69 corpus positions)
using symmetric alpha at 0.5
using symmetric eta at 0.5
using serial LDA version on this node
running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
-5.796 per-word bound, 55.6 perplexity estimate based on a held-out corpus of 9 documents with 69 words
PROGRESS: pass 0, at document #9/9
topic #0 (0.500): 0.057*""of"" + 0.043*""user"" + 0.041*""the"" + 0.040*""trees"" + 0.039*""interface"" + 0.036*""graph"" + 0.030*""system"" + 0.027*""time"" + 0.027*""response"" + 0.026*""eps""
topic #1 (0.500): 0.088*""of"" + 0.061*""system"" + 0.043*""survey"" + 0.040*""a"" + 0.036*""graph"" + 0.032*""trees"" + 0.032*""and"" + 0.032*""minors"" + 0.031*""the"" + 0.029*""computer""
topic diff=0.539396, rho=1.000000
</code></pre>

<p>It says ' too few updates, training might not converge' so I have tried increasing no of passes to 1000 but the output is still same.
(though it is not related to convergence , I have also tried increasing no of topics)</p>
",2017-05-23 11:46:16,2017-01-10 11:31:12,gensim LDA module : Always getting uniform topical distribution while predicting,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10219,40318719,2016-10-29 11:42:13,,"<p>I have made a sample program for getting topic distribution per document after doing LDA using gensim</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]   

stoplist=[""is"",""are"",""am"",""were"",""a"",""me"",""I""]

texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, update_every=1, chunksize=10000, passes=1)
lda.print_topics(2)
</code></pre>

<p>But the program is not printing anything.. Any changes required?</p>
",,2016-10-29 12:26:36,Printing topic distribution after LDA using gensim,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10231,40278023,2016-10-27 06:51:50,,"<pre><code>from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from gensim import corpora, models
import gensim
import os
from os import path
from time import sleep
import matplotlib.pyplot as plt
import random
from wordcloud import WordCloud, STOPWORDS
tokenizer = RegexpTokenizer(r'\w+')
en_stop = set(get_stop_words('en'))
with open(os.path.join('c:\users\kaila\jobdescription.txt')) as f:
    Reader = f.read()

Reader = Reader.replace(""will"", "" "")
Reader = Reader.replace(""please"", "" "")


texts = unicode(Reader, errors='replace')
tdm = []

raw = texts.lower()
tokens = tokenizer.tokenize(raw)
stopped_tokens = [i for i in tokens if not i in en_stop]
tdm.append(stopped_tokens)

dictionary = corpora.Dictionary(tdm)
corpus = [dictionary.doc2bow(i) for i in tdm]
sleep(3)
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=8, id2word = dictionary)
topics = ldamodel.print_topics(num_topics=8, num_words=200)
for i in topics:
    print(i)
    wordcloud = WordCloud().generate(i)
    plt.imshow(wordcloud)
    plt.axis(""off"")
    plt.show()
</code></pre>

<p>The issue is with the word cloud. I cannot get the word cloud for each of the 8 topics. I would want an output which gives 8 word clouds for the 8 topics. 
If anyone can help me regarding this issue, it will be great. </p>
",,2017-04-19 01:38:21,How do I print lda topic model and the word cloud of each of the topics,<python><topic-modeling><word-cloud>,,,CC BY-SA 3.0,True,False,True,False,False
10241,40326300,2016-10-30 05:32:50,,"<p>I am using Gensim's doc2vec method to read in my text file which contains 1 sentence per line. It reads my file into a dictionary where the keys are a tokenized list of terms and the values are the sentence number.</p>

<p>Here is my code:</p>

<pre><code>    from gensim import utils
    from gensim.models.doc2vec import LabeledSentence,TaggedLineDocument
    from gensim.models import Doc2Vec
    new_file = open('new_file.txt','w')
    with open('myfile.txt','r') as inp:
        for line in inp:
            utils.simple_preprocess(line)
            file1.write(str(utils.simple_preprocess(line)) + ""\n"")
    file1.close()
</code></pre>

<p>Example output of new file:</p>

<pre><code>[u'hi', u'how', u'are', u'you']
[u'its', u'such', u'great', u'day']
[u'its', u'such', u'great', u'day']
[u'its', u'such', u'great', u'day']
</code></pre>

<p>Then i feed that list into gensim's taggedlinedocument function:</p>

<pre><code>s = TaggedLineDocument('myfile.txt')
for k,v in s:
    print k, v
</code></pre>

<p>Example Output:</p>

<pre><code>[u'hi', u'how', u'are', u'you'] [0]
[u'hi', u'how', u'are', u'you'] [1]
[u'hi', u'how', u'are', u'you'] [2]
[u'its', u'such', u'a', u'great', u'day'] [3]
[u'its', u'such', u'a', u'great', u'day'] [4]
</code></pre>

<p>Question is, given the tag id (example 0), how do i get back the original sentence?</p>
",2016-10-30 05:37:52,2017-01-19 03:44:27,python gensim retrieve original sentences from doc2vec taggedlinedocument,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10252,40379531,2016-11-02 12:04:37,,"<p>I have used three different ways to calculate the matching between the resume and the job description. Can anyone tell me that what method is the best and why?</p>

<ol>
<li><p>I used NLTK for keyword extraction and then RAKE for
keywords/keyphrase scoring, then I applied cosine similarity.</p></li>
<li><p>Scikit for keywords extraction, tf-idf and cosine similarity
calculation.</p></li>
<li><p>Gensim library with LSA/LSI model to extract keywords and calculate
cosine similarity between documents and query.</p></li>
</ol>
",2016-11-02 16:32:20,2017-08-27 16:49:24,"Best for resume, document matching",<scikit-learn><nltk><information-retrieval><tf-idf><gensim>,,,CC BY-SA 3.0,True,False,True,False,True
10254,40436110,2016-11-05 08:12:51,,"<p>I am trying to calculate similarity. First of all i used RAKE library to extract the keywords from the crawled jobs. Then I put the keywords of every jobs into separate array and then combined all those arrays into documentArray.</p>
<blockquote>
<p>documentArray = ['Anger
command,Assertiveness,Approachability,Adaptability,Authenticity,Aggressiveness,Analytical
thinking,Molecular Biology,Molecular Biology,Molecular
Biology,molecular biology,molecular biology,Master,English,Molecular
Biology,,Islamabad,Islamabad District,Islamabad Capital
Territory,Pakistan,,Rawalpindi,Rawalpindi,Punjab,Pakistan'&quot;],
['competitive compensation,assay design,positive attitude,regular
basis,motivate others,meetings related,improve state,travel on,phd
degree,meeting abstracts,benefits package,daily basis,scientific
papers,application notes']</p>
</blockquote>
<hr />
<blockquote>
<p>queryStr = 'In Vitro,Biochemistry,PCR,Western
Blotting,Neuroscience,Molecular Biology,Cell
biology,Immunohistochemistry,Microscopy,Animal
Models,Presentations,Immunoprecipitation,Cell biology,Master's
Degree,Bachelor's Degree,,,,,'</p>
</blockquote>
<p>Then I wrote the following GENSIM code,</p>
<blockquote>
<p>class Gensim:</p>
<pre><code>def __init__(self):
    print(&quot;Init&quot;)

def calculateGensimSimilarity(self, texts, query):
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=2)
    index_lsi = similarities.MatrixSimilarity(lsi[corpus])
    index_lda = similarities.MatrixSimilarity(lda[corpus])
    vec_bow = dictionary.doc2bow(query.lower().split())
    vec_lsi = lsi[vec_bow]
    vec_lda = lda[vec_bow]
    print(&quot;LSI Model&quot;)
    sims_lsi = index_lsi[vec_lsi]
    print(&quot;LDA Model&quot;)
    print(sims_lsi)
    sims_lda = index_lda[vec_lda]
    print(sims_lda)
</code></pre>
</blockquote>
<p>It is printing LSA score 0 and LDA score 90%+ match. Kindly let me know where I am wrong and how can i modify to calculate the correct cosine similarity.</p>
<blockquote>
<p>LSA Score[ 0.  0.]
LDA Score[ 0.94234258  0.9477495 ]</p>
</blockquote>
",2020-06-20 09:12:55,2016-11-05 11:25:48,RAKE with GENSIM,<python><rake><information-retrieval><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
10278,40458742,2016-11-07 06:03:00,,"<p>In the word2vec model, there are two linear transforms that take a word in vocab space to a hidden layer (the ""in"" vector), and then back to the vocab space (the ""out"" vector). Usually this out vector is discarded after training. I'm wondering if there's an easy way of accessing the out vector in gensim python? Equivalently, how can I access the out matrix?</p>

<p>Motivation: I would like to implement the ideas presented in this recent paper: <a href=""https://arxiv.org/pdf/1602.01137v1.pdf"" rel=""noreferrer"">A Dual Embedding Space Model for Document Ranking</a></p>

<p>Here are more details. From the reference above we have the following word2vec model:</p>

<p><a href=""https://i.stack.imgur.com/OpupG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OpupG.png"" alt=""enter image description here""></a></p>

<p>Here, the input layer is of size $V$, the vocabulary size, the hidden layer is of size $d$, and an output layer of size $V$. The two matrices are W_{IN} and W_{OUT}. <em>Usually</em>, the word2vec model keeps only the W_IN matrix. This is what is returned where, after training a word2vec model in gensim, you get stuff like:</p>

<blockquote>
  <p>model['potato']=[-0.2,0.5,2,...] </p>
</blockquote>

<p>How can I access, or retain W_{OUT}? This is likely quite computationally expensive, and I'm really hoping for some built in methods in gensim to do this because I'm afraid that if I code this from scratch, it would not give good performance.</p>
",2016-11-12 18:37:16,2018-09-04 00:23:44,gensim word2vec accessing in/out vectors,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10287,40404411,2016-11-03 14:40:55,,"<p>I'm doing test development on how to use Lambda and S3 together with other possible AWS services to create a web service.</p>

<p>The background is to use a LDA model constructed using Gensim to analyse a text file that exist in S3. And the goal is to use Lambda's event trigger to automatically analyse texts that gets uploaded into the S3 bucket.</p>

<p>So far I have tried to create simple functions in Lambda to print and log an items properties when it gets uploaded to S3 bucket. However the analysis is an issue because of the LDA model is required to perform the analysis. </p>

<p>I can extract objects in S3 using the key by essentially calling:</p>

<pre><code>s3resource = boto3.resource('s3')
obj = s3resource.Object(BucketName, Key)
response = obj.get()
data = response['Body'].read()
</code></pre>

<p>However that makes the data variable as string. I need to extract the metadata so it can be loaded into a model object. So more in the style of:</p>

<pre><code>model = obj.get()...
</code></pre>

<p>First:
Is this possible? Loading a model file around 200-300mb into memory in Lambda to perform tasks?</p>

<p>Second:
Since this task should be invoked repeatedly for each file, my logic tells me to find ways to store the model in persistent memory to save fetch time... Is it possible to do more efficient object passing elsewhere?</p>

<p>P.S. to perform the analysis the required parameters are: the model, a wordid dict, the actual text that needs to be analysed. </p>

<p>Any help would be much appreciated and if there is an better fitting alternative, that would also be really awesome.</p>

<p>Thanks.</p>
",,2016-11-03 14:40:55,Use Lambda to load a non-string object from S3 to perform computation,<python><amazon-web-services><aws-lambda><lda><serverless-framework>,,,CC BY-SA 3.0,False,False,True,False,False
10302,40521982,2016-11-10 07:21:57,,"<p>I have referred the website <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a>. I have come across the error UnpicklingError was unhandled by user code : invalid load key,'%'. How do I clear that error? I had referred the other queries and included the klepto package but still that error persists. I am using anacoanda2. This is the code:-</p>

<pre><code>import logging
import xml.etree.cElementTree
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
level=logging.INFO)
import os
import klepto
from gensim import corpora

documents = [""Human machine interface for lab abc computer applications"",
         ""A survey of user opinion of computer system response time"",
         ""The EPS user interface management system"",
         ""System and human system engineering testing of EPS"",              
         ""Relation of user perceived response time to error measurement"",
         ""The generation of random binary unordered trees"",
         ""The intersection graph of paths in trees"",
         ""Graph minors IV Widths of trees and well quasi ordering"",
         ""Graph minors A survey""]
# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
     for document in documents]

# remove words that appear only once
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
for token in text:
    frequency[token] += 1

texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]

from pprint import pprint  # pretty-printer
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save_as_text('/tmp/deerwester.dict')  # store the dictionary, for future reference
print(dictionary)

print(dictionary.token2id)

new_doc = ""Human computer interaction""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)  # the word ""interaction"" does not appear in the dictionary and is ignored

corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.dict', corpus)  # store to disk, for later use
for c in corpus:
print(c)

class MyCorpus(object):
def __iter__(self):
    for line in open('/datasets/mycorpus.txt'):
        # assume there's one document per line, tokens separated by whitespace
        yield dictionary.doc2bow(line.lower().split())

corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!
print(corpus_memory_friendly)

for vector in corpus_memory_friendly:  # load one vector into memory at a time
print(vector)

from six import iteritems

# collect statistics about all tokens
dictionary = corpora.Dictionary(line.lower().split() for line in open('/datasets/mycorpus.txt'))

# remove stop words and words that appear only once
stop_ids = [dictionary.token2id[stopword] for stopword in stoplist 
        if stopword in dictionary.token2id]
once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]

# remove stop words and words that appear only once
dictionary.filter_tokens(stop_ids + once_ids)

# remove gaps in id sequence after words that were removed
dictionary.compactify()
print(dictionary)

# create a toy corpus of 2 documents, as a plain Python list
corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it

corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)

corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)
corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)
corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)

corpus = corpora.MmCorpus('/tmp/corpus.mm')

print(corpus)

# one way of printing a corpus: load it entirely into memory
print(list(corpus))  # calling list() will convert any sequence to a plain Python list


# another way of doing it: print one document at a time, making use of the streaming interface
for doc in corpus:
print(doc)

corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)

import gensim
import numpy as np
numpy_matrix = np.random.randint(10, size=[5,2])
corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)

import scipy.sparse
scipy_sparse_matrix = scipy.sparse.random(5,2)
corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)
scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)

from gensim import corpora, models, similarities
if (os.path.exists(""/tmp/deerwester.dict"")):
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(""Used files generated from first tutorial"")
else:
print(""Please run first tutorial to generate data set"")

tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

doc_bow = [(0, 1), (1, 1)]
print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi

lsi.print_topics(2)

for doc in corpus_lsi: # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly
print(doc)

lsi.save('/tmp/model.lsi') # same for tfidf, lda, ...
lsi = models.LsiModel.load('/tmp/model.lsi')

model = models.TfidfModel(corpus, normalize=True)

model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)

model.add_documents(another_tfidf_corpus) # now LSI has been trained on tfidf_corpus + another_tfidf_corpus
lsi_vec = model[tfidf_vec] # convert some new document into the LSI space, without affecting the model

model.add_documents(more_documents) # tfidf_corpus + another_tfidf_corpus + more_documents
lsi_vec = model[tfidf_vec]

model = models.RpModel(tfidf_corpus, num_topics=500)

model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)

model = models.HdpModel(corpus, id2word=dictionary)
</code></pre>
",,2016-11-10 07:21:57,"UnpicklingError was unhandled by user code : invalid load key,'%'",<python><dictionary><anaconda><gensim><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
10320,40432558,2016-11-04 22:25:51,,"<p>I am doing some topic modeling on newspaper articles, and have implemented LDA using gensim in Python3. Now I want to create a word cloud for each topic, using the top 20 words for each topic. I know I can print the words, and save the LDA model, but is there any way to just save the top words for each topic which I can further use for generating word clouds?</p>

<p>I tried to google it, but could not find anything relevant. Any help is appreciated.</p>
",,2017-10-04 06:38:15,How to generate word clouds from LDA models in Python?,<python><lda><word-cloud>,,,CC BY-SA 3.0,False,False,True,False,False
10327,40524768,2016-11-10 10:04:40,,"<p>I applied lda with both sklearn and with gensim. Then i checked perplexity of the held-out data. </p>

<p>I am getting negetive values for perplexity of gensim and positive values of perpleixy for sklearn. How do i compare those values.</p>

<p>sklearn perplexity = 417185.466838</p>

<p>gensim perplexity = -9212485.38144</p>
",,2016-11-15 17:52:49,Perplexity comparision issue in SKlearn LDA vs Gensim LDA,<python><scikit-learn><nlp><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
10335,40527326,2016-11-10 12:11:38,,"<p>I would like to extract data from the wikipedia summary page of ""machine learning"" and then use that data to build a word2vec model with gensim library.</p>

<p>So, first I get the wiki summary of ""machine learning"" (Wikipedia API for Python):</p>

<pre><code>sentences = wikipedia.summary(""machine learning"")
</code></pre>

<p>and then I create the model:</p>

<pre><code>model = gensim.models.Word2Vec(sentences, min_count=2, size=50, window=4)
</code></pre>

<p>The problem is that, if I print the vocabulary keys, I get a list of characters rather than a list of words. The following is the code that I use to print the vocabulary keys:</p>

<pre><code>print list(model.vocab.keys())
</code></pre>

<p>Where I am wrong?</p>

<p>Here I pasted the full code:</p>

<pre><code>import wikipedia, gensim.models
sentences = wikipedia.summary(""machine learning"")
model = gensim.models.Word2Vec(sentences, min_count=2, size=50, window=4)
print list(model.vocab.keys())
</code></pre>
",,2016-11-19 09:25:49,how to create a word2vec model with data extracted from wikipedia summary in python,<python><wikipedia><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10346,40531183,2016-11-10 15:25:29,,"<ul>
<li>I am running my code in ipython(running python version 2.7). 
Found error while calling ""pyLDAvis"" funtion to visualize my LDA generated topics. 
Following error :</li>
</ul>

<p>topics_df = pd.DataFrame([dict((y,x) for x, y in tuples) for tuples in topics])TypeError: 'int' object is not iterable</p>

<ul>
<li>Input data is in the form of a table which has two columns ""complaints_ID"" and ""complaints_txt"". I am trying to run topic model on each of the complaint_txt value</li>
</ul>

<p>Is this issue with Python version or arguments I am passing to this function?
Below is my code. </p>

<pre><code>from stop_words import get_stop_words
import pandas as pd
import numpy as np
from nltk import bigrams
from lib.lda import lda, visualizeLDA
from nltk.tokenize import RegexpTokenizer
from gensim import corpora, models
import gensim
import pyLDAvis.gensim

#provide path name here
mypath = "" ""
allcomplaints = pd.read_csv(mypath)
#combining complaints for each ID
myremarks= allcomplaints.groupby(['complaint_ID'])['complaint_txt'].agg(lambda x: ''.join(x)).values 
#create English stop words list
en_stop = get_stop_words('en')

#including domain specific stop words
my_stopwords = [""xx"",""xxxx""]
my_stopwords= [i.decode('utf-8') for i in my_stopwords]
en_stop = en_stop +my_stopwords

texts = []
for doc in myremarks:
        raw = doc.lower()
        tokens = bigrams(i for i in tokenizer.tokenize(raw)if not i in en_stop and len(i)&gt;1)
        mergedtokens = [i[0]+"" ""+i[1] for i in tokens]
        stopped_tokens = [i for i in mergedtokens if not i in en_stop]
        texts.append(stopped_tokens)
dictionary = corpora.Dictionary(texts)
print dictionary
    # convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

    # generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 5 , id2word = dictionary, passes = 1)
print(ldamodel.print_topics(num_topics=5))

#     Visualize ldamodel
vis= pyLDAvis.gensim.prepare(ldamodel,corpus,dictionary)
pyLDAvis.display(vis)
</code></pre>

<p># Following is the sample of data I am using to run LDA:</p>

<pre><code>Complaint_ID| Complaint_txt
------------| --------------
4545        | cust has billing issue
4545        | for $480 
6878        | connct issue for a day ne
6878        | ed immediate resoltn
</code></pre>
",2016-11-18 15:38:25,2020-06-17 05:53:07,'int' not iterable error in python While running pyldavis,<python-2.7><visualization><lda>,,,CC BY-SA 3.0,True,False,True,False,False
10349,40413866,2016-11-04 01:18:02,,"<p>i am going thorugh this paper <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"">http://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></p>

<p>and it states that</p>

<blockquote>
  <p>"" Theparagraph vector and word vectors are averaged or concatenated
  to predict the next word in a context. In the experiments, we use
  concatenation as the method to combine the vectors.""</p>
</blockquote>

<p>How does concatenation or averaging work?</p>

<p>example (if paragraph 1 contain word1 and word2):</p>

<pre><code>word1 vector =[0.1,0.2,0.3]
word2 vector =[0.4,0.5,0.6]

concat method 
does paragraph vector = [0.1+0.4,0.2+0.5,0.3+0.6] ?

Average method 
does paragraph vector = [(0.1+0.4)/2,(0.2+0.5)/2,(0.3+0.6)/2] ?
</code></pre>

<p>Also from this image:</p>

<p>It is stated that :</p>

<blockquote>
  <p>The paragraph token can be thought of as another word. It acts as a
  memory that remembers what is missing from the current context  or
  the topic of the paragraph. For this reason, we often call this model
  the Distributed Memory Model of Paragraph Vectors (PV-DM).</p>
</blockquote>

<p>Is the paragraph token equal to the paragraph vector which is equal to <code>on</code>?</p>

<p><a href=""https://i.stack.imgur.com/EQO9m.png""><img src=""https://i.stack.imgur.com/EQO9m.png"" alt=""enter image description here""></a></p>
",2016-11-04 06:53:29,2017-01-19 03:42:06,How does gensim calculate doc2vec paragraph vectors,<nlp><vectorization><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10353,40472070,2016-11-07 18:30:20,,"<p>I am trying to understand relation between word2vec and doc2vec vectors in Gensim's implementation. In my application, I am tagging multiple documents with same label (topic), I am training a doc2vec model on my corpus using dbow_words=1 in order to train word vectors as well. I have been able to obtain similarities between word and document vectors in this fashion which does make a lot of sense
For ex. getting documents labels similar to a word-
doc2vec_model.docvecs.most_similar(positive = [doc2vec_model[""management""]], topn = 50))</p>

<p>My question however is about theoretical interpretation of computing similarity between word2vec and doc2vec  vectors. Would it be safe to assume that when trained on the same corpus with same dimensionality (d = 200), word vectors and document vectors can always be compared to find similar words for a document label or similar document labels for a word. Any suggestion/ideas are most welcome.</p>

<p>Question 2: My other questions is about impact of high/low frequency of a word in final word2vec model. If wordA and wordB have similar contexts in a particular doc label(set) of documents but wordA has much higher frequency than wordB, would wordB have higher similarity score with the corresponding doc label or not. I am trying to train multiple word2vec models by sampling corpus in a temporal fashion and want to know if the hypothesis that as words get more and more frequent, assuming context relatively stays similar, similarity score with a document label would also increase. Am I wrong to make this assumption? Any suggestions/ideas are very welcome.</p>

<p>Thanks,
Manish</p>
",,2017-01-19 03:35:24,word vector and paragraph vector query,<similarity><gensim><word2vec><temporal><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10357,40564790,2016-11-12 16:06:36,,"<p>I am trying out Gensim for the first time and have a question now. I have trained a LSI Model with a corpus of prepared documents. My question is, how do i get to know if a new document is similar to my model generated from the corpus of documents. I dont want to know the similarity for the document to each document in my corpus like MatrixSimilarity does but rather know if the document is similar to my topic/model.</p>
",,2017-07-09 08:13:30,Doc2Vec Gensim Similarity between Document and Topic,<python><similarity><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10372,40289549,2016-10-27 16:12:17,,"<p>I am using word2vec for vectorization of texts and then k-means for clustering of texts using scikit-learn. After clustering, how do I get the top 5 or 10 words nearest to the centroid of each cluster? I am able to get all the words in the cluster but not able to get the nearest words. It was straight-forward when I was using tf-idf vectorizer as each feature in tf-idf maps to a word, but it is not the case with word2vec</p>

<p>Here's how I am using word2vec for k-means</p>

<pre><code>model = gensim.models.Word2Vec.load('w2v.mdel')
word_vectors =  vecTransform(input) #Convert input text to word vectors
km = KMeans(n_clusters=5)
idx = km.fit_predict(word_vectors)
</code></pre>
",2016-10-28 03:43:10,2016-10-28 12:18:17,k-means using word2vec : Find nearest words to centroids,<scikit-learn><nlp><k-means><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
10373,40581010,2016-11-14 02:17:09,,"<p>I want to visualize a word2vec created from gensim library. I tried sklearn but it seems I need to install a developer version to get it. I tried installing the developer version but that is not working on my machine . Is it possible to modify this code to visualize a word2vec model ?</p>

<p><a href=""https://lvdmaaten.github.io/tsne/code/tsne_python.zip"" rel=""nofollow noreferrer"">tsne_python</a></p>
",,2019-09-04 22:38:26,How to run tsne on word2vec created from gensim?,<scikit-learn><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
10376,40606524,2016-11-15 09:42:03,,"<p>I want to find N words with greatest TFIDF value for each document</p>

<p>I'm using gensim with a corpus to compute tfidf values:</p>

<pre><code>tfidf = models.TfidfModel(mmcorpus)
corpus_tfidf = tfidf[mmcorpus]
</code></pre>

<p>Then, I would like to get tf-idf of given words in each document of the corpus
By looking in my corpus_tfidf object, I see a corpus part with an index array of size 417 (which is the size of my corpus), but I doesn't find documentation to help me to get TFIDF of a word in a document of the corpus
Note: the answer of the question ""Getting TF-IDF Scores Of Words Using Gensim"" doesn't really solves the problem; it gives an unique value for each word</p>

<p>===EDIT===</p>

<p>From other sample codes, I've tried with success the following code (surely not as Pythonic than it can be): </p>

<pre><code>for doc in corpus_tfidf: 
    d = {} 
    for id, value in doc: 
        if value&gt;0: 
            d[corpus.dictionary.get(id)] = value
</code></pre>

<p>but I can't explain or document why I can do 'for doc in corpus_tfidf'and get something useful, corpus_tfidf is not clearly iterable and as it is, what define the objects obtained from each iteration?
Then, a subsequent question is: how can I find the source associated with the doc variable? I will create a separate question about that</p>
",2016-11-15 16:04:53,2016-11-15 16:04:53,Access to tfidf values in gensim,<python><tf-idf><gensim><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
10411,40651699,2016-11-17 09:55:53,,"<p>How can i compute the variance of the pre-trained word embeddings trained by word2Vec in gensim?<br>
In need the variance for the word2Vec-model trained by google (Google News corpus).<br>
Thanks.</p>
",,2016-11-17 09:55:53,Variance of the pre-trained word embeddings trained by word2Vec (Google News Corpus),<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10426,40671057,2016-11-18 06:53:42,,"<p>I've been trying to run an example of how t use word2vec from the gensim library of python but I keep getting this error </p>

<pre><code>    ValueError: The truth value of an array with more than one element is   ambiguous. Use a.any() or a.all()
</code></pre>

<p>This is my code, it's just a simple example :</p>

<pre><code>    from gensim.models import Word2Vec
    sentences = [['first', 'sentence'], ['second', 'sentence']]
    # train word2vec on the two sentences
    model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Note: I've made sure that gensim is installed with all its dependencies.</p>
",,2019-12-16 13:36:25,use a.all() or a.any() error while trying to use gensim word2vec,<python-2.7><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10438,40659565,2016-11-17 16:02:21,,"<p>I'm using Gensim TfidfModel model. this is my code:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split()) for line in open('aaa.txt'))

class MyCorpus(object):
    def __iter__(self):
        for line in open('aaa.txt'):
            yield dictionary.doc2bow(line.lower().split())

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus)

corpus_tfidf = tfidf[corpus]
</code></pre>

<p>now I want to extract tf-idf value of each word, I know that they are in corpus_tfidf variable and I tried some codes like below to view all of words tf-idf but I have a word like 'banana' and I want to find its tf-idf value. there is a access to find each word in dictionary like dictionary.token2id['banana'] but how can I get tf-idf of each word?</p>

<pre><code>{dictionary.get(id): value for doc in corpus_tfidf for id, value in doc}
</code></pre>

<p>My corpus has 6501598 documents, 585499 features, 64106768 non-zero entries, and it's important to get value of each word in minimum time.</p>
",,2016-11-17 16:02:21,Gensim Extracting TF-IDF value of a word in a corpus,<python><text><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10444,40727093,2016-11-21 18:33:30,,"<p>I have trained 26 million tweets with skipgram technique to create word embeddings as follows:</p>

<pre><code>sentences = gensim.models.word2vec.LineSentence('/.../data/tweets_26M.txt')
model = gensim.models.word2vec.Word2Vec(sentences, window=2, sg=1, size=200, iter=20)
model.save_word2vec_format('/.../savedModel/Tweets26M_All.model.bin', binary=True)
</code></pre>

<p>However, I am continuously collecting more tweets in my database. For example, when I have 2 million more tweets, I wanna update my embeddings with also considering this newcoming 2M tweets.</p>

<p>Is it possible to load previously trained model and update weights of embeddings (maybe adding new word embeddings to my model)? Or do I need to  28 (26+2) million tweets from beginning? It takes 5 hours with current parameters and will take longer with a bigger data.</p>

<p>One other question, is it possible to retrieve sentences parameter directly from database (instead of reading it from <em>txt</em>, <em>bz2</em> or <em>gz</em> files)? As our data to be trained is getting bigger, it would be better to bypassing text read/write operations.</p>
",,2016-11-21 18:33:30,gensim word2vec - updating word embeddings with newcoming data,<gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
10448,40637537,2016-11-16 16:41:03,,"<p>I am new to topic modelling.
My aim is to find key topics from a document. I am planning to use lda for the purpose. But in lda the number of topics should be predefined.I believe if a document from some other domain which was not in the training corpus comes,it will not give proper results. Is there any alternative solution? Is my thought is correct?    </p>
",,2017-05-05 21:48:42,Dynamic number of topics in topic models,<nlp><lda><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
10454,40643082,2016-11-16 21:55:53,,"<p>So I believe despite this being a common issue with many similar questions (especially on stackoverflow), the main reason behind this issue varies in each case</p>

<p>In my case I have a method named <code>readCorpus</code> (<strong>find code below</strong>) it reads a list of 21 files, extract docs from each file then yield them</p>

<p>The yield operation occurs at the end of reading each file</p>

<p>I have another method named <code>uploadCorpus</code> (<strong>find code below</strong>). The main aim of this method is to upload that corpus. </p>

<p>Obviously the main reason behind using yield is that the corpus can be very large and I only need to read it once.</p>

<p>Once I run the method <code>uploadCorpus</code> I receive the error below</p>

<p><code>TypeError: coercing to Unicode: need string or buffer, list found</code></p>

<p>The erros occurs at the line <code>self.readCorpus()])</code>. </p>

<p>Reading similar problems I came to understand that it happens when a list is misplaced .. I tried to uplate the line of question here to <code>docs for docs in self.readCorpus()])</code> but I ended with the same issue</p>

<p><strong>My code (uploadCorpus)</strong></p>

<pre><code>def uploadCorpus(self):
        #convert docs to corpus
        print ""uploading""

        utils.upload_chunked(
            self.service,
            [{'id': 'doc_%i' % num, 'tokens': utils.simple_preprocess(doc)}
            for num, doc in enumerate([ 
                self.readCorpus()])
                ],
            chunksize=1000) # send 1k docs at a time
</code></pre>

<p><strong>My code readCorpus()</strong></p>

<pre><code>def readCorpus(self):
    path = '../data/reuters'
    doc=''
    docs = []
    docStart=False

    fileCount=0

    print 'Reading Corpus'
    for name in glob.glob(os.path.join(path, '*.sgm')):
        print 'Reading File| ' + name
        docCount=0
        for line in open(name):
            if(len(re.findall(r'&lt;BODY&gt;', line)) &gt; 0 ): 
                docStart = True
                pattern = re.search(r'&lt;BODY&gt;.*', line)
                doc+= pattern.group()[6:]

            if(len(re.findall(r'&lt;/BODY&gt;\w*', line)) &gt; 0 ):
                docStart = False
                docs.append(doc)
                doc=''
                docCount+=1
                continue
                #break
            if(docStart):
                doc += line

        fileCount+=1
        print 'docuemnt[%d][%d]'%(fileCount,docCount)
        yield docs
        docs = []
</code></pre>
",2016-11-23 14:38:20,2016-11-23 14:38:20,"python gensim TypeError: coercing to Unicode: need string or buffer, list found",<python><python-2.7><typeerror><iterable><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10455,40768132,2016-11-23 15:18:19,,"<p>I have an already running python code of the <a href=""http://radimrehurek.com/gensim/simserver.html"" rel=""nofollow noreferrer"">document similarity server</a></p>

<p>The code runs fine from the commandline, however when I try to run from Jupyter notebook I get the following error (You can find the code below)</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last)<br>
   in ()<br>
  ----> 1 simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')</p>
</blockquote>

<pre><code>&lt;ipython-input-2-81df834abc60&gt; in queryIndex(self, queryText)
     58         print ""Querying the INDEX""
     59         doc = {'tokens': utils.simple_preprocess(queryText)}
---&gt; 60         print(self.service.find_similar(doc, min_score=0.4, max_results=50))
</code></pre>

<p>At first I got a different error message where the solution was to install simserver library within jupyter notebook using the command <code>!pip install --upgrade simserver</code> .. but now I do not think there is a missing library that needs to be downloaded</p>

<p>Relevant code from jupyter notebook:</p>

<p>Line where the issue occurs</p>

<pre><code>simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')

#!/usr/bin/env python
import pickle
import os
import re
import glob
import pprint
import json

from gensim import utils
from simserver import SessionServer

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

class SimilarityServer(object):
    def __init__(self):
        print ""Openning sesesion and setting it to true""
        self.service = SessionServer('tmp/my_server/')
        self.service.open_session()
        self.service.set_autosession(True)

    def indexDocs(self):
        print ""Docs indexing and training""
        #train and index
        print ""Training""
        self.service.session.train(None,method='lsi',clear_buffer=False)
        print ""Indexing""
        self.service.session.index(None)

    def queryIndex(self,queryText):
        print ""Querying the INDEX""
        doc = {'tokens': utils.simple_preprocess(queryText)}
        print(self.service.find_similar(doc, min_score=0.4, max_results=50))

simServer = SimilarityServer()
simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')
</code></pre>
",2016-12-06 06:33:41,2016-12-06 06:33:41,python code not running on Jupyter notebook,<python><jupyter-notebook><attributeerror><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10464,40622799,2016-11-16 01:53:49,,"<p>I am planning to do sentiment analysis on the customer reviews (a review can have multiple sentences) using word2vec. I have certain questions regarding this:</p>

<ol>
<li>Should I train my word2vec model (in gensim) using just the training data? Should I consider the test data for this too?</li>
<li>How should I represent the review for classification? Will this representation take into consideration the order of the word as this is important in representing a review for sentiment analysis?</li>
</ol>
",,2016-12-14 18:57:03,Sentiment Analysis using word2vec,<python><sentiment-analysis><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10467,40660127,2016-11-17 16:28:43,,"<p>My question is related to this post, <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda"">Document topical distribution in Gensim LDA</a>, the documentation for gensim.models.ldamodel states that ""minimum_probability controls filtering the topics returned for a document (bow)."" however, ldamodel[corpus] returns all possible topics with their probability (even below the number set in minimum_probability). what is the difference between these two? 
python 2.7.5
gensim 0.13.3</p>

<p>thank you</p>
",2017-05-23 12:30:20,2017-09-13 13:08:12,gensim Latent Dirichlet Allocation minimum_probability vs print_topics,<python><lda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10472,40732313,2016-11-22 01:20:34,,"<p>I am trying to install gensim on a google cloud instance using:</p>

<blockquote>
  <p>pip3 install gensim</p>
</blockquote>

<p>and this is the stacktrace when I am trying to import gensim:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.4/dist-packages/gensim/__init__.py"", line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/__init__.py"", line 7, in &lt;module&gt;
    from .coherencemodel import CoherenceModel
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/coherencemodel.py"", line 30, in &lt;module&gt;
    from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/wrappers/__init__.py"", line 5, in &lt;module&gt;
    from .ldamallet import LdaMallet
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/wrappers/ldamallet.py"", line 43, in &lt;module&gt;
    from smart_open import smart_open
  File ""/usr/local/lib/python3.4/dist-packages/smart_open/__init__.py"", line 1, in &lt;module&gt;
    from .smart_open_lib import *
  File ""/usr/local/lib/python3.4/dist-packages/smart_open/smart_open_lib.py"", line 36, in &lt;module&gt;
    import boto.s3.key
  File ""/usr/local/lib/python3.4/dist-packages/boto/s3/key.py"", line 33, in &lt;module&gt;
    import boto.utils
  File ""/usr/local/lib/python3.4/dist-packages/boto/__init__.py"", line 1216, in &lt;module&gt;
    boto.plugin.load_plugins(config)
AttributeError: 'module' object has no attribute 'plugin'
</code></pre>

<p>This is the linux version (output of lsb_release -a):</p>

<pre><code>No LSB modules are available.
Distributor ID: Debian
Description:    Debian GNU/Linux 8.6 (jessie)
Release:    8.6
Codename:   jessie
</code></pre>

<p>and this is the output of</p>

<blockquote>
  <p>pip3 freeze</p>
</blockquote>

<pre><code>Cython==0.25.1
Flask==0.11.1
Jinja2==2.8
MarkupSafe==0.23
Pillow==2.6.1
Werkzeug==0.11.11
beautifulsoup4==4.3.2
boto==2.43.0
bz2file==0.98
chardet==2.3.0
click==6.6
colorama==0.3.2
decorator==3.4.0
gensim==0.13.3
html5lib==0.999
itsdangerous==0.24
lxml==3.4.0
matplotlib==1.4.2
nltk==3.2.1
nose==1.3.4
numexpr==2.4
numpy==1.11.2
pandas==0.14.1
pyparsing==2.0.3
python-apt==0.9.3.12
python-dateutil==2.2
pytz==2012c
requests==2.4.3
scipy==0.14.0
six==1.8.0
smart-open==1.3.5
stop-words==2015.2.23.1
tables==3.1.1
unattended-upgrades==0.1
urllib3==1.9.1
wheel==0.24.0
</code></pre>

<p>Can anybody give me pointers! This is very frustrating.</p>
",,2019-05-10 12:47:24,Gensim installation problems,<python><pip><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
10497,40596702,2016-11-14 19:49:05,,"<p>I read the following code to learn a doc2vec model.Each document is defined as a text/line between two lines : </p>

<ul>
<li>clueweb09-en0001-XX-XXXXX</li>
<li>end_clueweb09-en0001-XX-XXXXX</li>
</ul>

<p>This is my code:</p>

<pre><code> path='/home/work/Step2/test-input/html'


alldocs = []  # will hold all docs in original order


for fname in os.listdir(path):
    with open(path+'/'+fname) as alldata:
        for line in alldata:
            docId= line
            print docId
            context= alldata.next()
            #print context
            tokens = gensim.utils.to_unicode(context).split()
            end=alldata.next()
            alldocs.append(LabeledSentence(tokens[:],[docId]))

model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
model.build_vocab(alldocs)
for epoch in range(10):
    model.train(alldocs)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay

# store the model to mmap-able files
model.save(path+'/my_html_model.doc2vec')
</code></pre>

<p>But I got the error when I wrote <em>model.docvecs['clueweb09-en0001-01-34238']</em> but when I write <em>model.docvecs[0]</em> I got the result.</p>

<p>This is the error I got:</p>

<pre><code>    Traceback (most recent call last):
  File ""getLearingDoc.py"", line 40, in &lt;module&gt;
    print model.docvecs['clueweb09-en0001-01-34238']
  File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 341, in __getitem__
    return self.doctag_syn0[self._int_index(index)]
  File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 315, in _int_index
    return self.max_rawint + 1 + self.doctags[index].offset
KeyError: 'clueweb09-en0001-01-34238'
</code></pre>

<p>I do not have experiences in python and gensim please tell me how can I solve this problem. </p>
",,2017-01-19 03:28:13,How solve gensim KeyError when I try to have a document's vector?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10505,40758017,2016-11-23 07:03:30,,"<pre><code># I am importing all the packages necessary for topic modelling in this step
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
import gensim
from gensim import corpora, models
import os
from os import path
from time import sleep
import matplotlib.pyplot as plt
import random
import wordcloud
from wordcloud import WordCloud, STOPWORDS
tokenizer = RegexpTokenizer(r'\w+')
en_stop = set(get_stop_words('en'))
with open(os.path.join('D:\Users\kaila\jobdescription.txt')) as f:
    Reader = f.read()
# I am replacing all the unnecessary words for wordcloud analysis
Reader = Reader.replace(""will"", "" "")
Reader = Reader.replace(""experience"", "" "")
Reader = Reader.replace(""work"", "" "")
Reader = Reader.replace(""years"", "" "")
Reader = Reader.replace(""please"", "" "")
Reader = Reader.replace(""apply"", "" "")


texts = unicode(Reader, errors='replace')
tdm = []

raw = texts.lower()
tokens = tokenizer.tokenize(raw)
stopped_tokens = [i for i in tokens if not i in en_stop]
tdm.append(stopped_tokens)

dictionary = corpora.Dictionary(tdm)
corpus = [dictionary.doc2bow(i) for i in tdm]
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word =   dictionary)
for t in range(ldamodel.num_topics):
    print(ldamodel)
    print(ldamodel.print_topics(num_topics=5, num_words=8))
    plt.figure()
    plt.imshow(WordCloud().fit_words(ldamodel.show_topic(t, 200)))
    plt.axis(""off"")
    plt.title(""Topic #"" + str(t))
    plt.show()
</code></pre>

<p>By doing this I am getting the topic models and wordclouds but the results are very similar to each other. Is there anything I can do to get unique topic model and word cloud results ? </p>
",,2016-11-23 07:03:30,I have executed the topic model code successfully but I want unique wordclouds for each topic which I am currently struggling with,<python><topic-modeling><word-cloud>,,,CC BY-SA 3.0,True,False,True,False,False
10526,40840731,2016-11-28 09:18:15,,"<p>Getting this error in python when trying to compute lda for a smaller size of corpus but works fine in other cases.</p>

<p>The size of corpus is 15 and I tried setting the number of topic to 5 then reduced it to 2 but it still gives the same error : <strong>ValueError: cannot compute LDA over an empty collection (no terms)</strong></p>

<p>getting error at this line :     <code>lda = models.LdaModel(corpus, num_topics=topic_number, id2word=dictionary, passes=passes)</code></p>

<p>where corpus is <code>corpus = [dictionary.doc2bow(text) for a, id, text, s_date, e_date, qd, qd_perc in texts]</code></p>

<p>Why is it giving no terms?</p>
",2018-03-18 07:14:06,2018-03-18 07:14:06,ValueError: cannot compute LDA over an empty collection (no terms),<python><python-3.x><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
10570,40865128,2016-11-29 11:46:08,,"<pre><code># -*- coding: utf-8 -*- 

'''
    word2Vec  
   
   
 model   
'''

import codecs
import gensim
import multiprocessing
import word2vec

import sys
reload(sys)
sys.setdefaultencoding('utf8')

#  configuration  
config = {
   'min_count': 15,  #   15   
   'size': 300,  # 300  embedding
   'sg': 1,  # 0 CBOW, 1 skip-gram
    'batch_words': 10000,  #      
    'iter': 10,  #    epoch ,  
    'workers': multiprocessing.cpu_count(),
}

modelTwitterNoStop = gensim.models.Word2Vec(**config)

class SentenceReader:

    def __init__(self, filepath):
        self.filepath = filepath


    def __iter__(self):
         for line in codecs.open(self.filepath, encoding='utf-8'):
             yield line.split(' ')

#      
sentences_vocab = SentenceReader('corpusAllNewsNoTagNoStop.txt')
sentences_train = SentenceReader('corpusAllNewsNoTagNoStop.txt')

#model = gensim.models.Word2Vec()
modelTwitterNoStop.build_vocab(sentences_vocab)
modelTwitterNoStop.train(sentences_train)

#   
modelTwitterNoStop.save('modelTwitterNoStop')


###   ##########
#    Test        

import codecs
import gensim
import multiprocessing

import sys
reload(sys)
sys.setdefaultencoding('utf8')

## 


#  gensim  
modelTwitterNoStop = gensim.models.Word2Vec.load('modelTwitterNoStop')


#most similar Test -  10   ..  ''       
print ' '.join([""{}-{}"".format(word, value) for word, value in 
(modelTwitterNoStop.most_similar(positive=[u"""", u""""], negative=[u""    ""], topn=10))])

print ""\n""

# positive     cosmul 
print ' '.join([""{}-{}"".format(word, value) for word, value in 
(modelTwitterNoStop.most_similar_cosmul(positive=[u"""", u""""], topn=20))])
print ""\n""

#doesn't match Test
print modelTwitterNoStop.doesnt_match(u""   "".split())

print ""\n""

#similarity Test
print modelTwitterNoStop.similarity(u"""", u"""")
print ""\n""

# no.of vocab.. in this model
print modelTwitterNoStop
print ""\n""
</code></pre>

<p>Warning (from warnings module):
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 840
    warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
UserWarning: detected Windows; aliasing chunkize to chunkize_serial</p>
",2016-12-01 09:12:47,2017-08-10 12:23:28,Problems with gensim install,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10584,40597319,2016-11-14 20:32:16,,"<p>I am training word2vec model in gensim using the sentences in a csv file as follows:</p>

<pre><code>import string
import gensim
import csv
import nltk

path = '/home/neel/Desktop/csci544_proj/test/sample.csv'
translator = str.maketrans({key: None for key in string.punctuation})

class gen(object):

    def __init__(self, path):
        self.path = path

    def __iter__(self):
        with open(path) as infile:
            reader = csv.reader(infile)
            for row in reader:
                rev = row[4]
                l = nltk.sent_tokenize(rev)
                for sent in l:
                    sent = sent.translate(translator)
                    yield sent.lower().split()

sentences = [path]
for p in gen(path):
    model = gensim.models.Word2Vec(p, min_count=1, iter=1)

print(model.vocab.keys())
</code></pre>

<p>I get the following result:
(['b', 'u', 'm', 'h', 'e', 'n', 'r', 'v', 'i', 'a', 't', 's', 'k', 'w', 'o', 'l'])</p>

<p>The result I am get is not words but the characters. Where is the program going wrong?</p>
",2016-11-15 04:42:01,2017-05-28 07:15:52,Gensim word2vec online training,<python><gensim><word2vec><yield-keyword>,,,CC BY-SA 3.0,True,False,True,False,False
10589,40936197,2016-12-02 15:56:07,,"<p>I want to replace the words of my gensim Word2Vec model with a mapping.</p>

<p><strong>Example</strong></p>

<p>My current model has the word <code>'foo'</code> that maps to a vector: </p>

<pre><code>&gt;&gt;&gt; model['foo']
[1.0 0.0]
</code></pre>

<p>I have the mapping: <code>d = {'foo': 'bar', ...}</code></p>

<p>How can I rebuild the model with this new mapping such that </p>

<pre><code>&gt;&gt;&gt; model['bar']  # in place of 'foo'
[1.0 0.0]
</code></pre>
",2016-12-03 04:06:25,2016-12-03 18:02:20,Rename gensim Word2Vec words with mapping,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10599,40966014,2016-12-05 01:54:42,,"<p>I found gensim has BM25 ranking function. However, i cannot find the tutorial how to use it.  </p>

<p>In my case, I had one query. a few documents which were retrieved from the search engine. How to use gensim BM 25 ranking to compare the query and documents to find the most similar one?</p>

<p>I am new to gensim. Thanks.</p>

<p>query:</p>

<pre><code>""experimental studies of creep buckling .""
</code></pre>

<p>document 1:</p>

<pre><code>"" the 7 x 7 in . hypersonic wind tunnel at rae farnborough, part 1, design, instrumentation and flow visualization techniques . this is the first of three parts of the calibration report on the r.a.e. some details of the design and lay-out of the plant are given, together with the calculated performance figures, and the major components of the facility are briefly described . the instrumentation provided for the wind-tunnel is described in some detail, including the optical and other methods of flow visualization used in the tunnel . later parts will describe the calibration of the flow in the working-section, including temperature measurements . a discussion of the heater performance will also be included as well as the results of tests to determine starting and running pressure ratios, blockage effects, model starting loads, and humidity of the air flow .""
</code></pre>

<p>document 2:</p>

<pre><code>"" the 7 in. x 7 in. hypersonic wind tunnel at r.a.e. farnborough part ii. heater performance . tests on the storage heater, which is cylindrical in form and mounted horizontally, show that its performance is adequate for operation at m=6.8 and probably adequate for flows at m=8.2 with the existing nozzles . in its present state, the maximum design temperature of 680 degrees centigrade for operation at m=9 cannot be realised in the tunnel because of heat loss to the outlet attachments of the heater and quick-acting valve which form, in effect, a large heat sink . because of this heat loss there is rather poor response of stagnation temperature in the working section at the start of a run . it is hoped to cure this by preheating the heater outlet cone and the quick-acting valve . at pressures greater than about 100 p.s.i.g. free convection through the fibrous thermal insulation surrounding the heated core causes the top of the heater shell to become somewhat hotter than the bottom, which results in /hogging/ distortion of the shell . this free convection cools the heater core and a vertical temperature gradient is set up across it after only a few minutes at high pressure . modifications to be incorporated in the heater to improve its performance are described .""
</code></pre>

<p>document 3:</p>

<pre><code>"" supersonic flow at the surface of a circular cone at angle of attack . formulas for the inviscid flow properties on the surface of a cone at angle of attack are derived for use in conjunction with the m.i.t. cone tables . these formulas are based upon an entropy distribution on the cone surface which is uniform and equal to that of the shocked fluid in the windward meridian plane . they predict values for the flow variables which may differ significantly from the corresponding values obtained directly from the cone tables . the differences in the magnitudes of the flow variables computed by the two methods tend to increase with increasing free-stream mach number, cone angle and angle of attack .""
</code></pre>

<p>document 4:</p>

<pre><code>"" theory of aircraft structural models subjected to aerodynamic heating and external loads . the problem of investigating the simultaneous effects of transient aerodynamic heating and external loads on aircraft structures for the purpose of determining the ability of the structure to withstand flight to supersonic speeds is studied . by dimensional analyses it is shown that .. constructed of the same materials as the aircraft will be thermally similar to the aircraft with respect to the flow of heat through the structure will be similar to those of the aircraft when the structural model is constructed at the same temperature as the aircraft . external loads will be similar to those of the aircraft . subjected to heating and cooling that correctly simulate the aerodynamic heating of the aircraft, except with respect to angular velocities and angular accelerations, without requiring determination of the heat flux at each point on the surface and its variation with time . acting on the aerodynamically heated structural model to those acting on the aircraft is determined for the case of zero angular velocity and zero angular acceleration, so that the structural model may be subjected to the external loads required for simultaneous simulation of stresses and deformations due to external loads .""
</code></pre>
",,2019-05-27 04:12:23,How to use gensim BM25 ranking in python,<python><ranking><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10604,40890226,2016-11-30 13:55:30,,"<p>I've used gensim to train a Word2Vec model, and would like to query nearby terms. But instead of just getting the words that are closest in all directions:</p>

<pre><code>model = models.Word2Vec.load('MyModel')        # load up my trained model
nearest = model.most_similar(['mushroom'])     # nearby words all around
</code></pre>

<p>I want to move in a particular direction and distance within the vector space and retrieve the nearest word, essentially:</p>

<pre><code>nearest = nearest_by_vector(word, direction_vector)
</code></pre>

<p>My vector math is terrible (ie non-existent), especially with so many dimensions in my model.</p>
",,2017-03-27 09:36:54,Move in Word2Vec vector space in specific direction,<python><machine-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10611,40910578,2016-12-01 12:22:28,,"<p>I am trying to build a Phrases model over a big corpus but I keep stumbling over a memory error. 
First I tried to fit my entire corpus into a big generator.
Then, I tried to save the model between each document :</p>

<pre><code>import codecs
import gensim
import os
import random
import string
import sys

def gencorp(file_path):
    with codecs.open(file_path, 'rb',encoding=""utf8"") as doc :
        for sentence in doc:
            yield sentence.split()

out_corpus_dir = ""C:/Users/Administrator/Desktop/word2vec/1billionwords_corpus_preprocessed/""
file_nb = 0
bi_detector = gensim.models.Phrases()
for file in os.listdir(out_corpus_dir):
    file_nb += 1
    file_path = out_corpus_dir+file
    bi_detector.add_vocab(gencorp(file_path))
    bi_detector.save(""generic_EN_bigrams_v%i""%(file_nb/10))
    bi_detector = gensim.models.Phrases.load(""generic_EN_bigrams_v%i""%(file_nb/10))
bi_detector.save(""generic_EN_bigrams"")
</code></pre>

<p>But none of these solutions work. However, generic_EN_bigrams_v0 is generated and saved.
So I am wondering if I can train a Phrases model per document and then find a way to merge them after.</p>

<p>Thanks you for any insight :)</p>
",,2017-10-10 13:08:24,Merging two gensim Phrases models,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10623,40986528,2016-12-06 01:52:52,,"<p>I was running Multi-label classification on text data I noticed TFIDF outperformed LDA by a large margin. TFIDF accuracy was aorund 50% and LDA was around 29%. </p>

<p>Is this expected or should LDA do better than this? </p>
",,2017-03-15 04:49:14,Classification LDA vs. TFIDF,<machine-learning><gensim><lda><text-classification>,,,CC BY-SA 3.0,False,False,True,False,False
10633,41012760,2016-12-07 08:39:16,,"<p>I am using gensim to construct an LSI corpus and then apply query similarity following gensim tutorials (<a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">tut1</a>, <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tut2</a> n <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">tut3</a>) </p>

<p>My issue is that when I try to calcualte query similarity as shown in the code below I get the result in form of (docID, simScore) tuples.</p>

<p>I need to use that <strong>docID</strong> to retrive a <strong>string representation of the document</strong>. (similar to the token2id mapping in the <code>corpora.Dictionary</code>)</p>

<p>Googling that I could not find anything useful</p>

<p><strong>My Code for searching</strong></p>

<pre><code>    def search(self):
    #Load necessary information
    dictionary = corpora.Dictionary.load('dictionary.dict')
    corpus_tfidf = corpora.MmCorpus('corpus.mm') # comes from the first tutorial, ""From strings to vectors""
    #print(corpus_tfidf)

    #Generate LSI model
    #lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
    lsi = LsiModel(corpus_tfidf,num_topics=2)

    #construct index
    index = similarities.MatrixSimilarity(lsi[corpus_tfidf]) # transform corpus to LSI space and index it

    #Construct query vector
    doc = ""Human machine interface for lab abc computer applications""
    vec_bow = dictionary.doc2bow(doc.lower().split())
    vec_lsi = lsi[vec_bow] # convert the query to LSI space

    #Calcualte similarity
    sims = index[vec_lsi] # perform a similarity query against the corpus
    sims = sorted(enumerate(sims), key=lambda item: -item[1])

    print(sims) # print sorted (document number, similarity score) 2-tuples
</code></pre>

<p><strong>Results sample</strong></p>

<pre><code>[(1, 0.9962855), (4, 0.99420911), (2, 0.98064679), (3, 0.97580492), (0, 0.9755646), (8, 0.34740543), (6, 0.1566827), (7, 0.15566549), (5, 0.13825497)]
</code></pre>
",,2016-12-11 18:58:16,doc2id mapping in gensim,<python><similarity><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10652,40924185,2016-12-02 03:14:53,,"<p>I am trying to use LDA module of GenSim to do the following task</p>

<p>""Train a LDA model with one big document and keep track of 10 latent topics. Given a new, unseen document, predict probability distribution of 10 latent topics"".</p>

<p>As per tutorial here: <a href=""http://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">http://radimrehurek.com/gensim/tut2.html</a>, this seems possible for a document in a corpus, but I am wondering if it it would be possible for an unseen document. </p>

<p>Thank you!  </p>
",,2016-12-02 19:12:19,Calculating topic distribution of an unseen document on GenSim,<python><nlp><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
10658,41101424,2016-12-12 12:55:05,,"<p>I am using the topic visualization library LDAvis:</p>

<pre><code>## visualization of the topics
import pyLDAvis
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
</code></pre>

<p>which produces an image of the Principal Components of the topics unveiled by the LDA (Latent Dirichlet Allocation) model. I will like to download the image but I am stuck. Any help much appreciated it!</p>
",,2017-01-20 12:21:13,Downloading the image produced by LDAvis library,<python><ipython><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
10664,41064594,2016-12-09 16:16:05,,"<p>I need to compare two strings, e.g. <em>Highly Toughened Steel - Unbreakable</em> and <em>Highly rugged Steel</em>. If they are the names of two products which are the same, I need to find out how similar the strings are. I also need to check for synonyms in the sentence.</p>

<p>I have already tried using Diff Sequence Matcher and the output is pretty good.</p>

<pre><code>seq=difflib.SequenceMatcher(None, a,b)
d=seq.ratio()*100
</code></pre>

<p>I was trying to look at Genism. Is it a library worth consider for such a small task? 
If so, can anyone provide a reproducible example for using Genism to compare strings?</p>
",2016-12-09 17:28:24,2016-12-09 17:28:24,Python Based Code To Compare Similarity Between Sentences,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10679,41182372,2016-12-16 10:33:16,,"<p>Please help me in understanding the difference between how <code>TaggedDocument</code> and <code>LabeledSentence</code> of <code>gensim</code> works. My ultimate goal is Text Classification using <code>Doc2Vec</code> model and any classifier. I am following this <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""noreferrer"">blog</a>!</p>

<pre><code>class MyLabeledSentences(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList


class MyTaggedDocument(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(TaggedDocument([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList

sentences = MyLabeledSentences(some_dir_name)
model_l = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5,     workers=7)
sentences_l = sentences.ToArray()
model_l.build_vocab(sentences_l )
for epoch in range(15): # 
    random.shuffle(sentences_l )
    model.train(sentences_l )
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha 

sentences = MyTaggedDocument(some_dir_name)
model_t = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)
sentences_t = sentences.ToArray()
model_l.build_vocab(sentences_t)
for epoch in range(15): # 
    random.shuffle(sentences_t)
    model.train(sentences_t)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha
</code></pre>

<p>My question is <code>model_l.docvecs['some_word']</code> is same as <code>model_t.docvecs['some_word']</code>?
Can you provide me weblink of good sources to get a grasp on how <code>TaggedDocument</code> or <code>LabeledSentence</code> works.</p>
",2016-12-16 10:37:24,2017-01-19 02:59:31,What is the difference between gensim LabeledSentence and TaggedDocument,<gensim><text-classification><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10686,41162876,2016-12-15 11:19:11,,"<p>I am using gensim word2vec package in python.
I would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>

<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>

<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance syn0 is not described as an attribute)</p>
",2017-12-21 15:21:25,2017-12-21 15:21:25,Get weight matrices from gensim word2Vec,<python><machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10690,41113211,2016-12-13 03:18:41,,"<p>I've using tensorflow to build word2vec model,reference here<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L118"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L118</a> </p>

<p>my question is that, how can i find top n similar words for a certain word.I know in gensim, I can save and load word2vec model,and then use model.most_similar to find what I want.but how in tensorflow and even more is there any way to save model in tensorflow since i find what i get is only an embedding vector,is that right? </p>
",,2017-02-18 06:24:29,how to find similar words for a certain word in tensorflow_word2vec like using model.most_similar in gensim?,<tensorflow><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10697,41129933,2016-12-13 20:23:03,,"<p>I have a trained word2vec models in geinsim with 300 dimensions and would like to cut the dimensions to 100 (simply drop the last 200 dimensions). What is the easiest and most efficient way using python?</p>
",,2020-03-14 22:47:51,Gensim Word2Vec model: Cut dimensions,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10715,41223299,2016-12-19 13:07:29,,"<p>I am learning <code>Doc2Vec</code> model from <code>gensim</code> library and using it as follows:</p>

<pre><code>class MyTaggedDocument(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            with open(os.path.join(self.dirname, fname),encoding='utf-8') as fin:
                print(fname)
                for item_no, sentence in enumerate(fin):
                    yield LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no])
sentences = MyTaggedDocument(dirname)
model = Doc2Vec(sentences,min_count=2, window=10, size=300, sample=1e-4, negative=5, workers=7)
</code></pre>

<p>The input <code>dirname</code> is a directory path which has , for the sake of simplicity, only 2 files located with each file containing more than 100 lines. I am getting following Exception.</p>

<p><a href=""https://i.stack.imgur.com/T6INA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T6INA.png"" alt=""Output""></a></p>

<p>Also, with <code>print</code> statement I could see that the iterator iterated over directory 6 times. Why is this so?</p>

<p>Any kind of help would be appreciated.</p>
",,2017-01-19 02:52:01,Gensim Doc2Vec Exception AttributeError: 'str' object has no attribute 'words',<python><neural-network><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10724,41133844,2016-12-14 02:28:25,,"<p>I am using <code>word2vec</code>, wiki corpus I trained, what can I do if the word I input not in vocabulary in <code>word2vec</code>?</p>

<p><strong>Test it a bit:</strong></p>

<pre><code>model = word2vec.Word2Vec.load('model/' + 'wiki_chinese_word2vec.model')    
model['boom']
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>KeyError(""word '%s' not in vocabulary"" % word)</p>
</blockquote>
",2016-12-14 02:29:35,2019-01-28 18:34:40,KeyError: word 'word' not in vocabulary in word2vec,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10736,41194588,2016-12-17 01:39:23,,"<p>I am trying to install gensim on EC2 (RedHat - t2-micro) to build a REST API. The overall installation process is really long so I shorten the code to replicate the error I am getting. Here below the shorter version:</p>

<ul>
<li><p>sudo yum update -y (EC2 update)</p></li>
<li><p>sudo yum install nano (nano editor)</p></li>
<li>sudo yum -y install gcc-c++ python27-devel atlas-sse3-devel lapack-devel (various Gensim related packages)</li>
<li>sudo yum -y install httpd (Apache)</li>
<li>sudo yum install mod_wsgi (WSGI)</li>
<li>curl -O <a href=""https://bootstrap.pypa.io/get-pip.py"" rel=""nofollow noreferrer"">https://bootstrap.pypa.io/get-pip.py</a> then sudo python2.7 get-pip.py (PIP)</li>
<li>virtualenv -p python2.7 /tmp/my_app then . /tmp/my_app/bin/activate (virtual env)</li>
</ul>

<p>With my virtual env setup, I then added numpy, scipy, and gensim:
- pip install -U --force numpy
- pip install -U --force scipy
- pip install -U --force </p>

<p>Now if I add a simple file with the following code:</p>

<pre><code># -*- coding: utf-8 -*-

import gensim

def main():
    return ""Hello my AVABBBCCCDDDEEE world!""

if __name__ == ""__main__"":
    main()
</code></pre>

<p>and run ""python file.py"" in the terminal (MACOS), i get the following error:</p>

<p>/tmp/ava_app/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.
  warnings.warn(""Pattern library is not installed, lemmatization won't be available."")
No handlers could be found for logger ""gensim.models.doc2vec""</p>

<p>Any idea? The error results in a 500 error in the browser.</p>

<p>Many thanks, Stephane</p>
",,2016-12-17 01:39:23,gensim on EC2: installation issue,<amazon-ec2><installation><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10763,41273573,2016-12-21 23:23:28,,"<p>I have a bIg data platform. On that I install Anaconda. When I ssh to my account on the platform and open up a Python interpreter in terminal it works fine when I import the gensim library. I had earlier installed this library using </p>

<pre><code>Conda install gensim

$ python
Python 2.7.12 |Anaconda 2.5.0 (64-bit)| (default, Jul  2 2016, 17:42:40) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
&gt;&gt;&gt; import gensim
/home/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.
  warnings.warn(""Pattern library is not installed, lemmatization won't be available."")
</code></pre>

<p>If you see it does import the library (just gives some warning for Pattern library). However when I open up Jupyter notebook and try to import the same library there it gives the following:</p>

<pre><code>In [11]:

import gensim 
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-11-0539d76422c6&gt; in &lt;module&gt;()
----&gt; 1 import gensim

ImportError: No module named gensim
</code></pre>

<p>Am not sure why the same library which is installed is not working in Jupyter. Please note that when I do </p>

<pre><code>pip list
</code></pre>

<p>it shows me all the libraries and it has gensim there. </p>
",2016-12-21 23:30:53,2019-10-27 13:33:37,Gensim Library not recognized in Jupyter notebook,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10773,41322521,2016-12-25 15:49:42,,"<p>I'm trying to create a similarity between two words using word2vec, I was successful, while doing it manually. but I have two big txt files. I want to create a loop. I tried a couple methods for looping but I was unsuccessful. so I decided to ask expert. </p>

<p>my code :</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
with open('myfile1.txt', 'r') as f:
    data1 = f.readlines()

with open('myfile2.txt', 'r') as f:
    data2 = f.readlines()

data = zip(data1, data2)

with open('myoutput.txt', 'a') as f:
    for x in data: 
        output = model.similarity(x[1], x[0])  # reading each word form each files
        out = '{} : {} : {}\n'.format(x[0].strip(), x[1].strip(),output)  
        f.write(out)
</code></pre>

<p>my input1, (text1)</p>

<pre><code>street 
spain 
ice
man
</code></pre>

<p>my input2 (text2)</p>

<pre><code>florist
paris 
cold 
kid
</code></pre>

<p>I want this output (output.txt)</p>

<pre><code>street florist 0.19991447551502498
spain paris 0.5380033328157873
ice cold 0.40968857572410483
man kid  0.42953233870042506
</code></pre>
",2016-12-26 15:25:28,2016-12-30 22:38:46,How to use Word2Vec with two inputs in a loop?,<python><nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10813,41326577,2016-12-26 04:56:52,,"<p>I am using python 3.5 on both windows and Linux but get the same error:
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc1 in position 0: ordinal not in range(128)
The error log is the following:
    Reloaded modules: lazylinker_ext
    Traceback (most recent call last):</p>

<pre><code>  File ""&lt;ipython-input-2-d60a2349532e&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/YZC/Google     Drive/sunday/data/RA/data_20100101_20150622/w2v_coherence.py',     wdir='C:/Users/YZC/Google Drive/sunday/data/RA/data_20100101_20150622')

  File ""C:\Users\YZC\Anaconda3\lib\site-    packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 699, in runfile
    execfile(filename, namespace)

  File ""C:\Users\YZC\Anaconda3\lib\site-    packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 88, in execfile
    exec(compile(open(filename, 'rb').read(), filename, 'exec'), namespace)

  File ""C:/Users/YZC/Google     Drive/sunday/data/RA/data_20100101_20150622/w2v_coherence.py"", line 70, in     &lt;module&gt;
    model = gensim.models.Word2Vec.load('model_all_no_lemma')

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\models\word2vec.py"",     line 1485, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\utils.py"", line 248,     in load
    obj = unpickle(fname)

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\utils.py"", line 912, in unpickle
    return _pickle.loads(f.read())

UnicodeDecodeError: 'ascii' codec can't decode byte 0xc1 in position 0:     ordinal not in range(128)
</code></pre>

<p>1.I checked and found the default decode method is utf-8 by:
    import sys
    sys.getdefaultencoding()
Out[2]: 'utf-8'</p>

<ol start=""2"">
<li>when read the file, I also added .decode('utf-8')</li>
<li>I did add shepang line in the beginning and declare utf-8
so I really dont know why python couldnt read the file. Can anybody help me out?</li>
</ol>

<p>Here are the code:</p>

<pre><code># -*- coding: utf-8 -*-
import gensim
import csv
import numpy as np
import math
import string
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob, Word



class SpeechParser(object):

    def __init__(self, filename):
        self.filename = filename
        self.lemmatize = WordNetLemmatizer().lemmatize
        self.cached_stopwords = stopwords.words('english')

    def __iter__(self):

        with open(self.filename, 'rb', encoding='utf-8') as csvfile:
            file_reader = csv.reader(csvfile, delimiter=',', quotechar='|', )
            headers = file_reader.next()
            for row in file_reader:
                parsed_row = self.parse_speech(row[-2])
                yield parsed_row

    def parse_speech(self, row):

        speech_words =  row.replace('\r\n', ' ').strip().lower().translate(None, string.punctuation).decode('utf-8', 'ignore')         

        return speech_words.split()

    # -- source: https://github.com/prateekpg2455/U.S-Presidential-    Speeches/blob/master/speech.py --
    def pos(self, tag):
        if tag.startswith('J'):
            return wordnet.ADJ
        elif tag.startswith('V'):
            return wordnet.VERB
        elif tag.startswith('N'):
            return wordnet.NOUN
        elif tag.startswith('R'):
            return wordnet.ADV
        else:
            return ''

if __name__ == '__main__':

    # instantiate object
    sentences = SpeechParser(""sample.csv"")

    # load an existing model
    model = gensim.models.Word2Vec.load('model_all_no_lemma')



    print('\n-----------------------------------------------------------')
    print('MODEL:\t{0}'.format(model))

    vocab = model.vocab

    # print log-probability of first 10 sentences
    row_count = 0
    print('\n------------- Scores for first 10 documents: -------------')
    for doc in sentences: 
        print(sum(model.score(doc))/len(doc))
        row_count += 1
        if row_count &gt; 10:
            break
    print('\n-----------------------------------------------------------')
</code></pre>
",2016-12-26 05:45:33,2016-12-28 12:51:12,"UnicodeDecodeError: 'ascii' codec can't decode, with gensim, python3.5",<encoding><utf-8><python-3.5><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
10815,41330637,2016-12-26 11:34:38,,"<p>It may be very basic problem but i have stuck with this since 2 hours.
I am trying to execute below line of code:</p>

<pre><code>from gensim.models.phrases import Phrases , Phraser
</code></pre>

<p>but i am getting error: <em>cannot import name 'Phraser'</em> as for as i know we get this kind of error when <em>Phraser</em> is neither variable nor function in <em>gensim.models.phrases</em> but i have checked gensim's homepage and found this:</p>

<blockquote>
  <p>class gensim.models.phrases.Phraser(phrases_model)</p>
</blockquote>

<p>I have gensim's latest module 0.13.4 and i am using Python 3.5.2 |Anaconda 4.1.1 (64-bit), on windows 10.</p>
",,2017-07-09 17:20:28,Gensim: cannot import name 'Phraser',<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10822,41360580,2016-12-28 10:58:21,,"<p>I'm trying to read my pretrained doc2vec model:</p>

<pre><code>from gensim.models import Doc2Vec
model = Doc2Vec.load('/path/to/pretrained/model')
</code></pre>

<p>However, an error appears during reading process. Could anyone suggest how to deal with this? Here is the error:</p>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-9-819b254ac835&gt; in &lt;module&gt;()
----&gt; 1 model = Doc2Vec.load('/path/to/pretrained/model')

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/models/word2vec.pyc in load(cls, *args, **kwargs)
   1682     @classmethod    
   1683     def load(cls, *args, **kwargs):
-&gt; 1684         model = super(Word2Vec, cls).load(*args, **kwargs)
   1685         # update older models
   1686         if hasattr(model, 'table'):

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/utils.pyc in load(cls, fname, mmap)
    246         compress, subname = SaveLoad._adapt_by_suffix(fname)
    247 
--&gt; 248         obj = unpickle(fname)
    249         obj._load_specials(fname, mmap, compress, subname)
    250         return obj

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/utils.pyc in unpickle(fname)
    909     with smart_open(fname) as f:
    910         # Because of loading from S3 load can't be used (missing readline in smart_open)
--&gt; 911         return _pickle.loads(f.read())
    912 
    913 

AttributeError: 'module' object has no attribute 'defaultdict'
</code></pre>
",,2017-05-15 23:02:46,Gensim: how to load pretrained doc2vec model?,<python><model><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10825,41430565,2017-01-02 16:55:15,,"<p>I'm writing my first app in python to use word2vec model.
Here is my simple code</p>

<pre><code>import gensim, logging
import sys
import warnings
from gensim.models import Word2Vec

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def main(): 
    ####LOAD MODEL
    model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  
    model.similarity('man', 'women')

if __name__ == '__main__':
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")
        #warnings.simplefilter(""ignore"")
    main()
</code></pre>

<p>I getting this the following error:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte 
</code></pre>

<p>I tried solving it by adding these two lines, but I'm still getting the error. </p>

<pre><code>reload(sys)  # Reload does the trick!
sys.setdefaultencoding('UTF8') #UTF8 #latin-1
</code></pre>

<p>The w2v model was trained on English sentences.</p>

<p>EDIT: Here is the full stack:</p>

<pre><code>**%run ""...\getSimilarity.py""**
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
**...\getSimilarity.py in &lt;module&gt;()**
     64         warnings.simplefilter(""error"")
     65         #warnings.simplefilter(""ignore"")
---&gt; 66     main()

**...\getSimilarity.py in main()**
     30     ####LOAD MODEL
---&gt; 31     model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  # C binary format
     32     model.similarity('man', 'women')

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors)**
   1090             else:
   1091                 for line_no, line in enumerate(fin):
-&gt; 1092                     parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
   1093                     if len(parts) != vector_size + 1:
   1094                         raise ValueError(""invalid vector on line %s (is this really the text format?)"" % (line_no))

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\utils.pyc in any2unicode(text, encoding, errors)**
    215     if isinstance(text, unicode):
    216         return text
--&gt; 217     return unicode(text, encoding, errors=errors)
    218 to_unicode = any2unicode
    219 

**...\AppData\Local\Enthought\Canopy\App\appdata\canopy-1.6.2.3262.win-x86_64\lib\encodings\utf_8.pyc in decode(input, errors)**
     14 
     15 def decode(input, errors='strict'):
---&gt; 16     return codecs.utf_8_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

**UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte** 
</code></pre>

<p>Any hints how to solve the problem?
Thanks in advance.</p>
",2017-01-02 17:25:53,2017-12-05 09:17:20,Encoding issue in python while using w2v,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10848,41467115,2017-01-04 15:13:46,,"<p>To extract the embedding representations of input data, the tensorflow documentation says we can use the following:</p>

<pre><code>embed = tf.nn.embedding_lookup(embeddings, input_data)
</code></pre>

<p>Accdg to the <a href=""https://www.tensorflow.org/api_docs/python/nn/embeddings#embedding_lookup"" rel=""nofollow noreferrer"">TF documentation</a>, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids:</p>

<blockquote>
  <p>ids: A Tensor with type int32 or int64 containing the ids to be looked up in params.</p>
</blockquote>

<p>My question is: Given a sentence, say, </p>

<blockquote>
  <p>""Welcome to the world""</p>
</blockquote>

<p>how can I represent and transform it into <code>ids</code>? In the code below, how can I transform my sentence into <code>input_data</code>. </p>

<pre><code>from gensim import models
embedding_path = ""../embeddings/GoogleNews-vectors-negative300.bin""
w = models.Word2Vec.load_word2vec_format(embedding_path, binary=True)
X = w.syn0
W = tf.Variable(tf.constant(0.0, shape=X.shape),trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, X.shape)
embedding_init = W.assign(embedding_placeholder)
embed = tf.nn.embedding_lookup(embedding_init, input_data)
sess = tf.Session()
sess.run(embed, feed_dict={embedding_placeholder: X})
</code></pre>
",2017-01-04 15:39:21,2017-01-04 16:58:49,"Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?",<python><tensorflow><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10869,41432760,2017-01-02 20:12:22,,"<p>I want to use gensim word2vec as input for neural network. I have 2 questions:</p>

<p>1) gensim.models.Word2Vec get as parameter the size. How this parameter is used? and size of what?</p>

<p>2) Once trained what is the output of gensim word2vec? As i could see this is not a probability values (not between 0 and 1). It seems to me for each word vector we get a distance (cosinus) between this word and some other words (but which words exactly?)</p>

<p>Thanks for your response.</p>
",,2019-08-28 07:58:03,What is the Gensim word2vec output,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10871,41440633,2017-01-03 09:50:20,,"<pre><code>sentences=gensim.models.doc2vec.TaggedLineDocument(""raw_docs.txt"")
model=gensim.models.Doc2Vec(sentences,min_count=1,iter=100)
sentence=TaggedDocument(words=[u''],tags=[u'T1'])
sentences1=[sentence]
model.build_vocab(sentences1,update=True)
model.train(sentences1)
print ""successful!""
</code></pre>

<p>I want to use a big data to train a doc2vec model. And I want to use this pretrained model to train a new text.</p>

<p>I only expect to train the new one with a pretrained model. How can I do that?The code above doesn't work...</p>
",2017-01-03 10:52:18,2017-01-03 10:52:18,How to train a new text with gensim doc2vec,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10891,41568556,2017-01-10 12:10:53,,"<p>I am currently working on gensim doc2vec model to implement sentence similarity. </p>

<p>I came across this <a href=""https://williambert.online/2012/05/relatively-quick-and-easy-gensim-example-code/"" rel=""nofollow noreferrer"">sample code</a> by William Bert where he has mentioned that to train this model I need to provide my own background corpus. The code is copied below for convenience:</p>

<pre><code>import logging, sys, pprint

logging.basicConfig(stream=sys.stdout, level=logging.INFO)

### Generating a training/background corpus from your own source of documents
from gensim.corpora import TextCorpus, MmCorpus, Dictionary

# gensim docs: ""Provide a filename or a file-like object as input and TextCorpus will be initialized with a
# dictionary in `self.dictionary`and will support the `iter` corpus method. For other kinds of corpora, you only
# need to override `get_texts` and provide your own implementation.""
background_corpus = TextCorpus(input=YOUR_CORPUS)

# Important -- save the dictionary generated by the corpus, or future operations will not be able to map results
# back to original words.
background_corpus.dictionary.save(
    ""my_dict.dict"")

MmCorpus.serialize(""background_corpus.mm"",
    background_corpus)  #  Uses numpy to persist wiki corpus in Matrix Market format. File will be several GBs.

### Generating a large training/background corpus using Wikipedia
from gensim.corpora import WikiCorpus, wikicorpus

articles = ""enwiki-latest-pages-articles.xml.bz2""  # available from http://en.wikipedia.org/wiki/Wikipedia:Database_download

# This will take many hours! Output is Wikipedia in bucket-of-words (BOW) sparse matrix.
wiki_corpus = WikiCorpus(articles)
wiki_corpus.dictionary.save(""wiki_dict.dict"")

MmCorpus.serialize(""wiki_corpus.mm"", wiki_corpus)  #  File will be several GBs.

### Working with persisted corpus and dictionary
bow_corpus = MmCorpus(""wiki_corpus.mm"")  # Revive a corpus

dictionary = Dictionary.load(""wiki_dict.dict"")  # Load a dictionary

### Transformations among vector spaces
from gensim.models import LsiModel, LogEntropyModel

logent_transformation = LogEntropyModel(wiki_corpus,
    id2word=dictionary)  # Log Entropy weights frequencies of all document features in the corpus

tokenize_func = wikicorpus.tokenize  # The tokenizer used to create the Wikipedia corpus
document = ""Some text to be transformed.""
# First, tokenize document using the same tokenization as was used on the background corpus, and then convert it to
# BOW representation using the dictionary created when generating the background corpus.
bow_document = dictionary.doc2bow(tokenize_func(
    document))
# converts a single document to log entropy representation. document must be in the same vector space as corpus.
logent_document = logent_transformation[[
    bow_document]]

# Transform arbitrary documents by getting them into the same BOW vector space created by your training corpus
documents = [""Some iterable"", ""containing multiple"", ""documents"", ""...""]
bow_documents = (dictionary.doc2bow(
    tokenize_func(document)) for document in documents)  # use a generator expression because...
logent_documents = logent_transformation[
                   bow_documents]  # ...transformation is done during iteration of documents using generators, so this uses constant memory

### Chained transformations
# This builds a new corpus from iterating over documents of bow_corpus as transformed to log entropy representation.
# Will also take many hours if bow_corpus is the Wikipedia corpus created above.
logent_corpus = MmCorpus(corpus=logent_transformation[bow_corpus])

# Creates LSI transformation model from log entropy corpus representation. Takes several hours with Wikipedia corpus.
lsi_transformation = LsiModel(corpus=logent_corpus, id2word=dictionary,
    num_features=400)

# Alternative way of performing same operation as above, but with implicit chaining
# lsi_transformation = LsiModel(corpus=logent_transformation[bow_corpus], id2word=dictionary,
#    num_features=400)

# Can persist transformation models, too.
logent_transformation.save(""logent.model"")
lsi_transformation.save(""lsi.model"")

### Similarities (the best part)
from gensim.similarities import Similarity

# This index corpus consists of what you want to compare future queries against
index_documents = [""A bear walked in the dark forest."",
             ""Tall trees have many more leaves than short bushes."",
             ""A starship may someday travel across vast reaches of space to other stars."",
             ""Difference is the concept of how two or more entities are not the same.""]
# A corpus can be anything, as long as iterating over it produces a representation of the corpus documents as vectors.
corpus = (dictionary.doc2bow(tokenize_func(document)) for document in index_documents)

index = Similarity(corpus=lsi_transformation[logent_transformation[corpus]], num_features=400, output_prefix=""shard"")

print ""Index corpus:""
pprint.pprint(documents)

print ""Similarities of index corpus documents to one another:""
pprint.pprint([s for s in index])

query = ""In the face of ambiguity, refuse the temptation to guess.""
sims_to_query = index[lsi_transformation[logent_transformation[dictionary.doc2bow(tokenize_func(query))]]]
print ""Similarities of index corpus documents to '%s'"" % query
pprint.pprint(sims_to_query)

best_score = max(sims_to_query)
index = sims_to_query.tolist().index(best_score)
most_similar_doc = documents[index]
print ""The document most similar to the query is '%s' with a score of %.2f."" % (most_similar_doc, best_score)
</code></pre>

<p>Where and how should I provide my own corpus in the code?</p>

<p>Thanks in advance for your help.</p>
",2017-01-10 12:44:19,2017-01-10 12:44:19,How to call a corpus file in python?,<python><machine-learning><gensim><corpus><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10908,41594324,2017-01-11 15:10:22,,"<p>Problem Statement - Classify a product review </p>

<p>classes - Travel,Hotel,Cars,Electronics,Food,Movies</p>

<p>I am approaching this problem with the famous <code>Text Classification</code> problem. Feature set is prepared by using <code>Doc2Vec</code> default model from <code>gensim</code> and for classification I am using <code>Logistic Regression</code> oneVSrest from <code>sklearn</code>. </p>

<p>For every class I feed 10000 reviews to <code>Doc2Vec</code>.( I am following this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow noreferrer"">Doc2Vec</a> tutorial). In this way the model learns vector for each sentence. From the resulting vectors, 80% from each class are given to <code>LogisticRegression</code> for training and 20% for testing. The accuracy of classifier is 98%. But for unseen data the accuracy is just 17%. Also <code>PCA</code> of all sentence vectors when plotted in a 2D graph resulted in one dense cluster. What I can conclude from the graph is that the data is inseparable but then how the classifier gave an accuracy of 98%? Also, why on unseen data the accuracy is very low? How can I evaluate/validate my results. </p>
",,2017-01-11 15:10:22,Classifier Accuracy - Too good to believe,<python><pca><gensim><text-classification><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10945,41689525,2017-01-17 05:18:53,,"<p>I am learning about <strong>Word2Vec</strong> and <strong>GloVe</strong> model in python so I am going through this getting started with <strong>GENSIM</strong> available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""nofollow noreferrer"">here</a>.</p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = word2vec.Text8Corpus('text8')
sentences = word2vec.Text8Corpus('~/Desktop/text8')
model = word2vec.Word2Vec(sentences, size=200)
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)
model.most_similar(['man'])
model.save('text8.model')
model.save_word2vec_format('text.model.bin', binary=True)
model1 = word2vec.Word2Vec.load_word2vec_format('text.model.bin', binary=True)
model1.most_similar(['girl', 'father'], ['boy'], topn=3)
more_examples = [""he is she"", ""big bigger bad"", ""going went being""]
for example in more_examples:
    a, b, x = example.split()
    predicted = model.most_similar([x, b], [a])[0][0]
    print (""'%s' is to '%s' as '%s' is to '%s'"" % (a, b, x, predicted))
model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am getting this error: </p>

<pre><code>2017-01-17 10:34:26,054 : INFO : loading projection weights from vectors.bin
Traceback (most recent call last):
  File ""&lt;pyshell#16&gt;"", line 1, in &lt;module&gt;
    model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1172, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'vectors.bin'
</code></pre>

<p>How do I rectify this. Where can I get the vector.bin file. 
Thanks for your help in advance.</p>
",,2017-02-09 08:12:02,Gensim getting started Error : No such file or directory: 'vectors.bin',<python><error-handling><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10946,41690885,2017-01-17 07:05:39,,"<p>There is Convolution1D example <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"" rel=""nofollow noreferrer"">https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py</a> without word2vec.</p>

<p>Currently, I am using gensim to train word2vec model.</p>

<p>I want to use word2vec and keras cnn(2D not 1D) to do document classifacation(Chinese Text). I learned the basic flow of text classification in cnn and want to do a test.</p>

<h2>For example(the steps I imagine):</h2>

<ol>
<li><p>Use a good Cinese Tokenized Text Set to train word2vec model</p>

<pre><code>model = gensim.models.Word2Vec(new_sentences, workers=10, size=200, min_count=2)
</code></pre></li>
<li><p>Tokenize my sentences dataset to words lists dataset(the longest sentence has over 8000 words, shortest is less 50)</p>

<pre><code>1     ['', '', '', '', '', '']
2     ['', '']
...
9999  ['', '', '']
</code></pre></li>
<li><p>Use a method to transform words lists dataset to word2vec dataset</p>

<p>transform every word in every sencence to a vec by trained model.</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200]]
...
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>Pad  word2vec dataset (with size=200 zero array)</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
....
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>go to the  CNN (using Convolution2D)</p></li>
</ol>

<hr>

<p>I search for a long time, but can't find any way to do the step 3 ( after step 3,  the parameter and layers setting in step 5 is hard to understand too). </p>
",,2017-01-17 11:43:53,How to use word2vec with keras CNN (2D) to do text classification?,<neural-network><deep-learning><keras><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10954,41628856,2017-01-13 06:46:13,,"<p>I am learning about word2vec and GloVe model in python so I am going through this available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""noreferrer"">here</a>. </p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>&gt;&gt;&gt;from gensim.models import word2vec
&gt;&gt;&gt;import logging
&gt;&gt;&gt;logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
&gt;&gt;&gt;sentences = word2vec.Text8Corpus('text8')
&gt;&gt;&gt;model = word2vec.Word2Vec(sentences, size=200)
</code></pre>

<p>I am getting this error : </p>

<pre><code>2017-01-13 11:15:41,471 : INFO : collecting all words and their counts
Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    model = word2vec.Word2Vec(sentences, size=200)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 469, in __init__
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 533, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 545, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1536, in __iter__
    with utils.smart_open(self.fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'text8'
</code></pre>

<p>How do I rectify this ?
Thanks in advance for your help.</p>
",2017-01-13 07:24:05,2017-05-28 15:35:19,gensim Getting Started Error: No such file or directory: 'text8',<python><python-3.x><error-handling><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10962,41658568,2017-01-15 06:43:50,,"<p>I have installed gensim (through pip) in Python. After the installation is over I get the following warning:</p>

<blockquote>
  <p><strong>C:\Python27\lib\site-packages\gensim\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>How can I rectify this? </p>

<p>I am unable to import word2vec from gensim.models due to this warning.</p>

<p>I have the following configurations: Python 2.7, gensim-0.13.4.1, numpy-1.11.3, scipy-0.18.1, pattern-2.6.</p>
",2017-01-15 07:05:26,2018-10-19 18:01:51,Chunkize warning while installing gensim,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
10971,41607976,2017-01-12 08:07:07,,"<p>I am using gensim <code>doc2vec</code>. I want know if there is any efficient way to know the vocabulary size from doc2vec. One crude way is to count the total number of words, but if the data is huge(1GB or more) then this won't be an efficient way.</p>
",,2019-05-07 11:24:24,Is there any way to get the vocabulary size from doc2vec model?,<gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
10981,41758755,2017-01-20 08:04:02,,"<p>I am currently going through <strong>Gensim</strong> tutorial on <strong>Corpora and Vector Spaces</strong> in that I am right now trying to understand  <a href=""http://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time"" rel=""nofollow noreferrer""><strong>Corpus Streaming  One Document at a Time</strong></a>.</p>

<p>After I compiled these lines of codes referring the above link in python3:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
        for line in open('mycorpus.txt'):
            # assume there's one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())

corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!

print(corpus_memory_friendly)

for vector in corpus_memory_friendly:  # load one vector into memory at a time
    print(vector)
</code></pre>

<p>I am getting this error: </p>

<pre><code>&lt;__main__.MyCorpus object at 0x7f2e37e17d68&gt;

Traceback (most recent call last):
  File ""&lt;pyshell#46&gt;"", line 1, in &lt;module&gt;
    for vector in corpus_memory_friendly:  # load one vector into memory at a time
  File ""&lt;pyshell#41&gt;"", line 3, in __iter__
    for line in open('mycorpus.txt'):
FileNotFoundError: [Errno 2] No such file or directory: 'mycorpus.txt'
</code></pre>

<p>I have already downloaded <strong><em>mycorpus.txt</em></strong> still I am getting this error. Where should I store mycorpus.txt file.</p>

<p>Thanks for your help.</p>
",,2017-01-20 08:04:02,(Gensim)Python : FileNotFoundError: [Errno 2] No such file or directory: 'mycorpus.txt',<python><error-handling><gensim><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
10986,41720864,2017-01-18 13:35:13,,"<p>I did research on Google also on Gensim Support forum, but I cannot find a good answer. </p>

<p>Basically, I am implementing online learning for Doc2Vec using Gensim, but Gensim keeps throwing me a random error called ""Segmentation </p>

<p>Please take a look at my sample code</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence
import random
import logging

if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO)

    sentence1 = ""this is a test""
    sentence2 = ""test test 123 test""
    sentence3 = ""qqq zzz""
    sentence4 = ""ppp""

    sentences = [
        LabeledSentence(sentence1.split(), [""p1""]),
        LabeledSentence(sentence2.split(), [""p2""])
    ]
    model = Doc2Vec(min_count=1, window=5, size=400, sample=1e-4, negative=5, workers=1)
    model.build_vocab(sentences)

    for a in range(2):
        random.shuffle(sentences)
        print([s.tags[0] for s in sentences])
        model.train(sentences)
    model.save(""test.d2v"")

    new_model = Doc2Vec.load(""test.d2v"")
    new_sentences = [
        LabeledSentence(sentence1.split(), [""n1""]),
        LabeledSentence(sentence3.split(), [""n2""])
    ]
    new_model.build_vocab(new_sentences, update=True)

    for a in range(4):
        random.shuffle(new_sentences)
        print([s.tags[0] for s in new_sentences])
        new_model.train(new_sentences)
</code></pre>

<p>Here is my error</p>

<pre><code>INFO:gensim.models.word2vec:training model with 1 workers on 7 vocabulary and 400 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5
INFO:gensim.models.word2vec:expecting 2 sentences, matching count from corpus used for vocabulary survey
Segmentation fault
</code></pre>

<p>Can anyone explain to me why? and how to solve this?</p>

<p>Thanks</p>
",,2017-01-19 00:21:24,Gensim Segmentation Fault,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11011,41765951,2017-01-20 14:28:54,,"<p>I am applying the LDA method using Gensim to extract keywords from documents.
I can extract topics, and then assign these topics and key words associated to the documents.</p>

<p>I would like to have the ids of these terms (or key words) instead of the terms themselves. I know that <code>corpus[i]</code> extract a list of couples  [(term_id, term_frequency) ...] of document <code>i</code> but I can't see how could I use this in my code to extract only the ids and assign it to my results.</p>

<p>My code is as follows :</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=passes, minimum_probability=0)

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))

threshold = sum(scores)/len(scores)
print(threshold)

for t in range(len(topic_tuple)):  

    key_words.append([topic_tuple[t][j][0] for j in range(num_words)])
    df_key_words = pd.DataFrame({'key_words' : key_words})

    documents_corpus.append([j for i,j in zip(lda_corpus,doc_set) if i[t][1] &gt; threshold])
    df_documents_corpus = pd.DataFrame({'documents_corpus' : documents_corpus})

    documents_corpus_id.append([i for d,i in zip(lda_corpus, doc_set_id) if d[t][1] &gt; threshold])
    df_documents_corpus_id = pd.DataFrame({'documents_corpus_id' : documents_corpus_id})


result.append(pd.concat([df_key_words, df_documents_corpus, df_documents_corpus_id ], axis=1))
</code></pre>

<p>Thank you in advance and ask me if more information are needed.</p>
",,2017-01-23 16:13:16,"Python, LDA : How to get the id of keywords instead of the keywords themselves with Gensim?",<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
11025,41709318,2017-01-18 00:15:02,,"<p><a href=""https://i.stack.imgur.com/ofJqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ofJqR.png"" alt=""Doc2Vec Figure 2""></a></p>

<p>The above picture is from <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>, the paper introducing Doc2Vec. I am using Gensim's implementation of Word2Vec and Doc2Vec, which are great, but I am looking for clarity on a few issues.</p>

<ol>
<li>For a given doc2vec model <code>dvm</code>, what is <code>dvm.docvecs</code>? My impression is that it is the averaged or concatenated vector that includes all of the word embedding <em>and</em> the paragraph vector, <code>d</code>. Is this correct, or is it d?</li>
<li>Supposing <code>dvm.docvecs</code> is not <code>d</code>, can one access d by itself? How?</li>
<li>As a bonus, how is <code>d</code> calculated? The paper only says:</li>
</ol>

<blockquote>
  <p>In our Paragraph Vector framework (see Figure 2), every
  paragraph is mapped to a unique vector, represented by a
  column in matrix D and every word is also mapped to a
  unique vector, represented by a column in matrix W.</p>
</blockquote>

<p>Thanks for any leads!</p>
",,2017-01-19 00:14:55,What is gensim's 'docvecs'?,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11034,41729287,2017-01-18 20:56:50,,"<p>I am using gensim for topic modeling. I've created a corpus using </p>

<pre><code>wordDict = corpora.Dictionary(trimmedTextTokens)

gsCorpus = [wordDict.doc2bow(text) for text in trimmedTextTokens]
</code></pre>

<p>where trimmedTextTokens are the result of removing stop words. Now I want to filter out the terms from the corpus that are not in a list of a restricted or constructed vocabulary. Any ideas? Thank you!!</p>
",,2017-01-19 10:56:02,How to filter out words in a corpus from a constrained vocabulary with gensim?,<python><nlp><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11040,41815880,2017-01-23 21:22:23,,"<p>I'm analyzing a corpus of roughly 2M raw words. I build a model using gensim's word2vec, embed the vectors using sklearn TSNE, and cluster the vectors (from word2vec, not TSNE) using sklearn DBSCAN. The TSNE output looks about right: the layout of the words in 2D space seems to reflect their semantic meaning. There's a group of misspellings, clothes, etc.</p>

<p>However, I'm having trouble getting DBSCAN to output meaningful results. It seems to label almost everything in the ""0"" group (colored teal in the images). As I increase epsilon, the ""0"" group takes over everything. Here are screenshots with epsilon=10, and epsilon=12.5. With epsilon=20, almost everything is in the same group.</p>

<p><a href=""https://i.stack.imgur.com/e9QvG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e9QvG.png"" alt=""epsilon 10""></a>
<a href=""https://i.stack.imgur.com/LLkzu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LLkzu.png"" alt=""epsilon 12.5""></a></p>

<p>I would expect, for instance, the group of ""clothing"" words to all get clustered together (they're unclustered @ eps=10). I would also expect more on the order of 100 clusters, as opposed to 5 - 12 clusters, and to be able to control the size and number of the clusters using epsilon.</p>

<p>A few questions, then. Am I understanding the use of DBSCAN correctly? Is there another clustering algorithm that might be a better choice? How can I know what a good clustering algorithm for my data is?</p>

<p>Is it safe to assume my model is tuned pretty well, given that the TSNE looks about right?</p>

<p>What other techniques can I use in order to isolate the issue with clustering? How do I know if it's my word2vec model, my use of DBSCAN, or something else?</p>

<p>Here's the code I'm using to perform DBSCAN:</p>

<pre><code>import sys
import gensim
import json
from optparse import OptionParser

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# snip option parsing

model = gensim.models.Word2Vec.load(options.file);
words = sorted(model.vocab.keys())
vectors = StandardScaler().fit_transform([model[w] for w in words])

db = DBSCAN(eps=options.epsilon).fit(vectors)
labels = db.labels_
core_indices = db.core_sample_indices_

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print(""Estimated {:d} clusters"".format(n_clusters), file=sys.stderr)

output = [{'word': w, 'label': np.asscalar(l), 'isCore': i in core_indices} for i, (l, w) in enumerate(zip(labels, words))]
print(json.dumps(output))
</code></pre>
",2018-06-06 14:20:29,2018-06-08 18:29:36,Troubleshooting tips for clustering word2vec output with DBSCAN,<python><machine-learning><scikit-learn><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
11070,41819761,2017-01-24 03:57:19,,"<p>Does anyone have an example of data visualization of an LDA model trained using the PySpark library (specifically using <a href=""https://github.com/bmabey/pyLDAvis"" rel=""noreferrer"">pyLDAvis</a>)? I've seen a lot of examples for GenSim and other libraries but not PySpark. Specifically I'm wondering what to pass into the <code>pyLDAvis.prepare()</code> function and how to get it from my lda model.
Here is my code:</p>

<pre><code> from pyspark.mllib.clustering import LDA, LDAModel
 from pyspark.mllib.feature import IDF
 from pyspark.ml.feature import CountVectorizer
 from pyspark.mllib.linalg import Vectors

 vectorizer = CountVectorizer(inputCol=""filtered1"", outputCol=""features"").fit(filtered_final)
 countVectors = vectorizer.transform(filtered_final).select(""status_id"", ""features"")
 countVectors.show()
 frequencyVectors = countVectors.rdd.map(lambda vector: vector[1])
 frequencyDenseVectors = frequencyVectors.map(lambda vector: Vectors.dense(vector))
 idf = IDF().fit(frequencyDenseVectors)
 print('fitting complete')
 tfidf = idf.transform(frequencyDenseVectors)
 print(""tf idf complete"")
 #prepare corpus for LDA
 corpus = tfidf.map(lambda x: [1, x]).cache()
 #train LDA
 ldaModel = LDA.train(corpus, k = 15, maxIterations=100, optimizer=""online"", docConcentration=2.0, topicConcentration=3.0)
 print(""lda model complete"")
</code></pre>
",2017-01-24 13:10:21,2019-06-11 13:48:08,pyLDAvis visualization of pyspark generated LDA model,<python><apache-spark><pyspark><lda>,,,CC BY-SA 3.0,False,False,True,False,False
11086,41936775,2017-01-30 13:10:29,,"<p>i am analysing text with topic modelling and using Gensim and pyLDAvis for that. Would like to share the results with distant colleagues, without a need for them to install python and all required libraries. 
Is there a way to export interactive graphs as HTML/JS files that could be uploaded to any web server?
I've found something mentioned in documentation, but have no idea how to implement it:
<strong><a href=""https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py"" rel=""noreferrer"">https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py</a></strong></p>
",,2017-01-30 13:14:12,Export pyLDAvis graphs as standalone webpage,<python><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11105,41884796,2017-01-26 23:25:06,,"<p>I would like to determine for a given URL as an input a category from a list of categories (e.g. programming, health, vegan food, computer science, math).</p>

<pre><code>cats = [ ""programming"", ""health"", ""raw vegan food"", ""vegan cooking"", ""computer science"", ""math"" ]

def getCategory(url, cats):
  ...
</code></pre>

<p>I would like to do it, without having to download a lot of data for deriving the category. I have already searched a lot of what is available, but I'm starting to information overload, losing in lot of data about NLP, topic modeling.</p>

<p>I have found gensim library, but not sure if it's able to do this conversion.
So, if you can provide certain direction, it would be really helpful.</p>
",,2017-01-26 23:25:06,Determine category for a given URL,<python><text-classification>,,,CC BY-SA 3.0,False,False,True,False,False
11107,41888085,2017-01-27 06:28:06,,"<p>I want to create a word embedding pretraining network which adds something on top of word2vec CBOW. Therefore, I'm trying to implement word2vec CBOW first. Since I'm very new to <a href=""https://keras.io/"" rel=""nofollow noreferrer"">keras</a>, I'm unable to figure out how to implement CBOW in it.</p>

<p><strong>Initialization</strong>:</p>

<p>I have calculated the vocabulary and have the mapping of word to integers.</p>

<p><strong>Input to the (yet to be implemented) network</strong>:</p>

<p>A list of <code>2*k + 1</code> integers (representing the central word and <code>2*k</code> words in context)</p>

<p><strong>Network Specification</strong></p>

<p>A shared <code>Embedding</code> layer should take this list of integers and give their corresponding vector outputs. Further a mean of <code>2*k</code> context vector is to be taken (I believe this can be done using <code>add_node(layer, name, inputs=[2*k vectors], merge_mode='ave')</code>).</p>

<p>It will be very helpful if anyone can share a small code-snippet of this.</p>

<p><strong>P.S.</strong>: I was looking at <a href=""https://github.com/niitsuma/word2vec-keras-in-gensim"" rel=""nofollow noreferrer"">word2veckeras</a>, but couldn't follow its code because it also uses a gensim.</p>

<p><strong>UPDATE 1</strong>:</p>

<p>I want to share the embedding layer in the network. The embedding layer should be able to take context words (2*k) and the current word as well. I can do this by taking all 2*k + 1 word indices in the input and write a custom lambda function which will do the needful. But, after that I also want to add negative sampling network for which I'll have to take embedding of more words and dot product with the context vector. Can someone provide with an example where Embedding layer is a shared node in the <code>Graph()</code> network</p>
",2017-01-27 10:36:46,2017-02-10 08:37:23,How to implement word2vec CBOW in keras with shared Embedding layer and negative sampling?,<keras><embedding><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11131,41960099,2017-01-31 14:31:00,,"<p>I have a textual dataset on which I trained a <code>gensim</code> w2v model. Now I want to use those vectors to recive the tf-idf values for the words and documents in my data set. What is the right way to do it? I tried to followe the <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tutorial</a> on gensim's site.</p>

<p>I expect something like <code>models.tfidfmodel(model.wv[model.wv.index2word])</code>
 but this fail since </p>

<p><code>File ""&lt;ipython-input-229-7946418f8a82&gt;"", line 1, in &lt;module&gt;
    models.tfidfmodel(model.wv[model.wv.index2word])
TypeError: 'module' object is not callable</code></p>

<p>does what I want makes since? Is BOW the only way to do that?</p>
",,2017-01-31 15:03:34,how to get tf-id from w2v on gensim,<python-3.x><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11150,42039964,2017-02-04 11:45:01,,"<p>I'm running a Lubuntu 16.04 Machine with <code>gcc</code> installed. I'm not getting <code>gensim</code> to work with <code>cython</code> because when I train a <code>doc2vec model</code>, it is only ever trained with one worker which is dreadfully slow. </p>

<p>As I said <code>gcc</code> was installed from the start. I then maybe made the mistake and installed <code>gensim</code> before <code>cython</code>. I corrected that by forcing a reinstall of <code>gensim</code> via <code>pip</code>. With no effect still just one worker.</p>

<p>The machine is setup as a <code>spark</code> master and I interface with <code>spark</code> via <code>pyspark</code>.  It works something like this, <code>pyspark</code> uses <code>jupyter</code> and <code>jupyter</code> uses python 3.5. This way I get a <code>jupyter</code> interface to my cluster. Now I have no idea if this is the reason why i cant get <code>gensim</code> to work with <code>cython</code>. I don't execute any gensim code on the cluster, it is just more convenient to fire up <code>jupyter</code> to also do <code>gensim</code>. </p>
",,2017-02-06 16:17:28,How to get cython and gensim to work with pyspark,<python><python-3.x><pyspark><cython><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11177,42119237,2017-02-08 16:58:43,,"<p>I'm using gensim to extract feature vector from a document.
I've downloaded the pre-trained model from Google named <code>GoogleNews-vectors-negative300.bin</code> and I loaded that model using the following command:</p>

<pre><code>model = models.Doc2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>My purpose is to get a feature vector from a document. For a word, it's very easy to get the corresponding vector:</p>

<pre><code>vector = model[word]
</code></pre>

<p>However, I don't know how to do it for a document. Could you please help?</p>
",,2019-12-17 09:43:27,load pre-trained word2vec model for doc2vec,<machine-learning><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11178,42096798,2017-02-07 18:01:16,,"<p>I have three NumPy arrays saved to disk in <code>.npy</code> format, together totaling about 40 GB (representing text count data from a very large document set). The three arrays represent the <code>data</code>, <code>indices</code>, and <code>indptr</code> attributes of a <a href=""https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.sparse.csc_matrix.html"" rel=""nofollow noreferrer""><code>scipy.sparse.csc_matrix</code></a> sparse matrix.</p>

<p>I want to use distributed gensim LsiModel on this data, specifically with the <a href=""https://radimrehurek.com/gensim/matutils.html#gensim.matutils.Scipy2Corpus"" rel=""nofollow noreferrer""><code>gensim.matutils.Scipy2Corpus</code></a> function to provide the corpus iterator from the underlying sparse matrix.</p>

<p>However, I do not want to materialize the whole matrix in memory. Instead, how can I tell gensim about the underlying disk data and have gensim stream from disk into the csc matrix as needed to distribute chunks to the worker processes? If I understand correctly, this is what <code>Scipy2Corpus</code> and the <a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow noreferrer"">distributed LsiModel examples</a> claim to do, but instead they require materializing the arrays into a csc matrix ahead of time.</p>

<p>I have tried loading each of the underlying arrays with <code>mmap_mode='r'</code>, but the function that constructs the materialized csc matrix, <code>scipy.sparse.csc_matrix</code> will pull all of the data in regardless.</p>
",,2017-02-08 00:01:37,Using gensim Scipy2Corpus without materializing sparse matrix in memory,<python><numpy><scipy><sparse-matrix><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11185,42064690,2017-02-06 09:47:22,,"<p>LSTM/RNN can be used for text generation.
<a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""nofollow noreferrer"">This</a> shows way to use pre-trained GloVe word embeddings for Keras model.</p>

<ol>
<li>How to use pre-trained Word2Vec word embeddings with Keras LSTM
model? <a href=""https://codekansas.github.io/gensim"" rel=""nofollow noreferrer"">This</a> post did help.</li>
<li>How to predict / generate next <em>word</em> when the model is provided with the sequence of words as its input?</li>
</ol>

<p>Sample approach tried:</p>

<pre><code># Sample code to prepare word2vec word embeddings    
import gensim
documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]
sentences = [[word for word in document.lower().split()] for document in documents]

word_model = gensim.models.Word2Vec(sentences, size=200, min_count = 1, window = 5)

# Code tried to prepare LSTM model for word generation
from keras.layers.recurrent import LSTM
from keras.layers.embeddings import Embedding
from keras.models import Model, Sequential
from keras.layers import Dense, Activation

embedding_layer = Embedding(input_dim=word_model.syn0.shape[0], output_dim=word_model.syn0.shape[1], weights=[word_model.syn0])

model = Sequential()
model.add(embedding_layer)
model.add(LSTM(word_model.syn0.shape[1]))
model.add(Dense(word_model.syn0.shape[0]))   
model.add(Activation('softmax'))
model.compile(optimizer='sgd', loss='mse')
</code></pre>

<p>Sample code / psuedocode to train LSTM and predict will be appreciated. </p>
",2020-01-21 19:19:15,2020-01-21 19:19:15,Using pre-trained word2vec with LSTM for word generation,<machine-learning><neural-network><keras><lstm><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
11204,41829323,2017-01-24 13:21:52,,"<p>I have a list of 10k words in a text file like so:</p>

<p>G15
KDN
C30A
Action Standard
Air Brush
Air Dilution</p>

<p>I am trying to convert them into lower cased tokens using this code for subsequent processing with GenSim:</p>

<pre><code>data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
texts = [[word for word in data.lower().split()] for word in data]
</code></pre>

<p>and I get the following callback:</p>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-84-33bbe380449e&gt; in &lt;module&gt;()
      1 data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
----&gt; 2 texts = [[word for word in data.lower().split()] for word in data]
      3 
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>Any suggestions on what I am doing wrong and how to correct it would be greatly appreciated!!! Thank you!!</p>
",,2018-09-26 09:29:55,AttributeError: 'list' object has no attribute 'lower' gensim,<python><string><split><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11237,42198720,2017-02-13 07:32:35,,"<p>With word2vec, to find similarity score/most similar words of a single word can be done by</p>

<pre><code>model.most_similar('man')
model.similarity('man', 'woman')
</code></pre>

<p>However, now i want to find similarity score of a word phrase, such as,</p>

<pre><code>model.most_similar('battery life')
model.similarity('battery life', 'battery')
model.similarity('battery life', 'sound quality')
</code></pre>

<p>which i get the KeyError: ""word 'battery life' not in vocabulary"",
so is it possible to do it with word2vec?</p>
",,2017-02-14 19:14:12,How to find similarity score of two word phrases with word2vec?,<text-mining><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11259,42186543,2017-02-12 10:38:17,,"<p>I am training a word2vec model from the tensorflow tutorial.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>

<p>After training I get the embedding matrix. I would like to save this and import it as a trained model in gensim.</p>

<p>To load a model in gensim, the command is:</p>

<pre><code>model = Word2Vec.load_word2vec_format(fn, binary=True)
</code></pre>

<p>But how do I generate the <code>fn</code> file from Tensorflow?</p>

<p>Thanks</p>
",2017-02-14 20:17:48,2017-12-24 15:07:14,"Training wordvec in Tensorflow, importing to Gensim",<python><machine-learning><tensorflow><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11269,42109463,2017-02-08 09:39:19,,"<p>I am a bit confused regarding an aspect of Doc2Vec. Basically, I am not sure if what I do makes sense. I have the following dataset :</p>

<pre><code>train_doc_0      --&gt; label_0
    ...               ...
train_doc_99     --&gt; label_0
train_doc_100    --&gt; label_1
    ...               ...
train_doc_199    --&gt; label_1
    ...               ...
    ...               ...
train_doc_239999 --&gt; label_2399

eval_doc_0
    ...
eval_doc_29
</code></pre>

<p>Where <code>train_doc_n</code> is a short document, belonging to some label. There are 2400 labels, with 100 training documents per label. <code>eval_doc_0</code> are evaluation documents where I would like to predict their label in the end (using a classifier).</p>

<p>I train a Doc2Vec model with these training documents &amp; labels. Once the model is trained, I reproject each of the original training document as well as my evaluation documents (the ones I would like to classify in the end) into the model's space using <code>infer_vector</code>. </p>

<p>The resulting is a matrix :</p>

<pre><code>X_train (240000,300) # doc2vec vectors for training documents
y_train (240000,)    # corresponding labels
y_eval  (30, 300)    # doc2vec vectors for evaluation documents
</code></pre>

<p>My problem is the following : If I run a simple cross validation on <code>X_train</code> and <code>y_train</code>, I have a decent accuracy. Once I try to classify my evaluation documents (even, using only 50 randomly sampled labels) I have a super bad accuracy, which makes me question my way of approaching this problem.</p>

<p>I followed this <a href=""https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1#.masebkx6n"" rel=""nofollow noreferrer"">tutorial</a> for the training of documents.</p>

<p>Does my approach make sense, especially with reprojecting all the training documents using <code>infer_vector</code> ?</p>
",2017-02-08 12:59:02,2017-02-08 17:31:37,Doc2Vec: reprojecting training documents into model space,<python><classification><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11274,42131107,2017-02-09 07:59:52,,"<p>I trained a word2vec model on my dataset using the word2vec gensim package. My dataset has about 131,681 unique words but the model outputs a vector matrix of shape (47629,100). So only 47,629 words have vectors associated with them. What about the rest? Why am I not able to get a 100 dimensional vector for every unique word?</p>
",2017-02-10 16:09:23,2017-02-10 16:09:23,Word2vec model query,<neural-network><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11288,42242521,2017-02-15 06:55:42,,"<p>I am just playing around with Doc2Vec from gensim, analysing stackexchange dump to analyze semantic similarity of questions to identify duplicates.</p>

<p>The tutorial on <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow noreferrer"">Doc2Vec-Tutorial</a> seems to describe the input as tagged sentences.</p>

<p>But the original paper: <a href=""https://arxiv.org/pdf/1405.4053.pdf"" rel=""nofollow noreferrer"">Doc2Vec-Paper</a> claims that the method can be used to infer fixed length vectors of paragraphs/documents.</p>

<p>Can someone explain the difference between a sentence and a document in this context, and how i would go about inferring paragraph vectors.</p>

<p>Since a question can sometimes span multiple sentences,
I thought, during training i will give sentences arising from the same question the same tags, but then how would i do this to infer_vector on unseen questions? </p>

<p>And this notebook : <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">Doc2Vec-Notebook</a></p>

<p>seems to be training vectors on TRAIN and TEST docs, can someone explain the rationale behind this and should i do the same?</p>
",,2017-02-16 03:49:52,Doc2Vec: Differentiate Sentence and Document,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11306,42212423,2017-02-13 19:54:33,,"<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>

<pre><code>dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(text) for text in dat]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
index = similarities.MatrixSimilarity(corpus_tfidf)
</code></pre>

<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>

<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>
",,2020-09-04 11:28:52,Python tf-idf: fast way to update the tf-idf matrix,<python><nlp><tf-idf><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
11325,42269313,2017-02-16 09:06:14,,"<p>First let's extract the TF-IDF scores per term per document:</p>

<pre><code>from gensim import corpora, models, similarities
documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>Printing it out:</p>

<pre><code>for doc in corpus_tfidf:
    print doc
</code></pre>

<p>[out]:</p>

<pre><code>[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.4301019571350565), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.2944198962221451)]
[(4, 0.3726494271826947), (7, 0.27219160459794917), (8, 0.3726494271826947), (9, 0.27219160459794917), (10, 0.3726494271826947), (11, 0.5443832091958983), (12, 0.3726494271826947)]
[(6, 0.438482464916089), (7, 0.32027755044706185), (9, 0.32027755044706185), (13, 0.6405551008941237), (14, 0.438482464916089)]
[(5, 0.3449874408519962), (7, 0.5039733231394895), (14, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]
[(9, 0.21953536176370683), (10, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]
[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]
[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]
[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]
[(8, 0.6282580468670046), (26, 0.45889394536615247), (29, 0.6282580468670046)]
</code></pre>

<p>If we want to find the ""saliency"" or ""importance"" of the words within this corpus, <strong>can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents?</strong> I.e. </p>

<pre><code>&gt;&gt;&gt; tfidf_saliency = Counter()
&gt;&gt;&gt; for doc in corpus_tfidf:
...     for word, score in doc:
...         tfidf_saliency[word] += score / len(corpus_tfidf)
... 
&gt;&gt;&gt; tfidf_saliency
Counter({7: 0.12182694202050007, 8: 0.11121194156107769, 26: 0.10886469856464989, 29: 0.10093919463036093, 9: 0.09022272408985754, 14: 0.08705221175200946, 25: 0.08482488519466996, 6: 0.08143359568202602, 10: 0.07480097322359022, 12: 0.07480097322359022, 4: 0.07411881371164887, 13: 0.07117278898823597, 5: 0.07104525967490458, 27: 0.07027283689263066, 28: 0.07027283689263066, 11: 0.060487023243988705, 15: 0.055997035904387725, 16: 0.055997035904387725, 21: 0.05389680556362955, 22: 0.05389680556362955, 23: 0.05389680556362955, 24: 0.05389680556362955, 17: 0.048785635947490406, 18: 0.048785635947490406, 19: 0.048785635947490406, 20: 0.048785635947490406, 0: 0.04778910634833961, 1: 0.04778910634833961, 2: 0.04778910634833961, 3: 0.04778910634833961, 30: 0.045480127933079706, 31: 0.045480127933079706, 32: 0.045480127933079706, 33: 0.045480127933079706, 34: 0.045480127933079706})
</code></pre>

<p>Looking at the output, could we assume that the most ""prominent"" word in the corpus is:</p>

<pre><code>&gt;&gt;&gt; dictionary[7]
u'system'
&gt;&gt;&gt; dictionary[8]
u'survey'
&gt;&gt;&gt; dictionary[26]
u'graph'
</code></pre>

<p>If so, <strong>what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?</strong></p>
",,2019-04-08 15:59:32,Interpreting the sum of TF-IDF scores of words across documents,<python><statistics><nlp><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11340,42094180,2017-02-07 15:50:16,,"<p>I've tried several methods of loading the google news word2vec vectors (<a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">https://code.google.com/archive/p/word2vec/</a>):</p>

<pre><code>en_nlp = spacy.load('en',vector=False)
en_nlp.vocab.load_vectors_from_bin_loc('GoogleNews-vectors-negative300.bin')
</code></pre>

<p>The above gives:</p>

<pre><code>MemoryError: Error assigning 18446744072820359357 bytes
</code></pre>

<p>I've also tried with the .gz packed vectors; or by loading and saving them with gensim to a new format:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
model.save_word2vec_format('googlenews2.txt')
</code></pre>

<p>This file then contains the words and their word vectors on each line.
I tried to load them with:</p>

<pre><code>en_nlp.vocab.load_vectors('googlenews2.txt')
</code></pre>

<p>but it returns ""0"".</p>

<p>What is the correct way to do this?</p>

<p><strong>Update:</strong></p>

<p>I can load my own created file into spacy.
I use a test.txt file with ""string 0.0 0.0 ...."" on each line. Then zip this txt with .bzip2 to test.txt.bz2.
Then I create a spacy compatible binary file:</p>

<pre><code>spacy.vocab.write_binary_vectors('test.txt.bz2', 'test.bin')
</code></pre>

<p>That I can load into spacy:</p>

<pre><code>nlp.vocab.load_vectors_from_bin_loc('test.bin')
</code></pre>

<p>This works!
However, when I do the same process for the googlenews2.txt, I get the following error:</p>

<pre><code>lib/python3.6/site-packages/spacy/cfile.pyx in spacy.cfile.CFile.read_into (spacy/cfile.cpp:1279)()

OSError: 
</code></pre>
",2019-05-13 17:26:27,2019-05-13 17:26:27,SpaCy: how to load Google news word2vec vectors?,<python><nlp><word2vec><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
11345,42329766,2017-02-19 16:28:49,,"<p>I'm currently working on NLP in python. However, in my corpus, there are both British and American English(realize/realise) I'm thinking to convert British to American. However, I did not find a good tool/package to do that. Any suggestions?</p>
",,2018-11-26 14:43:40,Python NLP British English vs American English,<python><nlp><nltk><gensim><linguistics>,,,CC BY-SA 3.0,True,False,True,False,False
11354,42363897,2017-02-21 09:49:33,,"<p>I am trying to implement word2vec model and getting Attribute error </p>

<blockquote>
  <p>AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'</p>
</blockquote>

<p>Below is the code :</p>

<pre><code>wv = Word2Vec.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
wv.init_sims(replace=True)
</code></pre>

<p>Please let me know the issue ?</p>
",2017-02-21 10:14:37,2017-03-17 09:34:24,AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format',<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11356,42368713,2017-02-21 13:31:26,,"<p>I am using word2vec from Gensim and I am feeding sentences to the model with the following iterator:</p>

<pre><code>class SentencesIterator(object):
    def __init__(self, source):
        self.source = source

        if os.path.isdir(self.source):
            self.type_source = 'dir'
        else:
            self.type_source = 'file'

    def __iter__(self):
        if self.type_source == 'dir':
            for fname in os.listdir(self.source):
                with open(os.path.join(self.source, fname)) as f:
                    for line in f:
                        yield line.split()
        else:
            with open(self.source) as f:
                    for line in f:
                        yield line.split()
</code></pre>

<p>Everything is working as expected but I noticed that the performance varies a lot depending on the type of input.</p>

<p>For a single file in input:
<code>INFO : PROGRESS: at 5.43% examples, 73483 words/s</code></p>

<p>For a directory with 1 file: <code>INFO : PROGRESS: at 17.09% examples, 71716 words/s</code></p>

<p>For a directory with 2 files: <code>INFO : PROGRESS: at 11.62% examples, 67678 words/s</code></p>

<p>For a directory with 30 files: <code>INFO : PROGRESS: at 1.19% examples, 54004 words/s</code></p>

<p>I don't understand the decrease in the speed of streaming. To me, all the operations are identical, it is just about opening a file and reading it line by line..</p>

<p>PS: I tried with 4 cores and 1 core and the same behavior is observed!</p>
",2017-02-22 05:17:06,2017-03-04 11:22:47,Performance of data streaming from file vs directory of files,<python><python-3.x><io><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11370,42289858,2017-02-17 05:09:35,,"<p>I have the LDA model and the document-topic probabilities.</p>

<pre><code># build the model on the corpus
ldam = LdaModel(corpus=corpus, num_topics=20, id2word=dictionary) 
# get the document-topic probabilities
theta, _ = ldam.inference(corpus)
</code></pre>

<p>I also need the distribution of words for all the topics i.e. a topic-word probability matrix. Is there a way to extract this information? </p>

<p>Thanks!</p>
",,2017-02-17 16:49:05,Extract topic word probability matrix in gensim LdaModel,<python><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11373,42316431,2017-02-18 14:30:56,,"<p>I am trying to classify paragraphs based on their sentiments. I have training data of 600 thousand documents. When I convert them to <code>Tf-Idf</code> vector space with words as analyzer and ngram range as 1-2 there are almost 6 million features. So I have to do Singular value decomposition (SVD) to reduce features.</p>

<p>I have tried gensim and sklearn's SVD feature. Both work fine for feature reduction till 100 but as soon as I try for 200 features they throw memory error. </p>

<p>Also I have not used entire document (600 thousand) as training data, I have taken 50000 documents only. So essentially my training matrix is:
50000 * 6 million and want to reduce it to  50000 * (100 to 500) </p>

<p>Is there any other way I can implement it in python, or do I have to implement sparks mllib SVD(written for only java and scala) ? If Yes, how much faster will it be?</p>

<p>System specification: 32 Gb RAM with 4 core processors on ubuntu 14.04</p>
",2017-02-18 14:38:37,2017-02-18 16:29:06,SVD using Scikit-Learn and Gensim with 6 million features,<python><scikit-learn><gensim><svd>,,,CC BY-SA 3.0,False,False,True,False,True
11387,42468394,2017-02-26 12:19:01,,"<p>I'm using PyCharm and loading models trained on words using Word2Vec. I tried to check the similarity between two words, but I get this error :</p>

<pre><code># Loading model trained on words
    model = word2vec.Word2Vec.load('models/text8.model')

    # Loading model enhanced with phrases (2-grams)
    model_phrase = word2vec.Word2Vec.load('models/text8.phrase.model')

    # Words that are similar are close in the sense of the cosine similarity.
    sim = model.similarity('woman', 'man')
    print 'Printing word similarity between ""woman"" and ""man"" : {0}'.format(sim)

Traceback (most recent call last):
File ""C:\Program Files (x86)\JetBrains\PyCharm 2016.3.2\helpers\pydev\pydevd.py"", line 1596, in &lt;module&gt;
globals = debugger.run(setup['file'], None, None, is_module)
File ""C:\Program Files (x86)\JetBrains\PyCharm 2016.3.2\helpers\pydev\pydevd.py"", line 974, in run
pydev_imports.execfile(file, globals, locals)  # execute the script
File ""C:/Users/XXX/Desktop/code/word2vec/embedding_word2vec_students.py"", line 144, in &lt;module&gt;
sim = model.similarity('woman', 'man')
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 1194, in similarity
return self.wv.similarity(w1, w2)
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 587, in similarity
return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 567, in __getitem__
return self.word_vec(words)
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 271, in word_vec
return self.syn0[self.vocab[word].index]
IndexError: list index out of range
</code></pre>

<p>When I debug, it seems that the problem comes from this line in the function word_vec :</p>

<pre><code>return self.syn0[self.vocab[word].index]
</code></pre>

<p>However I have no clue why I'm getting this. Thank you very much in advance if you can help me with this.</p>
",,2017-02-27 12:21:20,Word2Vec similarity function not working,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11394,42408666,2017-02-23 06:36:51,,"<p>I am using gensim to create a word2vec model of a sample file I have in a directory. I followed a tutorial online, which reads files in a directory and processes it line by line. My sample file has 9 lines in it. But this code gives my the same lines 9 times. Can someone please explain what's happening.</p>

<pre><code> class MySentences(object):
     def __init__(self, dirname):
         self.dirname = dirname   

     def __iter__(self): 
         for fname in os.listdir(self.dirname):
             for line in open(os.path.join(self.dirname, fname)):
                 print os.path.join(self.dirname, fname)
                 yield line.split() 

 sentences = MySentences('/fakepath/Folder')
</code></pre>

<p>Details:
Suppose filename contains 3 lines like</p>

<pre><code>hi how are you.
I am fine.
I am good.
</code></pre>

<p><code>line.split()</code> should give me: <code>['hi','how','are','you']</code> only once. But this happens 3 times so I get the above list thrice instead of once. If the total sentences are 5, then it returns the line 5 times.</p>
",2017-02-23 07:04:49,2017-02-23 07:30:00,Python File iterator running multiple times,<python><python-2.7><iterator><word2vec><listiterator>,,,CC BY-SA 3.0,False,False,True,False,False
11402,42357678,2017-02-21 02:35:31,,"<p>Word2Vec from gensim 0.13.4.1 to update the word vectors on the fly does not work.</p>

<pre><code>model.build_vocab(sentences, update=False)
</code></pre>

<p>works fine;  however, </p>

<pre><code>model.build_vocab(sentences, update=True)
</code></pre>

<p>does not.</p>

<hr>

<p>I am using <a href=""http://rutumulkar.com/blog/2015/word2vec"" rel=""noreferrer"">this website</a> to try and emulate what they have done; hence I use the following script at some point:</p>

<pre><code>model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.LineSentence(""./text8/text8"")
model.build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)
model.train(sentences)
</code></pre>

<p>However while this runs with <code>update=False</code>, using <code>update=True</code> gives me the following traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vecAttempt.py"", line 34, in &lt;module&gt;
    model.build_vocab(sentences, progress_per=10000, update=True)
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 535, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 708, in finalize_vocab
    self.update_weights()
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 1070, in update_weights
    self.wv.syn0 = vstack([self.wv.syn0, newsyn0])
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py"", line 230, in vstack
    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
</code></pre>
",2017-02-22 21:26:23,2017-02-23 15:35:04,gensim word2vec - array dimensions in updating with online word embedding,<python><numpy><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11403,42359220,2017-02-21 05:13:49,,"<p>I'm trying to classify user input text in two categories using doc2vec in python. I have the following code to train model and then classify input text. The problem is, i can't able find any method which classify string. 
I'm newbie so please ignore mistakes.</p>

<p>Here are links for class reference</p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.predict"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.predict</a>
<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>

<pre><code># gensim modules
from gensim import utils
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec

# random shuffle
from random import shuffle

# numpy
import numpy

# classifier
from sklearn.linear_model import LogisticRegression

import logging
import sys

log = logging.getLogger()
log.setLevel(logging.DEBUG)

ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
log.addHandler(ch)

class TaggedLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
        return self.sentences

    def sentences_perm(self):
        shuffle(self.sentences)
        return self.sentences


log.info('source load')
sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}

log.info('TaggedDocument')
sentences = TaggedLineSentence(sources)

log.info('D2V')
model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)
model.build_vocab(sentences.to_array())

log.info('Epoch')
for epoch in range(10):
    log.info('EPOCH: {}'.format(epoch))
    model.train(sentences.sentences_perm())

log.info('Model Save')
model.save('./imdb.d2v')
model = Doc2Vec.load('./imdb.d2v')

log.info('Sentiment')
train_arrays = numpy.zeros((25000, 100))
train_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]
    train_labels[i] = 1
    train_labels[12500 + i] = 0


test_arrays = numpy.zeros((25000, 100))
test_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_test_pos = 'TEST_POS_' + str(i)
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]
    test_labels[i] = 1
    test_labels[12500 + i] = 0

log.info('Fitting') 
classifier = LogisticRegression()
classifier.fit(train_arrays, train_labels)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)

print(classifier.score(test_arrays, test_labels))
# classify input text
text = input(""Enter Your text:"")
print(classifier.predict(text.split()))
</code></pre>
",,2017-02-22 00:28:03,Classify input Text using doc2vec and LogisticRegression,<python><machine-learning><logistic-regression><text-classification><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,True
11410,42372346,2017-02-21 16:07:53,,"<p>My code is running out of memory because of the question I asked in <a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">this page</a>. Then, I wrote the second code to have an iterable <code>alldocs</code>, not an all-in-memory <code>alldocs</code>. I changed my code based on the explanation of <a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">this page</a>. I am not familiar with stream concept and I could not solve the error I got.</p>

<p>This code read all files of all folders of given path.The context of each file is consist of a document name and its context in two lines.For instance:</p>

<blockquote>
  <p>clueweb09-en0010-07-00000</p>
  
  <p>dove   gif clipart pigeon  clip    art picture image   hiox    free    birds   india   web icons   clipart add stumble upon    </p>
  
  <p>clueweb09-en0010-07-00001</p>
  
  <p>google bookmarks   yahoo   bookmarks   php script  java    script  jsp script  licensed    scripts html    tutorials   css tutorials</p>
</blockquote>

<p>First code:</p>

<pre><code># coding: utf-8
 import string
 import nltk
 import nltk.tokenize 
 from nltk.corpus import stopwords
 import re
 import os, sys 

 import MySQLRepository

 from gensim import utils
 from gensim.models.doc2vec import Doc2Vec
 import gensim.models.doc2vec
 from gensim.models.doc2vec import LabeledSentence
 from boto.emr.emrobject import KeyValue


 def readAllFiles(path):
    dirs = os.listdir( path )
    for file in dirs:
        if os.path.isfile(path+""/""+file):
           prepareDoc2VecSetting(path+'/'+file)
       else:
           pf=path+""/""+file
           readAllFiles(pf)      

def prepareDoc2VecSetting (fname):
    mapDocName_Id=[]
    keyValues=set()
   with open(fname) as alldata:
        a= alldata.readlines()
        end=len(a)
        label=0
        tokens=[]
        for i in range(0,end):
            if a[i].startswith('clueweb09-en00'):
               mapDocName_Id.insert(label,a[i])
               label=label+1
               alldocs.append(LabeledSentence(tokens[:],[label]))
               keyValues |= set(tokens)
               tokens=[]
           else:
               tokens=tokens+a[i].split()  

   mydb.insertkeyValueData(keyValues) 

   mydb.insertDocId(mapDocName_Id)


   mydb=MySQLRepository.MySQLRepository()

  alldocs = [] 
  pth='/home/flr/Desktop/newInput/tokens'
  readAllFiles(ipth)

  model = Doc2Vec(alldocs, size = 300, window = 5, min_count = 2, workers = 4)
  model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>Second code:(I did not consider parts related to DB)</p>

<pre><code>import gensim
import os


from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence



class prepareAllDocs(object):

    def __init__(self, top_dir):
        self.top_dir = top_dir

    def __iter__(self):
    mapDocName_Id=[]
    label=1
    for root, dirs, files in os.walk(top_directory):
        for fname in files:
            print fname
            inputs=[]
            tokens=[]
            with open(os.path.join(root, fname)) as f:
                for i, line in enumerate(f):          
                    if line.startswith('clueweb09-en00'):
                        mapDocName_Id.append(line)
                        if tokens:
                            yield LabeledSentence(tokens[:],[label])
                            label+=1
                            tokens=[]
                    else:
                        tokens=tokens+line.split()
                yield LabeledSentence(tokens[:],[label])

pth='/home/flashkar/Desktop/newInput/tokens/'
allDocs = prepareAllDocs('/home/flashkar/Desktop/newInput/tokens/')
for doc in allDocs:
    model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, workers = 4)
model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 44, in 
      model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, >workers = 4)   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 618, in <strong>init</strong>
      self.build_vocab(documents, trim_rule=trim_rule)   File >""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/word2vec.py"",
  line 523, in build_vocab
      self.scan_vocab(sentences, progress_per=progress_per, >trim_rule=trim_rule)  # initial survey   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 655, in scan_vocab
      for document_no, document in enumerate(documents):   File >""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 40, in <strong>iter</strong>
      yield LabeledSentence(tokens[:],tpl<a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">1</a>) IndexError: list index out of range</p>
</blockquote>
",2017-02-23 17:52:43,2017-02-23 17:52:43,How build Doc2Vec model by useing an 'iterable' object,<python><iterator><gensim><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
11414,42376652,2017-02-21 19:51:13,,"<p>I tried the three default-options for alpha in gensim's lda implementation and now wonder about the result:
The sum of topic-probabilities over all documents is smaller than the number of documents in the corpus (see below). For example alpha = 'symmetric' yields about 9357 as sum of topic-probabilities, however, the number of topics is 9459. Could one tell me the reason for this unexpected result?</p>

<pre><code>alpha = symmetric
nr_of_docs = 9459
sum_of_topic_probs = 9357.12285605

alpha = asymmetric
nr_of_docs = 9459
sum_of_topic_probs = 9375.29253851

alpha = auto
nr_of_docs = 9459
sum_of_topic_probs = 9396.40123459
</code></pre>
",2018-12-13 20:03:15,2019-03-23 23:30:44,Gensim LDA alpha-parameter,<gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
11415,42381902,2017-02-22 03:00:46,,"<p>E.g. we train a word2vec model using <code>gensim</code>:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models.word2vec import Word2Vec

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
w2v_model = Word2Vec(texts, size=500, window=5, min_count=1)
</code></pre>

<p>And when we query the similarity between words, we find negative similarity scores:</p>

<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')
0.046929569156789336
&gt;&gt;&gt; w2v_model.similarity('graph', 'system')
0.063683518562347399
&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')
-0.040026775040430063
&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')
-0.0072684112978664561
</code></pre>

<p><strong>How do we interpret the negative scores?</strong> </p>

<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>

<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>

<p>Looking at the Python wrapper code, there isn't much too: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>

<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>
",,2017-03-07 03:12:36,Interpreting negative Word2Vec similarity from gensim,<python><nlp><similarity><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11424,42389748,2017-02-22 11:16:52,,"<p>Okay, this is a specific question about what data structure is required when providing training data to the Gensim python library. In particular, there must be an implicit understanding of what constitutes a document in any data that it is provided (otherwise it wouldn't, for instance, be able to find the tf-idf).</p>

<p>For a specific example, the wikipedia dump is used in the tutorials for the library for training purposes. The wikipedia dump is provided in XML. What gives gensim an understanding of separate documents? Is this understanding predicated on the nesing of xml elements? </p>
",,2017-03-16 10:36:23,Gensim data parsing,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11430,42382207,2017-02-22 03:32:50,,"<p>The <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">Word2Vec</a> object in <code>gensim</code> has a <code>null_word</code> parameter that isn't explained in the docs. </p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)</p>
</blockquote>

<p><strong>What is the <code>null_word</code> parameter used for?</strong></p>

<p>Checking the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680</a>, it states:</p>

<pre><code>    if self.null_word:
        # create null pseudo-word for padding when using concatenative L1 (run-of-words)
        # this word is only ever input  never predicted  so count, huffman-point, etc doesn't matter
        word, v = '\0', Vocab(count=1, sample_int=0)
        v.index = len(self.wv.vocab)
        self.wv.index2word.append(word)
        self.wv.vocab[word] = v
</code></pre>

<p><strong>What is ""concatenative L1""?</strong></p>
",,2017-02-22 07:02:14,What is the `null_word` parameter in gensim Word2Vec?,<python><null><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11433,42529467,2017-03-01 10:38:55,,"<p>I am making SMS categorizer. For this I want to classify my messages into different topics. So I want to use gensim for that. 
Can anybody provide me the source of any tutorial that can help me to begin topic modelling using gensim?</p>
",,2017-03-04 19:00:35,Topic Modelling using gensim,<gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11437,42493143,2017-02-27 18:26:15,,"<p>I wrote the following code to build a Doc2vec Model iteratively. As I read in <a href=""https://groups.google.com/forum/#!topic/gensim/4xPzZiemH1k"" rel=""nofollow noreferrer"">this page</a>, if the number of tokens is more than 10000 in  a document then we need to split tokens and repeat the label(s) for each segment. </p>

<p>The length of tokens is more than 10000 for most of my documents.I try to split my tokens by writing the following code.But I got error which shows the tokens after 10000 is not considered in my model. </p>

<pre><code>    def iter_documents(top_directory):
        mapDocName_Id=[]
        label=1
        for root, dirs, files in os.walk(top_directory):
            for fname in files:
                print fname
                inputs=[]
                tokens=[]
                with open(os.path.join(root, fname)) as f:
                    for i, line in enumerate(f):          
                        if line.startswith('clueweb09-en00'):
                            if tokens:
                                i=0
                                if len(tokens)&lt;10000:
                                    yield LabeledSentence(tokens[:],[label])
                                else:
                                    tLen=len (tokens)
                                    times= int(math.floor(tLen/10000))
                                    for i in range(0,times):
                                        s=i*10000
                                        e=(i*10000)+9999
                                        yield LabeledSentence(tokens[s:e],[label])
                                    start=times*10000
                                    yield LabeledSentence(tokens[start:tLen],[label])
                                label+=1
                                tokens=[]
                        else:
                            tokens=tokens+line.split()
                    yield LabeledSentence(tokens[:],[label])
class docIterator(object):
    def __init__(self,top_directory):
       self.top_directory = top_directory

    def __iter__(self):
       return iter_documents(self.top_directory)

allDocs = docIterator(inputPath)

model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, workers = 4)
model.save('my_model.doc2vec')
</code></pre>

<p>I test my model with the following code then I got this error:</p>

<pre><code>model= Doc2Vec.load('my_model.doc2vec')

#print model['school']
print model['philadelphia']
</code></pre>

<p>I got a vector as result of school but I got this error for philadelphia. philadelphia is in tokens after index 10000. </p>

<pre><code>2017-02-27 13:59:36,751 : INFO : loading Doc2Vec object from /home/fl/Desktop/newInput/tokens/my_model.doc2vec

2017-02-27 13:59:36,765 : INFO : loading docvecs recursively from /home/fl/Desktop/newInput/tokens/my_model.doc2vec.docvecs.* with mmap=None

2017-02-27 13:59:36,765 : INFO : setting ignored attribute syn0norm to None

2017-02-27 13:59:36,765 : INFO : setting ignored attribute cum_table to None
Traceback (most recent call last): 
File ""/home/fl/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/CreateAnnoyIndex.py"",
line 31, in &lt;module&gt;
     print model['philadelphia']   File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/word2vec.py"",
 line 1504, in __getitem__  
return self.syn0[self.vocab[words].index] 
KeyError: 'philadelphia'
</code></pre>
",,2017-03-10 04:21:24,words dos not exist in a doc2vec model when a document is added iteratively to the model,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11456,42514986,2017-02-28 17:18:24,,"<p>I am trying to implement a Word2Vec CBOW with negative sampling with Keras, following the code found <a href=""https://github.com/abaheti95/Deep-Learning/blob/master/word2vec/keras/cbow_model.py"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>EMBEDDING_DIM = 100

sentences = SentencesIterator('test_file.txt')
v_gen = VocabGenerator(sentences=sentences, min_count=5, window_size=3,
                       sample_threshold=-1, negative=5)

v_gen.scan_vocab()
v_gen.filter_vocabulary()
reverse_vocab = v_gen.generate_inverse_vocabulary_lookup('test_lookup')

# Generate embedding matrix with all values between -1/2d, 1/2d
embedding = np.random.uniform(-1.0 / (2 * EMBEDDING_DIM),
                              1.0 / (2 * EMBEDDING_DIM),
                              (v_gen.vocab_size + 3, EMBEDDING_DIM))

# Creating CBOW model
# Model has 3 inputs
# Current word index, context words indexes and negative sampled word indexes
word_index = Input(shape=(1,))
context = Input(shape=(2*v_gen.window_size,))
negative_samples = Input(shape=(v_gen.negative,))

# All inputs are processed through a common embedding layer
shared_embedding_layer = (Embedding(input_dim=(v_gen.vocab_size + 3),
                                    output_dim=EMBEDDING_DIM,
                                    weights=[embedding]))

word_embedding = shared_embedding_layer(word_index)
context_embeddings = shared_embedding_layer(context)
negative_words_embedding = shared_embedding_layer(negative_samples)

# Now the context words are averaged to get the CBOW vector
cbow = Lambda(lambda x: K.mean(x, axis=1),
              output_shape=(EMBEDDING_DIM,))(context_embeddings)

# Context is multiplied (dot product) with current word and negative
# sampled words
word_context_product = merge([word_embedding, cbow], mode='dot')
negative_context_product = merge([negative_words_embedding, cbow],
                                 mode='dot',
                                 concat_axis=-1)

# The dot products are outputted
model = Model(input=[word_index, context, negative_samples],
              output=[word_context_product, negative_context_product])

# Binary crossentropy is applied on the output
model.compile(optimizer='rmsprop', loss='binary_crossentropy')
print(model.summary())

model.fit_generator(v_gen.pretraining_batch_generator(reverse_vocab),
                    samples_per_epoch=10,
                    nb_epoch=1)
</code></pre>

<p>However, I get an  error during the merge part because Embedding layer is a 3D tensor while cbow is only 2 dimensions. I assume I need to reshape the embedding (which is [?, 1, 100]) to [1, 100] but I can't find how to reshape with the functional API.
I am using the Tensorflow backend.</p>

<p>Also, if someone can point to an other implementation of CBOW with Keras (Gensim free), I would love to have a look to it!</p>

<p>Thank you!</p>

<p>EDIT: Here is the error</p>

<pre><code>Traceback (most recent call last):
  File ""cbow.py"", line 48, in &lt;module&gt;
    word_context_product = merge([word_embedding, cbow], mode='dot')
    .
    .
    .
ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,1,100], [?,100].
</code></pre>
",2017-03-02 09:14:10,2017-03-02 09:14:10,Product merge layers with Keras functionnal API for Word2Vec model,<python><nlp><keras><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
11477,42552733,2017-03-02 10:23:07,,"<p>I'm trying to load a model of that contains spanish words using gensim-1.0 in python3.5, but when I do <code>gensim.models.KeyedVectors.load_word2vec_format(mymodel)</code> the CLI says this:</p>

<pre><code>Traceback (most recent call last):
  File ""./prueba.py"", line 30, in &lt;module&gt;
    model = KeyedVectors.load_word2vec_format('./data/WikiModelEsp/wiki.size.800.window.5.mincount.50.new.model', binary=True)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py"", line 192, in load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/utils.py"", line 231, in any2unicode
    return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>I try to call load function with <code>encoding='latin1'</code> and <code>binary=True</code> but still doesn't work.</p>
",,2017-05-09 08:58:59,Issue loading a model of Spanish data,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11478,42554289,2017-03-02 11:31:07,,"<p>I want to use output embedding of word2vec such as in <a href=""http://www2016.net/proceedings/companion/p83.pdf"" rel=""noreferrer"">this paper (Improving document ranking with dual word embeddings)</a>.</p>

<p>I know input vectors are in syn0, output vectors are in syn1 and syn1neg if negative sampling.</p>

<p>But when I calculated most_similar with output vector, I got same result in some ranges because of removing syn1 or syn1neg.</p>

<p>Here is what I got.</p>

<pre><code>IN[1]: model = Word2Vec.load('test_model.model')

IN[2]: model.most_similar([model.syn1neg[0]])

OUT[2]: [('of', -0.04402521997690201),
('has', -0.16387106478214264),
('in', -0.16650712490081787),
('is', -0.18117375671863556),
('by', -0.2527652978897095),
('was', -0.254993200302124),
('from', -0.2659570872783661),
('the', -0.26878535747528076),
('on', -0.27521973848342896),
('his', -0.2930959463119507)]
</code></pre>

<p>but another syn1neg numpy vector is already similar output.</p>

<pre><code>IN[3]: model.most_similar([model.syn1neg[50]])

OUT[3]: [('of', -0.07884830236434937),
('has', -0.16942456364631653),
('the', -0.1771494299173355),
('his', -0.2043554037809372),
('is', -0.23265135288238525),
('in', -0.24725285172462463),
('by', -0.27772971987724304),
('was', -0.2979024648666382),
('time', -0.3547973036766052),
('he', -0.36455872654914856)]
</code></pre>

<p>I want to get output numpy arrays(negative or not) with preserved during training.</p>

<p>Let me know how can I access pure syn1 or syn1neg, or code, or some word2vec module can get output embedding.</p>
",,2017-03-06 21:58:03,How can I access output embedding(output vector) in gensim word2vec?,<python><numpy><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11480,42517435,2017-02-28 19:43:33,,"<p>I'm using gensim implementation of Word2Vec. I have the following code snippet:</p>

<pre><code>print('training model')
model = Word2Vec(Sentences(start, end))
print('trained model:', model)
print('vocab:', model.vocab.keys())
</code></pre>

<p>When I run this in python2, it runs as expected. The final print is all the words in the vocabulary.</p>

<p>However, if I run it in python3, I get an error:</p>

<pre><code>trained model: Word2Vec(vocab=102, size=100, alpha=0.025)
Traceback (most recent call last):
  File ""learn.py"", line 58, in &lt;module&gt;
    train(to_datetime('-4h'), to_datetime('now'), 'model.out')
  File ""learn.py"", line 23, in train
    print('vocab:', model.vocab.keys())
AttributeError: 'Word2Vec' object has no attribute 'vocab'
</code></pre>

<p>What is going on? Is gensim word2vec not compatible with python3?</p>
",,2017-03-01 20:42:51,Gensim word2vec in python3 missing vocab,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11485,42502605,2017-02-28 07:26:00,,"<p>I am new to Deep Learning and I want to explore Deep Learning for NLP. I went through word embeddings and tested them in gensim word2vec. I also heard about pre-trained models. I am confused about the difference between pre-trained models and training the model yourself, and how to use the results.</p>

<p>I want to apply it in keras because I do not want to write formulas and all in Theano or Tensorflow.</p>
",2017-02-28 11:40:32,2017-02-28 11:40:32,Difference between pre-trained word embedding and training word embedding in keras,<python><python-2.7><nlp><deep-learning>,,,CC BY-SA 3.0,False,False,True,False,False
11489,42539384,2017-03-01 18:34:51,,"<p>I could not load a doc2vec model on my computer and I got the following error. But, when I load that model on other computers, I can use that model.Therefore, I know the model was built correctly.</p>

<p>what should I do.</p>

<p>This is the code:</p>

<pre><code># coding: utf-8
from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence
import os
import pickle
pth='/home/fatemeh/Step2/input-output/model/iterator'
model= Doc2Vec.load(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<pre><code>    Traceback (most recent call last):
  File ""CreateAnnoyIndex.py"", line 16, in &lt;module&gt;
    model= Doc2Vec.load(pth+'/my_model.doc2vec')
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py"", line 1762, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 248, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 912, in unpickle
    return _pickle.loads(f.read())
EOFError
</code></pre>
",2017-03-01 19:35:23,2017-03-01 20:19:32,Got EOFError during loading doc2vec model,<python-2.7><pickle><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11497,42399565,2017-02-22 18:33:14,,"<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = gensim.models.word2vec.LineSentence('dati.txt')
model = gensim.models.Word2Vec(
sentences, size=300, window=5, min_count=5, workers=5,
sg=1, hs=1, negative=0
)
model.save_word2vec_format('model.bin', binary=True)
</code></pre>

<p>The last method, save_word2vec_format, gives me this error:</p>

<p><code>
AttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'
</code></p>

<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=""https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56"" rel=""nofollow noreferrer"">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>

<p>Thank you in advance!</p>
",,2018-12-26 20:37:42,Save gensim Word2vec model in binary format .bin with save_word2vec_format,<python><attributes><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11513,42459373,2017-02-25 17:38:06,,"<p>Loading the complete pre-trained word2vec model by <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""noreferrer"">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>

<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>

<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>
",,2018-02-16 05:43:22,Reduce Google's Word2Vec model with Gensim,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11542,42697499,2017-03-09 14:08:15,,"<p>I'm new to python. What I'm trying to do is to read 2 parameters via console. </p>

<ol>
<li>parameter:path to a trained LDA model with gensim. </li>
<li>parameter:the number of most common words per topic which I want to get in return.</li>
</ol>

<p>Now I want to print for all the topics the number of the most common words per topic. Now my question is how to get all the topics.</p>

<p>This is what I have so far:</p>

<pre><code>import sys, getopt
import gensim

def main(argv):
   input_file = argv[0] #LDA Path
   number_of_words = argv[1] #Number of most common word per topic

   ldamodel = gensim.models.ldamodel.LdaModel.load(input_file, mmap=None) #load model
   ldamodel.print_topic(?, number_of_words)



if __name__ == ""__main__"":
   main(sys.argv[1:])
</code></pre>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">Gensim doc</a></p>

<p>Thanks</p>
",,2017-03-09 14:12:06,Load the computed LDA models and print the most common words per topic,<python><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11575,42626287,2017-03-06 12:59:32,,"<p>I downloaded Wikipedia word vectors from <a href=""https://github.com/clips/dutchembeddings"" rel=""nofollow noreferrer"">here</a>. I loaded the vectors with:</p>

<pre><code>model_160 = KeyedVectors.load_word2vec_format(wiki_160_path, binary=False)
</code></pre>

<p>and then want to train them with:</p>

<pre><code>model_160.train()
</code></pre>

<p>I get the error back:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-22a9f6312119&gt; in &lt;module&gt;()
----&gt; 1 model.train()

AttributeError: 'KeyedVectors' object has no attribute 'train'
</code></pre>

<p>My question is now:
It seems like KeyedVectors has no train function, but I want to continue training the vectors on my personal sentences, instead of just using the Wikipedia vectors. How is this possible?</p>

<p>Thanks in advance, Jan</p>
",,2017-03-21 20:30:45,Gensim: KeyedVectors.train(),<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11617,42746007,2017-03-12 09:58:26,,"<p>I have tried to train incrementally  word2vec model produced by gensim. But I found that the vocabulary size doesn't increased , only the word2vec model weights are updated . But i need to  update both vocabulary and model size .</p>

<pre><code>#Load data 
sentences = []
....................

#Training 
model = Word2Vec(sentences, size=100)
model.save(""modelbygensim.txt"")
model.save_word2vec_format(""modelbygensim_text.txt"")



#Incremental Training 
model = Word2Vec.load('modelbygensim.txt')
model.train(sentences)
model.save(""modelbygensim_incremental.txt"")
model.save_word2vec_format(""modelbygensim_text_incremental.txt"")
</code></pre>
",,2017-03-16 03:30:54,Incremental Word2Vec Model Training in gensim,<python><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11628,42727181,2017-03-10 20:29:18,,"<p>I am trying to load the pretrained word vectors from Google using the following code:</p>

<pre><code>from gensim import models
w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>But I am getting an error that tells me </p>

<blockquote>
  <p>File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 197, in load_word2vec_format
      result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)</p>
  
  <p>ValueError: array is too big; <code>arr.size * arr.dtype.itemsize</code> is larger than the maximum possible size.</p>
</blockquote>

<p>Could anyone suggest a possible solution. Thanks in advance.</p>
",,2017-04-02 06:25:34,ValueError: array is too big when loading GoogleNews-vectors-negative,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11631,42673590,2017-03-08 14:11:29,,"<p>I  getting memory error, when I use <strong>GoogleNews-vectors-negative300.bin</strong> or try to train a model with Gensim with wikipedia dataset corpus.(1 GB). I have 4GB RAM in my system. Is there any way to bypass this.</p>

<p>Can we host it on cloud service like AWS to get better speed ?</p>
",2017-11-20 07:37:39,2019-01-14 10:21:20,Gensim - Memory error using GoogleNews-vector model,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11637,42781292,2017-03-14 08:43:26,,"<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>

<pre><code>import gensim, re
import pandas as pd

# TOKENIZER
def tokenizer(input_string):
    return re.findall(r""[\w']+"", input_string)

# IMPORT DATA
data = pd.read_csv('mp_1002_prepd.txt')
data.columns = ['merged']
data.loc[:, 'tokens'] = data.merged.apply(tokenizer)
sentences= []
for item_no, line in enumerate(data['tokens'].values.tolist()):
    sentences.append(LabeledSentence(line,[item_no]))

# MODEL PARAMETERS
dm = 1 # 1 for distributed memory(default); 0 for dbow 
cores = multiprocessing.cpu_count()
size = 300
context_window = 50
seed = 42
min_count = 1
alpha = 0.5
max_iter = 200

# BUILD MODEL
model = gensim.models.doc2vec.Doc2Vec(documents = sentences,
dm = dm,
alpha = alpha, # initial learning rate
seed = seed,
min_count = min_count, # ignore words with freq less than min_count
max_vocab_size = None, # 
window = context_window, # the number of words before and after to be used as context
size = size, # is the dimensionality of the feature vector
sample = 1e-4, # ?
negative = 5, # ?
workers = cores, # number of cores
iter = max_iter # number of iterations (epochs) over the corpus)

# QUERY BASED DOC RANKING ??
</code></pre>

<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>
",2018-06-25 05:41:30,2018-06-25 05:41:30,Doc2Vec Get most similar documents,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
11649,42752356,2017-03-12 19:55:27,,"<p>I would like to do some text analysis on job descriptions and was going to use nltk. I can build a dictionary and remove the stopwords, which is part of what I want. However in addition to the single words and their frequencies I would like to keep meaningful 'word groups' and count them as well. </p>

<p>For example in job descriptions containing 'machine learning' I don't want to consider 'machine' and 'learning' separately but keep retain the word group in my dictionary if it frequently occurs together. What is the most efficient method to do that? (I think I wont need to go beyond word groups containing 2 or words). And: At which point should I do the stopword removal?</p>

<p>Here is an example:</p>

<pre><code>    text = 'As a Data Scientist, you will focus on machine 
            learning and Natural Language Processing'
</code></pre>

<p>The dictionary I would like to have is:</p>

<pre><code>     dict = ['data scientist', 'machine learning', 'natural language processing', 
             'data', 'scientist', 'focus', 'machine', 'learning', 'natural' 
             'language', 'processing']
</code></pre>
",2017-03-12 20:09:01,2017-03-20 17:17:37,Create a dictionary with 'word groups',<python><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
11650,42753119,2017-03-12 21:03:02,,"<p>I am calling load likethis .</p>

<p>.7/dist-packages/gensim/utils.py"", line 912, in </p>

<pre><code>  model = gensim.models.Word2Vec.load(""F:\\TrialGrounds\\gensimMODEL4\\model4"")

model = super(Word2Vec, cls).load(*args, **kwargs)
 File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 248, in load
 obj = unpickle(fname)
 File ""/usr/local/lib/python2unpickle
 return _pickle.loads(f.read())
AttributeError: 'module' object has no attribute 'call_on_class_only'
</code></pre>

<p>The model has split 500mb *2 numpy arrays. Can anyone help me in figuring out this issue</p>
",,2017-03-16 03:18:20,Error loading Pretrained vectors on gensim 0.12,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11661,42789612,2017-03-14 15:07:04,,"<p>I have this:</p>

<pre><code>texts = ['human', 'machine', 'interface']
</code></pre>

<p>When I do this with Gensim:</p>

<pre><code>dictionary = corpora.Dictionary(texts)
</code></pre>

<p>It leads to unicode <code>u'</code>'s being added... How can I suppress this?</p>
",,2017-03-19 05:11:58,Gensim and unicode in Python,<python><unicode><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11669,42881749,2017-03-19 01:05:34,,"<p>I'm having an issue running multicored LDA in gensim (generating 2000 topics, 1 pass using 15 workers). I get the error below, I initially thought it might not have to do with saving the model, but looking at the error (the code still keeps running, at least the process hasn't quit).
Anyone know what I can do to prevent this error from occurring?</p>

<pre><code>python3 run.py --method MultiLDA --ldaparams 2000 1 --workers 15 --path $DATA/gender_spectrum/

Traceback (most recent call last):
   File ""/usr/lib64/python3.5/multiprocessing/queues.py"", line 241, in _feed
   obj = ForkingPickler.dumps(obj)
   File ""/usr/lib64/python3.5/multiprocessing/reduction.py"", line 50, in dumps
   cls(buf, protocol).dump(obj)
OverflowError: cannot serialize a bytes object larger than 4 GiB```
</code></pre>
",2017-03-19 23:41:56,2017-03-19 23:41:56,Gensim multicore LDA overflow error,<python-3.x><python-multiprocessing><multicore><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
11673,42827175,2017-03-16 06:54:27,,"<p>I'm kinda newbie and not native english so have some trouble understanding <code>Gensim</code>'s <code>word2vec</code> and <code>doc2vec</code>.</p>

<p>I think both give me some words most similar with query word I request, by <code>most_similar()</code>(after training).</p>

<p>How can tell which case I have to use <code>word2vec</code> or <code>doc2vec</code>?</p>

<p>Someone could explain difference in short word, please?</p>

<p>Thanks.</p>
",,2017-03-16 10:04:35,Gensim: What is difference between word2vec and doc2vec?,<nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11700,42851859,2017-03-17 07:41:18,,"<p>As I understand, if i'm training a LDA model over a corpus where the size of the dictionary is say 1000 and no of topics (K) = 10, for each word in the dictionary I should have a vector of size 10 where each position in the vector is the probability of that word belongs to that particular topic, right? </p>

<p>So my question is given a word, what is the probability of that word belongs to to topic k where k could be from 1 to 10, how do I get this value in the gensim lda model?</p>

<p>I was using <code>get_term_topics</code> method but it doesn't output all the probabilities for all the topics. For eg.,</p>

<pre><code>lda_model1.get_term_topics(""fun"")
[(12, 0.047421702085626238)],
</code></pre>

<p>but I want to see what is the prob that ""fun"" could be in all the other topics as well?</p>
",,2017-03-30 23:46:41,How to get the topic-word probabilities of a given word in gensim LDA?,<gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11718,42857000,2017-03-17 11:58:57,,"<p>I have 2 lists of sentences. First list contains different questions, second contains different statements.</p>

<p>Little example:</p>

<pre><code>1st list:
[
    ""What are cultures that are associated with core values?"",
    ""How do bumblebees fly?"",
    ""It is possible that avocado seed can be a tea?"",
    ...
]

2nd list:
[
    ""The population was 388 at the 2010 census."",
    ""Grevillea helmsiae is a tree which is endemic to Queensland in Australia."",
    ""He played youth football for Tynecastle Boys Club."",
    ...
]
</code></pre>

<p>I want to write program which will be able to classify this 2 types of sentences. For this, I can create neural network and train it on my 2 lists. I guess, this must be recurrent neural network.</p>

<p>I have transformed each sentence to array of word2vec vectors. And now I want to set up keras recurrent neural network with LSTM layers. But I don't know how to do that correctly. Can you write keras model for this problem?</p>

<p>UPDATE</p>

<p>the form of above sentences after transforming it by word2vec is like this:</p>

<pre><code>[
    [vector_of_floats_for_word_""what"", vector_of_floats_for_word_""are"", vector_of_floats_for_word_""cultures"", vector_of_floats_for_word_""that"", ...],
    [vector_of_floats_for_word_""how"", vector_of_floats_for_word_""do"", vector_of_floats_for_word_""bumblebees"", ...]
]
</code></pre>

<p>and so on. each vector has 300 dimensions.</p>

<p>here is my model:</p>

<pre><code>X = []
Y = []

for i in range(1000):
    X.append(questions_vectors[i])
    Y.append([1, 0])
    X.append(statements_vectors[i])
    Y.append([0, 1])

model = Sequential()
model.add(LSTM(128, input_shape=(2000, None, 300)))
model.add(Dense(2, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.01))
</code></pre>

<p>there you can see magic numbers 2000 and 300. 2000 is 1000 questions + 1000 statements, 300 - word vector length</p>

<p>but I'm sure that my model is wrong. also I'm getting the error:</p>

<pre><code>ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4
</code></pre>
",2017-03-17 12:32:42,2017-03-17 12:32:42,How to classify sentences using word2vec and keras?,<keras><recurrent-neural-network><gensim><word2vec><keras-layer>,,,CC BY-SA 3.0,False,False,True,False,False
11727,42928438,2017-03-21 13:23:13,,"<p>I know that the creation of LDA models is probabilistic, and that two models trained under the same parameters on the same corpus will not necessarily be identical. However, I'm wondering if the topic distribution of a document fed into an LDA model is also probabilistic. </p>

<p>I have an LDA model as presented here:</p>

<pre><code>lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=numTopics,passes=10)
</code></pre>

<p>as well as two documents, Doc1 and Doc2. I want to find the cosine similarity of the two documents in lda space, so that:</p>

<pre><code>x = cossim(lda[Doc1], lda[Doc2]).
</code></pre>

<p>The problem I'm noticing is that when I run this through multiple iterations, the cosine similarity is not always identical. (even when I use the same saved LDA model). The similarity is extremely similar, but it's always a bit off each time. In my actual code I have hundreds of documents, so I'm converting the topic distributions to a dense vector and using numpy to do the calculations in a matrix:</p>

<pre><code>documentsList = np.array(documentsList)
calcMatrix=1-cdist(documentsList, documentsList, metric=self.metric)
</code></pre>

<p>Am I running into a rounding error with numpy (or another bug in my code), or is this behavior I should expect when using lda to find the topic distribution of a document?</p>

<p>Edit: I'm going to run a simple cosine similarity on 2 different documents using my lda model, and plot the spread of results. I will report back with what I find.</p>

<p>Ok, here are the results of running cossine similarity against 2 documents, using the same LDA model. </p>

<p>Here is my code:</p>

<pre><code>def testSpacesTwoDocs(doc1, doc2, dictionary):
    simList = []
    lda = gensim.models.ldamodel.LdaModel.load('LDA_Models/lda_bow_behavior_allFields_t385_p10')
    for i in range(50):
        doc1bow = dictionary.doc2bow(doc1)
        doc2bow = dictionary.doc2bow(doc2)

        vec1 = lda[doc1bow]
        vec2 = lda[doc2bow]

        S = matutils.cossim(vec1, vec2)
        simList.append(S)


    for entry in simList:
        print entry

    sns.set_style(""darkgrid"")
    plt.plot(simList, 'bs--')
    plt.show()


    return
</code></pre>

<p>Here are my results:
    0.0082616863035,
    0.00828413767524,
    0.00826550453411,
    0.00816756826185,
    0.00829832701338,
    0.00828970584276,
    0.00828578705814,
    0.00817109902484,
    0.00817138247141,
    0.00825297374028,
    0.008269435921,
    0.00826470121538,
    0.00818282042634,
    0.00824660449673,
    0.00818087532906,
    0.0081770261766,
    0.00817128310123,
    0.00817643202588,
    0.00827404791376,
    0.00832439428054,
    0.00816643128216,
    0.00828540881955,
    0.00825746652101,
    0.00816793513824,
    0.00828471827526,
    0.00827161219003,
    0.00817773114553,
    0.00826166001503,
    0.00828048713541,
    0.00817435544365,
    0.0082956702812,
    0.00826167470288,
    0.00829873425476,
    0.00825744872634,
    0.00826802120149,
    0.00829604894909,
    0.0081776752236,
    0.00817613482849,
    0.00825839326441,
    0.00817530362838,
    0.0081747561999,
    0.0082597447174,
    0.00828958180101,
    0.00827157760835,
    0.00826939127657,
    0.00826138381094,
    0.00817755590806,
    0.00827135780051,
    0.00827314260067,
    0.00817035250043</p>

<p>Am I correct to assume that the LDA model is infering the topic distribution of both documents at each iteration, and thus that the cosine similarities are stochastic rather than determanistic? Is this much variation a sign that I'm not training my model long enough? Or am I not properly normalizing the vectors? Thanks</p>

<p>Thanks</p>
",2017-03-21 14:42:13,2017-12-18 10:06:07,Are Topic Distributions of Documents in LDA Space Probabilistic?,<python><numpy><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
11728,42928647,2017-03-21 13:31:51,,"<p>I downloaded the stackoverflow dump (which is a 10GB file) and ran word2vec on the dump in order to get vector representations for programming terms (I require it for a project that I'm doing). Following is the code:</p>

<pre><code>from gensim.models import Word2Vec
from xml.dom.minidom import parse, parseString

titles, bodies = [], []
xmldoc = parse('test.xml') //this is the dump 
reflist = xmldoc.getElementsByTagName('row')
for i in range(len(reflist)):
    bitref = reflist[i]
    if 'Title' in bitref.attributes.keys():
        title = bitref.attributes['Title'].value
        titles.append([i for i in title.split()])
    if 'Body' in bitref.attributes.keys():
        body = bitref.attributes['Body'].value
        bodies.append([i for i in body.split()])

dimension = 8
sentences = titles + bodies
model = Word2Vec(sentences, size=dimension, iter=100)
model.save('snippet_1.model')
</code></pre>

<p>Now, in order to calculate the cosine similarity between a pair of sentences, I do the following:</p>

<pre><code>from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

model = Word2Vec.load('snippet_1.model')
dimension = 8
snippet = 'some text'
snippet_vector = np.zeros((1, dimension))
for word in snippet:
    if word in model.wv.vocab:
        vecvalue = model[word].reshape(1, dimension)
        snippet_vector = np.add(snippet_vector, vecvalue)

link_text = 'some other text'
link_vector = np.zeros((1, dimension))
for word in link_text:
    if word in model.wv.vocab:
        vecvalue = model[word].reshape(1, dimension)
        link_vector = np.add(link_vector, vecvalue)

print(cosine_similarity(snippet_vector, link_vector))
</code></pre>

<p>I am calculating the sum of word embedding for each word of a sentence to get some representation for the sentence as a whole. I do this for both sentences and then calculate the cosine similarity between them.</p>

<p>Now, the problem is I'm getting cosine similarity around 0.99 for any pair of sentences that I give. Is there anything that I'm doing wrong? Any suggestions for a better approach?</p>
",,2017-03-28 10:27:12,Cosine similarity between any two sentences is giving 0.99 always,<word2vec><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,True
11729,42836992,2017-03-16 14:32:11,,"<p>I installed Python 3.5 using Anaconda and gensim 1.0.1 (supports Python 3) using pip. I got the following error when running gensim:</p>

<pre><code>Exception in thread Thread-61:
Traceback (most recent call last):
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 838, in job_producer
    sentence_length = self._raw_word_count([sentence])
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in _raw_word_count
    return sum(len(sentence) for sentence in job)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in &lt;genexpr&gt;
    return sum(len(sentence) for sentence in job)
TypeError: object of type 'map' has no len()
</code></pre>

<p>The code causing this error is from <a href=""https://github.com/aditya-grover/node2vec"" rel=""nofollow noreferrer"">node2vec</a>. I am porting it to Python 3 but got this error.</p>

<p>I know that in Python 3, len(map) causes error, does it mean Gensim 1.0.1 does not support Python 3 although <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">pip website</a> says it supports? Or are there some hidden settings?</p>

<p>Anyone has any idea what is wrong? Thanks.</p>
",2017-03-16 15:00:42,2017-08-01 17:42:32,Gensim 1.0.1 on Python 3.5 TypeError: object of type 'map' has no len()?,<python><python-3.x><anaconda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11732,42986405,2017-03-23 20:30:58,,"<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>
",2017-03-29 18:42:16,2019-10-17 10:36:30,How to speed up Gensim Word2vec model load time?,<deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11745,42913090,2017-03-20 20:12:19,,"<p>Would like to  obtain an identifiable list of most relevant documents (to LDA model) using gensim, i.e. exactly as the OP in the link below</p>

<p><a href=""https://groups.google.com/forum/#!topic/gensim/lHi2MhoNDsY"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/lHi2MhoNDsY</a> has the relevant answer, but not sure of how to get this to work:</p>

<pre><code>tops = sorted(zip(my_ids, all_documents)), reverse=True, \
key=lambda my_id, doc: abs(dict(doc).get(topic_number, 0.0))) 
</code></pre>

<p>Apart from a parenthesis that seems to be in the wrong place, the two main questions are:</p>

<ol>
<li>How should my_ids be generated (do these map to the documents or to the words)? Something like <code>my_ids = [my_id for my_id in range(len(all_documents))]</code>
throws </li>
</ol>

<p>lambda () missing 1 required positional argument: 'doc'</p>

<ol start=""2"">
<li>What is the reason for this error, and how should <code>key=lambda my_id, doc: abs(dict(doc).get(topic_number, 0.0)))</code> be understood? Would <code>abs(dict(doc).get(topic_number, 0.0))</code> be unpacked as needed if the correct <code>my_ids</code> is supplied, or is some other fix needed?
Also have had a look at <a href=""https://stackoverflow.com/questions/26543349/python-3-map-lambda-method-with-2-inputs"">python 3 map/lambda method with 2 inputs</a>, but still unenlightened so far.</li>
</ol>
",2017-05-23 11:46:21,2017-03-20 20:40:49,"Gensim/Python - mapping document ids to documents, in sorted()",<python><lambda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11768,43014259,2017-03-25 08:34:41,,"<p>I am working on a topic modelling script that uses the gensim package to create a model based on a collection of documents.</p>

<p>When preparing to visualise the model using the pyLDAvis package, I run into this error:</p>

<pre><code>import pyLDAvis
pyLDAvis.enable_notebook()

Traceback (most recent call last):
  File ""/dev2.py"", line 2, in &lt;module&gt;
    pyLDAvis.enable_notebook()
  File ""/Users/username/Library/Python/3.6/lib/python/site-packages/pyLDAvis/_display.py"", line 311, in enable_notebook
    formatter = ip.display_formatter.formatters['text/html']
AttributeError: 'NoneType' object has no attribute 'display_formatter'
</code></pre>
",,2017-03-25 18:10:52,'display_formatter' attribute error in Python,<python><error-handling><lda>,,,CC BY-SA 3.0,False,False,True,False,False
11776,42879491,2017-03-18 20:29:11,,"<p>I'm trying to solve a Deep Learning text classification problem, so I have to vectorize the text input with Word2Vec to feed it into a neural network.</p>

<p>So I downloaded a Google pre trained Word2Vec model: <a href=""https://github.com/3Top/word2vec-api"" rel=""nofollow noreferrer"">https://github.com/3Top/word2vec-api</a></p>

<p>And load it using gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('Word2Vec.bin', binary=True)
</code></pre>

<p>When I try to print a specific word:</p>

<pre><code>print(model['cat'])
# =&gt; expected output: 0.47385435 (or something)
# =&gt; actual output: array with hundreds of floats between -1 and 1
</code></pre>

<p>Why don't I just get one vector for one word? Isn't that the point?</p>

<p>Bonus Question: Can I load the 3M word vectors in the Google pre trained Word2Vec model into a MongoDB database? (Columns: id - word(string) - vector(float)). Because loading the model from a .bin or .txt file takes over a minute.</p>
",,2017-03-29 22:44:37,How do I get a single vector for a single word using Word2Vec?,<python><deep-learning><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11790,42995073,2017-03-24 08:52:07,,"<p>Gensim has a tutorial saying how to, given a document/query string, say what other documents are most similar to it, in descending order:</p>

<p><a href=""http://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">http://radimrehurek.com/gensim/tut3.html</a></p>

<p>It can also display what topics are associated with an entire model <em>at all</em>:</p>

<p><a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">How to print the LDA topics models from gensim? Python</a></p>

<p>But how do you find what <em>topics</em> are associated with <em>a given document/query string</em>? Ideally with some numeric similarity metric for each topic? I haven't been able to find anything on that.</p>
",2017-05-23 12:10:13,2017-06-08 00:41:39,Displaying topics associated with a document/query in Gensim,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11795,43051902,2017-03-27 16:37:53,,"<p>I'm trying create an algorithm that's capable of show the top n documents similar to a specific document.
For that i used the gensim doc2vec. The code is bellow:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size=400, window=8, min_count=5, workers = 11, 
dm=0,alpha = 0.025, min_alpha = 0.025, dbow_words = 1)

model.build_vocab(train_corpus)

for x in xrange(10):
    model.train(train_corpus)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
    model.train(train_corpus)

model.save('model_EN_BigTrain')

sims = model.docvecs.most_similar([408], topn=10)
</code></pre>

<p>The sims var should give me 10 tuples, being the first element the id of the doc and the second the score.
The problem is that some id's do not correspond to any document in my training data.</p>

<p>I've been trying for some time now to make sense out of the ids that aren't in my training data but i don't see any logic.</p>

<p>Ps: This is the code that i used to create my train_corpus</p>

<pre><code>def readData(train_corpus, jData):

print(""The response contains {0} properties"".format(len(jData)))
print(""\n"")
for i in xrange(len(jData)):
    print ""&gt; Reading offers from Aux array""
    if i % 10 == 0: 
        print ""&gt;&gt;"", i, ""offers processed...""

      train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(jData[i][1]), tags=[jData[i][0]]))
print ""&gt; Finished processing offers""
</code></pre>

<p>Being each position of the aux array one array in witch the position 0 is an int (that i want to be the id) and the position 1 a description</p>

<p>Thanks in advance.</p>
",,2017-12-12 02:50:49,Gensim docvecs.most_similar returns Id's that dont exist,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11808,43070656,2017-03-28 13:16:37,,"<p>I have a <code>Word2Vec</code> model which is trained in <code>Gensim</code>. How can I use it in <code>Tensorflow</code> for <code>Word Embeddings</code>. I don't want to train Embeddings from scratch in Tensorflow. Can someone tell me how to do it with some example code?</p>
",,2017-03-28 19:45:55,How to use pretrained Word2Vec model in Tensorflow,<python><tensorflow><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
11828,43019447,2017-03-25 17:15:18,,"<p>I have a list of sentences, and I follow the instructions at the <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">tutorial</a> to make a corpora from it:</p>

<pre><code>texts = [[word for word in document.lower().split() if word.isalpha()] for document in documents]
corpus = corpora.Dictionary(texts)
</code></pre>

<p>I want to train a LDA model on this corpora and extract the topics keywords.</p>

<pre><code>lda = models.LdaModel(corpus, num_topics=10)
</code></pre>

<p>However, I receive an error while training: <code>TypeError: 'int' object is not iterable</code>. What am I doing wrong? What the format of a corpus should be?</p>
",,2017-03-28 15:46:18,Gensim: Unable to train the LDA model,<nlp><gensim><lda><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
11849,42976912,2017-03-23 13:05:13,,"<p>After reading <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">this article</a>, I start to train my own model. The problem is that the author does not make it clear what the <code>sentences</code> in  <code>Word2Vec</code> should be like. </p>

<p>I download the text from a Wikipedia page, as it is written is the article, and I make a list of sentences from it:</p>

<pre><code>sentences = [word for word in wikipage.content.split('.')]
</code></pre>

<p>So, for example, <code>sentences[0]</code> looks like:</p>

<pre><code>'Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed'
</code></pre>

<p>Then I try to train a model with this list:</p>

<pre><code>model = Word2Vec(sentences, min_count=2, size=50, window=10,  workers=4)
</code></pre>

<p>But the dictionary of the model consists of letters! For example, the output of <code>model.wv.vocab.keys()</code> is:</p>

<pre><code>dict_keys([',', 'q', 'D', 'B', 'p', 't', 'o', '(', ')', '0', 'V', ':', 'j', 's', 'R', '{', 'g', '-', 'y', 'c', '9', 'I', '}', '1', 'M', ';', '`', '\n', 'i', 'r', 'a', 'm', '', 'v', 'N', 'h', '/', 'P', 'F', '8', '""', '', 'W', 'T', 'u', 'U', '?', ' ', 'n', '2', '=', 'w', 'C', 'O', '6', '&amp;', 'd', '4', 'S', 'J', 'E', 'b', 'L', '$', 'l', 'e', 'H', '', 'f', 'A', ""'"", 'x', '\\', 'K', 'G', '3', '%', 'k', 'z'])
</code></pre>

<p>What am I doing wrong? Thanks in advance!</p>
",2017-03-24 06:57:36,2017-03-24 06:57:36,How to train Word2Vec model on Wikipedia page using gensim?,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11867,43045295,2017-03-27 11:34:55,,"<p>I have a set of users and their content(1 document per user containing tweets of that user). I am planning to use a distributed vector representation of some size N for each user. One way is to take pre trained wordvectors on twitter data and average them to get distributed vector of an user. I am planning to use doc2vec for better results.But I am not quite sure if I understood the DM model given in <a href=""http://www.jmlr.org/proceedings/papers/v32/le14.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>.</p>

<p>I understand that we are assigning one vector per paragraph and while predicting next word we are using that and then backpropagating the error to update the paragraph vector as well as word vector. How to use this to predict paragraph vector of a new paragraph? </p>

<p>Edit : Any toy code for gensim to compute paragraph vector of new document would be appreciated.</p>
",,2017-08-27 02:33:12,How to get paragraph vector for a new paragraph?,<machine-learning><deep-learning><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
11875,43065843,2017-03-28 09:32:41,,"<p>I want to make word2vec with gensim. I heard that vocabulary corpus should be unicode so I converted it to unicode.</p>

<pre><code># -*- encoding:utf-8 -*-
# !/usr/bin/env python
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
from gensim.models import Word2Vec
import pprint

with open('parsed_data.txt', 'r') as f:
    corpus = map(unicode, f.read().split('\n'))

model = Word2Vec(size=128, window=5, min_count=5, workers=4)
model.build_vocab(corpus,keep_raw_vocab=False)
model.train(corpus)
model.save('w2v')

pprint.pprint(model.most_similar(u''))
</code></pre>

<p>Above is my source code. It seems like work well. However there are problem with vocabulary key. I want to make korean word2vec which use unicode. For example word <code></code> which means apology in english and it's unicode is <code>\xC0AC\xACFC</code> If I try to find <code></code> in word2vec, key error occur...<br>
Instead of <code>\xc0ac\xacfc</code> <code>\xc0ac</code> and <code>\xacfc</code> stores separately. 
What's the reason and how to solve it?</p>
",,2017-03-28 23:56:15,Python Gensim word2vec vocabulary key,<python><unicode><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11877,43098535,2017-03-29 15:57:20,,"<p>I have studied <code>word2vec</code> implementation in gensim, I am aware that input vectors are in <code>syn0</code>, output vectors are in <code>syn1</code> and <code>syn1neg</code> if negative sampling.</p>

<p>I know I can access similarity between input and output embeddings like this:</p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab
outv.index2word = model.wv.index2word  
outv.syn0 = model.syn1neg 
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p>My question is, if it is possible to save output embeddings (from <code>syn1</code> or <code>syn1neg</code> matrix) as final model. For example, when <code>model.save()</code>, so that it outputs output embeddings (or where exactly in the code of <code>word2vec.py</code> I could access and modify that). I need this in order to use these output embeddings as input to classifier. I have done it previously in brute-force approach, so I would like to access output embeddings easily.</p>
",2017-03-29 16:08:11,2017-03-30 02:52:55,Saving output (context) embeddings in word2vec (gensim implementation) as a final model,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
11885,43074949,2017-03-28 16:25:57,,"<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p>the code loads the google_news binary file to model. 
my question is, how the line 3 computes the output from a binary file ( Since Binary files contains 0's and 1's).</p>
",2017-03-28 19:54:16,2017-03-28 20:30:29,How word2vec retrieves result from binary files?,<neural-network><nlp><semantics><text-mining><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11887,43080291,2017-03-28 21:29:10,,"<p>Gensim's HDP model for topic modeling (gensim.models.hdpmodel.HdpModel) has a constructor that takes an argument called <code>max_chunks</code>.</p>

<p>On the documentation, it says <code>max_chunks</code> is the number of chunks the model will go over, and if that is larger than the number of chunks in supplied corpus, the training will wrap around the corpus.</p>

<p>Since I was warned by INFO logs that the likelihood function has been decreasing, I figure I may need multiple passes on corpus to converge.</p>

<p>LDA model provides with the <code>passes</code> argument the functionality to train on corpus for multiple iterations. I have difficulty figuring out how <code>max_chunks</code> in HDP maps to <code>passes</code> in LDA.</p>

<p>For example, let say my corpus has 1000000 documents. what <code>max_chunks</code> needs to be exactly in order to train, say, 3 passes on my corpus.</p>

<p>Any suggestion? Many many thanks</p>
",,2018-11-02 07:36:01,Gensim HDP topic model: How to train on multiple passes of corpus?,<nlp><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11904,43213046,2017-04-04 16:41:20,,"<p>Is it possible to use a gensim Random Projection to train a SVM in sklearn?<br>
I need to use <code>gensim</code>'s tfidf implementation because it's better at dealing with large inputs and then want to put that into a random projection on which I will train my SVM. I'd also be happy to just pass the tfidf model generated by <code>gensim</code> to <code>sklearn</code> and use their random projection, if that makes things easier.<br>
But so far I haven't found a way to get either model out of gensim into sklearn.  </p>

<p>I have tried using <code>gensim.matutils.corpus2csc</code>but of course that doesn't work: neither TfidfModel nor RpModel are corpi, so now I'm clueless at what to try next.</p>
",,2018-03-12 20:41:17,Use gensim Random Projection in sklearn SVM,<scikit-learn><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
11907,43158631,2017-04-01 15:25:04,,"<p>In a paper titled, ""<a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363760"" rel=""noreferrer"">Machine Learning at the Limit</a>,"" Canny, et. al. report substantial <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">word2vec</a> processing speed improvements. </p>

<p>I'm working with the <a href=""https://github.com/BIDData/BIDMach"" rel=""noreferrer"">BIDMach</a> library used in this paper, and cannot find any resource that explains how Word2Vec is implemented or how it should be used within this framework.</p>

<p>There are several scripts in the repo:</p>

<ul>
<li><a href=""https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.sh"" rel=""noreferrer"">getw2vdata.sh</a></li>
<li><a href=""https://github.com/BIDData/BIDMach/blob/master/scripts/getw2vdata.ssc"" rel=""noreferrer"">getwv2data.ssc</a></li>
</ul>

<p>I've tried running them (after building the referenced <code>tparse2.exe</code> file) with no success.</p>

<p>I've tried modifying them to get them to run but have nothing but errors come back.</p>

<p>I emailed the author, and posted <a href=""https://github.com/BIDData/BIDMach/issues/96"" rel=""noreferrer"">an issue on the github repo</a>, but have gotten nothing back. I only got somebody else having the same troubles, who says he got it to run but at much slower speeds than reported on newer GPU hardware.</p>

<p>I've searched all over trying to find anyone that has used this library to achieve these speeds with no luck. There are multiple references floating around that point to this library as the fastest implementation out there, and cite the numbers in the paper:</p>

<ul>
<li><a href=""https://pdfs.semanticscholar.org/cced/c38f68ffaf51cf8c31cd6c6b5c2cf033f91a.pdf"" rel=""noreferrer"">Intel research references the reported numbers without running the code on GPU (they cite numbers reported in the original paper)</a></li>
<li><a href=""https://www.reddit.com/r/MachineLearning/comments/4p3enc/advice_library_for_training_word2vec_on_gpu/"" rel=""noreferrer"">old reddit post pointing to BIDMach as the best</a> (but the OP says ""I haven't tested BIDMach myself yet"")</li>
<li><a href=""https://stackoverflow.com/questions/30573873/how-to-train-word2vec-on-very-large-datasets"">SO post citing BIDMach as the best</a> (OP doesn't actually run the library to make this claim...)</li>
<li>many more not worth listing citing BIDMach as the best/fastest without example or claims of ""I haven't tested myself...""</li>
</ul>

<p>When I search for a similiar library (gensim), and the <code>import</code> code required to run it, <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22from+gensim.models+import+Word2Vec%22&amp;*"" rel=""noreferrer"">I find thousands of results and tutorials</a> but a similar search for the BIDMach code <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22import+BIDMach.networks.Word2Vec%22&amp;*"" rel=""noreferrer"">yields only the BIDMach repo</a>.</p>

<p>This BIDMach implementation certainly carries the reputation for being the best, but <strong>can anyone out there tell me how to use it</strong>?</p>

<p>All I want to do is run a simple training process to compare it to a handful of other implementations on my own hardware.</p>

<p>Every other implementation of this concept I can find either has works with the <a href=""https://github.com/svn2github/word2vec/blob/master/demo-word.sh"" rel=""noreferrer"">original shell script test file</a>, <a href=""https://github.com/yindlib/cuda-word2vec"" rel=""noreferrer"">provides actual instructions</a>, or <a href=""https://github.com/IntelLabs/pWord2Vec/tree/master/sandbox"" rel=""noreferrer"">provides shell scripts of their own</a> to <a href=""https://github.com/facebookresearch/fastText/blob/master/word-vector-example.sh"" rel=""noreferrer"">test</a>.</p>

<hr>

<p>UPDATE:
The author of the library has added additional shell scripts to get the previously mentioned scripts running, but exactly what they mean or how they work is still a total mystery and I can't understand how to get the word2vec training procedure to run on my own data.</p>

<hr>

<p><strong>EDIT (for bounty)</strong></p>

<p>I'll give out the bounty to anywone that can explain how I'd use my own corpus (text8 would be great), and then train a model, and then save the ouput vectors and the vocabulary to files that can be read by <a href=""https://bitbucket.org/omerlevy/hyperwords"" rel=""noreferrer"">Omar Levy's Hyperwords</a>.</p>

<p>This is exactly what the original C implementation would do with arguments <code>-binary 1 -output vectors.bin -save-vocab vocab.txt</code></p>

<p>This is also what Intel's implementation does, and other CUDA implementations, etc, so this is a great way to generate something that can be easily compared with other versions...</p>

<hr>

<p><strong>UPDATE (bounty expired without answer)</strong>
John Canny has updated a few scripts in the repo and added a <code>fmt.txt</code> file, thus making it possible to run test scripts that are package in the repo.</p>

<p>However, my attempt to run this with the <strong>text8</strong> corpus yields near 0% accuracy on they hyperwords test.</p>

<p>Running the training process on the billion word benchmark (which is what the repo scripts now do) also yields well-below-average accuracy on the hyperwords test.</p>

<p>So, either the library never yielded accuracy on these tests, or I'm still missing something in my setup. </p>

<p><a href=""https://github.com/BIDData/BIDMach/issues/96"" rel=""noreferrer"">The issue remains open on github</a>.</p>
",2017-05-23 12:10:24,2017-04-11 18:52:27,Can anyone explain how to get BIDMach's Word2vec to work?,<machine-learning><nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11914,43045418,2017-03-27 11:40:25,,"<p>I used to python 3.5 and Based on gensim samples I created a project and added these codes in my project:</p>

<pre><code>    class MyCorpus(object):
    def __iter__(self):
        for line in open('files/2/mycorpus.txt'):
            # assume there's one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())


corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!
print(corpus_memory_friendly)
</code></pre>

<p>But after running I have these error in my pycharm console :</p>

<pre><code>    Traceback (most recent call last):
  File ""D:/Python-Workspace(s)/GensimSamples/2.Gensim_CorpusStreaming.py"", line 31, in &lt;module&gt;
    for vector in corpus_memory_friendly:  # load one vector into memory at a time
  File ""D:/Python-Workspace(s)/GensimSamples/2.Gensim_CorpusStreaming.py"", line 17, in __iter__
    yield dictionary.doc2bow(line.lower().split())
AttributeError: module 'gensim.corpora.dictionary' has no attribute 'doc2bow'
</code></pre>

<p>How can I solve this issue?</p>
",,2017-07-11 02:40:16,gensim Memory-friendly corpora error,<python-3.x><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
11918,43171573,2017-04-02 17:55:28,,"<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
",,2017-04-02 17:59:45,can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?,<python><nlp><semantics><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11932,43165724,2017-04-02 07:25:25,,"<p>I used <code>Gensim</code>'s <code>Word2Vec</code> for training most similar words.</p>

<p>My dataset is all posts from my college community site.</p>

<p>Each dataset consists of like this:</p>

<pre><code>(title) + (contents) + (all comments)  // String
</code></pre>

<p>For example, </p>

<pre><code>data[0] =&gt; ""This is title. Contents is funny. What so funny?. Not funny for me""
</code></pre>

<p>So, I have around 400,000 datas like above and make them as a vector and try to train these data via <code>Word2Vec</code>. </p>

<p>I wonder that whether it is possible to make <code>Word2Vec</code> consider WEIGHT, which means, if I give an weight to certain data vector, <code>Word2Vec</code> train this data in a way that each word in this data vector has more strong relationship(similarity).</p>

<p>For example, If I gave a weight 5 to dataset, <code>I like Pizza, Chicken</code>, the word <code>Pizza</code> and <code>Chicken</code> (or <code>like</code> and <code>Pizza</code> etc) has strong relations than other data vector's words.</p>

<p>Would that be possible?</p>

<p>Sorry for poor explanation but I'm not native english speaker. If need more detailed info, please post comment.</p>
",,2017-04-02 18:40:31,Word2Vec: Is it possible to train with respect to weight in NLP?,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11934,43166762,2017-04-02 09:29:02,,"<p>As I know of, <code>tsne</code> is reducing dimension of word vector. </p>

<p><code>Word2vec</code> is generate word embedding model with huge amount of data.</p>

<p>What is the relation between two?</p>

<p>Does <code>Word2vec</code> use <code>tsne</code> inside? </p>

<p>(I use <code>Word2vec</code> from <code>Gensim</code>)</p>
",,2017-04-02 19:27:03,What is relation between tsne and word2vec?,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11943,43146077,2017-03-31 17:01:18,,"<p>I trained a doc2vec (gensim.models.Doc2Vec) model and now I'm using this line:</p>

<pre><code>print(dict([(model.index2word[i], similarity) for i, similarity in enumerate(model.similar_by_word('igdumd32.dll@0x', topn=False))])['igdumd64.dll@0x'])
</code></pre>

<p>but it yields this error:
AttributeError: 'Doc2Vec' object has no attribute 'index2word'</p>

<p>I am using gensim 1.0.1</p>

<p>Can you help?</p>
",,2017-04-02 06:19:28,Index2word in Gensim's Doc2vec raises an Attribute error,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11944,43146420,2017-03-31 17:24:05,,"<p>I'm loading pretrained Doc2Vec model using:</p>

<pre><code>from gensim.models import Doc2Vec
model = Doc2Vec.load('/path/to/pretrained/model')
</code></pre>

<p>I'm getting the following error:</p>

<blockquote>
  <p>AttributeError: 'module' object has no attribute 'call_on_class_only'</p>
</blockquote>

<p>Does anyone know how to fix it. The model was trained with gensim 0.13.3 and I'm using  gensim 0.12.4. </p>
",2017-03-31 17:26:05,2017-03-31 22:22:24,Gensim: error while loading pretrained doc2vec model?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11955,43202548,2017-04-04 08:56:07,,"<p>Im <code>gensims</code> latest version, loading trained vectors from a file is done using <code>KeyedVectors</code>, and dosent requires instantiating a new Word2Vec object. But now my code is broken because I can't use the <code>model.vector_size</code> property. What is the alternative to that? I mean something better than just <code>kv[kv.index2word[0]].size</code>.</p>
",,2017-09-04 10:02:10,gensim KeydVectors dimensions,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
11963,43186733,2017-04-03 14:03:06,,"<p>I'm using the Phrases class and want to visualize the vectors in a 2D space. In order to do this with Word2Vec I've used T-SNE and it worked perfectly. When I'm trying to do the same with Phrases it doesn't make any sense (words appear next to irrelevant words). </p>

<p>Any suggestions on how to visualize the Phrases output?</p>
",,2017-04-12 18:20:55,Visualize Gensim's Phrases' vectors in 2D,<data-visualization><gensim><word2vec><phrases>,,,CC BY-SA 3.0,False,False,True,False,False
11969,43321492,2017-04-10 11:03:23,,"<p>I use <code>Gensim</code> <code>Word2Vec</code> to train word sets in my database.</p>

<p>I have about 400,000 phrase(Each phrase is short. Total 700MB) in my <code>PostgreSQL</code> database.</p>

<p>This is how I train these data using <code>Django ORM</code>: </p>

<pre><code>post_vector_list = []
for post in Post.objects.all():
    post_vector = my_tokenizer(post.category.name)
    post_vector.extend(my_tokenizer(post.title))
    post_vector.extend(my_tokenizer(post.contents))
    post_vector_list.append(post_vector)
word2vec_model = gensim.models.Word2Vec(post_vector_list, window=10, min_count=2, size=300) 
</code></pre>

<p>But this job getting a lot of time and feels like not efficient.</p>

<p>Especially, creating <code>post_vector_list</code> part took a lot of time and space..</p>

<p>I want to improve speed of training but have no idea how to do.</p>

<p>Want to get your advices. Thanks.</p>
",,2017-04-12 18:06:52,Word2Vec: Any way to train model fastly?,<orm><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
11995,43260074,2017-04-06 15:52:26,,"<p>I need to process the topics in the LDA output (lda.show_topics(num_topics=-1, num_words=100...) and then compare what I do with the pyLDAvis graph but the topic numbers are differently numbered. Is there a way I can match them? </p>
",,2017-10-19 04:22:28,Is there any way to match Gensim LDA output with topics in pyLDAvis graph?,<python-3.x><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12019,43343034,2017-04-11 10:13:58,,"<p>I am able to create the lda model and save it. Now I am trying load the model, and pass a new document</p>

<p><code>lda = LdaModel.load('..\\models\\lda_v0.1.model')
doc_lda = lda[new_doc_term_matrix]
print(doc_lda )</code></p>

<p>On printing the doc_lda I am getting the object. <code>&lt;gensim.interfaces.TransformedCorpus object at 0x000000F82E4BB630&gt;</code>
 However I want to get the topic words associated with it. What is the method I have to use. I was referring to <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">this</a>.</p>
",2017-04-12 06:41:49,2019-04-14 13:20:39,How to view and interpret the output of lda model using gensim,<python-3.x><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12027,43249717,2017-04-06 08:27:18,,"<p>I want to use word2vec with PySpark to process some data.
I was previously using Google trained model <code>GoogleNews-vectors-negative300.bin</code> with <code>gensim</code> in Python.</p>

<p>Is there a way I can load this bin file with <code>mllib.word2vec</code> ?
Or does it make sense to export the data as a dictionary from Python <code>{word : [vector]}</code> (or <code>.csv</code> file) and then load it in <code>PySpark</code>? </p>

<p>Thanks</p>
",2019-10-09 07:27:58,2019-10-09 07:27:58,Pyspark - Load trained model word2vec,<python><load><pyspark><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
12055,43317056,2017-04-10 07:21:48,,"<p>I am trying to visually depict my topics in python using pyldavis. However i am unable to view the graph. Is it that we have to view the graph in the browser or will it get popped upon execution. Below is my code</p>

<pre><code>import pyLDAvis
import pyLDAvis.gensim as gensimvis
print('Pyldavis ....')
vis_data = gensimvis.prepare(ldamodel, doc_term_matrix, dictionary)
pyLDAvis.display(vis_data)
</code></pre>

<p>The program is continuously in execution mode on executing the above commands. Where should I view my graph? Or where it will be stored? Is it integrated only with the Ipython notebook?Kindly guide me through this. 
P.S My python version is 3.5.</p>
",2017-04-10 09:16:42,2019-11-21 08:29:38,pyldavis Unable to view the graph,<python-3.x><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12068,43444261,2017-04-17 02:14:36,,"<p>I am trying to calculate the similarity of 2 texts using WMD. I have tried to use the following code in Python 3, using gensim:</p>

<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
word2vec_model.init_sims(replace=True) # normalizes vectors
distance = word2vec_model.wmdistance(""string 1"", ""string 2"")  # Compute WMD as normal.
</code></pre>

<p>However, I don't think this is returning me the right value. How should I do this in python?</p>
",2017-04-17 14:28:38,2019-10-30 04:23:51,Word Mover's Distance in Python,<python><python-3.x><text><nlp><information-retrieval>,,,CC BY-SA 3.0,False,False,True,False,False
12073,43337242,2017-04-11 05:13:18,,"<p>I want to use a word2vec module containing tons of Indian characters. The module was trained by Facebook - <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a> .
(I am using Gujarati vectors)</p>

<p>I installed gensim and tries to load the module, but following error occurred:</p>

<pre><code>In [1]: import gensim  

In [2]: from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('wiki.gu/wiki.gu.bin', binary=True,unicode_errors='ignore')

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 32: invalid start byte
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
",,2018-04-05 20:09:08,'utf8' decode error while loading a word2vec module,<python><utf-8><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12092,43393248,2017-04-13 13:09:52,,"<p>I'm trying to import a downloaded model from Google. I'm doing this using the following code:</p>

<pre><code>import gensim

model= gensim.models.KeyedVectors.load_word2vec_format('C://gensim/model/GoogleNews-vectors-negative300.bin.gz', binary=True)  
</code></pre>

<p>However, when run, I get this error:</p>

<pre><code>File ""C:\Users\Acer\AppData\Local\Programs\Python\Python36-32\lib\site packages\smart_open\smart_open_lib.py"", line 309, in __init__
raise NotImplementedError(""unknown URI scheme %r in %r"" % (self.scheme, uri))
NotImplementedError: unknown URI scheme 'c' in 'C://gensim/model/GoogleNews-vectors-negative300.bin.gz'
</code></pre>

<p>The file path is correct, and name for the model, however I cannot get it to import correctly. I have been using <a href=""http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">this</a> guide. </p>

<p>Any suggestions?</p>

<p>Thanks</p>
",2017-07-01 00:09:03,2017-07-01 00:09:03,Gensim error when importing Word2Vec model,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12100,43357247,2017-04-11 22:29:51,,"<p>The <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py"" rel=""noreferrer"">ldamodel</a> in gensim has the two methods: <code>get_document_topics</code> and <code>get_term_topics</code>. </p>

<p>Despite their use in this gensim tutorial <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb"" rel=""noreferrer"">notebook</a>, I do not fully understand how to interpret the output of <code>get_term_topics</code> and created the self-contained code below to show what I mean:</p>

<pre><code>from gensim import corpora, models

texts = [['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]

# build the corpus, dict and train the model
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, 
                                 random_state=0, chunksize=2, passes=10)

# show the topics
topics = model.show_topics()
for topic in topics:
    print topic
### (0, u'0.159*""system"" + 0.137*""user"" + 0.102*""response"" + 0.102*""time"" + 0.099*""eps"" + 0.090*""human"" + 0.090*""interface"" + 0.080*""computer"" + 0.052*""survey"" + 0.030*""minors""')
### (1, u'0.267*""graph"" + 0.216*""minors"" + 0.167*""survey"" + 0.163*""trees"" + 0.024*""time"" + 0.024*""response"" + 0.024*""eps"" + 0.023*""user"" + 0.023*""system"" + 0.023*""computer""')

# get_document_topics for a document with a single token 'user'
text = [""user""]
bow = dictionary.doc2bow(text)
print ""get_document_topics"", model.get_document_topics(bow)
### get_document_topics [(0, 0.74568415806946331), (1, 0.25431584193053675)]

# get_term_topics for the token user
print ""get_term_topics: "", model.get_term_topics(""user"", minimum_probability=0.000001)
### get_term_topics:  [(0, 0.1124525558321441), (1, 0.006876306738765027)]
</code></pre>

<p>For <code>get_document_topics</code>, the output makes sense. The two probabilities add up to 1.0, and the topic where <code>user</code> has a higher-probability (from <code>model.show_topics()</code>) has also the higher probability assigned.</p>

<p>But for <code>get_term_topics</code>, there are questions:</p>

<ol>
<li>The probabilities do not add up to 1.0, why?</li>
<li>While numerically, the topic where <code>user</code> has a higher-probability (from <code>model.show_topics()</code>) has also a higher number assigned, what does this number mean?</li>
<li>Why should we use <code>get_term_topics</code> at all, when <code>get_document_topics</code> can provide (seemingly) the same functionality and has meaningful output?</li>
</ol>
",2017-04-21 19:39:04,2017-07-18 18:48:52,get_document_topics and get_term_topics in gensim,<python><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12111,43396677,2017-04-13 15:49:36,,"<p>Executing this function row by row using a loop works. Executing the same function using pandas.DataFrame.apply returns ValueError: operands could not be broadcast together with shapes. Should the pandas.DataFrame.apply work? If it is one of those things that is not easily explainable, any ideas on how to speed up processing (other than multiprocessing)?</p>

<pre><code>#python 3.6
import pandas as pd # version 0.19.2  
import numpy as np  # 
#gensim version 1.0.1
from gensim import models #https://radimrehurek.com/gensim/models/word2vec.html

df=pd.DataFrame({""q1"":[['how', 'I', 'from', 'iPhone', 'keep', 'them', 'my', 'but', 'delete', 'iCloud', 'photos', 'in', 'can'],
                   ['use', 'are', 'radio', 'What', 'commercial', 'cognitive', 'technology', 'in'],
                   ['how', 'I', 'razor', 'prevent', 'burns', 'the', 'stomach', 'on', 'can']],
             ""q2"":[['Can', 'remove', 'from', 'I', 'iPhone', 'removing', 'them', 'my', 'storage', 'photos', 'iCloud', 'without'],
                  ['radio', 'from', 'Where', 'do', 'come', 'cognitive', 'distinction'],
                   ['how', 'I', 'razor', 'prevent', 'can', 'burn']]})

#using pretrained model https://code.google.com/archive/p/word2vec/
w2v = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) 

#This works
df['w2v_sim']=np.nan
for i in range(len(df)):       
df['w2v_sim'].ix[i]=w2v.n_similarity(df['q1'].ix[i],df['q2'].ix[i])
print(str(df['w2v_sim'].ix[i]))

#this doesn't work
df['w2v_sim']=np.nan
df['w2v_sim']=df.apply(w2v.n_similarity(df['q1'],df['q2']),axis=1)
</code></pre>

<p>ValueError: operands could not be broadcast together with shapes (13,300) (8,300) </p>

<p>Thank you</p>
",2017-04-13 18:20:11,2017-04-13 18:40:18,pandas.DataFrame.apply ValueError: operands could not be broadcast together with shapes,<python-3.x><pandas><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12138,43524301,2017-04-20 15:48:51,,"<p>I was wondering if it is possible to update spacys default vocabulary. What I am trying doing is this:</p>

<ul>
<li>run word2vec on my own corpus with gensim</li>
<li>load the vectors into my model with <code>nlp.vocab.load_vectors_from_bin_loc(\path)</code></li>
</ul>

<p>But since a lot of words in my corpus aren't in spacys default vocabulary I can't make use of the imported vectors. Is there an (easy) way to add those missing types?  </p>

<p><strong>Edit:</strong><br>
I realize it might be problematic to mix vectors. So my question is:<br>
How can I import a custom vocabulary into spacy?</p>
",2017-04-21 09:04:13,2017-05-08 04:27:02,Update spaCy Vocabulary,<python><word2vec><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
12142,43505696,2017-04-19 20:43:08,,"<p>I'm fitting a Hierarchical Dirichlet Process (HDP) topic model using the python gensim package on the 20newsgroups dataset, and I discover that my topics are not very informative (the top word probability is very small). </p>

<p>I'm using standard text pre-processing that includes tokenization, stop-words removal, and stemming. I was thinking that reducing dictionary size can help generate more meaningful topics. What are some of ways of reducing the dictionary size in gensim?</p>
",,2017-04-19 21:12:59,How to reduce dictionary size in gensim?,<python><dictionary><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12150,43543762,2017-04-21 13:16:57,,"<p>I'm using the Doc2Vec tags as an unique identifier for my documents, each document has a different tag and no semantic meaning. I'm using the tags to find specific documents so I can calculate the similarity between them. </p>

<p>Do the tags influence the results of my model? </p>

<p>In this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">tutorial</a> they talk about a parameter <code>train_lbls=false</code>, with this set to false there are no representations learned for the labels (tags). </p>

<p>That tutorial is somewhat dated and I guess the parameter does no longer exist, how does Doc2Vec handle tags? </p>
",2017-04-24 07:32:21,2017-05-16 05:30:47,Does Doc2Vec learn representations for the tags?,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12155,43476869,2017-04-18 15:53:34,,"<p>I have multiple documents that contain multiple sentences. I want to use <strong>doc2vec</strong> to cluster (e.g. k-means) the sentence vectors by using <strong>sklearn</strong>. </p>

<p>As such, the idea is that similar sentences are grouped together in several clusters. However, it is not clear to me if I have to train every single document separately and then use a clustering algorithm on the sentence vectors. Or, if I could infer a sentence vector from doc2vec without training every new sentence.</p>

<p>Right now this is a snippet of my code:</p>

<pre><code>sentenceLabeled = []
for sentenceID, sentence in enumerate(example_sentences):
    sentenceL = TaggedDocument(words=sentence.split(), tags = ['SENT_%s' %sentenceID])
    sentenceLabeled.append(sentenceL)

model = Doc2Vec(size=300, window=10, min_count=0, workers=11, alpha=0.025, 
min_alpha=0.025)
model.build_vocab(sentenceLabeled)
for epoch in range(20):
    model.train(sentenceLabeled)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay
textVect = model.docvecs.doctag_syn0

## K-means ##
num_clusters = 3
km = KMeans(n_clusters=num_clusters)
km.fit(textVect)
clusters = km.labels_.tolist()

## Print Sentence Clusters ##
cluster_info = {'sentence': example_sentences, 'cluster' : clusters}
sentenceDF = pd.DataFrame(cluster_info, index=[clusters], columns = ['sentence','cluster'])

for num in range(num_clusters):
     print()
     print(""Sentence cluster %d: "" %int(num+1), end='')
     print()
     for sentence in sentenceDF.ix[num]['sentence'].values.tolist():
        print(' %s ' %sentence, end='')
        print()
    print()
</code></pre>

<p>Basically, what I am doing right now is training on every labeled sentence in the document. However, if have the idea that this could be done in a simpler way.</p>

<p>Eventually, the sentences that contain similar words should be clustered together and be printed. At this point training every document separately, does not clearly reveal any logic within the clusters.</p>

<p>Hopefully someone can steer me in the right direction.
Thanks.</p>
",,2017-04-19 17:09:51,Doc2Vec Sentence Clustering,<python><scikit-learn><text-mining><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,True
12156,43546165,2017-04-21 15:08:58,,"<p>I've just installed Gensim and have run a test. 484 tests run successfully however I hit an error:</p>

<pre><code>======================================================================
ERROR: testAddMorphemesToEmbeddings (gensim.test.test_varembed_wrapper.TestVarembed)
Test add morphemes to Embeddings
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/franciskim/Downloads/gensim-2.0.0/gensim/test/test_varembed_wrapper.py"", line 56, in testAddMorphemesToEmbeddings
    vectors=varembed_model_vector_file, morfessor_model=varembed_model_morfessor_file)
  File ""/Users/franciskim/Downloads/gensim-2.0.0/gensim/models/wrappers/varembed.py"", line 69, in load_varembed_format
    morfessor_model = morfessor.MorfessorIO().read_binary_model_file(morfessor_model)
  File ""/Users/franciskim/anaconda/lib/python3.6/site-packages/morfessor/io.py"", line 179, in read_binary_model_file
    model = pickle.load(fobj)
AttributeError: Can't get attribute 'FixedCorpusWeight' on &lt;module 'morfessor.baseline' from '/Users/franciskim/anaconda/lib/python3.6/site-packages/morfessor/baseline.py'&gt;

----------------------------------------------------------------------
</code></pre>

<p>I can only find 1 other instance of this happening on the internet, but there are no answers to this yet.</p>
",,2017-04-21 15:08:58,Gensim - AttributeError: Can't get attribute 'FixedCorpusWeight',<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12185,43588290,2017-04-24 12:37:39,,"<p>When creating model,there is not any more model with extension finish </p>

<blockquote>
  <p>.syn1neg.npy </p>
  
  <p>syn0.npy</p>
</blockquote>

<p>My code is below:</p>

<pre><code>corpus= x+y
tok_corp= [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus]
model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)
model.save('/home/Desktop/test_model')

model = gensim.models.Word2Vec.load('/home/kafein/Desktop/chatbot/test_model')
</code></pre>

<p>There is only 1 model file</p>

<pre><code>test_model
</code></pre>

<p>Which part i am wrong ?</p>
",,2017-05-15 22:47:20,creating word2vec model syn1neg.npy extension,<python><python-3.x><deep-learning><word2vec><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
12209,43633092,2017-04-26 11:37:02,,"<p>I'm new to deeplearning4j, i want to make sentence classifier using words vector as input for the classifier. 
I was using python before, where the vector model was generated using gensim, and i want to use that model for this new classifier. 
Is it possible to use gensim's word2vec model in deeplearning4j.word2vec and how i can do that?</p>
",2019-02-04 21:31:28,2019-02-04 21:31:28,Is it possible to use gensim word2vec model in deeplearning4j.word2vec?,<java><gensim><word2vec><deeplearning4j>,,,CC BY-SA 4.0,False,False,True,False,False
12218,43500996,2017-04-19 16:14:54,,"<p>I want to train a word2vec model on the english wikipedia using python with gensim. I closely followed <a href=""https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw</a> for that.</p>

<p>It works for me but what I don't like about the resulting word2vec model is that named entities are split which makes the model unusable for my specific application. The model I need has to represent named entities as a single vector. </p>

<p>Thats why I planned to parse the wikipedia articles with spacy and merge entities like ""north carolina"" into ""north_carolina"", so that word2vec would represent them as a single vector. So far so good.</p>

<p>The spacy parsing has to be part of the preprocessing, which I originally did as recommended in the linked discussion using:</p>

<pre><code>...
wiki = WikiCorpus(wiki_bz2_file, dictionary={})
for text in wiki.get_texts():
    article = "" "".join(text) + ""\n""
    output.write(article)
...
</code></pre>

<p>This removes punctuation, stop words, numbers and capitalization and saves  each article in a separate line in the resulting output file. The problem is that spacy's NER doesn't really work on this preprocessed text, since I  guess it relies on punctuation and capitalization for NER (?).</p>

<p><strong>Does anyone know if I can ""disable"" gensim's preprocessing so that it doesn't remove punctuation etc. but still parses the wikipedia articles to text directly from the compressed wikipedia dump? Or does someone know a better way to accomplish this? Thanks in advance!</strong></p>
",,2019-10-07 14:14:32,Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus,<python><nlp><gensim><word2vec><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
12228,43551426,2017-04-21 20:23:15,,"<p>I have been developing a python script to classify if an article is related to a body text or not. For that I have been using ML (SVM classifier) with some features, including average of word embeddings. </p>

<p>The code for calculating the average of word embeddings between the list of articles and bodies is the following:</p>

<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
setm = set(word2vec_model.index2word)

def avg_feature_vector(words, model, num_features, index2word_set):
        #function to average all words vectors in a given paragraph 
        featureVec = np.zeros((num_features,), dtype=""float32"")
        nwords = 0
        for word in words:
            if word in index2word_set and word not in stop:
                try:
                    featureVec = np.add(featureVec, model[word])
                    nwords = nwords+1
                except:
                    pass
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

def doc_similatiry(headlines, bodies):
    X = []
    docs = []
    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        headline_avg_vector = avg_feature_vector(lemmatize_str(clean(headline)).split(), word2vec_model, 300, setm)
        body_avg_vector = avg_feature_vector(lemmatize_str(clean(body)).split(), word2vec_model, 300, setm)
        similarity =  1 - distance.cosine(headline_avg_vector, body_avg_vector)
        X.append(similarity)
    return X, docs
</code></pre>

<p>It seems like the average word2vec is being calculated correctly. However, it has worse scores than the TF-IDF cosine alone. Therefore, my idea was to group these 2 features, by that means, multiplying the TF-IDF score of each word to the word2vec.</p>

<p>Here is my code to do that:</p>

<pre><code>def avg_feature_vector(words, model, num_features, index2word_set, tfidf_vec, vec_repr, pos):
        #function to average all words vectors in a given paragraph (with tfidf feature)
        featureVec = np.zeros((num_features,), dtype=""float32"")
        nwords = 0

        for word in words:
            if word in index2word_set and word not in stop:
                try:
                    a = tfidf_vec.vocabulary_[word]
                    featureVec = np.add(featureVec, model[word]) * vec_repr[pos, a]
                    nwords = nwords+1
                except:
                    pass    
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

def doc_similatiry_with_tfidf(headlines, bodies):

    X = []
    docs = []
    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        docs.append(lemmatize_str(clean(headline)))
        docs.append(lemmatize_str(clean(body)))
    vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=True, stop_words=stop, sublinear_tf=True)
    sklearn_representation = vectorizer.fit_transform(docs)

    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):
        a = (clean(headline))
        headline_avg_vector = avg_feature_vector(nltk.word_tokenize(a), word2vec_model, 300, setm, vectorizer, sklearn_representation, 2*i)
        a = (clean(body))
        body_avg_vector = avg_feature_vector(nltk.word_tokenize(a), word2vec_model, 300, setm, vectorizer, sklearn_representation, 2*i+1)

        similarity =  1 - distance.cosine(headline_avg_vector, body_avg_vector)
        X.append(similarity)

    return X, docs
</code></pre>

<p>My problem is that this method is getting awful results and I don't know know if there's some logic that explains that (because in theory it should have better results) or if I'm doing something wrong in my code.</p>

<p>Can anyone help me figure this out? Also, I am open to new solutions to solve this problem.</p>

<p>Note: There are some functions used there which I didn't post the code since I thought they were not necessary. If there is something you don't understand I am here to explain it better.</p>
",,2017-04-21 20:23:15,Average of word embeddings with TF-IDF score,<python><machine-learning><nlp><tf-idf><word2vec>,,,CC BY-SA 3.0,True,False,True,False,True
12231,43554048,2017-04-22 00:55:21,,"<p>When I load doc2vec model from pkl file, I get this error.</p>

<pre><code>    ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-45-5ed9dc71f3a6&gt; in &lt;module&gt;()
----&gt; 1 temp_docky.infer_vector(['paypal'])

C:\Users\Laxmikant\Anaconda\lib\site-packages\gensim\models\doc2vec.pyc in infer_vector(self, doc_words, alpha, min_alpha, steps)
    750                 train_document_dm(self, doc_words, doctag_indexes, alpha, work, neu1,
    751                                   learn_words=False, learn_hidden=False,
--&gt; 752                                   doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
    753             alpha = ((alpha - min_alpha) / (steps - i)) + min_alpha
    754 

C:\Users\Laxmikant\Anaconda\lib\site-packages\gensim\models\doc2vec_inner.pyx in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5165)()
    406     # default vectors, locks from syn0/doctag_syn0
    407     if word_vectors is None:
--&gt; 408        word_vectors = model.wv.syn0
    409     _word_vectors = &lt;REAL_t *&gt;(np.PyArray_DATA(word_vectors))
    410     if doctag_vectors is None:

AttributeError: 'Doc2Vec' object has no attribute 'wv'
</code></pre>

<p>Can you please help with the error?</p>
",,2017-05-15 22:54:31,'Doc2Vec' object has no attribute 'wv',<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12233,43540857,2017-04-21 10:53:43,,"<p>Trying to load up a file in gensim with this line of code :    </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(r""C:/Users/dan/txt_sentoken/pos/cv000_29590.tx"", binary=False)
</code></pre>

<p>However, I am getting this error: </p>

<pre><code>ValueError: invalid literal for int() with base 10:'films'
</code></pre>

<p>Help how do I solve this error ?</p>
",,2017-05-08 04:23:54,Gensim - Trying to load a text file in gensim,<python-3.x><jupyter-notebook><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12242,43617836,2017-04-25 17:57:08,,"<p>I have trained a Latent Dirichlet Allocation (LDA) model on a corpus of documents using the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">gensim package</a> in python. </p>

<p>I am able to retrieve the following:</p>

<ol>
<li>Distribution of topics over <strong>one document</strong></li>
<li>Distribution of words in a topic</li>
</ol>

<p>However, how can we obtain the distribution over topics (probability of a topic) in the <strong>entire corpus</strong>? </p>

<p>For example, if we 3 topics can be get a distribution as follows for the entire corpus (not just one document): 
[topic 1: 0.5, topic 2: 0.3, topic 3: 0.2]</p>

<p>Any help would be appreciated. Thank You. </p>
",,2017-04-25 17:57:08,Distribution over topics for LDA using Gensim,<gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
12248,43647749,2017-04-27 02:40:34,,"<p>I have already trained a Word2Vec model with gensim.models.Word2Vec.
by which means can I acquire the frequency of each word in this model?</p>
",,2020-10-01 08:40:31,how to acquire word frequency from Word2Vec model,<python><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12252,43580272,2017-04-24 05:16:36,,"<p>I went through the word2vec tutorial and was able to train with given example data of Text8Corpus. <a href=""https://rare-technologies.com/word2vec-tutorial"" rel=""nofollow noreferrer"">Tutorial link</a> But when I tried to test on custom data, model is not training. The input are in unicode string format in python list. And min_count is also set to 1. Since it was not training in above input format, I tried to check type of input from the given tutorial, but I receive the format as this: class 'gensim.models.word2vec.Text8Corpus'. So not sure how to train my custom data of unicode string sentences in a list. Can anyone please guide me in right direction ?</p>

<blockquote>
  <p>Example of the input : [[u'SENTENCE_START', u'dont', u'let', u'him', u'treat', u'you', u'like', u'garbage', u'SENTENCE_END']] </p>
</blockquote>

<pre><code>sentences_clean = []
data = pandas.read_excel('from my folder/3_captions.xlsx', parse_cols = ""A"")
sentences = data.iloc[:, 0].tolist()

for line in sentences:
  line = re.sub(r""""""[""?,$!]|'(?!(?&lt;! ')[ts])"""""", """", line)
  line = re.sub(r""\."", """", line) 
  line = line.lower().replace(""'"", """")
  sentences_clean.append(line)

tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences_clean]

import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model_word2vec = gensim.models.Word2Vec(sentences_clean, min_count=1, size=300, workers=4)
model_word2vec.save('/Users/rajesh/Documents/word2vec')
print (model_word2vec.similarity('freudian','slip'))
</code></pre>

<p>that's all there is to train to word2vec I understand from tutorial and example ? But this below tutorial works.    </p>

<pre><code>sentences = word2vec.Text8Corpus('/Users/rajesh/Downloads/text8')
model_word2vec = gensim.models.Word2Vec(sentences, min_count=1, size=20)
print (model_word2vec.similarity('queen','woman'))
&gt;&gt;&gt; 0.659536897647
</code></pre>
",2017-04-24 06:05:32,2017-05-16 05:41:59,"Word2Vec model is not training, input sentences tried with both sequence of sentence and tokenized words in list",<python><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
12259,43598212,2017-04-24 21:43:49,,"<p>I'm looking for a way to dinamically add pre-trained word vectors to a word2vec gensim model.</p>

<p>I have a pre-trained word2vec model in a txt (words and their embedding) and I need to get Word Mover's Distance (for example via <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html"" rel=""noreferrer"" title=""gensim.models.Word2Vec.wmdistance"">gensim.models.Word2Vec.wmdistance</a>) between documents in a specific corpus and a new document. </p>

<p>To prevent the need to load the whole vocabulary, I would want to load only the subset of the pre-trained model's words that are found in the corpus. But if the new document has words that are not found in the corpus but they are in the original model vocabulary add them to the model so they are considered in the computation.</p>

<p>What I want is to save RAM, so possible things that would help me:</p>

<ul>
<li>Is there a way to add the word vectors directly to the model?</li>
<li>Is there a way to load to gensim from a matrix or another object? I could have that object in RAM and append to it the new words before loading them in the model</li>
<li>I don't need it to be on gensim, so if you know a different implementation for WMD that gets the vectors as input that would work (though I do need it in Python)</li>
</ul>

<p>Thanks in advance. </p>
",,2020-01-22 22:27:23,Add word embedding to word2vec gensim model,<python><nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12283,43668207,2017-04-27 21:28:16,,"<p>I am unable to understand how to print the output for the below code</p>

<pre><code># make gensim dictionary and corpus
dictionary = gensim.corpora.Dictionary(boc_texts)
corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>I want to print the keyphrases and their tfidf scores</p>

<p>Thank you</p>
",,2017-05-03 09:34:11,How to print gensim dictionary and corpus,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12291,43712401,2017-04-30 23:25:09,,"<p>I have those 2 functions which differs in only 1 line, so to avoid code duplication, I want to create a base class with a general form of those functions then inherit it for each class.</p>

<p>function 1:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>function 2:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>You can see the only difference is at</p>

<pre><code>        doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
        doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
</code></pre>
",2018-02-17 06:20:13,2018-02-17 06:20:13,How to refactor repeated code,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12300,43656999,2017-04-27 11:52:14,,"<p>I have unstructured data of about 150k documents. I am trying to group these documents using unsupervised learning algorithm. Currently I am using LDA (Latent Dirichlet allocation) in gensim Python. For LDAModel I have passed num_topics=20. Hence my whole 150k data is falling into 20 topics. </p>

<p>Now that I have these groups, I have 2 questions:</p>

<ol>
<li>How should I assign new documents to these topics? </li>
</ol>

<p>The approach I am taking is:
Calculate the sum of the word scores of the document per topic and assign the document to the topic with the highest score. However this is not giving me good results. </p>

<p>Is there any better way to get this?</p>

<ol start=""2"">
<li>How do I assign the main keywords that denote the topic? </li>
</ol>
",2017-05-02 06:18:57,2017-09-13 12:49:51,LDA - Assigning keywords to topics,<python-3.x><cluster-analysis><gensim><lda><unsupervised-learning>,,,CC BY-SA 3.0,False,False,True,False,False
12311,43772128,2017-05-04 00:51:45,,"<p>I have an LDA model trained through Mallet in Java. Three files are generated from the Mallet LDA model, which allow me to run the model from files and infer the topic distribution of a new text.</p>

<p>Now I would like to implement a Python tool which is able to infer a topic distribution given a new text, based on the trained LDA model. I do not want to re-trained the LDA model in Python. Therefore, I wonder if it is possible to load the trained Mallet LDA model into Gensim or any other python LDA package. If so, how can I do it?</p>

<p>Thanks for any answers or comments.</p>
",,2019-04-02 18:10:44,Use Gensim or other python LDA packages to use trained LDA model from Mallet,<gensim><lda><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
12338,43776572,2017-05-04 07:31:22,,"<p>I have trained a doc2vec and corresponding word2vec on my own corpus using gensim. I want to visualise the word2vec using t-sne with the words. As in, each dot in the figure has the ""word"" also with it.</p>

<p>I looked at a similar question here : <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">t-sne on word2vec</a></p>

<p>Following it, I have this code : </p>

<p>import gensim
import gensim.models as g</p>

<pre><code>from sklearn.manifold import TSNE
import re
import matplotlib.pyplot as plt

modelPath=""/Users/tarun/Desktop/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin""
model = g.Doc2Vec.load(modelPath)

X = model[model.wv.vocab]
print len(X)
print X[0]
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X[:1000,:])

plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.show()
</code></pre>

<p>This gives a figure with dots but no words. That is I don't know which dot is representative of which word. How can I display the word with the dot?</p>
",2017-05-23 12:26:07,2020-09-05 20:07:36,Visualise word2vec generated from gensim,<scikit-learn><data-visualization><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
12353,43740256,2017-05-02 14:19:50,,"<p>i keep getting this error. how to solve it ?</p>
<blockquote>
<p>tok_corp= [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus]</p>
<p>AttributeError: 'float' object has no attribute 'decode'</p>
</blockquote>
<p>my code :</p>
<pre><code>import os
import pandas as pd
import nltk
import gensim
from gensim import corpora, models, similarities
import sys
reload(sys)
sys.setdefaultencoding('utf8')

os.chdir(&quot;C:/Users/Lubi.Lubi-GF/Desktop/final&quot;);
df=pd.read_csv('xx.csv');



#x=df['sentiment'].values.tolist()
x=df['TITLE'].values.tolist()
#y=df['ID'].values.tolist()
corpus= x 
tok_corp= [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus]
model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)
</code></pre>
",2020-06-20 09:12:55,2017-05-02 14:19:50,'float' object has no attribute 'decode' nltk tokenize csv file on python2.7,<python-2.7><csv><utf-8><nltk><tokenize>,,,CC BY-SA 3.0,True,False,True,False,False
12375,43855348,2017-05-08 19:01:26,,"<p>I'm getting the following error when training a doc2vec model in a Jupyter notebook on OS X. The error is reproducible (although the specific thread in which it occurs changes) for my current dataset, although I have successfully trained models on other datasets. </p>

<pre><code>Exception in thread Thread-82:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 428, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5455)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
Exception in thread Thread-77:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 458, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5963)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
</code></pre>
",,2017-05-15 22:36:58,RuntimeError: release unlocked lock while training doc2vec,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12404,43815130,2017-05-05 23:39:21,,"<p>The exercise from intro to deep learning <a href=""https://github.com/yala/introdeeplearning/blob/master/Lab1.ipynb"" rel=""nofollow noreferrer"">this assignment</a>. It uses bag-of-words to represent a tweet.<br> How to <code>use word embeddings</code> to achieve the same?<br>I played around the <code>word2vec</code> tool, I came across following questions: <br><br>
 (i) How to <code>obtain pre-trained embeddings</code> to represent these tweets? (To use word2vec directly instead of training these tweets for embedding vectors.)How to use word2vec to use such pre-trained model?<br><br>
 (ii) How to train a tensorflow <code>2 hidden layer</code> architecture once we obtain embeddings from word2vec (i.e. dimensions will change due to <code>embedding_size</code>) or (continuation of previous bow model what will be additional changes due to embeddings)<br>
Previously it was: </p>

<pre><code>input dimension : (None, vocab_size)
Layer-1: (input_data * weights_1) + biases_1 
Layer-2: (layer_1 * weights_2) + biases_2 
output layer: (layer_2 * n_classes) + n_classes
output dimension: (None, n_classes)
</code></pre>

<p>(iii) Is it necessary to obtain embeddings for given data of tweets by training word2vec from scratch? How to train data of around 14k tweets using word2vec (not gensim or GloVe)? Will word2vec preprocess <code>@</code> as stopping word?</p>
",,2017-05-05 23:39:21,Understanding word embeddings when converting from bag of words for tweets,<tensorflow><sentiment-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
12412,43896195,2017-05-10 14:59:56,,"<p>I have trained an LDA algorithm on a corpus , and what I'd like to do is getting for each sentence the topic on which it corresponds, in order, to make a comparison between what the algorithm finds and the labels I have.</p>

<p>I have tried with the code below, but the results are quite bad I find a great deal of topic 17 (maybe 25% of the volume, it should be closer to 5%)</p>

<p>Thanks for your help</p>

<pre><code># text lemmatized: list of string lemmatized
dico = Dictionary(texts_lemmatized)
corpus_lda = [dico.doc2bow(text) for text in texts_lemmatized]

lda_ = LdaModel(corpus_lda, num_topics=18)

df_ = pd.DataFrame([])
data = []

# theme_commentaire = label of the string
for i in range(0, len(theme_commentaire)):
     # lda_.get_document_topics() gives the distribution of all topic for a specific sentence
     algo = max(lda_.get_document_topics(corpus_lda[i]))[0]
     human = theme_commentaire[i]
     data.append([str(algo), human])

cols = ['algo', 'human']
df_ = pd.DataFrame(data, columns=cols)
df_.head()
</code></pre>
",,2017-05-11 07:37:11,Gensim find topics in sentences,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12415,43878332,2017-05-09 19:26:59,,"<p>I have saved a Gensim dictionary to disk. When I load it, the <code>id2token</code> attribute dict is not populated.</p>

<p>A simple piece of the code that saves the dictionary:</p>

<pre><code>dictionary = corpora.Dictionary(tag_docs)
dictionary.save(""tag_dictionary_lda.pkl"")
</code></pre>

<p>Now when I load it (I'm loading it in an jupyter notebook), it still works fine for mapping tokens to IDs, but <code>id2token</code> does not work (I cannot map IDs to tokens) and in fact <code>id2token</code> is not populated at all.</p>

<pre><code>&gt; dictionary = corpora.Dictionary.load(""../data/tag_dictionary_lda.pkl"")
&gt; dictionary.token2id[""love""]
Out: 1613

&gt; dictionary.doc2bow([""love""])
Out: [(1613, 1)]

&gt; dictionary.id2token[1613]
Out: 
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input&gt; in &lt;module&gt;()
----&gt; 1 dictionary.id2token[1613]

KeyError: 1613

&gt; list(dictionary.id2token.keys())
Out: []
</code></pre>

<p>Any thoughts? </p>
",,2017-05-11 13:36:18,Gensim saved dictionary has no id2token,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12423,43918566,2017-05-11 14:37:58,,"<p>I'm trying to mimick the n_gram parameter in CountVectorizer() with gensim. My goal is to be able to use LDA with Scikit or Gensim and to find very similar bigrams. </p>

<p>For example, we can find the following bigrams with scikit: ""abc computer"", ""binary unordered"" and with gensim ""A survey"", ""Graph minors""...</p>

<p>I have attached my code below to make a comparison between Gensim and Scikit in terms of bigrams/unigrams.</p>

<p>Thanks for your help</p>

<pre><code>documents = [[""Human"" ,""machine"" ,""interface"" ,""for"" ,""lab"", ""abc"" ,""computer"" ,""applications""],
      [""A"", ""survey"", ""of"", ""user"", ""opinion"", ""of"", ""computer"", ""system"", ""response"", ""time""],
      [""The"", ""EPS"", ""user"", ""interface"", ""management"", ""system""],
      [""System"", ""and"", ""human"", ""system"", ""engineering"", ""testing"", ""of"", ""EPS""],
      [""Relation"", ""of"", ""user"", ""perceived"", ""response"", ""time"", ""to"", ""error"", ""measurement""],
      [""The"", ""generation"", ""of"", ""random"", ""binary"", ""unordered"", ""trees""],
      [""The"", ""intersection"", ""graph"", ""of"", ""paths"", ""in"", ""trees""],
      [""Graph"", ""minors"", ""IV"", ""Widths"", ""of"", ""trees"", ""and"", ""well"", ""quasi"", ""ordering""],
      [""Graph"", ""minors"", ""A"", ""survey""]]
</code></pre>

<p>With the gensim model we find 48 unique tokens, we can print the unigram/bigrams with print(dictionary.token2id)</p>

<pre><code># 1. Gensim
from gensim.models import Phrases

# Add bigrams and trigrams to docs (only ones that appear 20 times or more).
bigram = Phrases(documents, min_count=1)
for idx in range(len(documents)):
    for token in bigram[documents[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            documents[idx].append(token)

documents = [[doc.replace(""_"", "" "") for doc in docs] for docs in documents]
print(documents)

dictionary = corpora.Dictionary(documents)
print(dictionary.token2id)
</code></pre>

<p>And with the scikit 96 unique tokens, we can print scikit's vocabulary with print(vocab)</p>

<pre><code># 2. Scikit
import re
token_pattern = re.compile(r""\b\w\w+\b"", re.U)

def custom_tokenizer( s, min_term_length = 1 ):
    """"""
    Tokenizer to split text based on any whitespace, keeping only terms of at least a certain length which start with an alphabetic character.
    """"""
    return [x.lower() for x in token_pattern.findall(s) if (len(x) &gt;= min_term_length and x[0].isalpha() ) ]

from sklearn.feature_extraction.text import CountVectorizer

def preprocess(docs, min_df = 1, min_term_length = 1, ngram_range = (1,1), tokenizer=custom_tokenizer ):
    """"""
    Preprocess a list containing text documents stored as strings.
    doc : list de string (pas tokeniz)
    """"""
    # Build the Vector Space Model, apply TF-IDF and normalize lines to unit length all in one call
    vec = CountVectorizer(lowercase=True,
                      strip_accents=""unicode"",
                      tokenizer=tokenizer,
                      min_df = min_df,
                      ngram_range = ngram_range,
                      stop_words = None
                     ) 
    X = vec.fit_transform(docs)
    vocab = vec.get_feature_names()

    return (X,vocab)

docs_join = list()

for i in documents:
    docs_join.append(' '.join(i))

(X, vocab) = preprocess(docs_join, ngram_range = (1,2))

print(vocab)
</code></pre>
",,2017-05-11 16:25:48,Trying to mimick Scikit ngram with gensim,<python><scikit-learn><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
12434,43881924,2017-05-10 00:49:37,,"<p>I had trained a paragraph vector model from gensim by using a considerable amount text data. I did the next test: I verified the index of any sentence and then inferred a vector for it</p>

<pre><code>&gt;&gt;&gt; x=m.docvecs[18638]
&gt;&gt;&gt; g=m.infer_vector(""The seven OxyR target sequences analyzed previously and two new sites grxA at position 207 in GenBank entry M13449 and a second Mu phage mom site at position 59 in GenBank entry V01463 were used to generate an individual information weight matrix"".split())
</code></pre>

<p>When I computed the cosine similarity, it was very low (the opposite is expected). </p>

<pre><code>&gt;&gt;&gt; 1 - spatial.distance.cosine(g, x)
0.20437437837633066
</code></pre>

<p>Can someone tell me if I'm doing something wrong, please?</p>

<p>Thanks</p>
",2018-11-28 10:43:05,2018-11-28 10:43:05,inconsistent similarity betwen inferred and trained vectors in doc2vec,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12447,43868822,2017-05-09 11:39:10,,"<p>In order to use the Latent semantic indexation method from gensim, I want to begin with a small ""classique"" example like :</p>

<pre><code>import logging, gensim, bz2
id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400)
etc..
</code></pre>

<p>My question is : How to get the corpus iterator 'wiki_en_tfidf.mm' ? Must I download it from somewhere ? I have searched on the Internet but I did not find anything. Help please ? </p>
",,2017-05-09 13:48:09,Latent Semantic Indexation with gensim,<gensim><wikidata><latent-semantic-indexing><bz2><latent-semantic-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
12453,43922531,2017-05-11 17:55:58,,"<p>In order to make it clear, I would like to get your feedback whether the following code/gensim-usage is right or not? </p>

<p>Thank you in advance for your valuable time. </p>

<pre><code>import gensim    

train = [""John likes to watch movies Mary likes movies too"" ,
         ""John also likes to watch football games"" ]

test = [""Football is my dream""]

train_texts = [[word for word in document.lower().split()] for document in train]
test_texts = [[word for word in document.lower().split()] for document in test]

dictionary =gensim.corpora.Dictionary(train_texts)

train_corpus = [dictionary.doc2bow(text) for text in train_texts]
test_corpus = [dictionary.doc2bow(text) for text in test_texts]

ldaModel = gensim.models.LdaModel(corpus=train_corpus , 
             id2word=dictionary , num_topics=2)
bound_perplex = ldaModel.bound(test_corpus)
</code></pre>
",,2017-05-14 06:10:59,id2word_token2Id usage confusion in Gensim,<python><python-2.7><python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12459,43904029,2017-05-10 23:09:25,,"<p>I want to analyze the vectors looking for patterns and stuff, and use SVM on them to complete a classification task between class A and B, the task should be supervised. (I know it may sound odd but it's our homework.) so as a result I really need to know:</p>

<p>1- how to extract the coded vectors of a document using a trained model?</p>

<p>2- how to interpret them and how does word2vec code them?</p>

<p>I'm using gensim's word2vec.</p>
",2017-05-11 03:25:59,2017-05-15 10:16:43,How extract vocabulary vectors from gensim's word2vec?,<python><machine-learning><gensim><word2vec><text-classification>,,,CC BY-SA 3.0,False,False,True,False,False
12460,43942790,2017-05-12 16:48:23,,"<p>I am executing the following line:</p>

<pre><code>id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
</code></pre>

<p>This code is available at <strong>""<a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a>""</strong>. I downloaded the wikipedia corpus and generated the required files and wiki_en_wordids.txt is one of those files. This file is available in the following location:</p>

<pre><code>~/gensim/results/wiki_en
</code></pre>

<p>So when i execute the code mentioned above I get the following error:</p>

<pre><code>Traceback (most recent call last):
          File ""~\Python\Python36-32\temp.py"", line 5, in &lt;module&gt;
            id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
          File ""~\Python\Python36-32\lib\site-packages\gensim\corpora\dictionary.py"", line 344, in load_from_text
            with utils.smart_open(fname) as f:
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 129, in smart_open
            return file_smart_open(parsed_uri.uri_path, mode)
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 613, in file_smart_open
            return open(fname, mode)
          FileNotFoundError: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'
</code></pre>

<p>Even though the file is available in the required location I get that error. Should I place the file in any other location? How do I determine what the right location is?</p>
",,2017-05-14 05:59:53,gensim file not found error,<python><python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12470,43850721,2017-05-08 14:49:35,,"<p>I have a file that I thought was a Gensim Dictionary file, but apparently it is not one.  I created it by merging dictionaries: <code>baseddict.merge_with(addddict)</code>. </p>

<p>However, now when I try to call doc2bow on it like this:</p>

<pre><code>class MyCorpus(object):    
    def __init__(self,arrayxx,dictionaryx):
        self.arr = arrayxx
        self.diction = dictionaryx

    def __iter__(self):
        for each in self.arr:
            yield self.diction.doc2bow(each)
</code></pre>

<p>I get an error:</p>

<blockquote>
  <p>File ""/home/rnczf01/Desktop/Files/Patent_sim/newpython/parse_xml_patentsall_embedded_pftaps_allfiles.py"",
  line 58, in <strong>iter</strong>
      yield self.diction.doc2bow(each) AttributeError: 'VocabTransform' object has no attribute 'doc2bow'</p>
</blockquote>
",,2017-05-08 14:49:35,Calling a merged dictionary in Gensim,<nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12485,44000997,2017-05-16 12:06:17,,"<p>I am trying to run gensim WMD similarity faster. Typically, this is what is in the docs:
Example corpus:</p>

<pre><code>    my_corpus = [""Human machine interface for lab abc computer applications"",
&gt;&gt;&gt;              ""A survey of user opinion of computer system response time"",
&gt;&gt;&gt;              ""The EPS user interface management system"",
&gt;&gt;&gt;              ""System and human system engineering testing of EPS"",
&gt;&gt;&gt;              ""Relation of user perceived response time to error measurement"",
&gt;&gt;&gt;              ""The generation of random binary unordered trees"",
&gt;&gt;&gt;              ""The intersection graph of paths in trees"",
&gt;&gt;&gt;              ""Graph minors IV Widths of trees and well quasi ordering"",
&gt;&gt;&gt;              ""Graph minors A survey""]

my_query = 'Human and artificial intelligence software programs'
my_tokenized_query =['human','artificial','intelligence','software','programs']

model = a trained word2Vec model on about 100,000 documents similar to my_corpus.
model = Word2Vec.load(word2vec_model)
</code></pre>

<hr>

<pre><code>from gensim import Word2Vec
from gensim.similarities import WmdSimilarity

def init_instance(my_corpus,model,num_best):
    instance = WmdSimilarity(my_corpus, model,num_best = 1)
    return instance
instance[my_tokenized_query]
</code></pre>

<p>the best matched document is <code>""Human machine interface for lab abc computer applications""</code> which is great. </p>

<p>However the function <code>instance</code> above takes an extremely long time. So I thought of breaking up the corpus into <code>N</code> parts and then doing <code>WMD</code> on each with <code>num_best = 1</code>, then at the end of it, the part with the max score will be the most similar. </p>

<pre><code>    from multiprocessing import Process, Queue ,Manager

    def main( my_query,global_jobs,process_tmp):
        process_query = gensim.utils.simple_preprocess(my_query)

        def worker(num,process_query,return_dict):  
            instance=init_instance\
(my_corpus[num*chunk+1:num*chunk+chunk], model,1)
            x = instance[process_query][0][0]
            y = instance[process_query][0][1]
            return_dict[x] = y
        manager = Manager()
        return_dict = manager.dict()

        for num in range(num_workers):
            process_tmp = Process(target=worker, args=(num,process_query,return_dict))
            global_jobs.append(process_tmp)
            process_tmp.start()
        for proc in global_jobs:
            proc.join()

        return_dict = dict(return_dict)
        ind = max(return_dict.iteritems(), key=operator.itemgetter(1))[0]
        print corpus[ind]
        &gt;&gt;&gt; ""Graph minors A survey""
</code></pre>

<p>The problem I have with this is that, even though it outputs something, it doesn't give me a good similar query from my corpus even though it gets the max similarity of all the parts. </p>

<p>Am I doing something wrong?</p>
",2017-05-19 18:22:16,2017-06-24 12:46:43,Python Gensim how to make WMD similarity run faster with multiprocessing,<python><multithreading><multiprocessing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12500,43985180,2017-05-15 17:07:37,,"<p>Loading the wiki-fasttext model with the gensim library takes <strong>six</strong> minutes. </p>

<p>I'm aware of ways to cache the model but I'm looking for ways to speedup the initial model loading. The specific api is below:</p>

<pre><code>en_model = KeyedVectors.load_word2vec_format(os.path.join(root_dir, model_file))
</code></pre>

<p>Granted, wiki-fasttext a very large model, however I have load the same model in many languages.  </p>
",,2017-05-16 12:36:24,Is there a way to load the wiki-fasttext model faster with load_word2vec_format,<nlp><stanford-nlp><gensim><fasttext>,,,CC BY-SA 3.0,False,False,True,True,False
12521,44022180,2017-05-17 10:23:45,,"<p>I am trying to load a binary file using <code>gensim.Word2Vec.load(fname)</code> but I get the error:</p>

<blockquote>
  <p>File ""file.py"", line 24, in 
      model = gensim.models.Word2Vec.load('ammendment_vectors.model.bin')   </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1396, in load
      model = super(Word2Vec, cls).load(*args, **kwargs) </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 271, in load
      obj = unpickle(fname)  </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 933, in unpickle
      return _pickle.load(f, encoding='latin1')</p>
  
  <p>_pickle.UnpicklingError: could not find MARK</p>
</blockquote>

<p>I googled but I am unable to figure out why this error is coming up. Please let me know if any other information is required.</p>
",2017-05-17 11:27:13,2017-05-18 01:56:16,Unpickling Error while using Word2Vec.load(),<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12524,44026369,2017-05-17 13:31:21,,"<p>I wanted to parallelize the word2vec execution with gensim. Well, let me describe what are the steps I have done:</p>

<ol>
<li>Have installed c compiler using minGW and set the Env variable path</li>
<li>Then I downloaded the .tar.gz file of gensim-2.1.0 and installed gensim through CMD with command ""python setup.py install""</li>
<li>Gensim got installed without any error</li>
<li>My Python version, Gensim FAST_VERSION result is as below:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/5nk70.jpg"" rel=""nofollow noreferrer"">Python, Gensim, FAST_VERSION screenshot</a></p>

<p>N.B: I am using the versions: Windows 7, Python version 2.7.13, gensim version 2.1.0, scipy version 0.18.1</p>
",2017-05-17 19:38:52,2017-05-24 11:27:10,Couldn't parallelize my execution of word2vec using gensim,<python><cython><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12526,44060759,2017-05-19 03:08:01,,"<p>I used the gensim LDAModel for topic extraction for customer reviews as follows:</p>

<pre><code>dictionary = corpora.Dictionary(clean_reviews)
dictionary.filter_extremes(keep_n=11000) #change filters
dictionary.compactify()
dictionary_path = ""dictionary.dict""
corpora.Dictionary.save(dictionary, dictionary_path)

# convert tokenized documents to vectors

corpus = [dictionary.doc2bow(doc) for doc in clean_reviews]
vocab = lda.datasets.load_reuters_vocab()  

# Training lda using number of topics set = 10 (which can be changed)

lda = gensim.models.LdaModel(corpus, id2word = dictionary,
                        num_topics = 20,
                        passes = 20,
                        random_state=1,
                        alpha = ""auto"")
</code></pre>

<p>This returns unigrams in topics like:</p>

<pre><code>topic1 -delivery,parcel,location

topic2 -app, login, access
</code></pre>

<p>But I am looking for ngrams. I came across sklearn's LatentDirichletAllocation which uses Tfidf vectorizer as follows:</p>

<pre><code>vectorizer = TfidfVectorizer(analyzer='word', ngram_range=[2,5], stop_words='english', min_df=2)    
X = vectorizer.fit_transform(new_review_list)
clf = decomposition.LatentDirichletAllocation(n_topics=20, random_state=3, doc_topic_prior = .1).fit(X)
</code></pre>

<p>where we can specify range for ngrams in the vectorizer. Is it possible to do so in the gensim LDA Model as well.</p>

<p>Sorry, I'm very new to using all these models, so don't know much about them.</p>
",2017-05-19 05:05:27,2018-10-30 16:08:39,How to implement Latent Dirichlet Allocation to give bigrams/trigrams in topics instead of unigrams,<python><scikit-learn><nlp><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,True
12535,44005974,2017-05-16 15:44:16,,"<p>Suppose I filter in a list the word that i want to use in my next word2vec model load. How can I construct my own KeyedVectors that contain only these filtered words list?</p>

<p>I tried to make:</p>

<pre><code>w2v_model_keyed = w2v_model.wv
w2v_model_keyed.drop(word)
</code></pre>

<p>for a given word but i get the following error:</p>

<pre><code> AttributeError: 'KeyedVectors' object has no attribute 'drop'
</code></pre>

<p>Thank you</p>
",,2017-05-16 18:39:57,How to speed up Gensim Word2vec model by filtering out some words?,<word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12547,44045881,2017-05-18 10:57:39,,"<p>I'm trying to load the pre-trained words2vecs which I've found here (<a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a>)
I used the following command:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('word2vec.bin.gz', binary=False)
</code></pre>

<h2>And it throws this error:</h2>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/deeplearning/anaconda3/lib/python3.6/site-
packages/gensim/models/keyedvectors.py"", line 193, in 
 load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 374, 
in readline
    return self._buffer.readline(size)
  File ""/home/deeplearning/anaconda3/lib/python3.6/_compression.py"", 
line 68, in readinto
    data = self.read(len(byte_view))
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 463, 
in read
    if not self._read_gzip_header():
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 411, 
in _read_gzip_header
    raise OSError('Not a gzipped file (%r)' % magic)
OSError: Not a gzipped file (b've')
</code></pre>
",2017-05-18 18:26:04,2020-09-26 12:29:49,Failed to load a .bin.gz pre trained words2vecx,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12558,44050492,2017-05-18 14:23:39,,"<p>I'm new to gensim, I was reading about <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Experiments on the English Wikipedia</a> and from what I understand, it creates a model with topics and words and tries to relate them.</p>

<p>On my company, we have a list of phrases that we cluster manually after filtering them with a script that uses the Damerau-Levenshtein distance formula (actually, this data is on Elasticsearch and we use the fuzzyness search and the score to understand if the matching should be considered).</p>

<p>Example:</p>

<p><code>PHP Developer</code> is in the cluster <code>Developer</code>.</p>

<p>Let's say there is <code>Java Developer</code>, this too should be clustered as <code>Developer</code>.</p>

<p>The fuzzy search of Elasticsearch matches <code>Java Developer</code> to be similar to <code>PHP Developer</code> (Elasticsearch uses the Damerau-Levenshtein distance formula) so the script considers to put the same clusters of <code>PHP Developer</code> that are already validated (this validation is done manually).</p>

<p>My question is: can this gensim be useful to cluster words using Wikipedia's database as a ""dictionary""? </p>

<p>I also find this <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md?utm_campaign=buffer&amp;utm_content=buffer0df9b&amp;utm_medium=social&amp;utm_source=linkedin.com"" rel=""nofollow noreferrer"">pre-trained vectors</a> done by Facebook, I don't know if I can use this for my problem.</p>

<p>I tried to load one of the <code>.txt</code> files with this Python script:</p>

<pre><code>import gensim

sentences = gensim.models.KeyedVectors.load_word2vec_format('/Users/genesisxyz/Downloads/wiki.it.vec')
print(sentences)

p = sentences.similarity('uomo', 'donna')
print(p)
</code></pre>

<p>This was just a first experiment I was doing, but I still don't know where to start, I did a little of neural networks on other topics not related to words semantics, but here I have no clue.</p>

<p>Thanks in advance!</p>
",2017-05-19 07:50:36,2017-05-24 11:17:36,Word clustering with gensim,<python><neural-network><cluster-analysis><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
12564,44143441,2017-05-23 19:26:36,,"<p>I am using the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> code from the word2vec HTTP server running at <a href=""https://rare-technologies.com/word2vec-tutorial/#bonus_app"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/#bonus_app</a>. I changed the loaded file to a file of vectors trained with the original C version of word2vec. I load the file with </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)
</code></pre>

<p>and it seems to load without problems. But when I test the HTTP service with, let's say </p>

<pre><code>curl 'http://127.0.0.1/most_similar?positive%5B%5D=woman&amp;positive%5B%5D=king&amp;negative%5B%5D=man' 
</code></pre>

<p>I got an empty result with only the execution time.</p>

<pre><code>{""taken"": 0.0003361701965332031, ""similars"": [], ""success"": 1}
</code></pre>

<p>I put a <code>traceback.print_exc()</code> on the except part of the related method, which is in this case <code>def most_similar(self, *args, **kwargs):</code> and I got: </p>

<pre><code>Traceback (most recent call last):
  File ""./w2v_server.py"", line 114, in most_similar
    topn=5)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 304, in most_similar
    self.init_sims()
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 817, in init_sims
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>

<p>Any idea on why this might happens? </p>

<p>Note: I use python 2.7 and I installed gensim using pip, which gave me gensim 2.1.0.</p>
",,2017-05-23 23:21:57,Code for gensim Word2vec as an HTTP service 'KeyedVectors' Attribute error,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12566,44101714,2017-05-21 20:56:02,,"<p>I've been trying to determine the similarity between a set of documents, and one of the methods I'm using is the cosine similarity with the results of the TF-IDF.</p>

<p>I tried to use both sklearn and gensim's implementations, which give me similar results, but my own implementation results in a different matrix.</p>

<p>After analyzing, I noticed that the their implementations are different from the ones I've studied and came across:</p>

<p>Sklearn and gensim use raw counts as the TF, and apply L2 norm
on the resulting vectors.</p>

<p>On the other side, the implementations I found will normalize the term count,
like</p>

<pre><code>TF = term count / sum of all term counts in the document
</code></pre>

<p>My question is, what is the difference with their implementations? Do they give better results in the end, for clustering or other purposes?</p>

<p>EDIT(So the question is clearer):
What is the difference between normalizing the end result vs normalizing the term count at the beggining?</p>
",2017-05-21 21:12:53,2017-05-23 14:26:36,Sklearn and gensim's TF-IDF implementation,<scikit-learn><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,True
12572,44090503,2017-05-20 20:26:35,,"<p>that might actually be a dumb question but I just can't figure out why my script with gensim.models.word2vec is not working. Here is the thing, I'm using the stanford sentiment analysis databank dataset (~11000 reviews), and i'm trying to build word2vec using gensim, this is my script: </p>

<pre><code>import gensim as gs 
import sys 

# open the datas
sentences = gs.models.word2vec.LineSentence('../processedWords.txt')
print(""size in RAM of the sentences: {}"".format(sys.getsizeof(sentences)))

# transform them
# bigram_transformer = gs.models.Phrases(sentences)

model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5)
model.save('firstModel')
print(model.similarity('film', 'test'))
print(model.similarity('film', 'movie'))
</code></pre>

<p>Now, my problem is that the script runs in 2s, and gives only huge similarity between every pair of words. In addition, some words which are in the sentences are not in the built vocabulary. </p>

<p>I must be doing something obviously wrong, but can't figure what. </p>

<p>Thank you for your help. </p>
",2017-05-23 10:28:57,2017-06-09 13:22:44,Gensim Word2Vec: poor training performance.,<python-3.x><dataset><text-mining><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12584,43970727,2017-05-15 02:22:16,,"<p>I am working on the LDA topic model in python which gives output of of the following topics:</p>

<pre><code>(0, u'0.559*""delivery"" + 0.124*""area"" + 0.018*""mile"" + 0.016*""option"" + 0.012*""partner"" + 0.011*""traffic"" + 0.011*""hub"" + 0.011*""thanks"" + 0.010*""city"" + 0.009*""way""')
(1, u'0.397*""package"" + 0.073*""address"" + 0.055*""time"" + 0.047*""customer"" + 0.045*""apartment"" + 0.037*""delivery"" + 0.031*""number"" + 0.026*""item"" + 0.021*""support"" + 0.018*""door""')
(2, u'0.190*""time"" + 0.127*""order"" + 0.113*""minute"" + 0.075*""pickup"" + 0.074*""restaurant"" + 0.031*""food"" + 0.027*""support"" + 0.027*""delivery"" + 0.026*""pick"" + 0.018*""min""')
(3, u'0.072*""code"" + 0.067*""gps"" + 0.053*""map"" + 0.050*""street"" + 0.047*""building"" + 0.043*""address"" + 0.042*""navigation"" + 0.039*""access"" + 0.035*""point"" + 0.028*""gate""')
(4, u'0.434*""hour"" + 0.068*""time"" + 0.034*""min"" + 0.032*""amount"" + 0.024*""pay"" + 0.019*""gas"" + 0.018*""road"" + 0.017*""today"" + 0.016*""traffic"" + 0.014*""load""')
(5, u'0.245*""route"" + 0.154*""warehouse"" + 0.043*""minute"" + 0.039*""need"" + 0.039*""today"" + 0.026*""box"" + 0.025*""facility"" + 0.025*""bag"" + 0.022*""end"" + 0.020*""manager""')
(6, u'0.371*""location"" + 0.110*""pick"" + 0.097*""system"" + 0.040*""im"" + 0.038*""employee"" + 0.022*""evening"" + 0.018*""issue"" + 0.015*""request"" + 0.014*""while"" + 0.013*""delivers""')
(7, u'0.182*""schedule"" + 0.181*""please"" + 0.059*""morning"" + 0.050*""application"" + 0.040*""payment"" + 0.026*""change"" + 0.025*""advance"" + 0.025*""slot"" + 0.020*""date"" + 0.020*""tomorrow""')
(8, u'0.138*""stop"" + 0.110*""work"" + 0.062*""name"" + 0.055*""account"" + 0.046*""home"" + 0.043*""guy"" + 0.030*""address"" + 0.026*""city"" + 0.025*""everything"" + 0.025*""feature""') 
</code></pre>

<p>Is there a way that the model automatically creates human readable topic names (instead of the topic numbers) for the above topic list based of the features/words in each topic? I don't want to create topic names manually for each of the 0-9 topics.</p>

<p>I'm creating the LDA model as follows:</p>

<pre><code>import gensim
import json
import pandas as pd
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
import re
from nltk.tokenize import RegexpTokenizer
import pyLDAvis.gensim as gensimvis
import pyLDAvis
from gensim import corpora
from gensim import corpora, models, similarities 
import xlrd
from collections import OrderedDict
import simplejson as json

wb = xlrd.open_workbook(""excel.xlsx"")
sh = wb.sheet_by_index(0)

feedback_list = []

for rownum in range(1, sh.nrows):
    feedback = OrderedDict()
    row_values = sh.row_values(rownum)
    feedback['timestamp'] = row_values[0]
    feedback['text'] = row_values[1]
    feedback['header'] = row_values[2]
    feedback['transporter'] = row_values[3]
    feedback['device-type'] = row_values[4]
    feedback['app-version'] = row_values[5]
    feedback['locale'] = row_values[6]
    feedback['company-type'] = row_values[7]
    feedback['detected-language'] = row_values[8]  

    feedback_list.append(feedback)

j = json.dumps({'feedback': feedback_list})

with open('data.json', 'w') as f:
    f.write(j)  

data_file = ""data.json""  
pd.read_json(data_file, typ = ""series"")


with open(data_file, ""rb"") as f:
    data = f.readlines()

# Reading the data to a dataframe

data_json_str = ""[""+','.join(data) + ""]""
data_df = pd.read_json(data_json_str)

num_reviews_tpadv = len(data_df[""feedback""][0])
all_reviews = []

# Adding all the reviews to all_reviews list 

for i in range(num_reviews_tpadv):
    all_reviews.append(data_df[""feedback""][0][i][""text""])

stopwords = {}
with open('stopwords.txt', 'rU') as f:
    for line in f:
        stopwords[line.strip()] = 1

def clean_review(text):

    words = []
    nouns = []
    if type(text) != int:
        new_text = text.lower()
        sentences = nltk.sent_tokenize(new_text)

        for sentence in sentences:
            tokens = nltk.word_tokenize(sentence)
            text1 = [word for word in tokens if word not in stopwords]
            tagged_text = nltk.pos_tag(text1)

            for word, tag in tagged_text:
                words.append({""word"": word, ""pos"": tag})       

        lem = WordNetLemmatizer()

        words = [word for word in words if word[""pos""] in [""NN"", ""NNS""]]

        for word in words:
            nouns.append(lem.lemmatize(word[""word""]))

    return nouns          

clean_reviews = []

for i in range(num_reviews_tpadv):
    clean_reviews.append(clean_review(data_df[""feedback""][0][i][""text""]))

#----------------------------------------------------
# Creating Dictionary and Corpus to train LDA model
#----------------------------------------------------

dictionary = corpora.Dictionary(clean_reviews)
dictionary.filter_extremes(keep_n=11000) #change filters
dictionary.compactify()
dictionary_path = ""dictionary.dict""
corpora.Dictionary.save(dictionary, dictionary_path)

corpus = [dictionary.doc2bow(doc) for doc in clean_reviews]

# Training lda using number of topics set = 10 (which can be changed)

lda = gensim.models.LdaModel(corpus, id2word = dictionary,
                        num_topics = 20,
                        passes = 20,
                        random_state=1,
                        alpha = ""auto"")
lda_model_path = ""lda_model.lda""
lda.save(lda_model_path)

i = 0
for topic in lda.show_topics(20):
    print topic

    i += 1
</code></pre>
",2017-05-15 05:52:38,2017-05-15 05:52:38,How to create topic names using LDA topic model,<python><nlp><nltk><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,False
12591,44146401,2017-05-23 23:03:42,,"<p>The title pretty much says it all. Here's some test code:</p>

<pre><code>import os
os.environ.update({'MALLET_HOME': r'C:/Users/somebody/a/place/LDA/mallet-2.0.8/',
                  'JAVA_HOME': r'C:/Program Files/Java/jdk1.8.0_131/'})

from gensim.corpora import mmcorpus, Dictionary
texts = [['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

import gensim

mallet_path = r'C:\Users\somebody\a\place\LDA\mallet-2.0.\bin\mallet'
gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=dictionary, num_topics=5, alpha=1)
</code></pre>

<p>This raises the following error (truncated, since most of it is irrelevant stack info): </p>

<pre><code>CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-99-7343c192afd1&gt; in &lt;module&gt;()
      5 mallet_path = r'C:\Users\somebody\a\place\LDA\mallet-2.0.8\bin\mallet'
----&gt; 6 gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=dictionary, num_topics=5, alpha=1)
.
.
.
CalledProcessError: Command 'C:\Users\somebody\a\place\LDA\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\somebody\AppData\Local\Temp\33b805_corpus.txt --output C:\Users\somebody\AppData\Local\Temp\33b805_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>Fine, super, <em>but when I go run that exact command in cmd.exe, or in cygwin, There is no error, the code runs just fine!</em> I can even test the return code: ""echo $?"" in cygwin returns 0. Any help much appreciated!</p>
",,2019-08-07 02:59:23,"gensim LdaMallet raising CalledProcessError, but running mallet at command line runs with no error",<python><subprocess><gensim><lda><mallet>,,,CC BY-SA 3.0,False,False,True,False,False
12602,44011706,2017-05-16 21:15:43,,"<p>I read this <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">page</a> but I do not understand what is different between models which are built based on the following codes.
I know when dbow_words is 0, training of doc-vectors is faster.</p>

<p>First model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>Second model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4,dbow_words=1)
</code></pre>
",,2017-05-17 01:07:38,What is different between doc2vec models when the dbow_words is set to 1 or 0?,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12604,44163836,2017-05-24 16:24:45,,"<p>I trained a doc2vec model using train(..) with default settings. That worked, but now I'm wondering how infer_vector combines across input words, is it just the average of the individual word vectors?</p>

<pre><code>model.random.seed(0)
model.infer_vector(['cat', 'hat'])
model.random.seed(0)
model.infer_vector(['cat'])
model.infer_vector(['hat']) #doesn't average up to the ['cat', 'hat'] vector
model.random.seed(0)
model.infer_vector(['hat'])
model.infer_vector(['cat']) #doesn't average up to the ['cat', 'hat'] vector
</code></pre>

<p>Those don't add up, so I'm wondering what I'm misunderstanding. </p>
",,2017-05-25 07:34:35,How does doc2vec.infer_vector combine across words?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12607,44169631,2017-05-24 22:42:20,,"<p>I am trying to use gensim's summarizer and keywords to extract important keywords and summarizing contents. However, I am getting the following error:</p>

<pre><code>from gensim.summarization import summarize 
</code></pre>

<p>Traceback:</p>

<pre><code> ImportError                               Traceback (most recent call last)
&lt;ipython-input-12-70743b938b65&gt; in &lt;module&gt;()
----&gt; 1 from gensim.summarization import summarize

ImportError: No module named summarization
</code></pre>

<p>I checked the version which is gensim 0.10.0. I am using Anaconda distribution and installed gensim using </p>

<pre><code>conda install gensim
</code></pre>

<p>Any help would greatly help. </p>

<p>Thanks </p>
",,2017-05-26 16:51:40,Getting import error when using gensim.summeraization,<python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12608,44051051,2017-05-18 14:48:50,,"<p>I am using gensim version <code>0.12.4</code> and have trained two separate word embeddings using the same text and same parameters. After training I am calculating the Pearsons correlation between the word occurrence-frequency and vector-length.  One model I trained using <code>save_word2vec_format(fname, binary=True)</code> and then loaded using <code>load_word2vec_format</code> the other I trained using <code>model.save(fname)</code> and then loaded using <code>Word2Vec.load()</code>. I understand that the word2vec algorithm is non deterministic so the results will vary however the difference in the correlation between the two models is quite drastic. Which method should I be using in this instance?</p>
",,2017-05-18 15:11:01,Gensim save_word2vec_format() vs. model.save(),<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12614,44150201,2017-05-24 06:06:14,,"<p>My question concerns the proper training of the model for unique and really specific use of the Word2Vec model. <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">See Word2Vec details here</a></p>

<p>I am working on identifying noun-adjective (or ) relationships within the word embeddings.</p>

<p>(E.g. we have 'nice car' in a sentence of the data-set. Given the word embeddings of the corpus and the nouns and adjectives all labeled, I am trying to design a technique to find the proper vector that connects 'nice' with 'car'.)</p>

<p>Of course I am not trying to connect only that pair of words, but the technique should would for all  relationships. A supervised approach is taken at this moment, then try to work towards designing an unsupervised method.</p>

<p>Now that you understand what I am trying to do, I will explain the problem. I obviously know that word2vec needs to be trained on large amounts of data, to learn the proper embeddings as accurately as possible, but I am afraid to give it more data than the data-set with labelled  sentences (500-700).</p>

<p>I am afraid that if I give it more data to train on (e.g. Latest Wikipedia dump data-set), it will learn better vectors, but the extra data will influence the positioning of my  words, then this word relationship is biased by the extra training data. (e.g. what if there is also 'nice Apple' in the extra training data, then the positioning of the word 'nice' could be compromised).</p>

<p>Hopefully this makes sense and I am not making bad assumptions, but I am just in the dilemma of having bad vectors because of not enough training data, or having good vectors, but compromised  vector positioning in the word embeddings.</p>

<p>What would be the proper way to train on ? As much training data as possible (billions of words) or just the labelled data-set (500-700 sentences) ?</p>

<p>Thank you kindly for your time, and let me know if anything that I explained does not make sense.</p>
",,2017-05-24 11:04:27,how to train Word2Vec model properly for a special purpose,<vector><deep-learning><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
12658,44158856,2017-05-24 12:47:53,,"<p>I have implemented finding similar documents based on a particular document using LDA Model (using Gensim). Next thing i want to do is if I have multiple documents then how to get similar document based on the multiple documents provided as input. </p>

<p>I implemented LDA using this <a href=""https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model"">link</a></p>

<p>sample code for single query -</p>

<pre><code>dictionary = corpora.Dictionary.load('dictionary.dict')
corpus = corpora.MmCorpus(""corpus.mm"")
lda = models.LdaModel.load(""model.lda"") #result from running online lda (training)

index = similarities.MatrixSimilarity(lda[corpus])
index.save(""simIndex.index"")

docname = ""docs/the_doc.txt""
doc = open(docname, 'r').read()
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lda = lda[vec_bow]

sims = index[vec_lda]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
print sims
</code></pre>

<p>Now if I have another doc then how to implement it.</p>
",,2018-05-26 13:07:15,how to calculate document similarity using more than one query?,<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
12664,44249358,2017-05-29 19:32:17,,"<p>I would like to find outliers in my dataset by using LDA. In order to specify outliers, For this case, I am planning to use a bound or perplexity value of the new unseen document on trained model?
After that, I will sort the values in ascending order to check whether it is the outlier or not? 
My issue is that I could not get a bound/perplex value of individual doc, the model throws me <strong>""TypeError: 'int' object is not subscriptable""</strong> error.</p>

<p>I would appreciate if you help me to solve my case?</p>

<p>Just in case, I am attaching my code : </p>

<pre><code>tokenized_corpora = dictionary.doc2bow(_acc[2])
total_number_of_words_tokenized_corpora = len(tokenized_corpora)
bound_corpora = ldaModel.bound(tokenized_corpora)
per_word_perplex_corpora = np.exp2(-bound_corpora / 
total_number_of_words_tokenized_corpora)
</code></pre>

<p>Thanks in advance. </p>
",,2017-06-02 17:30:23,how to get a bound or perplexity value of the new unseen document on trained model?,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12669,44233296,2017-05-29 00:37:15,,"<p>I'm trying to use gensim's (ver 1.0.1) <code>doc2vec</code> to get the cosine similarities of documents. This should be relatively simple, but I'm having problems retrieving the vector of the documents so I can do cosine similarity. When I try to retrieve a document by the label I gave it in training, I get a key error. </p>

<p>For example, 
<code>print(model.docvecs['4_99.txt'])</code> 
will tell me that there is no such key as <code>4_99.txt</code>.</p>

<p>However if I print <code>print(model.docvecs.doctags)</code> I see things like this:
<code>'4_99.txt_3': Doctag(offset=1644, word_count=12, doc_count=1)</code></p>

<p>So it appears that for every document, <code>doc2vec</code> is saving each sentence as the ""document name underscore number""</p>

<p>So I'm either 
A) training incorrectly or
B) Don't understand how to retrieve the doc vector so that I can do <code>similarity(d1, d2)</code></p>

<p>Can anyone help me out here? </p>

<p>Here is how I train my doc2vec:</p>

<pre><code>#Obtain txt abstracts and txt patents 
filedir = os.path.abspath(os.path.join(os.path.dirname(__file__)))
files = os.listdir(filedir)

#Doc2Vec takes [['a', 'sentence'], 'and label']
docLabels = [f for f in files if f.endswith('.txt')]

sources = {}  #{'2_139.txt': '2_139.txt'}
for lable in docLabels:
    sources[lable] = lable
sentences = LabeledLineSentence(sources)


model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())
for epoch in range(10):
    model.train(sentences.sentences_perm())

model.save('./a2v.d2v')
</code></pre>

<p>This uses this class </p>

<p><code>class LabeledLineSentence(object):</code></p>

<pre><code>def __init__(self, sources):
    self.sources = sources

    flipped = {}

    # make sure that keys are unique
    for key, value in sources.items():
        if value not in flipped:
            flipped[value] = [key]
        else:
            raise Exception('Non-unique prefix encountered')

def __iter__(self):
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

def to_array(self):
    self.sentences = []
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
    return self.sentences

def sentences_perm(self):
    shuffle(self.sentences)
    return self.sentences
</code></pre>

<p>I got this class from a web tutorial (<a href=""https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1"" rel=""nofollow noreferrer"">https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1</a>) to help me get around Doc2Vec's weird data formatting requirements and I don't completely understand it to be honest. It does look like this class written here is adding the <code>_n</code> for each sentence, but in the tutorial it seems that they still retrieve the document vector with just giving it the filename... So what am I doing wrong here?</p>
",,2017-05-29 06:57:08,Problems accessing docvectors with gensim,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12673,44213766,2017-05-27 06:32:19,,"<p>My purpose is build something like Q&amp;A bot that can generate sentences according to the input sentences of user. I use pre-trained word2vec in gensim for my input of model.<br>
My words are chinese, but I think it doesn't matter to word2vec. 
I first turned each sentence into a 3d-array. The shape is</p>

<p>(sample_n, time_step, word_dim)</p>

<pre><code>x = [sentence_1, sentence_2, ... , sentence_n]
sentence = [word_1, word_2, ...]
word = [250 dimensions array]
</code></pre>

<p>time_step is equal to the length of sequence.The sequences have already done the zero padding. Thus, the length is fixed.</p>

<p>Next, I build a simple seq2seq model like that:<br>
(I said ""simple"", because it doesn't have any attention layer and feed last output to current input in decoder.)</p>

<pre><code>model = Sequential

# Encoder
model.add(LSTM(units=HIDDEN_SIZE, input_shape=(X_TIME_STEP, WORD_DIM), return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(units=HIDDEN_SIZE, input_shape=(X_TIME_STEP, WORD_DIM), return_sequences=False))
model.add(Dense(HIDDEN_SIZE, activation=""relu""))
model.add(RepeatVector(Y_TIME_STEP))

# Decoder
model.add(LSTM(units=WORD_DIM, return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(units=WORD_DIM, return_sequences=True))
model.add(TimeDistributed(Dense(WORD_DIM, activation=""linear"")))

optimizer = optimizers.Adam()
model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=['acc'])

model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS)
</code></pre>

<p>The loss value was negative during training.
Afterward, I just use training data to do prediction and turn the output vector back to the sentence.<br>
However, the words in sentence are all the same or something weird.  </p>

<p>I changed loss function to ""mse"". It seems that it improved a little bit and at least the loss wasn't negative, but I think that is not a correct way to solve this.<br>
What I can figure out that is the value of word2vec in each dimension is <strong>not between 0 and 1</strong>.</p>

<ul>
<li><p>In order to use ""categorical_crossentropy"", It need to do normalization with word vector before training and use softmax for activation function of output layer.<br>
After prediction, convert the normalized vector back to origin(called denormalized?) and then turn it into sentences.<br>
If this is fine, what method that I can use to normalized?</p></li>
<li><p>just change the loss function or layer that can deal with the unbound value.</p></li>
</ul>

<p>I am new to NN and Keras, is there any way to solve it? Thanks!</p>
",,2017-05-27 06:32:19,Use word2vec and seq2seq model in Keras,<nlp><deep-learning><keras><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12741,44352552,2017-06-04 09:22:06,,"<p>There is a dataframe like this:  </p>

<pre><code>  index  terms   
  1345  ['jays', 'place', 'great', 'subway']    
  1543  ['described', 'communicative', 'friendly']    
  9874  ['great', 'sarahs', 'apartament', 'back']    
  2456  ['great', 'sarahs', 'apartament', 'back']  
</code></pre>

<p>I try to create a dictionary from the corpus of comments[ 'terms' ], but I face an error message !  </p>

<pre><code>from gensim import corpora, models
dictionary = corpora.Dictionary( comments['terms'] )

TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>
",,2019-01-07 23:51:29,"TypeError: doc2bow expects an array of unicode tokens on input, not a single string when using gensim.corpora.Dictionary()",<python><dictionary><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12743,44371835,2017-06-05 15:00:06,,"<p>I want to import word vecters created from tensorflow and utilize it at gensim.</p>

<p>there is a method <code>gensim.models.KeyedVectors.load_word2vec_format</code></p>

<p>so I tried this method by following exactly the same way in <a href=""https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim"">Training wordvec in Tensorflow, importing to Gensim</a></p>

<p>Example:</p>

<blockquote>
  <p>2 3</p>
  
  <p>word0 -0.000737 -0.002106 0.001851</p>
  
  <p>word1 -0.000878 -0.002106 0.002834</p>
</blockquote>

<p>Save the file and then load with kwarg binary=False:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)
</code></pre>

<p>but error like</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    model=gensim.models.KeyedVectors.load_word2vec_format('test.w2v')
  File ""C:\Users\cbj\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 243, in load_word2vec_format
    raise EOFError(""unexpected end of input; is count incorrect or file otherwise damaged?"")
EOFError: unexpected end of input; is count incorrect or file otherwise damaged?
</code></pre>

<p>raised</p>

<p>how can I solve this problem?</p>
",,2017-06-05 17:41:28,Importing word vectors from tensorflow into gensim,<python><tensorflow><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
12760,44282320,2017-05-31 10:30:49,,"<p>I want to use gensim to train a word2vec model <br>
python 3.5.3 <br>
gensim 2.1.0 <br>
numpy 1.12.1+mkl <br>
scipy 0.19.0 <br></p>

<pre><code>import gensim
import codecs
class MySentences(object):
    def __init__(self,filename):
        self.filename=filename
    def __iter__(self):
        with codecs.open(self.filename) as f:
            for line in f.readlines():
                wordlist=list()
                for word in line:
                    wordlist.append(word)
                yield wordlist

sentences=MySentences('D:/Documents/Data/icwb2-data-processed/pku_training.rmspace.utf8')
model=gensim.models.Word2Vec(sentences)
model.save('w.model')
</code></pre>

<p>I run this code, and i cause the error:</p>

<blockquote>
  <p>AttributeError: module 'gensim' has no attribute 'models'</p>
</blockquote>

<p>I make this error due to i named this file 'gensim.py' <br>
thank @BurhanKhalid !!!</p>
",2018-04-15 07:55:44,2019-07-24 04:50:31,How to solve this error: module 'gensim' has no attribute 'models',<python><python-3.x><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12762,44315325,2017-06-01 19:09:36,,"<p>I have a blackbox LDA model which outputs word to topic distribution like follows:</p>

<pre><code>word_1 topic_b:freq_of_word_1 topic_a:freq_of_word_1 ....
word_2 topic_a:freq_of_word_2 topic_d:freq_of_word_2 ....
.
.
</code></pre>

<p>How do I proceed to calculate perplexity? Looking at Gensim LDA code I found they use variational lower bound methods, but they have access to internal topic parameters to calculate them. In my blackbox model, all I have access is to these dumps, and the initial parameters alpha and beta.</p>
",,2017-06-01 19:09:36,"Given a word to topic distribution, how do I calculate perplexity?",<text-mining><evaluation><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12772,44301061,2017-06-01 07:23:07,,"<p>I have a set of documents. I also have the title of topics based on which I want to categorize documents. My preference is to use LDA in Gensim. is there any way to feed my own list of topics in the topic modeling algorithm ?</p>
",,2017-10-29 14:37:35,LDA/LSI Topic modelling in Gensim with predefined list of topics,<gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12773,44306123,2017-06-01 11:20:24,,"<p>Sentiment prediction based on document vectors works pretty well, as examples show:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a>
<a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">http://linanqiu.github.io/2015/10/07/word2vec-sentiment/</a></p>

<p>I wonder what pattern is in the vectors making that possible. I thought it should be similarity of vectors making that somehow possible. Gensim similarity measures rely on cosine similarity. Therefore, I tried the following:</p>

<p>Randomly initialised a fix compare vector, get cosine similarity of the compare vector with all other vectors in training and test set, use the similarities and the labels of the train set to estimate a logistic regression model, evaluate the model with the test set.</p>

<p>Looks like this, where train/test_arrays contain document vectors and train/test_labels labels either 0 or 1. (Notice, document vectors are obtained from genism doc2vec and are well trained, predicting the test set 80% right if directly used as input for the logistic regression):</p>

<pre><code>fix_vec = numpy.random.rand(100,1)
def cos_distance_to_fix(x):
    return scipy.spatial.distance.cosine(fix_vec, x)

train_arrays_cos =  numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=train_arrays), newshape=(-1,1))
test_arrays_cos = numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=test_arrays), newshape=(-1,1))

classifier = LogisticRegression()
classifier.fit(train_arrays_cos, train_labels)
classifier.score(test_arrays_cos, test_labels)
</code></pre>

<p>It turns out, that this approach does not work, predicting the test set only to 50%....
So, my question is, what information is in the vectors, making the prediction based on vectors work, if it is not the similarity of vectors? Or is my approach simply not possible to capture similarity of vectors correct?</p>
",2017-06-01 20:06:53,2017-06-02 01:06:04,What information in document vectors makes sentiment prediction work?,<machine-learning><sentiment-analysis><gensim><feature-selection><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12774,44338377,2017-06-02 22:41:55,,"<p>I am trying, unsuccefully, to install a python package (gensim).</p>

<pre><code>System details:
I am using Python 3.6.0 :: Anaconda custom (64-bit) with Ubuntu16.04LTS.
</code></pre>

<ol>
<li><p>First I followed the directions from <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">here</a> (including updating before trying to install), I entered </p>

<pre><code>easy_install --upgrade gensim
</code></pre>

<p>and got the following error massage:</p>

<pre><code>Searching for gensim
Reading https://pypi.python.org/simple/gensim/
Download error on https://pypi.python.org/simple/gensim/: unknown urltype: https -- Some packages may not be found!
Couldn't find index page for 'gensim' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading https://pypi.python.org/simple/
Download error on https://pypi.python.org/simple/: unknown url type:https -- Some packages may not be found!
No local packages or working download links found for gensim 
error: Could not find suitable distribution for 
Requirement.parse('gensim')
</code></pre></li>
<li><p>When I tried to install using anaconda, </p>

<pre><code>conda install -c anaconda gensim=1.0.1
</code></pre>

<p>as described <a href=""https://anaconda.org/anaconda/gensim"" rel=""nofollow noreferrer"">here</a>, I got </p>

<pre><code>Fetching package metadata ...
CondaHTTPError: HTTP None None for url 
&lt;https://conda.anaconda.org/anaconda/linux-64/repodata.json&gt;
Elapsed: None

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
SSLError(SSLError(""Can't connect to HTTPS URL because the SSL module is not available."",),)
</code></pre></li>
<li><p>When I tried to install directly from git</p>

<pre><code>pip install https://github.com/RaRe-Technologies/gensim.git
</code></pre>

<p>I got a similar error</p>

<pre><code>SSLError(""Can't connect to HTTPS URL because the SSLpip._vendor.requests.packages.urllib3.exceptions.SSLError: 
Can't connect to HTTPS URL because the SSL module is not available.
</code></pre></li>
<li><p>I tried to work my way around this by using</p>

<pre><code>PYTHONUSERBASE=/home/usr/anaconda3 pip3 install --user --upgrade gensim
</code></pre>

<p>which was able to install the gensim package, but under a newly created folder <code>/home/usr/anaconda3/python3.5</code> even though my default python is python3.6</p></li>
</ol>

<p><strong>Conclusion(?)</strong>: </p>

<p>From <a href=""https://docs.python.org/3/library/http.client.html"" rel=""nofollow noreferrer"">this</a> I understood that probably my Python was not compiled with SSL support, and if I fix this I may be able to win this long battle. BUT I don't understand how I can fix this D_:</p>

<p>PLUS, I don't understand why it insists on being installed under python3.5 when the <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">manual</a> says the package is also compatible with python3.6.</p>
",,2017-06-06 13:35:50,Can't install gensim with Ubuntu16.04 and anaconda,<ubuntu><ssl><anaconda><gensim><python-3.6>,,,CC BY-SA 3.0,False,False,True,False,False
12785,44323816,2017-06-02 08:05:15,,"<p>I have about 30 word2vec models. When loading them in a python script each consumes a few GB of RAM so it is impossible to use all of them at once. Is there any way to use the models without loading the complete model into RAM?</p>
",,2017-06-02 08:30:22,word2vec - reduce RAM consumption when loading model,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12796,44345576,2017-06-03 15:29:31,,"<p>I have calculated document similarities using Doc2Vec.docvecs.similarity() in gensim. Now, I would either expect the cosine similarities to lie in the range [0.0, 1.0] if gensim used the absolute value of the cosine as the similarity metric, or roughly half of them to be negative if it does not.</p>

<p>However, what I am seeing is that <em>some</em> similarities are negative, but they are very rare  less than 1% of pairwise similarities in my set of 30000 documents.</p>

<p>Why are almost all of the similarities positive?</p>
",,2017-06-05 14:51:37,Why are almost all cosine similarities positive between word or document vectors in gensim doc2vec?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12799,44497215,2017-06-12 10:29:01,,"<p>I am training a model with <code>gensim</code>, my corpus is many short sentences, and each sentence has a frequency which indicates times it occurs in total corpus. I implement it as follow, as you can see, I just choose to do repeat <code>freq</code> times. Any way, if the data is small, it should work, but when data grows, <strong>the frequency can be very large, it costs too much memory</strong> and my machine cannot afford it. </p>

<p>So 
1. <strong>can I just count the frequency in every record instead of repeat <code>freq</code> times?</strong> 2. <strong>Or any other ways to save memory?</strong></p>

<pre><code>class AddressSentences(object):
    def __init__(self, raw_path, path):
        self._path = path

    def __iter__(self):
        with open(self.path) as fi:
            headers = next(fi).split("","")
            i_address, i_freq = headers.index(""address""), headers.index(""freq"")
            index = 0
            for line in fi:
                cols = line.strip().split("","")
                freq = cols[i_freq]
                address = cols[i_address].split()
                # Here I do repeat
                for i in range(int(freq)):
                    yield TaggedDocument(address, [index])
                index += 1

print(""START %s"" % datetime.datetime.now())
train_corpus = list(AddressSentences(""/data/corpus.csv""))
model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
print(""END %s"" % datetime.datetime.now())
</code></pre>

<p>corpus is something like this:</p>

<pre><code>address,freq
Cecilia Chapman 711-2880 Nulla St.,1000
The Business Centre,1000
61 Wellfield Road,500
Celeste Slater 606-3727 Ullamcorper. Street,600
Theodore Lowe Azusa New York 39531,700
Kyla Olsen Ap #651-8679 Sodales Av.,300
</code></pre>
",2017-06-12 10:42:51,2017-06-12 18:46:45,How to count frequency in gensim.Doc2Vec?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12805,44386503,2017-06-06 09:39:09,,"<p>I 'm working on word2vec model using gensim in Python, but I found that the result are the words having the same theme, synonyms are only part of the result.</p>

<p>Can I find synonyms of a word based on the work I have done?</p>

<p>Any replies will be appreciated!</p>
",,2017-06-12 03:44:45,How to find synonyms based on word2vec,<word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12819,44461656,2017-06-09 15:32:12,,"<p>My setup is as follows:</p>

<p>Python version: 3.6.0</p>

<p>Numpy version: 1.13.0</p>

<p>Scipy version: 0.19.0</p>

<p>Gensim version: 2.1.0</p>

<p>GCC Compiler version: 5.3.0</p>

<p>System: Windows 7, 64bit</p>

<p>I get the following error with the setup above</p>

<pre><code>import gensim
&gt;&gt;&gt;Slow version of gensim.models.doc2vec is being used
</code></pre>

<p>This makes the run time far too slow when training models on gensim. I feel there is some problem with the package versions I am using or how I installed them because: I had to install numpy using <code>pip</code>; I had to install scipy using <code>conda</code>; and I had to install gensim using <code>pip</code> again. The reason for this setup, is because if I try to install scipy using <code>pip</code>, I get the error</p>

<pre><code>&gt;&gt;&gt;ImportError: DLL load failed: The specified procedure could not be found.
</code></pre>

<p>So I had to install scipy via <code>conda</code>. Also, if I try to install gensim using</p>

<p><code>conda install gensim</code></p>

<p>or</p>

<p><code>conda update gensim</code></p>

<p>it only installs version 1 - I have tried <code>conda install -c anaconda gensim=2.1.0</code> but I get the error</p>

<pre><code>PackageNotFoundError: Package missing in current win-64 channels:
- gensim 2.1.0*
</code></pre>

<p>Numpy and Scipy work fine independently when I import them into a script - that is, they import fine and I can use all their functionality. However, when they are being used by Gensim, clearly there is a problem and I don't know why.</p>

<p><strong>Would anyone be able to advise possible fixes? Ideally I would like to keep all the latest versions of these packages if possible. Thank you in advance</strong></p>

<p><strong>NOTE: Gensim works fine with the ""fast"" version when I have Gensim version 1 installed and with the same versions of the dependencies above!</strong></p>
",2017-06-09 16:01:30,2017-10-10 10:02:38,Gensim: Slow version of gensim.models.doc2vec is being used,<python><numpy><scipy><pip><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12822,44439291,2017-06-08 15:01:24,,"<p>I have a a gensim Word2Vec KeyedVectors model. For speed purposes, I would like to parallelize my program so that it can run in a Spark environment. However, as far as I am aware Spark's RDDs only work on collections and iterables. I don't think I could actually see a performance boost by simply putting the KeyedVectors model into a RDD.</p>

<p>I have explored storing the model as a broadcast variable, but it is far too large. Partitioning (using an RDD) looks like the best option.</p>

<p>If I wanted to boost my program's performance by converting the model to a parallel collection in Spark, how would I go about doing this?</p>
",,2017-06-08 15:01:24,Parallelizing a gensim KeyedVectors model in Spark,<apache-spark><pyspark><rdd><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12826,44443675,2017-06-08 19:02:46,,"<p>I am using a pre-trained doc2vec BOW model <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">(AP-news)</a>. I am doing the following:</p>

<pre><code>import gensim.models as g 
start_alpha=0.01
infer_epoch=1000
model=""\\apnews_dbow\\doc2vec.bin""
m = g.Doc2Vec.load(model)
text='this is a sample text'
vec=m.infer_vector(text,alpha=start_alpha, steps=infer_epoch)
</code></pre>

<p>But if I compute the vec again for the same text then I am getting a different vector representation of the same text. Why is this happening and how can I aviod this. I want to have the same vector returned if I give exactly the same text. 
I tried following <a href=""https://github.com/RaRe-Technologies/gensim/issues/447"" rel=""nofollow noreferrer"">this post</a> but does not seem to help. </p>
",,2020-01-07 17:52:06,removing randomization of vector initialization for doc2vec,<python><random><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12833,44516295,2017-06-13 08:37:05,,"<p>I have one tf-idf example from an ISI paper. Im trying to validate my code by this example. But I get different result from my code.I dont know what the reason is!</p>

<p>Term-document matrix from paper:  </p>

<pre><code>acceptance     [ 0 1 0 1 1 0
information      0 1 0 1 0 0
media            1 0 1 0 0 2
model            0 0 1 1 0 0
selection        1 0 1 0 0 0 
technology       0 1 0 1 1 0]
</code></pre>

<p>Tf-idf matrix from paper: </p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.3  0   0.2   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.9  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My tf-idf matrix:</p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.5  0   0.4   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.8  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My code:</p>

<pre><code>tfidf = models.TfidfModel(corpus)   
corpus_tfidf=tfidf[corpus]
</code></pre>

<p>Ive tried another code like this:</p>

<pre><code>transformer = TfidfTransformer()
tfidf=transformer.fit_transform(counts).toarray() ##counts is term-document matrix
</code></pre>

<p>But I didnt get appropriate answer</p>
",,2017-06-14 06:03:49,Tf-idf calculation using gensim,<python><tf-idf><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12862,44449132,2017-06-09 04:05:11,,"<p>I have made doc2vec file by training data using gensim model now while processing it. I am getting an error.
I am running the below code:-</p>

<p>model = Doc2Vec.load('sentiment140.d2v')</p>

<pre><code>if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>I am getting a Keyerror - ""TEST_POS_72""<a href=""https://i.stack.imgur.com/z9TEW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z9TEW.png"" alt=""ERROR""></a></p>

<p>I want to know what I am doing wrong.</p>
",,2017-06-12 03:25:59,Getting error while using gensim model in python,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12869,44506950,2017-06-12 19:05:05,,"<p>I'm trying to pull into python the English Wikipedia corpus (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) to perform some deep learning. I'm using gensim.</p>

<p>It's 16GB and I've got it sitting on a large EC2 machine in AWS. I load it with</p>

<pre><code>from gensim.corpora.wikicorpus import WikiCorpus
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from pprint import pprint
import multiprocessing

wiki = WikiCorpus(""enwiki-latest-pages-articles.xml.bz2"")
</code></pre>

<p>I run this in a jupyter notebook, but its basically hung trying to load this. I'm watching memory consumption and its loading extremely slowly. (12+ hours and only ~2 GB). Any way I can speed this up?</p>
",2017-06-12 19:06:23,2017-06-22 09:48:08,Can I speed up loading xml bz2 files into memory?,<python><deep-learning><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12874,44471602,2017-06-10 09:14:02,,"<p>I have a problem deploying python modules on Heroku.
It's based on python2.</p>

<p>requirements.txt file:</p>

<pre><code>Flask==0.11.1
Jinja2==2.8
MarkupSafe==0.23
Werkzeug==0.11.10
click==6.6
gunicorn==19.6.0
itsdangerous==0.24
requests==2.10.0
wsgiref==0.1.2
konlpy==0.4.4
gensim==2.1.0
</code></pre>

<p>The modules that I'm having trouble uploading is konlpy and gensim.
I already installed all the modules on virtual environment. I also set the buildpack for gensim (not konlpy).</p>

<p>The error log is:</p>

<pre><code>2017-06-10T08:56:17.851884+00:00 app[web.1]: [2017-06-10 08:56:17 +0000] [4] [INFO] Shutting down: Master
2017-06-10T08:56:17.851975+00:00 app[web.1]: [2017-06-10 08:56:17 +0000] [4] [INFO] Reason: Worker failed to boot.
2017-06-10T08:56:17.946195+00:00 heroku[web.1]: State changed from starting to crashed
2017-06-10T08:56:17.936020+00:00 heroku[web.1]: Process exited with status 3
2017-06-10T08:56:55.589601+00:00 heroku[router]: at=error code=H10 desc=""App crashed"" method=POST path=""/"" host=afternoon-beach-22534.herokuapp.com request_id=8e14e22a-b473-4aa8-959e-c26a80f8775e fwd=""69.63.188.105"" dyno= connect= service= status=503 bytes= protocol=https
2017-06-10T08:58:15.541365+00:00 heroku[router]: at=error code=H10 desc=""App crashed"" method=POST path=""/"" host=afternoon-beach-22534.herokuapp.com request_id=1bd4f45b-1db6-4e24-be10-28b0b62b7c40 fwd=""173.252.124.202"" dyno= connect= service= status=503 bytes= protocol=https
2017-06-10T08:59:04.103999+00:00 heroku[web.1]: State changed from crashed to starting
2017-06-10T08:59:19.881438+00:00 heroku[web.1]: Starting process with command `gunicorn app:app --log-file=-`
2017-06-10T08:59:23.010876+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [4] [INFO] Starting gunicorn 19.6.0
2017-06-10T08:59:23.012294+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [4] [INFO] Using worker: sync
2017-06-10T08:59:23.020541+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [9] [INFO] Booting worker with pid: 9
2017-06-10T08:59:23.089805+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [11] [INFO] Booting worker with pid: 11
2017-06-10T08:59:23.011687+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [4] [INFO] Listening at: http://0.0.0.0:42198 (4)
2017-06-10T08:59:23.730044+00:00 heroku[web.1]: Process exited with status 1
2017-06-10T08:59:23.520921+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [11] [ERROR] Exception in worker process
2017-06-10T08:59:23.520934+00:00 app[web.1]: Traceback (most recent call last):
2017-06-10T08:59:23.520936+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 557, in spawn_worker
2017-06-10T08:59:23.520936+00:00 app[web.1]:     worker.init_process()
2017-06-10T08:59:23.520937+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 126, in init_process
2017-06-10T08:59:23.520938+00:00 app[web.1]:     self.load_wsgi()
2017-06-10T08:59:23.520938+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 136, in load_wsgi
2017-06-10T08:59:23.520939+00:00 app[web.1]:     self.wsgi = self.app.wsgi()
2017-06-10T08:59:23.520940+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/base.py"", line 67, in wsgi
2017-06-10T08:59:23.520941+00:00 app[web.1]:     self.callable = self.load()
2017-06-10T08:59:23.520941+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 65, in load
2017-06-10T08:59:23.520942+00:00 app[web.1]:     return self.load_wsgiapp()
2017-06-10T08:59:23.520942+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 52, in load_wsgiapp
2017-06-10T08:59:23.520943+00:00 app[web.1]:     return util.import_app(self.app_uri)
2017-06-10T08:59:23.520944+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/util.py"", line 357, in import_app
2017-06-10T08:59:23.520944+00:00 app[web.1]:     __import__(module)
2017-06-10T08:59:23.520945+00:00 app[web.1]:   File ""/app/app.py"", line 8, in &lt;module&gt;
2017-06-10T08:59:23.520946+00:00 app[web.1]:     from konlpy.tag import Twitter; t = Twitter()
2017-06-10T08:59:23.520947+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/konlpy/tag/_twitter.py"", line 72, in __init__
2017-06-10T08:59:23.520947+00:00 app[web.1]:     jvm.init_jvm(jvmpath)
2017-06-10T08:59:23.520948+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/konlpy/jvm.py"", line 40, in init_jvm
2017-06-10T08:59:23.520949+00:00 app[web.1]:     jvmpath = jvmpath or jpype.getDefaultJVMPath()
2017-06-10T08:59:23.520949+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_core.py"", line 114, in get_default_jvm_path
2017-06-10T08:59:23.520950+00:00 app[web.1]:     return finder.get_jvm_path()
2017-06-10T08:59:23.520951+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 121, in get_jvm_path
2017-06-10T08:59:23.520951+00:00 app[web.1]:     jvm = method()
2017-06-10T08:59:23.520952+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 164, in _get_from_known_locations
2017-06-10T08:59:23.520953+00:00 app[web.1]:     for home in self.find_possible_homes(self._locations):
2017-06-10T08:59:23.520954+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 95, in find_possible_homes
2017-06-10T08:59:23.520954+00:00 app[web.1]:     for childname in sorted(os.listdir(parent)):
2017-06-10T08:59:23.520955+00:00 app[web.1]: OSError: [Errno 2] No such file or directory: '/usr/lib/jvm'
2017-06-10T08:59:23.520962+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [9] [ERROR] Exception in worker process
2017-06-10T08:59:23.520962+00:00 app[web.1]: Traceback (most recent call last):
2017-06-10T08:59:23.520963+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 557, in spawn_worker
2017-06-10T08:59:23.520964+00:00 app[web.1]:     worker.init_process()
2017-06-10T08:59:23.520964+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 126, in init_process
2017-06-10T08:59:23.520965+00:00 app[web.1]:     self.load_wsgi()
2017-06-10T08:59:23.520966+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 136, in load_wsgi
2017-06-10T08:59:23.520966+00:00 app[web.1]:     self.wsgi = self.app.wsgi()
2017-06-10T08:59:23.520967+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/base.py"", line 67, in wsgi
2017-06-10T08:59:23.520967+00:00 app[web.1]:     self.callable = self.load()
2017-06-10T08:59:23.520968+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 65, in load
2017-06-10T08:59:23.520969+00:00 app[web.1]:     return self.load_wsgiapp()
2017-06-10T08:59:23.520969+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 52, in load_wsgiapp
2017-06-10T08:59:23.520970+00:00 app[web.1]:     return util.import_app(self.app_uri)
2017-06-10T08:59:23.520970+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/util.py"", line 357, in import_app
2017-06-10T08:59:23.520971+00:00 app[web.1]:     __import__(module)
2017-06-10T08:59:23.520972+00:00 app[web.1]:   File ""/app/app.py"", line 8, in &lt;module&gt;
2017-06-10T08:59:23.520972+00:00 app[web.1]:     from konlpy.tag import Twitter; t = Twitter()
2017-06-10T08:59:23.520973+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/konlpy/tag/_twitter.py"", line 72, in __init__
2017-06-10T08:59:23.520974+00:00 app[web.1]:     jvm.init_jvm(jvmpath)
2017-06-10T08:59:23.520974+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/konlpy/jvm.py"", line 40, in init_jvm
2017-06-10T08:59:23.520975+00:00 app[web.1]:     jvmpath = jvmpath or jpype.getDefaultJVMPath()
2017-06-10T08:59:23.520976+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_core.py"", line 114, in get_default_jvm_path
2017-06-10T08:59:23.520977+00:00 app[web.1]:     return finder.get_jvm_path()
2017-06-10T08:59:23.520977+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 121, in get_jvm_path
2017-06-10T08:59:23.520978+00:00 app[web.1]:     jvm = method()
2017-06-10T08:59:23.520979+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 164, in _get_from_known_locations
2017-06-10T08:59:23.520979+00:00 app[web.1]:     for home in self.find_possible_homes(self._locations):
2017-06-10T08:59:23.520980+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/jpype/_jvmfinder.py"", line 95, in find_possible_homes
2017-06-10T08:59:23.520981+00:00 app[web.1]:     for childname in sorted(os.listdir(parent)):
2017-06-10T08:59:23.520981+00:00 app[web.1]: OSError: [Errno 2] No such file or directory: '/usr/lib/jvm'
2017-06-10T08:59:23.521316+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [9] [INFO] Worker exiting (pid: 9)
2017-06-10T08:59:23.521493+00:00 app[web.1]: [2017-06-10 08:59:23 +0000] [11] [INFO] Worker exiting (pid: 11)
2017-06-10T08:59:23.552400+00:00 app[web.1]: Traceback (most recent call last):
2017-06-10T08:59:23.552442+00:00 app[web.1]:   File ""/app/.heroku/python/bin/gunicorn"", line 11, in &lt;module&gt;
2017-06-10T08:59:23.552493+00:00 app[web.1]:     sys.exit(run())
2017-06-10T08:59:23.552512+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 74, in run
2017-06-10T08:59:23.552559+00:00 app[web.1]:     WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()
2017-06-10T08:59:23.552605+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/base.py"", line 192, in run
2017-06-10T08:59:23.552667+00:00 app[web.1]:     super(Application, self).run()
2017-06-10T08:59:23.552685+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/app/base.py"", line 72, in run
2017-06-10T08:59:23.552727+00:00 app[web.1]:     Arbiter(self).run()
2017-06-10T08:59:23.552769+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 218, in run
2017-06-10T08:59:23.552834+00:00 app[web.1]:     self.halt(reason=inst.reason, exit_status=inst.exit_status)
2017-06-10T08:59:23.552851+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 331, in halt
2017-06-10T08:59:23.552954+00:00 app[web.1]:     self.stop()
2017-06-10T08:59:23.552972+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 381, in stop
2017-06-10T08:59:23.553058+00:00 app[web.1]:     time.sleep(0.1)
2017-06-10T08:59:23.553101+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 231, in handle_chld
2017-06-10T08:59:23.553166+00:00 app[web.1]:     self.reap_workers()
2017-06-10T08:59:23.553182+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 506, in reap_workers
2017-06-10T08:59:23.553314+00:00 app[web.1]:     raise HaltServer(reason, self.WORKER_BOOT_ERROR)
2017-06-10T08:59:23.553400+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;
2017-06-10T08:59:23.748346+00:00 heroku[web.1]: State changed from starting to crashed
2017-06-10T08:59:36.316423+00:00 heroku[router]: at=error code=H10 desc=""App crashed"" method=POST path=""/"" host=afternoon-beach-22534.herokuapp.com request_id=e7341e48-6af5-4d10-8d6d-4a485fdda85c fwd=""173.252.124.218"" dyno= connect= service= status=503 bytes= protocol=https
2017-06-10T08:59:53.796217+00:00 heroku[router]: at=error code=H10 desc=""App crashed"" method=GET path=""/"" host=afternoon-beach-22534.herokuapp.com request_id=898717bb-1ce1-4c36-97ef-fa844585effb fwd=""59.18.254.244"" dyno= connect= service= status=503 bytes= protocol=https
</code></pre>

<p>What I'm trying to do is deploying word2vec on heroku.
Thank you.</p>
",,2018-02-16 13:57:06,Deploy Python Modules on Heroku?,<python-2.7><heroku><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12882,44490739,2017-06-12 02:46:16,,"<p>I am trying to run a program using the Gensim library of the Python with the version 3.6.<br>
Whenever I ran the program, I came across these statements: </p>

<pre class=""lang-none prettyprint-override""><code>C:\Python36\lib\site-packages\gensim-2.0.0-py3.6-win32.egg\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
Slow version of gensim.models.doc2vec is being used
</code></pre>

<p>I do not understand what is the meaning behind <code>Slow version of gensim.models.doc2vec is being used</code>. How the gensim is selecting the slow version and if I want the fastest version then what I need to do?   </p>
",2017-06-12 03:04:24,2017-10-10 10:06:35,"Using Gensim shows ""Slow version of gensim.models.doc2vec being used""",<python><python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
12883,44492006,2017-06-12 05:33:04,,"<p>I'm trying to run Word2Vec first on a very tiny toy dataset that I made up by hand -- just to convince myself I'm doing it correctly before I go for my main dataset. But despite doing 99000 iterations the results weren't great. (Tiger and Lion didn't have as high a similarity as I thought they would). </p>

<p>Toy dataset:</p>

<pre><code>s= [['Tiger', 'Zebra'], ['Tiger', 'Lion', 'Cheetah'],
     ['Orangutan', 'Bonobo', 'Orangutan', 'Chimpanzee'],
     ['Dog', 'Cat', 'Mouse'], ['Tiger', 'Rhino'],
     ['House', 'Car'], ['Antelope', 'Gazelle'],
     ['Zebra', 'Horse'], ['Tiger', 'Lion', 'Leopard'],
     ['Cat', 'Mouse'], ['Mouse', 'Hampster', 'Gerbil'],
     ['Rhino', 'Zebra'], ['Zebra', 'Antelope'],
     ['Tiger', 'Lion'], ['Lion', 'Tiger', 'Giraffe'],
     ['Leopard', 'Lion'], ['Leopard', 'Tiger', 'Lion'],
     ['Tiger', 'Lion'], ['Tiger', 'Lion'],
     ['Car', 'Van'], ['Car', 'Lorry'],
     ['Car', 'Van'], ['Car', 'Lorry'],
     ['Car', 'Van'], ['Car', 'Lorry']
     ]
</code></pre>

<p>In theory should I expect a toy dataset like this to show amazing results if I did large amount of iterations?</p>

<p>Here is the code I'm using:</p>

<pre><code>model = gensim.models.Word2Vec(s, min_count=0, iter=iterations,size=100)
</code></pre>

<p>Ps. <a href=""https://stats.stackexchange.com/questions/215637/word2vec-vector-quality-vs-number-of-training-iterations/284803#284803"">See here</a> for related discussion.</p>
",,2017-06-12 05:54:32,Does Word2Vec with high iterations work for very small toy datasets?,<word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12901,44432464,2017-06-08 09:55:47,,"<p>Code:-</p>

<pre><code>import sys
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
model = Doc2Vec.load('sentiment140.d2v')

if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>Getting the below error:-
2017-06-08 15:24:18,013 : INFO : loading Doc2Vec object from C:/Users/Desktop/sentiment140.d2v
2017-06-08 15:24:21,556 : INFO : loading wv recursively from C:/Users/Desktop/sentiment140.d2v.wv.* with mmap=None
2017-06-08 15:24:21,556 : INFO : setting ignored attribute syn0norm to None
2017-06-08 15:24:21,571 : INFO : loading docvecs recursively from C:/Users/Desktop/sentiment140.d2v.docvecs.* with mmap=None
2017-06-08 15:24:21,571 : INFO : setting ignored attribute cum_table to None
2017-06-08 15:24:21,571 : INFO : loaded C:/Users/Desktop/sentiment140.d2v
Please input train_pos_count, train_neg_count and classifier!
C:\Users\AppData\Local\Continuum\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
An exception has occurred, use %tb to see the full traceback.</p>

<p>SystemExit</p>
",,2017-06-08 09:55:47,Getting system exit error while modelling test data,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,True
12910,44581914,2017-06-16 06:07:00,,"<p>I am building a NLP chat application in Python using <code>gensim</code> library through <code>doc2vec</code> model. I have hard coded documents and given a set of training examples, I am testing the model by throwing a user question and then finding most similar documents as a first step. In this case my test question is an exact copy of a document from training example. </p>

<pre><code>import gensim
from gensim import models
sentence = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'tell',u'me',u'about'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'what',u'is',u'my',u'limit',u'how',u'much',u'can',u'I',u'claim'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'I',u'am',u'retiring',u'how',u'much',u'can',u'claim',u'have', u'resigned'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'what',u'is',u'my',u'eligibility',u'post',u'my',u'promotion'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'what',u'is', u'my',u'eligibility' u'post',u'my',u'promotion'], tags=[""SENT_4""])
sentences = [sentence, sentence1, sentence2, sentence3, sentence4]
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])
model = models.Doc2Vec(alpha=0.03, min_alpha=.025, min_count=2)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)
    model.alpha -= 0.002  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model_loaded.docvecs.most_similar([""SENT_4""]))
</code></pre>

<p>Result:</p>

<pre><code>[('SENT_1', 0.043695494532585144), ('SENT_2', 0.0017897281795740128), ('SENT_0', -0.018954679369926453), ('SENT_3', -0.08253869414329529)]
</code></pre>

<p>Similarity of <code>SENT_4</code> and <code>SENT_3</code> is only <code>-0.08253869414329529</code> when it should be 1 since they are exactly same. How should I improve this accuracy? Is there a specific way of training documents and I am missing something out?  </p>
",2017-06-16 06:23:46,2017-06-16 18:36:40,How can I improve the cosine similarity of two documents(sentences) in doc2vec model?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12921,44553278,2017-06-14 19:34:58,,"<p>I am planning to use Multi Layer Perceptron Classifier from Scikit Learn for this purpose.<br>
Output is the Gender of that word which shall be represented in a one-hot encoding like [1,0,0] for male, [0, 1, 0] for female and [0, 0, 1] for female. 
Now one of the inputs is the word vector for the word. Each of these vectors has 20 dimensions.
The other features are it's Part Of Speech Tags and Singularity(0)/Plurality(1) state. 
My question is how do I use the word vector which is an array as a feature in MLPClassifier?</p>
",,2017-06-14 22:26:18,How do I use the word vector returned by word2vec as features?,<python><scikit-learn><neural-network><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
12927,44605649,2017-06-17 14:15:59,,"<p>I'm recently interested in NLP, and would like to build up search engine for product recommendation. (Actually I'm always wondering about how search engine for Google/Amazon is built up)</p>

<p>Take Amazon product as example, where I could access all ""word"" information about one product:</p>

<pre><code>Product_Name    Description      ReviewText
""XXX brand""    ""Pain relief""    ""This is super effective""
</code></pre>

<p>By applying <code>nltk</code> and <code>gensim</code> packages I could easily compare similarity of different products and make recommendations.</p>

<p>But here's another question I feel very vague about:
How to build a search engine for such products? </p>

<p>For example, if I feel pain and would like to search for medicine online, I'd like to type-in <code>""pain relief""</code> or <code>""pain""</code>, whose searching results should include <code>""XXX brand""</code>.</p>

<p>So this sounds more like keyword extraction/tagging question? How should this be done in NLP? I know <strong>corpus</strong> should contain <strong>all</strong> but <strong>single</strong> words, so it's like:</p>

<pre><code>[""XXX brand"" : (""pain"", 1),(""relief"", 1)]
</code></pre>

<p>So if I typed in either <code>""pain""</code> or <code>""relief""</code> I could get <code>""XXX brand""</code>; but what about I searched <code>""pain relief""</code>?</p>

<p>I could come up with idea that directly call python in my javascript for calculate similarities of input words <code>""pain relief""</code> on browser-based server and make recommendation; but that's kind of do-able? </p>

<p>I still prefer to build up very big lists of keywords at backends, stored in datasets/database and directly visualized in web page of search engine.</p>

<p>Thanks!</p>
",,2017-06-18 15:47:35,Natural language processing keywords for building search engine,<nlp><nltk><search-engine><gensim><corpus>,,,CC BY-SA 3.0,True,False,True,False,False
12928,44606735,2017-06-17 16:17:55,,"<p>I want to extract topics from articles, the test article is ""<a href=""https://julien.danjou.info/blog/2017/announcing-scaling-python"" rel=""nofollow noreferrer"">https://julien.danjou.info/blog/2017/announcing-scaling-python</a>"".</p>

<p>It's an aticle about python and scalling. I've tried lsi and lda, most of time , lda seems works better. But the output of both of them isn't stable. </p>

<p>Of course, the first three or five keywords seem to hit the target. ""python"", ""book"", 'project' ( I don't think 'project' should be an useful topic and will drop it in stopwords list.) , scaling or scalable or openstack should be in keywords list, but not stable at all.</p>

<p>Topic list and stopwords list might improve the results, but it's not scalable. I have to maintain different list for different domain.</p>

<p>So the question here, is there any better solution to improve the algorithm?</p>

<pre><code>num_topics = 1
num_words = 10
passes = 20
</code></pre>

<h3>lda model demo code, code of lsi is the same.</h3>

<pre><code>for topic in lda.print_topics(num_words=num_words):
    termNumber = topic[0]
    print(topic[0], ':', sep='')
    listOfTerms = topic[1].split('+')
    for term in listOfTerms:
        listItems = term.split('*')
        print('  ', listItems[1], '(', listItems[0], ')', sep='')
        lda_list.append(listItems[1])
</code></pre>

<h3>Test Result 1</h3>

<pre><code>Dictionary(81 unique tokens: ['dig', 'shoot', 'lot', 'world', 'possible']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""application"" ( 0.130)
  ""topic"" ( 0.130)
  ""new"" ( 0.130)
  ""openstack"" ( 0.130)
  ""way"" ( 0.130)
  ""decided""( 0.130)

# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""scalable"" ( 0.015)
  ""turn"" ( 0.015)
  ""working"" ( 0.015)
  ""openstack"" ( 0.015)
  ""scaling"" ( 0.015)
  ""different""( 0.015)
</code></pre>

<h3>Test Result 2</h3>

<pre><code>Dictionary(81 unique tokens: ['happy', 'idea', 'tool', 'new', 'shoot']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""scaling"" ( 0.130)
  ""application"" ( 0.130)
  ""turn"" ( 0.130)
  ""working"" ( 0.130)
  ""openstack"" ( 0.130)
  ""topic""( 0.130)
# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""decided"" ( 0.015)
  ""different"" ( 0.015)
  ""turn"" ( 0.015)
  ""writing"" ( 0.015)
  ""working"" ( 0.015)
  ""application""( 0.015)
</code></pre>
",,2017-06-22 09:39:25,how to improve topic model of gensim,<python><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12938,44571617,2017-06-15 15:36:38,,"<p>Sometimes it returns probabilities for all topics and all is fine, but  sometimes it returns probabilities for just a few topics and they don't add up to one, it seems it depends on the document. Generally when it returns few topics, the probabilities add up to more or less 80%, so is it returning just the most relevant topics? Is there a way to force it to return all probabilities?</p>

<p>Maybe I'm missing something but I can't find any documentation of the method's parameters.</p>
",,2020-04-27 16:51:03,probabilities returned by gensim's get_document_topics method doesn't add up to one,<text-mining><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
12955,44610300,2017-06-17 23:20:19,,"<p>I have a list of titles: </p>

<pre><code>&gt; print(data)
&gt; 
  0   Manager
  1   Electrician
  3   Carpenter
  4   Electrician &amp; Carpenter
  ...
</code></pre>

<p>I am trying to use gensim to find the closest related titles. </p>

<p>The code I have is: </p>

<pre><code>import os
import pandas as pd
import nltk
import gensim
from gensim import corpora, models, similarities
from nltk.tokenize import word_tokenize 

df = pd.read_csv('df.csv')
corpus = pd.DataFrame(df, columns=['Job Title'])
tokenized_sents = [word_tokenize(i) for i in corpus]

model = gensim.models.Word2Vec(tokenized_sents, min_count=1)

model.most_similar(""Electrician"")
</code></pre>

<p>When I am running tokenization to tokenize each title as a sentence  (tokenized_sents variable), it only tokenizes the header:</p>

<pre><code>&gt; tokenzied_sents 
&gt; [['Job', 'Title']]
</code></pre>

<p>What am I doing wrong?</p>
",2017-06-17 23:51:46,2017-06-17 23:51:46,Unable to tokenize sentences using gensim and nltk in python,<python><pandas><nltk><tokenize><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
12959,44613624,2017-06-18 09:32:35,,"<p>after train the model, I use  infer_vector() to get the vector successfully.
but after I save the model and load again, error appears as follows:</p>

<pre><code>print ""infer:"", model.infer_vector(sents[0]).tolist()
File ""/Users/zhangweimin/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 752, in infer_vector
    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 426, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5401)
TypeError: object of type 'NoneType' has no len()
</code></pre>

<p>the whole code is:`    </p>

<pre><code>model = train_d2v(labeled_docs, model_file, word_file, 3)

# OK
print ""before infer:"", model.infer_vector(sents[0]).tolist()

model = Doc2Vec.load(model_file)

print ""sents[0]:"", sents[0]
print ""type:"", type(model)
print ""infer:"", model.infer_vector(sents[0]).tolist() #ERROR`
</code></pre>
",2017-06-18 09:34:38,2017-06-20 20:09:07,How to save gensim doc2vec model,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12973,44710644,2017-06-22 23:06:43,,"<p>I trained a word2vec model using gensim package and saved it with the following name. </p>

<pre><code>model_name = ""300features_1minwords_10context""
model.save(model_name)
</code></pre>

<p>I got these log message info. while the model was getting trained and saved.</p>

<pre><code>INFO : not storing attribute syn0norm
INFO : not storing attribute cum_table
</code></pre>

<p>Then, I tried to load the model using this, </p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(""300features_1minwords_10context"")
</code></pre>

<p>I got the following error. </p>

<pre><code>2017-06-22 21:27:14,975 : INFO : loading Word2Vec object from 300features_1minwords_10context
2017-06-22 21:27:15,496 : INFO : loading wv recursively from 300features_1minwords_10context.wv.* with mmap=None
2017-06-22 21:27:15,497 : INFO : setting ignored attribute syn0norm to None
2017-06-22 21:27:15,498 : INFO : setting ignored attribute cum_table to None
2017-06-22 21:27:15,499 : INFO : loaded 300features_1minwords_10context
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-9d90db0f07c0&gt; in &lt;module&gt;()
      1 from gensim.models import Word2Vec
      2 model = Word2Vec.load(""300features_1minwords_10context"")
----&gt; 3 model.syn0.shape

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Also, in the file ""300features_1minwords_10context"", it shows that </p>

<pre><code>""300features_1minwords_10context"" is not UTF-8 encoded
Saving disabled.
Open console for more details 
</code></pre>

<p>To fix the above attribute error, I have also tried the following from the google forum, </p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(""300features_1minwords_10context"")
model.syn0.shape
</code></pre>

<p>It resulted in another error which is </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The model is trained with UTF-8 encoded sentences. I am not sure why is it throwing this error ?</p>

<p>More info : </p>

<pre><code>df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)
features = ['MaterialDescription']
temp_features = df[features]
temp_features.to_csv('materialDescription', encoding='UTF-8')
X = pd.read_csv('materialDescription',encoding='UTF-8')
</code></pre>

<p>Here, I had to use 'mac_roman' encoding in order to access it using pandas dataframe. Since the text in the dataframe has to be in UTF-8 while training the model, I have saved that particular feature in a separate csv file by encoding it with UTF-8 and later, I have the accessed that particular column.</p>

<p>Any help is appreciable </p>
",2017-06-23 02:31:07,2017-06-23 07:37:30,Word2vec saved model is not UTF-8 encoded but the sentence input to the Word2vec model is UTF-8 encoded,<python-3.x><utf-8><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12982,44693241,2017-06-22 07:44:56,,"<p>The file <code>GoogleNews-vectors-negative300.bin</code> contains 300 million word-vectors. I think (not sure) this file is loaded when the following line is written:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
</code></pre>

<p>I want to download the vectors for words that I give externally in a list called <code>words</code>. This is my code to do this:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport', 'amenity', 'area', 'atm', 'barrier', 'bay', 'bench', 'boundary', 'bridge', 'building', 'bus', 'cafe', 'car', 'coast', 'continue', 'created', 'defibrillator', 'drinking', 'ele', 'embankment', 'entrance', 'ferry', 'foot', 'fountain', 'fuel', 'gate', 'golf', 'gps', 'grave', 'highway', 'horse', 'hospital', 'house', 'landuse', 'layer', 'leisure', 'man', 'manmade', 'market', 'marketplace', 'maxheight', 'name', 'natural', 'noexit', 'oneway', 'park', 'parking', 'pgs', 'place', 'worship', 'playground', 'police', 'police station', '', 'post', 'post box or mail', 'power', 'powerstation', 'private', 'public', 'railway', 'ref', 'residential', 'restaurant', 'road', 'route', 'school', 'shelter', 'shop', 'source', 'sport', 'toilet', 'toilets', 'tourism', 'unknown', 'vehicle', 'vending', 'vending machine', 'village', 'wall', 'waste', 'water', 'waterway', 'worship'];

model = gensim.models.KeyedVectors.load_word2vec_format(words, binary=True)

M = len(words)
count = 0
for i in range(1,M):
    wi = id2word[words[i]]
    if wi in word2vec.vocab:
        vector[:,count] = model[:,i]
        count = count+1

f = open('word_vectors.csv', 'w')
print(vector, file=f)
f.close()
</code></pre>

<p>But when I run the code, it just freezes up my system. Is it because it is loading the whole of the binary file before searching for the words in <code>words</code>? If yes, how do I get around this issue? I think of this as I get the following warning, which is why I use the <code>warning</code> package to suppress it:</p>

<pre><code>c:\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
</code></pre>

<p>And the error it gives is:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vec.py"", line 18, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format(topic, binary=True) 
  File ""c:\Python35\lib\site-packages\gensim\models\keyedvectors.py"", line 196, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""c:\Python35\lib\site-packages\smart_open\smart_open_lib.py"", line 208, in smart_open
    raise TypeError('don\'t know how to handle uri %s' % repr(uri))
TypeError: don't know how to handle uri [['access'], ['aeroway'], ['airport'], ['amenity'], ['area'], ['atm'], ['barrier'], ['bay'], ['bench'], ['boundary'], ['bridge'], ['building'], ['bus'], ['cafe'], ['car'], ['coast'], ['continue'], ['created'], ['defibrillator'], ['drinking'], ['ele'], ['embankment'], ['entrance'], ['ferry'], ['foot'], ['fountain'], ['fuel'], ['gate'], ['golf'], ['gps'], ['grave'], ['highway'], ['horse'], ['hospital'], ['house'], ['landuse'], ['layer'], ['leisure'], ['man'], ['manmade'], ['market'], ['marketplace'], ['maxheight'], ['name'], ['natural'], ['noexit'], ['oneway'], ['park'], ['parking'], ['pgs'], ['place'], ['worship'], ['playground'], ['police'], ['police station'], [''], ['post'], ['post box or mail'], ['power'], ['powerstation'], ['private'], ['public'], ['railway'], ['ref'], ['residential'], ['restaurant'], ['road'], ['route'], ['school'], ['shelter'], ['shop'], ['source'], ['sport'], ['toilet'], ['toilets'], ['tourism'], ['unknown'], ['vehicle'], ['vending'], ['vending machine'], ['village'], ['wall'], ['waste'], ['water'], ['waterway'], ['worship']]
</code></pre>

<p>This I guess means that the program is not able to search for the words in the binary file. So, how to solve it? </p>
",2017-06-22 08:13:53,2017-06-22 09:01:44,How to extract a word vector from the Google pre-trained model for word2vec?,<python><file-handling><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
12986,44615340,2017-06-18 13:09:18,,"<p>Loading the pretrained fasttext wordvectors released by Facebook Research take a very long time on a local machine, which I do like this:</p>

<pre><code>model =  fs.load_word2vec_format('wiki.en.vec') 
print(model['test']) # get the vector of the word 'test'
</code></pre>

<p>I am seeking to reduce the load time by <strong>removing wordvectors for words that don't appear in my dataset</strong>. I.e. I want to reduce the pretrained vector model to the words that comprise the dataset I need to analyse, which is a subset of the pre-trained model.</p>

<p>I was about to try and build a new model by extracting the wordvectors I need and saving to a new model, but the type would change from <em>FastTextKeyedVectors</em> to <em>FastText</em>:</p>

<pre><code>#model2 = gensim.models.Word2Vec(iter=1)
#model2 = FastText()
for word in words:
    if (word in model):
       model2[] = model[word]
</code></pre>

<p>How can I reduce my load time ? Do my approaches make sense or am I on the wrong path ?</p>
",,2017-06-19 12:32:20,Load fasttext model faster by excluding certain vocabulary,<pandas><scikit-learn><nlp><stanford-nlp><fasttext>,,,CC BY-SA 3.0,False,False,True,True,True
12996,44732303,2017-06-24 02:56:11,,"<p>I am trying to build a Docker application that uses Python's gensim library, version 2.1.0, which is being installed via pip from a requirements.txt file.</p>

<p>However, Docker seems to have trouble installing numpy, scipy, and gensim. I googled the error messages and found other users who experienced the same problem, but in other environments. Many of their solutions do not seem to work in Docker.</p>

<p>The following is the error message:</p>

<pre><code>&lt;pre&gt; Step 4 : RUN pip install -r requirements.txt
 ---&gt; Running in a86d07e229d7
Collecting Flask==0.12 (from -r requirements.txt (line 1))
  Downloading Flask-0.12-py2.py3-none-any.whl (82kB)
Collecting requests==2.17.3 (from -r requirements.txt (line 2))
  Downloading requests-2.17.3-py2.py3-none-any.whl (87kB)
Collecting numpy==1.12.1 (from -r requirements.txt (line 3))
  Downloading numpy-1.12.1.zip (4.8MB)
Collecting nltk==3.2.2 (from -r requirements.txt (line 4))
  Downloading nltk-3.2.2.tar.gz (1.2MB)
Collecting scipy==0.19.0 (from -r requirements.txt (line 5))
  Downloading scipy-0.19.0.zip (15.3MB)
    Complete output from command python setup.py egg_info:
    /bin/sh: svnversion: not found
    /bin/sh: svnversion: not found
    non-existing path in 'numpy/distutils': 'site.cfg'
    Could not locate executable gfortran
    Could not locate executable f95
    Could not locate executable ifort
    Could not locate executable ifc
    Could not locate executable lf95
    Could not locate executable pgfortran
    Could not locate executable f90
    Could not locate executable f77
    Could not locate executable fort
    Could not locate executable efort
    Could not locate executable efc
    Could not locate executable g77
    Could not locate executable g95
    Could not locate executable pathf95
    don't know how to compile Fortran code on platform 'posix'
    Running from numpy source directory.
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py:367: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
      run_build = parse_setuppy_commands()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Atlas (http://math-atlas.sourceforge.net/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [atlas]) or by setting
        the ATLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [blas]) or by setting
        the BLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [blas_src]) or by setting
        the BLAS_SRC environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      self.calc_info()
    /usr/local/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'define_macros'
      warnings.warn(msg)
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 416, in &lt;module&gt;
        setup_package()
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 412, in setup_package
        setup(**metadata)
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 320, in __init__
        self.fetch_build_eggs(attrs['setup_requires'])
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 377, in fetch_build_eggs
        replace_conflicting=True,
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 852, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1124, in best_match
        return self.obtain(req, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1136, in obtain
        return installer(requirement)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 445, in fetch_build_egg
        return cmd.easy_install(req)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 673, in easy_install
        return self.install_item(spec, dist.location, tmpdir, deps)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 699, in install_item
        dists = self.install_eggs(spec, download, tmpdir)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 880, in install_eggs
        return self.build_and_install(setup_script, setup_base)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1119, in build_and_install
        self.run_setup(setup_script, setup_base, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1105, in run_setup
        run_setup(setup_script, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 257, in run_setup
        raise
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 170, in save_modules
        saved_exc.resume()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 145, in resume
        six.reraise(type, exc, self._tb)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
        raise value.with_traceback(tb)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program
    /bin/sh: gcc: not found
    /bin/sh: gcc: not found

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-j8py_tat/scipy/
Removing intermediate container a86d07e229d7
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1 &lt;/pre&gt;
</code></pre>

<p>I'm using the <code>python:3.5-alpine</code> image. The versions of the packages are <code>numpy==1.12.1</code>, <code>scipy==0.19.0</code>, and <code>gensim==2.1.0</code>.</p>
",,2017-06-30 12:28:01,"Docker unable to install numpy, scipy, or gensim",<python><numpy><docker><scipy><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
13010,44698910,2017-06-22 12:02:19,,"<p>I'm building a NLP chat application using Doc2Vec technique in Python using its <code>gensim</code> package. I have already done tokenizing and stemming. I want to remove the stop words (to test if it works better) from both the training set as well as the question which user throws. </p>

<p>Here is my code.</p>

<pre><code>import gensim
import nltk
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampl',u'what',u'is'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampl',u'tell',u'me',u'about'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'i',u'am',u'how',u'much',u'can',u'elig',u'claim'],tags=[""SENT_5""])
sentence6 = models.doc2vec.LabeledSentence(words=[u'resign',u'i',u'have',u'how',u'much',u'can',u'i',u'claim',u'elig'],tags=[""SENT_6""])
sentence7 = models.doc2vec.LabeledSentence(words=[u'promot',u'what',u'is',u'my',u'elig',u'post',u'my'],tags=[""SENT_7""])
sentence8 = models.doc2vec.LabeledSentence(words=[u'claim',u'can,',u'i',u'for'],tags=[""SENT_8""])
sentence9 = models.doc2vec.LabeledSentence(words=[u'product',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_9""])
sentence10 = models.doc2vec.LabeledSentence(words=[u'hotel',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_10""])
sentence11 = models.doc2vec.LabeledSentence(words=[u'onlin',u'product',u'can',u'i',u'for',u'bought',u'through',u'claim',u'sampl'],tags=[""SENT_11""])
sentence12 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'guidelin',u'where',u'do',u'i',u'apply',u'form',u'sampl'],tags=[""SENT_12""])
sentence13 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'procedur',u'rule',u'and',u'regul',u'what',u'is',u'the',u'for'],tags=[""SENT_13""])
sentence14 = models.doc2vec.LabeledSentence(words=[u'can',u'i',u'submit',u'expenditur',u'on',u'behalf',u'of',u'my',u'friend',u'and',u'famili',u'claim',u'and',u'reimburs'],tags=[""SENT_14""])
sentence15 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'procedur',u'can',u'i',u'submit',u'from',u'shopper stop',u'claim'],tags=[""SENT_15""])
sentence16 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'can',u'i',u'submit',u'from',u'pantaloon',u'claim'],tags=[""SENT_16""])
sentence17 = models.doc2vec.LabeledSentence(words=[u'invoic',u'procedur',u'can',u'i',u'submit',u'invoic',u'from',u'spencer',u'claim'],tags=[""SENT_17""])

# User asks a question.

document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
#print(type(tokenized_document))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))
sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

# Building vocab.
sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence19]

#I tried to remove the stop words but it didn't work out as LabeledSentence object has no attribute lower.
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
          for document in sentences]
..
</code></pre>

<p>Is there a way I can remove stop words from <code>sentences</code> directly and get a new set of vocab without stop words ?</p>
",,2017-06-22 21:05:31,How to remove stop words from documents in gensim?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
13012,44657908,2017-06-20 16:01:51,,"<p>I am not expert in NLP related models. So I may be missing a simple point here. So please bear with me. </p>

<p>I have obtained topics and the corresponding keywords for each of those topic. I want to first cluster the documents based on these topics. That is, I want to see which document belongs to which topic. Next, given a new document, I need to find out which document clusters it belongs to. 
How do I do it ? </p>

<p>I think we need to pass the new document through the LDA model used to obtain the topics and then use the generated topics to measure its similarity with the previously obtained topics. However, I am not sure if this will work. For example, if the new document is a short document I don't know if it will work. </p>

<p>Any help will be extremely useful. </p>

<p>Thank you. </p>

<p>BTW: I am using Python 2.7 and the gensim package for the LDA algorithm</p>
",,2017-06-20 16:01:51,Deciding which document cluster a new document belong to,<python-2.7><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
13034,44787568,2017-06-27 18:37:50,,"<p>I am trying to determine the optimum number of topics for my LDA model using log perplexity in python. That is, I am graphing the log perplexity for a range of topics and determining the minimum perplexity. However, the graph I have obtained has negative values for log perplexity, when it should have positive values between 0 and 1.</p>

<pre><code>#calculating the log perplexity per word as obtained by gensim code 
##https://radimrehurek.com/gensim/models/atmodel.html
#parameters: pass in trained corpus
#return: graph of perplexity per word for varying number of topics
parameter_list = range(1, 500, 100)
grid ={}

for parameter_value in parameter_list:
model = models.LdaMulticore(corpus=corpus, workers=None, id2word=None, 
                            num_topics=parameter_value, iterations=10)
grid[parameter_value]=[]

perplex=model.log_perplexity(corpus, total_docs=len(corpus))
grid[parameter_value].append(perplex)


df = pd.DataFrame(grid)
ax = plt.figure(figsize=(7, 4), dpi=300).add_subplot(111)
df.iloc[0].transpose().plot(ax=ax,  color=""#254F09"")
plt.xlim(parameter_list[0], parameter_list[-1])
plt.ylabel('Perplexity')
plt.xlabel('topics')
plt.show()     
</code></pre>
",,2017-06-27 18:37:50,Determining log_perplexity using ldamulticore for optimum number of topics,<python-2.7><gensim><lda><topic-modeling><perplexity>,,,CC BY-SA 3.0,False,False,True,False,False
13038,44831480,2017-06-29 17:25:57,,"<p>Using <code>gensim</code> <code>word2vec</code> model in order to calculate similarities between two words. Training the model with a 250mb Wikipedia text gave a good result - about 0.7-0.8 similarity score for a related pair of words.</p>

<p>The problem is when I am using the <code>Phraser</code> model to add up phrases the similarity score drops to nearly zero for the same exact words.</p>

<p><strong>Results with the phrase model:</strong></p>

<pre><code>speed - velocity - 0.0203503432178
high - low - -0.0435703782446
tall - high - -0.0076987978333
nice - good - 0.0368784716958
computer - computational - 0.00487748035808
</code></pre>

<p>That probably means I am not using the Phraser model correctly.</p>

<p><strong>My Code:</strong></p>

<pre><code>    data_set_location = **
    sentences = SentenceIterator(data_set_location)

    # Train phrase locator model
    self.phraser = Phraser(Phrases(sentences))

    # Renewing the iterator because its empty
    sentences = SentenceIterator(data_set_location)

    # Train word to vector model or load it from disk
    self.model = Word2Vec(self.phraser[sentences], size=256, min_count=10, workers=10)



class SentenceIterator(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
                yield line.lower().split()
</code></pre>

<p>Trying the pharser model alone looks like it worked fine:</p>

<p><code>&gt;&gt;&gt;vectorizer.phraser['new', 'york', 'city', 'the', 'san', 'francisco']
['new_york', 'city', 'the', 'san_francisco']</code></p>

<p>What can cause such behavior?</p>

<p><strong>Trying to figure out the solution:</strong></p>

<p>according to gojomo answer, I've tried to create a <code>PhraserIterator</code>:</p>

<pre><code>import os

class PhraseIterator(object):
def __init__(self, dirname, phraser):
    self.dirname = dirname
    self.phraser = phraser

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
            yield self.phraser[line.lower()]
</code></pre>

<p>using this iterator I've tried to train my <code>Word2vec</code> model.</p>

<pre><code>phrase_iterator = PhraseIterator(text_dir, self.phraser)
self.model = Word2Vec(phrase_iterator, size=256, min_count=10, workers=10
</code></pre>

<p>Word2vec training log:</p>

<pre><code>    Using TensorFlow backend.
2017-06-30 19:19:05,388 : INFO : collecting all words and their counts
2017-06-30 19:19:05,456 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
2017-06-30 19:20:30,787 : INFO : collected 6227763 word types from a corpus of 28508701 words (unigram + bigrams) and 84 sentences
2017-06-30 19:20:30,793 : INFO : using 6227763 counts as vocab in Phrases&lt;0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000&gt;
2017-06-30 19:20:30,793 : INFO : source_vocab length 6227763
2017-06-30 19:21:46,573 : INFO : Phraser added 50000 phrasegrams
2017-06-30 19:22:22,015 : INFO : Phraser built with 70065 70065 phrasegrams
2017-06-30 19:22:23,089 : INFO : saving Phraser object under **/Models/word2vec/phrases_model, separately None
2017-06-30 19:22:23,441 : INFO : saved **/Models/word2vec/phrases_model
2017-06-30 19:22:23,442 : INFO : collecting all words and their counts
2017-06-30 19:22:29,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-06-30 19:33:06,667 : INFO : collected 143 word types from a corpus of 163438509 raw words and 84 sentences
2017-06-30 19:33:06,677 : INFO : Loading a fresh vocabulary
2017-06-30 19:33:06,678 : INFO : min_count=10 retains 95 unique words (66% of original 143, drops 48)
2017-06-30 19:33:06,679 : INFO : min_count=10 leaves 163438412 word corpus (99% of original 163438509, drops 97)
2017-06-30 19:33:06,683 : INFO : deleting the raw counts dictionary of 143 items
2017-06-30 19:33:06,683 : INFO : sample=0.001 downsamples 27 most-common words
2017-06-30 19:33:06,683 : INFO : downsampling leaves estimated 30341972 word corpus (18.6% of prior 163438412)
2017-06-30 19:33:06,684 : INFO : estimated required memory for 95 words and 256 dimensions: 242060 bytes
2017-06-30 19:33:06,685 : INFO : resetting layer weights
2017-06-30 19:33:06,724 : INFO : training model with 10 workers on 95 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-06-30 19:33:14,974 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:23,229 : INFO : PROGRESS: at 0.24% examples, 607 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:31,445 : INFO : PROGRESS: at 0.48% examples, 810 words/s, 
...
2017-06-30 20:19:00,864 : INFO : PROGRESS: at 98.57% examples, 1436 words/s, in_qsize 0, out_qsize 1
2017-06-30 20:19:06,193 : INFO : PROGRESS: at 99.05% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:11,886 : INFO : PROGRESS: at 99.29% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:17,648 : INFO : PROGRESS: at 99.52% examples, 1438 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:22,870 : INFO : worker thread finished; awaiting finish of 9 more threads
2017-06-30 20:19:22,908 : INFO : worker thread finished; awaiting finish of 8 more threads
2017-06-30 20:19:22,947 : INFO : worker thread finished; awaiting finish of 7 more threads
2017-06-30 20:19:22,947 : INFO : PROGRESS: at 99.76% examples, 1439 words/s, in_qsize 0, out_qsize 8
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 6 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 5 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 4 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-06-30 20:19:22,949 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-06-30 20:19:22,949 : INFO : training on 817192545 raw words (4004752 effective words) took 2776.2s, 1443 effective words/s
2017-06-30 20:19:22,950 : INFO : saving Word2Vec object under **/Models/word2vec/word2vec_model, separately None
2017-06-30 20:19:22,951 : INFO : not storing attribute syn0norm
2017-06-30 20:19:22,951 : INFO : not storing attribute cum_table
2017-06-30 20:19:22,958 : INFO : saved **/Models/word2vec/word2vec_model
</code></pre>

<p>After this training - any of two similarity calculation produce zero:</p>

<pre><code>speed - velocity - 0
high - low - 0
</code></pre>

<p>So it seems that the iterator is not working well so I've checked it using gojomo trick:</p>

<pre><code>print(sum(1 for _ in s))
1

print(sum(1 for _ in s))
1
</code></pre>

<p>And its working. </p>

<p>What may be the problem?</p>
",2017-07-01 15:06:56,2017-07-01 15:57:13,Word2vec gensim - Calculating similarity between word isn't working when using phrases,<python><deep-learning><gensim><word2vec><phrases>,,,CC BY-SA 3.0,False,False,True,False,False
13052,44849368,2017-06-30 14:56:28,,"<p>I have multiple text files and I am trying to find a way to identify similar bodies of text. The files themselves consist of an ""average"" sized paragraph. On top of this I also have some data that could be used as lables for the data if I were to go down the root of a neural networks such as a saimese network.</p>

<p>While that was one option another possibility I was wondering about was using something such as doc2vec in order to process all of the paragraphs (with the removal of stopwords and such) and then attempting to find similar files of text based upon the cosine from doc2vec. </p>

<p>I was wondering how do the methods outlined above generally compare to each other in terms of results they produce and is doc2vec robust and accurate enough to consider it a viable option? Also I may be overlooking a good method for this.</p>
",,2017-07-10 23:59:22,Training a network to find similar bodies of text,<nlp><nltk><gensim><spacy><doc2vec>,,,CC BY-SA 3.0,True,True,True,False,False
13056,44871728,2017-07-02 14:14:36,,"<p>I used the gensim package in Python to load the pre-trained Google word2vec dataset. I then want to use k-means to find meaningful clusters on my word vectors, and find the representative word for each cluster. I am thinking to use the word whose corresponding vector is closest to the centroid of a cluster to represent that cluster, but don't know whether this is a good idea as my experiment did not give me good results.</p>

<p>My example code is like below:</p>

<pre><code>import gensim
import numpy as np
import pandas as pd
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import pairwise_distances_argmin_min

model = gensim.models.KeyedVectors.load_word2vec_format('/home/Desktop/GoogleNews-vectors-negative300.bin', binary=True)  

K=3

words = [""ship"", ""car"", ""truck"", ""bus"", ""vehicle"", ""bike"", ""tractor"", ""boat"",
       ""apple"", ""banana"", ""fruit"", ""pear"", ""orange"", ""pineapple"", ""watermelon"",
       ""dog"", ""pig"", ""animal"", ""cat"", ""monkey"", ""snake"", ""tiger"", ""rat"", ""duck"", ""rabbit"", ""fox""]
NumOfWords = len(words)

# construct the n-dimentional array for input data, each row is a word vector
x = np.zeros((NumOfWords, model.vector_size))
for i in range(0, NumOfWords):
    x[i,]=model[words[i]] 

# train the k-means model
classifier = MiniBatchKMeans(n_clusters=K, random_state=1, max_iter=100)
classifier.fit(x)

# check whether the words are clustered correctly
print(classifier.predict(x))

# find the index and the distance of the closest points from x to each class centroid
close = pairwise_distances_argmin_min(classifier.cluster_centers_, x, metric='euclidean')
index_closest_points = close[0]
distance_closest_points = close[1]

for i in range(0, K):
    print(""The closest word to the centroid of class {0} is {1}, the distance is {2}"".format(i, words[index_closest_points[i]], distance_closest_points[i]))
</code></pre>

<p>The output is as below:</p>

<pre><code>[2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]
The closest word to the centroid of class 0 is rabbit, the distance is 1.578625818679259
The closest word to the centroid of class 1 is fruit, the distance is 1.8351978219013796
The closest word to the centroid of class 2 is car, the distance is 1.6586030662247868
</code></pre>

<p>In the code I have 3 categories of words: vehicle, fruit and animal. From the output we can see that k-means correctly clustered the words for all 3 categories, but the representative words derived using the centroid method are not very good, as for class 0 I want to see ""animal"" but it gives ""rabbit"", and for class 2 I want to see ""vehicle"" but it returns ""car"".</p>

<p>Any help or suggestion in finding the good representative word for each cluster will be highly appreciated.</p>
",2017-07-02 14:21:18,2017-07-03 11:38:53,How to find the meaningful word to represent each k-means cluster derived from word2vec vectors?,<python><k-means><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
13061,44757639,2017-06-26 10:25:46,,"<p>I am building a data product (an NLP chat application) for which I am learning Flask so that the user can have a better UI to interact with my product. </p>

<p>I have written down the following code in Flask to get the user input and stored it in a variable.</p>

<p><strong>main.py</strong></p>

<pre><code>from flask import Flask, render_template, request
app = Flask(__name__)

@app.route('/')
def index():
   return render_template('init.html')

@app.route('/handle_data', methods = ['POST', 'GET'])
def handle_data():
    userQuestion = request.form['userQuestion']
    print(userQuestion)
    return render_template('init.html', userQuestion = userQuestion)

if __name__ == '__main__':
   app.run()
</code></pre>

<p><strong>init.html</strong></p>

<pre><code>&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;body&gt;


&lt;form action=""{{ url_for('handle_data') }}"" method=""post""&gt;
    &lt;input type=""text"" name=""userQuestion""&gt;
    &lt;input type=""submit""&gt;
&lt;/form&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I've handled the form data and stored it in a variable <code>userQuestion</code>. I want to pass this variable to another python script which contains the code of my training model. </p>

<p><strong>doc2vec_main.py</strong></p>

<pre><code>import gensim
import nltk
import numpy
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'sampling'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampling',u'tell',u'me',u'about',u'sampling'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my',u'elig'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'eligiblity',u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'eligiblity',u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'eligibility',u'claim',u'i',u'am',u'how',u'much',u'can',u'i'],tags=[""SENT_5""])
# ... list of all the training set.

# User inputs a question
document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))

sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence18, sentence19]

model = models.Doc2Vec(size=4, alpha=0.25, min_alpha=.025, min_count=1)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = 
    model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model.docvecs.most_similar([""SENT_19""]))
</code></pre>

<p>My problem is I cannot find a way to connect <code>doc2vec_main.py</code> to <code>main.py</code> and pass the value of <code>userQuestion</code> to the <code>document</code> variable in <code>doc2main.py</code> 
script. That is when a user inputs a question in the form and clicks submit, the value of the form get passed down to <code>document</code> in <code>doc2vec_main.py</code> and the remaining script runs. </p>

<p>I have searched a lot on the internet but it didn't help. Can you suggest me a way to do it? I'm a complete beginner in Flask so forgive me for any mistake.</p>
",,2017-06-27 08:04:26,How to pass HTML form data stored in a variable to a Python script in Flask?,<python><flask><nlp>,,,CC BY-SA 3.0,True,False,True,False,False
13071,44781047,2017-06-27 13:04:51,,"<p>I'm following the 'English Wikipedia' gensim tutorial at <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation</a></p>

<p>where it explains that tf-idf is used during training (at least for LSA, not so clear with LDA).</p>

<p>I expected to apply a tf-idf transformer to new documents, but instead, at the end of the tut, it suggests to simply input a bag-of-words.</p>

<pre><code>doc_lda = lda[doc_bow]
</code></pre>

<p>Does LDA require bag-of-words vectors only?</p>
",,2018-08-20 10:01:09,Necessary to apply TF-IDF to new documents in gensim LDA model?,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13108,44763743,2017-06-26 15:55:56,,"<p>I have thousands of documents with associated tag information. However i also have many documents without tags.</p>

<p>I want to train a model on the documents WITH tags and then apply the trained classifier to the UNTAGGED documents; the classifier will then suggest the most appropriate tags for each UNTAGGED document.</p>

<p>I have done quite a lot of research and there doesn't seem to be a SUPERVISED implementation to document tag classification.</p>

<p>I know NLTK, gensim, word2vec and other libraries will be useful for this problem.</p>

<p>I will be coding the project in Python.</p>

<p>Any help would be greatly appreciated.</p>
",,2017-06-26 18:01:56,supervised tag suggestion for documents,<python><machine-learning><nlp><text-classification>,,,CC BY-SA 3.0,True,False,True,False,False
13110,44910999,2017-07-04 16:54:25,,"<p>I am fairly new to the NLP embedding world. I used gensim's word2vec model and tensorflow vector representation.</p>

<p>I have a question that while training gensim's word2vec model it takes tokenize sentences, while tensorflow takes a long list of words. How does it differ in training. Is there any quality impact?
Also how does then tensorflow cater to the needs of skip-gram as now the data is a list of words and no more sentences. 
I am referring to the tensorflow's tutorial found at link <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec</a></p>

<p>Pardon me if my understanding in this domain is wrong would appreciate if my understanding is cleared.<br>
Thank you for your guidance and help.</p>
",,2017-07-04 16:54:25,how does gensim's word2vec differ from tensorflow vector representation?,<vector><tensorflow><nlp><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
13139,44882814,2017-07-03 10:04:28,,"<p>I have tried lots of different pre-trained models, most of them have 0 documentation the few that have some sort of documentation say that it is possible to load it with</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

word_vectors = KeyedVectors.load_word2vec_format('model102938_that_cannot_be_loaded.bin', binary=True)
</code></pre>

<p>but it never works and I always get a random error.
Is there actually a way to load it? Or am I better off buying a couple hundred server racks and training it myself?</p>
",,2017-07-03 10:21:00,How can I load word2vec vectors?,<nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13168,44993240,2017-07-09 05:19:44,,"<pre><code>def cosine(vector1,vector2):
    cosV12 = np.dot(vector1, vector2) / (linalg.norm(vector1) * linalg.norm(vector2))
    return cosV12
model=gensim.models.doc2vec.Doc2Vec.load('Model_D2V_Game')
string='       ...'
list=string.split(' ')
vector1=model.infer_vector(doc_words=list,alpha=0.1, min_alpha=0.0001,steps=5)
vector2=model.docvecs.doctag_syn0[0]
print cosine(vector2,vector1)
</code></pre>

<p>-0.0232586</p>

<p>I use a train data to train a <code>doc2vec</code> model. Then, I use <code>infer_vector()</code> to generate a vector given a document which is in trained data. But they are different. The value of cosine was so small (<code>-0.0232586</code>) distance between the <code>vector2</code> which was saved in <code>doc2vec</code> model and the <code>vector1</code> which was generated by <code>infer_vector()</code>. But this is not reasonable ah ...</p>

<p><strong>I find where i have error in. I should use 'string=u'       ...'' instead 'string='       ...''. When I correct this way, the cosine distance is up to 0.889342.</strong> </p>
",2017-07-09 06:15:57,2017-07-09 16:15:45,How to use the infer_vector in gensim.doc2vec?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13171,44944249,2017-07-06 08:58:52,,"<p>I'm trying to install <code>pyemd</code> package in Python through <code>pip</code> and getting following error:</p>

<pre><code>C:\Users\dipanwita.neogy&gt;pip install pyemd
Collecting pyemd
  Using cached pyemd-0.4.3.tar.gz
Requirement already satisfied: numpy&lt;2.0.0,&gt;=1.9.0 in c:\users\dipanwita.neogy\a
naconda3\lib\site-packages (from pyemd)
Building wheels for collected packages: pyemd
  Running setup.py bdist_wheel for pyemd ... error
  Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -u
-c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Loca
l\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(
__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __fil
e__, 'exec'))"" bdist_wheel -d C:\Users\DIPANW~1.NEO\AppData\Local\Temp\tmpngn2np
rmpip-wheel- --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win32-3.6
  creating build\lib.win32-3.6\pyemd
  copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
  copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
  running build_ext
  building 'pyemd.emd' extension
  error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C+
+ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

  ----------------------------------------
  Failed building wheel for pyemd
  Running setup.py clean for pyemd
Failed to build pyemd
Installing collected packages: pyemd
  Running setup.py install for pyemd ... error
    Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -
u -c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Lo
cal\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open
)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __f
ile__, 'exec'))"" install --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-e
rihhtfj-record\install-record.txt --single-version-externally-managed --compile:

    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.6
    creating build\lib.win32-3.6\pyemd
    copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
    copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
    running build_ext
    building 'pyemd.emd' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual
C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

    ----------------------------------------
Command ""C:\Users\dipanwita.neogy\Anaconda3\python.exe -u -c ""import setuptools,
 tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Local\\Temp\\pip-build-nk1
3uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read(
).replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install
 --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-erihhtfj-record\install-r
ecord.txt --single-version-externally-managed --compile"" failed with error code
1 in C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-build-nk13uh5b\pyemd\
</code></pre>

<p>I cannot find anything regarding this error. Please suggest me what should I do? </p>
",,2020-03-03 05:08:50,pip install pyemd error?,<python><nlp><pip><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13172,44957082,2017-07-06 19:10:13,,"<p>What is Spacy's built in method of creating vector representations?
I performed NLP on my corpus, and then used .similarity (cosine similarity) to map out documents that were ""similar"". However, I am unsure what method spacy uses to create vector representations. To my knowledge, I am thinking that it is probably word2vec skip-gram with negative sampling, however, I would like to be sure!</p>
",,2017-07-07 10:37:54,What does Spacy use to create vector representations?,<python><nlp><gensim><word2vec><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
13182,44964380,2017-07-07 06:56:08,,"<p>I have a corpus built out of Wikimedia Dump files stored at <strong><em>sentences.txt</em></strong>
I have a sentence say '   !      '</p>

<p>Now when I try to extract the word vectors there is always one or two words which have been missed out while training (despite being included in the list to be trained upon) and I get the KeyError.
Is there any way to improve the training so that it doesn't miss out words that frequently? </p>

<p>Here is a proof that it does happen. <code>tok.wordtokenize</code> is a word tokenizer. <code>sent.drawlist()</code> as well as <code>sents.drawlist()</code> returns a list of sentences from the corpus stored inside <strong><em>sentences.txt</em></strong>. </p>

<hr>

<pre><code>&gt;&gt;&gt; sentence = '   !      '
&gt;&gt;&gt; sentence = tok.wordtokenize(sentence) #tok.wordtokenize() is simply a word tokenizer.
&gt;&gt;&gt; sentences = sent.drawlist()
&gt;&gt;&gt; sentences = [tok.wordtokenize(i) for i in sentences]
&gt;&gt;&gt; sentences2 = sents.drawlist()
&gt;&gt;&gt; sentences2 = [tok.wordtokenize(i) for i in sentences2]
&gt;&gt;&gt; sentences = sentences2 + sentences + sentence
&gt;&gt;&gt; """" in sentences #proof that the word is present inside sentences
True
&gt;&gt;&gt; sentences[0:10] #list of tokenized sentences.
[['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '-', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences, size =10,  min_count=1) 
&gt;&gt;&gt; pred = []
&gt;&gt;&gt; for word in sentence:
...         pred.append(model.wv[word].tolist())
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 574, in __getitem__
    return self.word_vec(words)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 273, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word '' not in vocabulary""
</code></pre>

<hr>

<p>As you can see, I check for the word """" inside the list of tokenized sentences. It is present in the list that I feed into the Word2Vec trainer and yet after training it is not in the vocabulary. </p>
",2017-07-08 22:14:19,2017-07-10 19:51:19,Gensim: Loss of Words/Tokens while Training,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13186,45037860,2017-07-11 14:46:45,,"<p>For gensim(1.0.1) doc2vec, I am trying to load google pre-trained word vectors instead of using <code>Doc2Vec.build_vocab</code> </p>

<pre><code>wordVec_google = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)    
model0 = Doc2Vec(size=300, alpha=0.05, min_alpha=0.05, window=8, min_count=5, workers=4, dm=0, hs=1)    
model0.wv = wordVec_google    
##some other code 
model0.build_vocab(sentences=allEmails, max_vocab_size = 20000)
</code></pre>

<p>but this object <code>model0</code> can not be further trained with ""labeled Docs"", and can't infer vectors for documents. </p>

<p>Anyone knows how to use doc2vec with google pretrained word vectors?<br>
I tried this post:  <a href=""http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a>
but it does not work to load into <code>gensim.models.Word2Vec</code> object, perhaps it is a different gensim version.</p>
",2018-10-23 23:51:20,2018-10-23 23:51:20,gensim(1.0.1) Doc2Vec with google pretrained vectors,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
13193,44983315,2017-07-08 06:40:33,,"<p>I applied LDA from gensim package on the corpus and I get the probability with each term. My problem is how I get only the terms without their probability.
Here is my code:</p>

<pre><code>K = ldamodel.num_topics
t = 0
topicWordProbMat = ldamodel.print_topics(K)
for  topic_dist in topicWordProbMat:
    print('Topic #',t,topic_dist)
    t = t + 1
</code></pre>

<p>The output as example is like this:</p>

<pre><code>Topic # 0 '0.181*things + 0.181*amazon + 0.181*good
Topic # 1 '0.031*nokia + 0.031*microsoft + 0.031*apple  
</code></pre>

<p>and I want it as this:</p>

<pre><code>Topic # 0 things amazon good
Topic # 1 nokia microsoft apple
</code></pre>

<p>any idea how? Thanks in advance</p>
",,2017-07-09 08:40:50,Get topics terms only with LDA,<python-3.x><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13198,44948661,2017-07-06 12:17:46,,"<p>I am new to python and word2vec and keep getting a ""you must first build vocabulary before training the model"" error. What is wrong with my code?</p>

<p>Here is my code:</p>

<pre><code>file_object=open(""SupremeCourt.txt"",""w"")
from gensim.models import word2vec

data = word2vec.Text8Corpus('SupremeCourt.txt')
model = word2vec.Word2Vec(data, size=200)

out=model.most_similar()

print(out[1])
print(out[2])
</code></pre>
",2017-07-06 12:20:32,2017-07-06 20:37:14,Word2Vec Vocabulary not definded error,<python><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13209,44929582,2017-07-05 14:50:39,,"<p>I am using windows 7. I have installed python 2.7 and gensim using (pip install gensim). When I try to import gensim in a python console, I get the following error : </p>

<pre><code>C:\HOMEWARE\Anaconda\lib\site-packages\gensim\utils.py:860:UserWarning:detectedWindows;aliasingchunkizetochunkize_serial
warnings.warn(""detectedWindows;aliasingchunkizetochunkize_serial"")
Traceback(mostrecentcalllast):
File""&lt;stdin&gt;"",line1,in&lt;module&gt;
File""C:\HOMEWARE\Anaconda\lib\site-packages\gensim\__init__.py"",line6,in&lt;module&gt;
fromgensimimportparsing,matutils,interfaces,corpora,models,similarities,summarization
File""C:\HOMEWARE\Anaconda\lib\site-packages\gensim\matutils.py"",line21,in&lt;module&gt;
fromscipy.statsimportentropy
File""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\stats\__init__.py"",line348,in&lt;module&gt;
from.statsimport*
File""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\stats\stats.py"",line175,in&lt;module&gt;
importscipy.specialasspecial
File""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\special\__init__.py"",line640,in&lt;module&gt;
from._ufuncsimport*
ImportError:DLLloadfailed:Thespecifiedmodulecouldnotbefound.
</code></pre>

<p>I have seen similar error on stackoverflow <a href=""https://stackoverflow.com/questions/20201868/importerror-dll-load-failed-the-specified-module-could-not-be-found"">here</a> and <a href=""https://stackoverflow.com/questions/8111664/boost-python-examples-windows-7-x64-importerror-dll-load-failed-the-specifi"">here</a> but it doesn't seem to do the trick for me. </p>

<p>Thank you for your help !</p>
",,2017-10-07 14:54:53,Gensim : ImportError: DLL load failed: The specified module could not be found,<python><windows><installation><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13226,45007902,2017-07-10 09:05:57,,"<p>I want to know the loss for my w2v model and I upgrade <code>gensim</code> to the latest version, but still can't use the argument <code>compute_loss</code>, am I miss something??  </p>

<p><a href=""https://i.stack.imgur.com/Io83T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Io83T.png"" alt=""enter image description here""></a></p>
",,2017-07-13 20:10:12,'Word2Vec' object has no attribute 'compute_loss',<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13235,45046058,2017-07-11 23:34:25,,"<p>I am new to doc2vec and I wish to classify set of texts using it.</p>

<p>I am confused about TaggedDocument and TaggedLineDocument.</p>

<p>1) What is the difference between two? Is it that TaggedLineDocument is collection of TaggedDocuments?</p>

<p>2) If I have a directory containing all the files, How to generate feature vectors for them? Should I create a new file where each line contains text from the file from the directory?</p>
",,2017-07-12 01:36:48,Difference between TaggedDocument and TaggedLineDocument in gensim? and How to work with files in a directory?,<nlp><gensim><word2vec><text-classification><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13249,45159693,2017-07-18 06:59:03,,"<p>I am trying to make a word2vec model by Gensim on Persian language which has ""space"" as the character delimiter, I use python 3.5. The problem that I encounter was I gave a text file as input and it returns a model which only consists of each character separately instead of words. I also gave the input as a list of words which is recommended on :</p>

<p><a href=""https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key]"">Python Gensim word2vec vocabulary key</a></p>

<p>It doesn't work for me and I think it doesn't consider sequence of words in a sentence so it wouldn't be correct.</p>

<p>I did some preprocessing on my input which consist of:</p>

<p>collapse multiple whitespaces into a single one<br>
tokenize by splitting on whitespace<br>
remove words less than 3 characters long
remove stop words</p>

<p>I gave the text to word2vec which gave me result correctly, but I need it on python so my choice is limited to use Gensim.</p>

<p>Also I tried to load the model which made by word2vec source on gensim I get error so I need create the word2vec model by Gensim.</p>

<p>my code is:</p>

<pre><code>  wfile = open('aggregate.txt','r')    
  wfileRead = wfile.read()    
  model = word2vec.Word2Vec(wfileRead , size=100)   
  model.save('Word2Vec.txt')
</code></pre>
",2019-07-07 05:05:36,2019-07-07 05:05:36,word2vec models consist of characters instead of words,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
13257,45069715,2017-07-13 00:47:01,,"<p>I loaded a word2vec model using Google News dataset. Now I want to get the Word2Vec representations of a list of sentences that I wish to cluster. After going through the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a> I found this <code>gensim.models.word2vec.LineSentence</code>but I'm not sure this is what I am looking for. </p>

<p>There should be a way to get word2vec representations of a list of sentences from a pretrained model right? None of the links I searched had anything about it. Any leads would be appreciated. </p>
",,2017-07-13 06:12:44,"After loading a pretrained Word2Vec model, how do I get word2vec representations of new sentences?",<cluster-analysis><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13259,45070186,2017-07-13 01:49:54,,"<p>What I'm trying to do is to ask the user to input a company name, for example Microsoft, and be able to predict that it is in the Computer Software industry. I have around 150 000 names and 60+ industries. Some of the names are not English company names.</p>

<p>I have tried training a Word2Vec model using Gensim based on company names only and averaged up the word vectors before feeding it into SKlearn's logistic regression but had terrible results. My questions are:</p>

<ol>
<li><p>Has anyone tried these kind of tasks? Googling on short text classification shows me results on classifying short sentences instead of pure names. If anyone had tried this before, mind sharing a few keywords or research papers regarding this task?</p></li>
<li><p>Would it be better if I have a brief description for each company instead of only using their names? How much would it help for my Word2Vec model rather than using only the company names?</p></li>
</ol>
",2017-07-18 16:11:45,2017-07-18 16:11:45,Machine learning to classify company names to their industries,<python><machine-learning><text-classification><multilabel-classification>,2017-07-18 16:14:09,,CC BY-SA 3.0,False,False,True,False,True
13270,45091817,2017-07-13 22:20:01,,"<p>There are 2 arrays containing 30,000 vectors and 50000 vectors respectively.</p>

<pre><code>Item_array = [item1,item2,...,item30000]
User_array = [user1,user2,...,user50000]
</code></pre>

<p>Each vector in array is the tfidf value using Gensim.</p>

<p>For example:  </p>

<pre><code>Item_array[0] = [(0, 0.03663947221807151),(2, 0.09781584692664856),(10, 0.07212302141012294)]
</code></pre>

<p>I'm trying to build a item-user matrix for sorting each user's similar items.
For-loop method cost me lots of time to finish it.</p>

<p>How should i handle this efficiently. Any help should be appreciable..</p>

<p>It's my expected output:</p>

<pre><code>           user1 user2 user3 ... user50000
item1      0.35  0.45  0.86        0.46
item2      0.42  0.32  0.53        0.53          
item3      0.65  0.33  0.45        0.46        
...        ...   ...   ...         ... 
item50000  0.54  0.33  0.00        1.00
</code></pre>
",,2017-07-13 22:24:34,Python: How to get cosine similarity matrix efficiently,<python><numpy><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
13272,45164340,2017-07-18 10:36:34,,"<p>I am new to Gensim Word2Vec. I was trying to use Word2Vec to build word vectors for some raw html files. So I first convert the html file into txt file.</p>

<h3>My First Question:</h3>

<p>When I train the word2vec model, everything is fine. But when I want to test the accuracy of the model by doing</p>

<pre><code>model.accuracy(file_name)
</code></pre>

<p>it produced error: </p>

<pre><code>Traceback (most recent call last):
  File ""build_w2v.py"", line 82, in &lt;module&gt;
    main()
  File ""build_w2v.py"", line 77, in main
    gen_w2v_model()
  File ""build_w2v.py"", line 71, in gen_w2v_model
    accuracy = model.accuracy(target)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1330, in accuracy
    return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 679, in accuracy
    raise ValueError(""missing section header before line #%i in %s"" % (line_no, questions))
ValueError: missing section header before line #0
</code></pre>

<p>Below is the sample file:</p>

<pre><code>zGR='ca-about-health_js';var ziRfw=0;zobt="" Vision Ads"";zOBT="" Ads"";function zIpSS(u){zpu(0,u,280,375,""ssWin"")}function zIlb(l,t,f){zT(l,'18/1Pp/wX')}


zWASL=1;zGRH=1
#rs{margin:0 0 10px}#rs #n5{font-weight:bold}#rs a{padding:7px;text-transform:capitalize}Poking Eyelashes - Poking Eyelashes Problem


&lt;!--
zGOW=0;xd=0;zap="""";zAth='25752';zAthG='25752';zTt='11';zir='';zBTS=0;zBT=0;zSt='';zGz=''
ch='health';gs='vision';xg=""Vision"";zcs=''
zFDT='0'
zFST='0'
zOr='BA15WT26OkWA0O1b';zTbO=zRQO=1;zp0=zp1=zp2=zp3=zfs=0;zDc=1;
zSm=zSu=zhc=zpb=zgs=zdn='';zFS='BA110BA0110B00101';zFD='BA110BA0110B00101'
zDO=zis=1;zpid=zi=zRf=ztp=zpo=0;zdx=20;zfx=100;zJs=0;
zi=1;zz=';336280=2-1-1299;72890=2-1-1299;336155=2-1-12-1;93048=2-1-12-1;30050=2-1-12-1';zx='100';zde=15;zdp=1440;zds=1440;zfp=0;zfs=66;zfd=100;zdd=20;zaX=new Array(11, new Array(100,1051,8192,2,'336,300'),7, new Array(100,284,8196,12,'336,400'));zDc=1;;zDO=1;;zD336=1;zhc='';;zGTH=1;
zGo=0;zG=17;zTac=2;zDot=0;
zObT=""Vision"";zRad=5;var tp="" primedia_""+(zBT?"""":""non_"")+""site_targeting"";if(!this.zGCID)zGCID=tp
else zGCID+=tp;
if(zBT&gt;0){zOBR=1}
if(!this.uy)uy='about.com';if(typeof document.domain!=""undefined"")document.domain=uy;//--&gt;


function zob(p){if(!this.zOfs)return;var a=zOfs,t,i=0,l=a.length;if(l){w('&lt;div id=""oF""&gt;&lt;b&gt;'+(this.zobt?zobt:xg+' Ads')+'&lt;/b&gt;&lt;ul&gt;');while((i&lt;l)&amp;&amp;i&lt;zRad){t=a[i++].line1;w('&lt;li&gt;&lt;a href=""/z/js/o'+(p?p:'')+'.htm?k='+zUriS(t.toLowerCase())+(this.zobr?zobr:'')+'&amp;d='+zUriS(t)+'&amp;r='+zUriS(zWl)+'"" target=""_'+(this.zOBNW?'new'+zr(9999):'top')+'""&gt;'+t+'&lt;/a&gt;&lt;/li&gt;');}w('&lt;/ul&gt;&lt;/div&gt;')}}function rb600(){if(gEI('bb'))gEI('bb').height=600}zJs=10
zJs=11
zJs=12
zJs=13
zc(5,'jsc',zJs,9999999,'')
zDO=0
</code></pre>

<p>So This file actually begins with many (I don't know) space or \n. When I open in the vim.<a href=""https://i.stack.imgur.com/jOz4T.png"" rel=""nofollow noreferrer"">It looks like this</a>.</p>

<p><strong>So what is the problem here?</strong></p>

<h3>My second question:</h3>

<p>Also, I am doing text classification of some biomedical papers. The files I was given are all raw html files in either Japanese or English. After I do the ascii conversion and some stop_words cleaning, there are still many HTML code left in the file. </p>

<p>When I try to clean these files and restrict the characters to [a-zA-Z0-9], I found some medical terms like [4protein...] or something get not properly cleaned as well.</p>

<p><strong>Are there any suggestions in how to clean up these files?</strong></p>
",,2017-07-18 16:46:33,Gensim Word2Vec Error: ValueError: missing section header before line #0,<python><html><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13277,45125798,2017-07-16 06:35:56,,"<p>I have two directories from which I want to read their text files and label them, but I don't know how to do this via <code>TaggedDocument</code>. I thought it would work as TaggedDocument([Strings],[Labels]) but this doesn't work apparently. </p>

<p>This is my code: </p>

<pre><code>from gensim import models
from gensim.models.doc2vec import TaggedDocument
import utilities as util
import os
from sklearn import svm
from nltk.tokenize import sent_tokenize
CogPath = ""./FixedCog/""
NotCogPath = ""./FixedNotCog/""
SamplePath =""./Sample/""
docs = []
tags = []
CogList = [p for p in os.listdir(CogPath) if p.endswith('.txt')]
NotCogList = [p for p in os.listdir(NotCogPath) if p.endswith('.txt')]
SampleList = [p for p in os.listdir(SamplePath) if p.endswith('.txt')]
for doc in CogList:
     str = open(CogPath+doc,'r').read().decode(""utf-8"")
     docs.append(str)
     print docs
     tags.append(doc)
     print ""###########""
     print tags
     print ""!!!!!!!!!!!""
for doc in NotCogList:
     str = open(NotCogPath+doc,'r').read().decode(""utf-8"")
     docs.append(str)
     tags.append(doc)
for doc in SampleList:
     str = open(SamplePath + doc, 'r').read().decode(""utf-8"")
     docs.append(str)
     tags.append(doc)

T = TaggedDocument(docs,tags)

model = models.Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1,size=50)
</code></pre>

<p>and this is the error I get: </p>

<pre><code>Traceback (most recent call last):
  File ""/home/farhood/PycharmProjects/word2vec_prj/doc2vec.py"", line 34, in &lt;module&gt;
    model = models.Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1,size=50)
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 635, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 544, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 674, in scan_vocab
    if isinstance(document.words, string_types):
AttributeError: 'list' object has no attribute 'words'
</code></pre>
",2018-12-15 19:32:14,2019-07-22 20:13:46,How to use TaggedDocument in gensim?,<python><nltk><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,True
13288,45186094,2017-07-19 09:16:06,,"<p>I want to use Word2vec in a web server (production) in two different variants where I fetch two sentences from the web and compare it in real-time. For now, I am testing it on a local machine which has 16GB RAM. </p>

<p>Scenario:
w2v = load w2v model </p>

<pre><code>If condition 1 is true:
   if normalize:
      reverse normalize by w2v.init_sims(replace=False) (not sure if it will work)
   Loop through some items:
   calculate their vectors using w2v
else if condition 2 is true:
   if not normalized:
       w2v.init_sims(replace=True)
   Loop through some items:
   calculate their vectors using w2v
</code></pre>

<p>I have already read the solution about reducing the vocabulary size to a small size but I would like to use all the vocabulary. </p>

<p>Are there new workarounds on how to handle this? Is there a way to initially load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary? </p>
",2017-07-20 08:28:14,2018-06-04 20:45:33,How to make word2vec model's loading time and memory use more efficient?,<python><nlp><nltk><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
13303,45170589,2017-07-18 15:05:37,,"<p>When training, what will word2vec do to cope with the words at the end of a sentence . Will it use the exact words at the beginning of another sentence as the context words of the center words which is 
at the end of last sentence.  </p>
",,2017-07-18 16:48:58,How word2vec deal with the end of a sentence,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13316,45193550,2017-07-19 14:26:08,,"<p>I recently updated a conda environment from python=3.4 to python 3.6. The environment is made for a project using <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> wich worked perfectly on 3.4. After this update, using the library generates multiple errors such as:</p>

<pre><code>TypeError: object of type 'itertools.chain' has no len()
</code></pre>

<p>or</p>

<pre><code>AssertionError: decomposition not initialized yet
</code></pre>

<p>Do you guys know why this happens while gensim explicitly says python 3.5 and 3.6 are supported ?</p>

<p>The used code:</p>

<pre><code># Create Texts
texts = src.data.raw.extract_clean_merge_titles_abstracts(papers)
src.data.raw.train_phraser(texts)
texts = src.data.raw.tokenize_stream(texts)

print(""Size of corpus: "", len(texts)) # ERROR 1 HERE

# Create Dictionary
dictionary = gensim.corpora.dictionary.Dictionary(texts, prune_at=None)
dictionary.filter_extremes(no_below=3 ,no_above=0.1, keep_n=None)
dictionary.compactify()
print(dictionary)
dictionary.save(config.paths.PATH_DATA_GENSIM_TEMP_DICTIONARY)

# Create corpus
corpus = [dictionary.doc2bow(text) for text in texts]
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS, corpus)
corpus_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX, corpus, num_features=len(dictionary))
corpus_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX)

#tf-idf
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]    #gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF, corpus_tfidf)
tfidf.save(config.paths.PATH_DATA_GENSIM_TEMP_TFIDF)
corpus_tfidf_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX, corpus_tfidf, num_features=len(dictionary))
corpus_tfidf_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX)

# lsa
lsa_num_topics = 100
lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=lsa_num_topics)
corpus_lsa = lsa[corpus_tfidf] # ERROR 2 HERE
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA, corpus_lsa)
lsa.save(config.paths.PATH_DATA_GENSIM_TEMP_LSA)
corpus_lsa_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX, corpus_lsa, num_features=lsa_num_topics)
corpus_lsa_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX)
</code></pre>

<p>Here is the list of the packages installed:</p>

<pre><code>bkcharts                  0.2                      py36_0  
bokeh                     0.12.6                   py36_0  
boto                      2.47.0                   py36_0  
bz2file                   0.98                     py36_0  
cycler                    0.10.0                   py36_0  
dbus                      1.8.20                        1    ostrokach
decorator                 4.0.11                   py36_0  
expat                     2.1.0                         0    ostrokach
fontconfig                2.12.1                        3  
freetype                  2.5.5                         2  
gensim                    2.2.0               np113py36_0  
gettext                   0.19.5                        2    ostrokach
glib                      2.48.2                        0    ostrokach
gst-plugins-base          1.8.0                         0  
gstreamer                 1.8.0                         0  
icu                       54.1                          0    ostrokach
jinja2                    2.9.6                    py36_0  
jpeg                      9b                            0  
libffi                    3.2.1                         8    ostrokach
libgcc                    5.2.0                         0  
libgfortran               3.0.0                         1  
libiconv                  1.14                          0  
libpng                    1.6.27                        0  
libsigcpp                 2.4.1                         3    ostrokach
libxcb                    1.12                          1  
libxml2                   2.9.4                         0  
markupsafe                0.23                     py36_2  
matplotlib                2.0.2               np113py36_0  
mkl                       2017.0.3                      0  
networkx                  1.11                     py36_0  
nltk                      3.2.4                    py36_0  
numpy                     1.13.1                   py36_0  
openssl                   1.0.2l                        0  
pcre                      8.39                          1  
pip                       9.0.1                    py36_1  
pymysql                   0.7.9                    py36_0  
pyparsing                 2.1.4                    py36_0  
pyqt                      5.6.0                    py36_2  
python                    3.6.1                         2  
python-dateutil           2.6.0                    py36_0  
pytz                      2017.2                   py36_0  
pyyaml                    3.12                     py36_0  
qt                        5.6.2                         4  
readline                  6.2                           2  
requests                  2.14.2                   py36_0  
scikit-learn              0.18.2              np113py36_0  
scipy                     0.19.1              np113py36_0  
setuptools                27.2.0                   py36_0  
sip                       4.18                     py36_0  
six                       1.10.0                   py36_0  
smart_open                1.5.3                    py36_0  
sqlite                    3.13.0                        0  
system                    5.8                           2  
tk                        8.5.18                        0  
tornado                   4.5.1                    py36_0  
wheel                     0.29.0                   py36_0  
xz                        5.2.2                         1  
yaml                      0.1.6                         0  
zlib                      1.2.8                         3  
</code></pre>
",2017-07-20 07:27:28,2017-07-20 08:02:28,Gensim errors after updating python version with conda,<python-3.x><conda><gensim>,,,CC BY-SA 3.0,True,False,True,False,True
13326,45102484,2017-07-14 12:01:35,,"<p>I have the predict_output_word method from the official github repository. which takes only wod2vec models trained with skip-gram and tries to predict the middle word by summing the vectors of all the input word's indices and 
divids this by the length of np_sum of the input word indices. Then you consider output and take softmax to get probabilities of the predicted word after you sum all these probabilities to get the most likely word. Is there a better way to approach this in other to get better words since this gives very bad results for shorter sentences.
below is the code from github.</p>

<pre><code>def predict_output_word(model, context_words_list, topn=10):

from numpy import exp,  dtype, float32 as REAL,\
ndarray, empty, sum as np_sum,
from gensim import utils, matutils 

""""""Report the probability distribution of the center word given the context words as input to the trained model.""""""
if not model.negative:
    raise RuntimeError(""We have currently only implemented predict_output_word ""
        ""for the negative sampling scheme, so you need to have ""
        ""run word2vec with negative &gt; 0 for this to work."")

if not hasattr(model.wv, 'syn0') or not hasattr(model, 'syn1neg'):
    raise RuntimeError(""Parameters required for predicting the output words not found."")

word_vocabs = [model.wv.vocab[w] for w in context_words_list if w in model.wv.vocab]
if not word_vocabs:
    warnings.warn(""All the input context words are out-of-vocabulary for the current model."")
    return None


word2_indices = [word.index for word in word_vocabs]

#sum all the indices
l1 = np_sum(model.wv.syn0[word2_indices], axis=0)

if word2_indices and model.cbow_mean:
    #l1 = l1 / len(word2_indices)
    l1 /= len(word2_indices)

prob_values = exp(dot(l1, model.syn1neg.T))     # propagate hidden -&gt; output and take softmax to get probabilities
prob_values /= sum(prob_values)
top_indices = matutils.argsort(prob_values, topn=topn, reverse=True)

return [(model.wv.index2word[index1], prob_values[index1]) for index1 in top_indices]   #returning the most probable output words with their probabilities
</code></pre>
",,2017-07-14 16:30:05,Predict middle word word2vec,<machine-learning><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13328,45195169,2017-07-19 15:37:04,,"<p>I am trying to run doc2vec library from gensim package. My problem is that when I am training and saving the model the model file is rather large(2.5 GB) I tried using this line :</p>

<pre><code>model.estimate_memory()
</code></pre>

<p>But it didn't change anything. I also have tried to change max_vocab_size to decrease the space. But there was not luck. Can somebody help me with this matter?</p>
",,2017-07-19 17:48:30,Gensim Doc2Vec generating huge file for model,<python><semantics><gensim><word2vec><doc2vec>,2017-07-19 20:34:04,,CC BY-SA 3.0,False,False,True,False,False
13340,45243316,2017-07-21 17:19:59,,"<p>I did document similarity on my corpus using Doc2Vec and it outputting not that good of similarities. I was wondering if I could do a topic model from what Doc2Vec is giving me to increase the accuracy of my model in order to get better similarities? </p>
",,2017-07-21 23:46:52,Can I create a topic model (such as LDA) from the output of doc2vec model?,<nlp><gensim><lda><topic-modeling><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13345,45108291,2017-07-14 17:05:14,,"<p>I would like to use embeddings made by w2v in order to obtain the most likely substitute words GIVEN a context (surrounding words), rather than supplying an individual word.</p>

<p>Example:
sentence = 'I would like to go to the park tomorrow after school'</p>

<p>If I want to find candidates similar to ""park"", typically I would just leverage the similarity function from the Gensim model</p>

<pre><code>model.most_similar('park')
</code></pre>

<p>and obtain semantically similar words. However this could give me similar words to the verb 'park' instead of the noun 'park', which I was after.</p>

<p>Is there any way to query the model and give it surrounding words as context to provide better candidates?</p>
",,2017-07-15 01:55:41,python word2vec context similarity using surrounding words,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
13350,45152693,2017-07-17 20:01:01,,"<p>I have a panda series around 300k lines (one text per line). I wanted to extract the output array for each line of a trained W2V on a testing data set using the following code:</p>

<pre><code>import gensim

w2v = gensim.models.Word2Vec(list(X_train), hs=1, negative=0)
l = [w2v.score(e) for e in list(X_test)]
</code></pre>

<p>Here is the error I received (when it does not kill my ipython session)</p>

<pre><code>error: can't start new thread
</code></pre>

<p>How can I solve this?</p>
",2017-07-17 20:20:33,2017-07-17 20:20:33,Gensim Word2Vec - can't start new thread,<python><multithreading><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13352,45154833,2017-07-17 22:50:11,,"<p>As you know, the skip-gram model learns vector representations of elements based on long sequences of elements and the contexts of each. This model has most commonly been applied to natural language by concatenating giant collections of text. These documents of are often concatenated into a single very long line of text, with no distinction of when a new document begins and ends. This ends up not being much of an issue in NLP because the percentage of the model training instances involving overlapping documents is a small percentage of the total number of instances. In Education data, this overlap can be much higher because of shorter sequences and high numbers of users (formerly ""documents"" in NLP). This is also a problem in other behavioral datasets, not just education. The problem manifests itself when inspecting the learned vectors and finding that the model has determined that many of the students' first encountered elements are very similar to the students' very last encountered elements. This is a bi-product of the ""wrapping"" of lines in the input to gensim (instances spanning the end of one student's sequence and the beginning of another). How can I identify where in the code this overlapping occurs and prohibit this overlap from happening during training in gensim. </p>
",,2017-07-17 23:24:20,how to prevent overlapping in word2vec?,<python><nlp><gensim><word2vec>,2017-07-18 08:55:16,,CC BY-SA 3.0,False,False,True,False,False
13384,45310409,2017-07-25 17:51:48,,"<p>I need to use gensim to get vector representations of words, and I figure the best thing to use would be a word2vec module that's pre-trained on the english wikipedia corpus. Does anyone know where to download it, how to install it, and how to use gensim to create the vectors?</p>
",,2017-12-14 04:26:24,Using a Word2Vec model pre-trained on wikipedia,<wikipedia><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13385,45310925,2017-07-25 18:21:31,,"<p>When I train my lda model as such</p>

<pre><code>dictionary = corpora.Dictionary(data)
corpus = [dictionary.doc2bow(doc) for doc in data]
num_cores = multiprocessing.cpu_count()
num_topics = 50
lda = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, 
workers=num_cores, alpha=1e-5, eta=5e-1)
</code></pre>

<p>I want to get a full topic distribution for all <code>num_topics</code> for each and every document. That is, in this particular case, I want each document to have 50 topics contributing to the distribution <strong><em>and</em></strong> I want to be able to access all 50 topics' contribution. This output is what LDA should do if adhering strictly to the mathematics of LDA. However, gensim only outputs topics that exceed a certain threshold as shown <strong><a href=""https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396"">here</a></strong>. For example, if I try</p>

<pre><code>lda[corpus[89]]
&gt;&gt;&gt; [(2, 0.38951721864890398), (9, 0.15438596408262636), (37, 0.45607443684895665)]
</code></pre>

<p>which shows only 3 topics that contribute most to document 89. I have tried the solution in the link above, however this does not work for me. I still get the same output:</p>

<pre><code>theta, _ = lda.inference(corpus)
theta /= theta.sum(axis=1)[:, None]
</code></pre>

<p>produces the same output i.e. only 2,3 topics per document.</p>

<p>My question is how do I change this threshold so I can access the <strong><em>FULL</em></strong> topic distribution for <strong><em>each</em></strong> document? How can I access the full topic distribution, no matter how insignificant the contribution of a topic to a document? The reason I want the full distribution is so I can perform a <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""noreferrer"">KL similarity</a> search between documents' distribution.</p>

<p>Thanks in advance</p>
",,2018-10-19 13:42:25,How to get a complete topic distribution for a document using gensim LDA?,<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13388,45234310,2017-07-21 09:40:29,,"<p>I'm training a <code>Word2Vec</code> model like:</p>

<pre><code>model = Word2Vec(documents, size=200, window=5, min_count=0, workers=4, iter=5, sg=1)
</code></pre>

<p>and <code>Doc2Vec</code> model like:</p>

<pre><code>doc2vec_model = Doc2Vec(size=200, window=5, min_count=0, iter=5, workers=4, dm=1)
doc2vec_model.build_vocab(doc2vec_tagged_documents)
doc2vec_model.train(doc2vec_tagged_documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)
</code></pre>

<p>with the <strong>same data</strong> and comparable parameters.</p>

<p>After this I'm using these models for my classification task. And I have found out that simply averaging or summing the <code>word2vec</code> embeddings of a document performs considerably better than using the <code>doc2vec</code> vectors.  I also tried with much more <code>doc2vec</code> iterations (25, 80 and 150 - makes no difference).</p>

<p>Any tips or ideas why and how to improve <code>doc2vec</code> results?</p>

<p><strong>Update</strong>: This is how <code>doc2vec_tagged_documents</code> is created:</p>

<pre><code>doc2vec_tagged_documents = list()
counter = 0
for document in documents:
    doc2vec_tagged_documents.append(TaggedDocument(document, [counter]))
    counter += 1
</code></pre>

<p><strong>Some more facts about my data:</strong></p>

<ul>
<li>My training data contains 4000 documents</li>
<li>with 900 words on average.</li>
<li>My vocabulary size is about 1000 words.</li>
<li>My data for the classification task is much smaller on average (12 words on average), but I also tried to split the training data to lines and train the <code>doc2vec</code> model like this, but it's almost the same result.</li>
<li>My data is <strong>not</strong> about natural language, please keep this in mind.</li>
</ul>
",2017-07-22 13:14:19,2017-07-22 13:14:19,Doc2Vec Worse Than Mean or Sum of Word2Vec Vectors,<python><machine-learning><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13395,45276029,2017-07-24 08:47:11,,"<p>According to the <strong>gensim.models.Word2Vec</strong> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">API reference</a>, ""compute_loss"" is a valid keyword. However, I get an error that says it's an <code>unexpected keyword</code>.</p>

<p><strong>UPDATE</strong>:</p>

<p>The Word2Vec class on GitHub <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">does have</a> the 'compute_loss' keyword, but my local library does not.
I see that the gensim documentation and library deviate from each other.
I found that the <code>win-64/gensim-2.2.0-np113py35_0.tar.bz2</code>-file in <a href=""https://anaconda.org/anaconda/gensim/files"" rel=""nofollow noreferrer"">conda repository</a> is not up to date.</p>

<p>However after uninstalling gensim with conda, <code>pip install gensim</code> did not change anything as it still doesn't work.</p>

<p>Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p><strong>/END OF UPDATE</strong></p>

<p>I followed and downloaded the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">tutorial notebook on Word2Vec</a>.</p>

<p>In input [25], first cell after ""Training Loss Computation"" headline, I get an error in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> class' initializer. </p>

<p>Input:</p>

<pre><code># instantiating and training the Word2Vec model
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, 
compute_loss=True, hs=0, sg=1, seed=42)

# getting the training loss value
training_loss = model_with_loss.get_latest_training_loss()
print(training_loss)
</code></pre>

<p>Output:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-c2933abf4b08&gt; in &lt;module&gt;()
      1 # instantiating and training the Word2Vec model
----&gt; 2 model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)
      3 
      4 # getting the training loss value
      5 training_loss = model_with_loss.get_latest_training_loss()

TypeError: __init__() got an unexpected keyword argument 'compute_loss'
</code></pre>

<p>I have gensim 2.2.0 installed via conda and have a new new clone from the gensim repository (with the tutorial notebook). I'm using 64-bit Python 3.5.3 on windows 10. (Anaconda)</p>

<p>I've tried to search for others with same encounter, but I haven't been successful. </p>

<p>Do you know the reason for this, and how to fix this? Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p>I've also previously <a href=""https://groups.google.com/forum/#!topic/gensim/J1J2zTZwD7Q"" rel=""nofollow noreferrer"">posted the question</a> in the official mailing list.</p>
",2017-07-25 10:44:31,2017-07-31 09:04:15,Why doesn't gensim's Word2Vec recognize 'compute_loss' keyword?,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13399,45346233,2017-07-27 09:10:01,,"<p>I wonder how the <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">similarity</a> works with gensim ? How the different shards are created and does it increase performance when looking only for top-N similar document ? More generally, is there a documentation about the internal structures of gensim ?</p>
",2017-07-27 12:56:46,2017-07-27 17:32:34,Gensim's similarity: how does it work?,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13400,45346418,2017-07-27 09:17:59,,"<p>I am trying to generate word vectors using PySpark. Using gensim I can see the words and the closest words as below:</p>



<pre class=""lang-python prettyprint-override""><code>sentences = open(os.getcwd() + ""/tweets.txt"").read().splitlines()
w2v_input=[]
for i in sentences:
    tokenised=i.split()
    w2v_input.append(tokenised)
model = word2vec.Word2Vec(w2v_input)
for key in model.wv.vocab.keys():
    print key
    print model.most_similar(positive=[key])
</code></pre>

<p>Using PySpark</p>

<pre class=""lang-python prettyprint-override""><code>inp = sc.textFile(""tweet.txt"").map(lambda row: row.split("" ""))
word2vec = Word2Vec()
model = word2vec.fit(inp)
</code></pre>

<p>How can I generate the words from the vector space in model? That is the pyspark equivalent of the gensim <code>model.wv.vocab.keys()</code>?</p>

<p>Background: I need to store the words and the synonyms from the model in a map so I can use them later for finding the sentiment of a tweet. I cannot reuse the word-vector model in the map functions in pyspark as the model belongs to the spark context (error pasted below). I want the pyspark word2vec version instead of gensim because it provides better synonyms for certain test words.</p>

<pre class=""lang-python prettyprint-override""><code> Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation.SparkContext can only be used on the driver, not in code that it run on workers.
</code></pre>

<p>Any alternative solution is also welcome.</p>
",2018-01-19 22:51:23,2019-12-09 10:46:40,How to obtain the word list from pyspark word2vec model?,<apache-spark><nlp><pyspark><apache-spark-mllib><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13413,45289256,2017-07-24 20:13:05,,"<p>I have a big dataset and I'm trying to run Word2Vec model on it, but the vocabulary is constantly lowered to just 28.</p>

<pre><code>&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=1,trim_rule=None, workers=4,sg=0, hs=1)
&gt;&gt;&gt; len(model.wv.vocab)
28
</code></pre>

<p>I've tried with different setup of the constructor still the same.</p>

<p>My dataset consists of machine logs:</p>

<pre><code>wc eventlog_dataset
  4421775 124189284 978608310 eventlog_dataset
</code></pre>

<p>I previously ran tfidf model on this same dataset and I know for sure that I have ~100k unique words.</p>

<p>When I use a different dataset in gensim I have no such problem, so I definately know that the problem is my dataset, but I don't know why exactly...</p>

<p>Here's a sample :</p>

<pre><code>2017-05-16 10:55:58.91 CDT     3 61617032 Notification    Minor           Command error   sw_cli     {user super all {{0 8}} -1 10.0.188.216 3136} {Command: getfs  Error: Error: File Services is not configured on this array.} {}
2017-05-16 10:55:32.58 CDT     3 61616917 Notification    Minor           Command error   sw_cli     {user super all {{0 8}} -1 10.0.51.11 3727} {Command: getcage -e cage12 Error:    Opcode         = SCCMD_DOCDB    Node           = 253    Tpd error code = TE_INVALID          -- Invalid input parameter    Tpd error info = Cage (cage12) does not support this function } {}
</code></pre>

<p>As per the gensim documentation <code>trim_rule=None,min_count=1</code> should leave the full vocabulary.</p>

<p>Has anyone had such problems on datasets before ?</p>

<p><strong>EDIT</strong></p>

<p>Here's the code </p>

<pre><code>class FileToSent(object):
    def __init__(self, filename):
        self.filename = filename
       def __iter__(self):
            for line in open(self.filename, 'r'):
             ll = [i for i in unicode(line, 'utf-8').lower().split()]
             print ll
            yield ll


    sentences = FileToSent('/home/veselin/eventlog_dataset')
    model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=2,workers=4, hs=1)
</code></pre>

<p>And here's the output from the first line :</p>

<pre><code>/usr/bin/python2.7 /home/veselin/PycharmProjects/test/word2vec.py
[u'2016-10-16', u'17:55:19.55', u'cest', u'1', u'1788217', u'notification', u'minor', u'cli', u'command', u'error', u'sw_cli', u'{3parsvc', u'super', u'all', u'{{0', u'8}}', u'-1', u'172.16.24.110', u'12539}', u'{command:', u'getsralertcrit', u'all', u'error:', u'this', u'system', u'is', u'not', u'licensed', u'for', u'system', u'reporter', u'features}', u'{}']
</code></pre>

<p>You can see that words like cli,system or license, etc are not included in the vocabulary.</p>

<p>INFO logging (on full dataset)</p>

<pre><code>/usr/bin/python2.7 /home/veselin/PycharmProjects/test/word2vec.py
2017-07-28 11:32:56,966 : INFO : collecting all words and their counts
2017-07-28 11:33:35,580 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-07-28 11:33:35,582 : INFO : collected 28 word types from a corpus of 29 raw words and 1 sentences
2017-07-28 11:33:35,582 : INFO : Loading a fresh vocabulary
2017-07-28 11:33:35,582 : INFO : min_count=2 retains 1 unique words (3% of original 28, drops 27)
2017-07-28 11:33:35,582 : INFO : min_count=2 leaves 2 word corpus (6% of original 29, drops 27)
2017-07-28 11:33:35,583 : INFO : deleting the raw counts dictionary of 28 items
2017-07-28 11:33:35,584 : INFO : sample=0.001 downsamples 1 most-common words
2017-07-28 11:33:35,584 : INFO : downsampling leaves estimated 0 word corpus (3.3% of prior 2)
2017-07-28 11:33:35,584 : INFO : estimated required memory for 1 words and 100 dimensions: 1900 bytes
2017-07-28 11:33:35,584 : INFO : constructing a huffman tree from 1 words
2017-07-28 11:33:35,585 : INFO : built huffman tree with maximum node depth 0
2017-07-28 11:33:35,585 : INFO : resetting layer weights
2017-07-28 11:33:35,585 : INFO : training model with 4 workers on 1 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5
2017-07-28 11:36:43,871 : INFO : PROGRESS: at 100.00% examples, 0 words/s, in_qsize 2, out_qsize 2
2017-07-28 11:36:43,872 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-07-28 11:36:43,873 : INFO : training on 145 raw words (0 effective words) took 188.3s, 0 effective words/s
2017-07-28 11:36:43,873 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay

Process finished with exit code 0
</code></pre>
",2017-07-27 17:34:09,2020-01-13 15:05:14,gensim is always trimming the vocabulary,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13416,45352522,2017-07-27 13:39:51,,"<p>I am trying to install gensim in Python on my Ubuntu. I tried with  easy_install but getting errors. Could someone help with identifying what is going wrong?</p>

<p><strong>easy_install</strong></p>

<pre><code>easy_install -U gensim

Running scipy-0.19.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-    QXO1dA/scipy-0.19.1/egg-dist-tmp-AxijnA
/tmp/easy_install-QXO1dA/scipy-0.19.1/setup.py:323: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates 

warnings.warn(""Unrecognized setuptools command, proceeding with ""                                                                 /usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found.                                                                     
Directories to search for the libraries can be specified in the                                                                     numpy/distutils/site.cfg file (section [atlas]) or by setting                                                                       the ATLAS environment variable.    

self.calc_info()                                                                                                                    
/usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning:                                                 
Lapack (http://www.netlib.org/lapack/) libraries not found.                                                                          
Directories to search for the libraries can be specified in the                                                                     
numpy/distutils/site.cfg file (section [lapack]) or by setting                                                                      
the LAPACK environment variable.                                                                                                  
self.calc_info()                                                                                                                  
/usr/local/lib/python2.7/dist-packages/numpy/distutils
/system_info.py:572: UserWarning:                                                   
Lapack (http://www.netlib.org/lapack/) sources not found.                                                                           
Directories to search for the sources can be specified in the                                                                       
numpy/distutils/site.cfg file (section [lapack_src]) or by setting                                                                  
the LAPACK_SRC environment variable.                                                                                              
self.calc_info()                                                                                                                  
Running from scipy source directory.                                                                                                
non-existing path in 'scipy/integrate': 'quadpack.h'                                                                                
Warning: Can't read registry to find the necessary compiler setting                                                                 
Make sure that Python modules _winreg, win32api or win32con are  installed.                                                          
error: no lapack/blas resources found
</code></pre>

<p>Thank you</p>
",2017-07-27 15:52:51,2017-07-28 14:40:22,Issues installing gensim on Ubuntu,<python><python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13419,45280020,2017-07-24 11:56:23,,"<p>I have generated a word2vec model using gensim for  a huge corpus and I need to cluster the vocabularies using k means clustering for that i need:</p>

<ol>
<li>cosine distance matrix (word to word, so the size of the matrix the number_of_words x number_of_words )</li>
<li>features matrix (word to features, so the size of the matrix is the number_of_words x number_of_features(200) )</li>
</ol>

<p>for the feature matrix i tried to use x=model.wv and I got the object type as gensim.models.keyedvectors.KeyedVectors and its much smaller than what I expected a feature matrix will be </p>

<p>is there a way to use this object directly to generate the k-means clustering ?</p>
",2017-07-24 20:03:44,2017-07-24 20:03:44,getting distance matrix and features matrix from word2vec model,<python><k-means><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13430,45317151,2017-07-26 03:54:12,,"<p>I'm relative new in the world of Latent Dirichlet Allocation.
I am able to generate a LDA Model following the Wikipedia tutorial and I'm able to generate a LDA model with my own documents.
My step now is try understand how can I use a previus generated model to classify unseen documents.
I'm saving my ""lda_wiki_model"" with</p>

<pre><code>id2word =gensim.corpora.Dictionary.load_from_text('ptwiki_wordids.txt.bz2')

    mm = gensim.corpora.MmCorpus('ptwiki_tfidf.mm')

    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
    lda.save('lda_wiki_model.lda')
</code></pre>

<p>And I'm loading the same model with:</p>

<pre><code>new_lda = gensim.models.LdaModel.load(path + 'lda_wiki_model.lda') #carrega o modelo
</code></pre>

<p>I have a ""new_doc.txt"", and I turn my document into a id&lt;-> term dictionary and converted this tokenized document to ""document-term matrix""</p>

<p>But when I run <code>new_topics = new_lda[corpus]</code> I receive a 
<strong><em>'gensim.interfaces.TransformedCorpus object at 0x7f0ecfa69d50'</em></strong></p>

<p>how can I extract topics from that?</p>

<p>I already tried </p>

<pre><code>`lsa = models.LdaModel(new_topics, id2word=dictionary, num_topics=1, passes=2)
corpus_lda = lsa[new_topics]
print(lsa.print_topics(num_topics=1, num_words=7)
</code></pre>

<p>and</p>

<p><code>print(corpus_lda.print_topics(num_topics=1, num_words=7</code>)
`</p>

<p>but that return topics not relationed to my new document.
Where is my mistake? I'm miss understanding something?</p>

<p>**If a run a new model using the dictionary and corpus created above, I receive the correct topics, my point is: how re-use my model? is correctly re-use that wiki_model?</p>

<p>Thank you.</p>
",2017-07-26 04:32:02,2019-04-29 21:15:29,gensim.interfaces.TransformedCorpus - How use?,<gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13434,45425070,2017-07-31 20:50:44,,"<p>This might be the naive question which I am about to ask. I have a tokenized corpus on which I have trained Gensim's Word2vec model. The code is as below</p>

<pre><code>site = Article(""http://www.datasciencecentral.com/profiles/blogs/blockchain-and-artificial-intelligence-1"")
site.download()
site.parse()

def clean(doc):
    stop_free = "" "".join([i for i in word_tokenize(doc.lower()) if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    snowed = "" "".join(snowball.stem(word) for word in normalized.split())
    return snowed   

b = clean(site.text)
model = gensim.models.Word2Vec([b],min_count=1,size=32)
print(model) ### Prints: Word2Vec(vocab=643, size=32, alpha=0.025) ####
</code></pre>

<p>To cluster similar words, I am using PCA to visualize the clusters of similar words. But the problem is that it is forming only big cluster as seen in the image.</p>

<p><strong>PCA &amp; scatter plot Code:</strong></p>

<pre><code>vocab = list(model.wv.vocab)
X = model[vocab]
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

df = pd.concat([pd.DataFrame(X_pca),
                pd.Series(vocab)],
               axis=1)
df.columns = ['x','y','word']

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.scatter(df['x'],df['y'])
plt.show()
</code></pre>

<p>So, I have three questions here:</p>

<p>1) Is just one article enough to have the clear segregation of the clusters?</p>

<p>2) If I have a model trained with huge corpus and I want to predict the similar words in the new article and visualize them (i.e. words in the article I'm predicting) in the form of clusters, is there a way to do that?</p>

<p>I highly appreciate your suggestions. Thank you.</p>
",2017-07-31 20:55:50,2017-08-01 17:55:29,Python: clustering similar words based on word2vec,<python><nlp><cluster-analysis><word2vec><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
13448,45427552,2017-08-01 01:22:30,,"<p>Hi I have files present in bbc folder like this
<a href=""https://i.stack.imgur.com/TVEiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TVEiB.png"" alt=""Files present in parent folder""></a></p>

<p>Each sub folder inside bbc folder contains text files
<a href=""https://i.stack.imgur.com/beygd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/beygd.png"" alt=""Text files in sub folders""></a></p>

<p>This code helps accessing files inside the folder</p>

<pre><code>class MySentences(object):
def __init__(self, dirname):
    self.dirname = dirname

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname)):
            yield line.split()
sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc/business')
</code></pre>

<p>But I want to access files from each subfolder.I am getting following error when I do this </p>

<pre><code>sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc')
IOError                                   Traceback (most recent call last)
&lt;ipython-input-29-26fb31de4fec&gt; in &lt;module&gt;()
      1 sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc') # a memory-friendly iterator
----&gt; 2 model = gensim.models.Word2Vec(sentences)

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss)
    501             if isinstance(sentences, GeneratorType):
    502                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
--&gt; 503             self.build_vocab(sentences, trim_rule=trim_rule)
    504             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,
    505                        start_alpha=self.alpha, end_alpha=self.min_alpha)

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in build_vocab(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)
    575 
    576         """"""
--&gt; 577         self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
    578         self.scale_vocab(keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, update=update)  # trim by min_count &amp; precalculate downsampling
    579         self.finalize_vocab(update=update)  # build tables &amp; arrays

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in scan_vocab(self, sentences, progress_per, trim_rule)
    587         vocab = defaultdict(int)
    588         checked_string_types = 0
--&gt; 589         for sentence_no, sentence in enumerate(sentences):
    590             if not checked_string_types:
    591                 if isinstance(sentence, string_types):

&lt;ipython-input-28-48533b12127a&gt; in __iter__(self)
      5     def __iter__(self):
      6         for fname in os.listdir(self.dirname):
----&gt; 7             for line in open(os.path.join(self.dirname, fname)):
      8                 yield line.split()

IOError: [Errno 13] Permission denied: 'C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc\\business'
</code></pre>

<p>Kindly suggest me changes in the code</p>
",,2017-08-01 18:07:56,Read files present in subfolder,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13459,45357552,2017-07-27 17:38:00,,"<p>I am new to both TensorFlow and also Document Similarity / Topic Modeling therefore I apologize if my questions don't make complete sense.</p>

<p>From my limited understanding, topic modelling is done using algorithms such as LSA,LDA,etc. I have seen code using gensim and LSA but the time to train is very high for the large set of documents I have in mind. Consequently the CPU and RAM resources are very heavy.</p>

<p>Tensorflow doesn't seem to have a native LSA or LDA implementation. </p>

<p>I would appreciate an opinion on :</p>

<ul>
<li><p>Would LDA implemented using Tensorflow have a better performance than implemented using gensim?</p></li>
<li><p>Could someone tell me of other Tensorflow primitives that I should look at for document similarity rather than LDA?</p></li>
</ul>

<p>Once again I am sorry if my questions are too vague and do not cover sufficient information to give a proper response. I am new to this domain and I would appreciate any directions someone could point me to.</p>

<p>Thank you for your time.</p>

<p>Regards,
Jeetu</p>
",,2018-07-07 19:35:15,Document Similarity using Tensorflow,<tensorflow><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
13465,45467699,2017-08-02 17:46:00,,"<p>I am using gensim <strong>Doc2Vec</strong> model to generate my feature vectors. Here is the code I am using (I have explained what my problem is in the code):</p>

<pre><code>cores = multiprocessing.cpu_count()

# creating a list of tagged documents
training_docs = []

# all_docs: a list of 53 strings which are my documents and are very long (not just a couple of sentences)
for index, doc in enumerate(all_docs):
    # 'doc' is in unicode format and I have already preprocessed it
    training_docs.append(TaggedDocument(doc.split(), str(index+1)))

# at this point, I have 53 strings in my 'training_docs' list 

model = Doc2Vec(training_docs, size=400, window=8, min_count=1, workers=cores)

# now that I print the vectors, I only have 10 vectors while I should have 53 vectors for the 53 documents that I have in my training_docs list.
print(len(model.docvecs))
# output: 10
</code></pre>

<p>I am just wondering if I am doing a mistake or if there is any other parameter that I should set?</p>

<blockquote>
  <p><strong>UPDATE</strong>: I was playing with the <strong><em>tags</em></strong> parameter in <strong><em>TaggedDocument</em></strong>, and when I changed it to a mixture of text and numbers like: <em>Doc1, Doc2, ...</em> I see a different number for the count of generated vectors, but still I do not have the same number of feature vectors as expected.</p>
</blockquote>
",2017-08-02 22:06:59,2017-08-03 01:29:10,Gensim Doc2Vec model only generates a limited number of vectors,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13471,45419049,2017-07-31 14:48:32,,"<p>From the Doc2Vec wikipedia tutorial at <a href=""https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<pre><code>for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), 
           pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
</code></pre>

<p>Output is:</p>

<pre><code>min_count: 0, size of vocab: 8545782.0
min_count: 1, size of vocab: 8545782.0
min_count: 2, size of vocab: 4227783.0
min_count: 3, size of vocab: 3008772.0
min_count: 4, size of vocab: 2439367.0
min_count: 5, size of vocab: 2090709.0
min_count: 6, size of vocab: 1856609.0
min_count: 7, size of vocab: 1681670.0
min_count: 8, size of vocab: 1546914.0
min_count: 9, size of vocab: 1437367.0
min_count: 10, size of vocab: 1346177.0
min_count: 11, size of vocab: 1267916.0
min_count: 12, size of vocab: 1201186.0
min_count: 13, size of vocab: 1142377.0
min_count: 14, size of vocab: 1090673.0
min_count: 15, size of vocab: 1043973.0
min_count: 16, size of vocab: 1002395.0
min_count: 17, size of vocab: 964684.0
min_count: 18, size of vocab: 930382.0
min_count: 19, size of vocab: 898725.0
</code></pre>

<blockquote>
  <p>In the original paper, they set the vocabulary size 915,715. It seems similar size of vocabulary if we set min_count = 19. (size of vocab = 898,725)</p>
</blockquote>

<p><code>700</code> seems rather arbitrary, and I don't see any mentioning of this in the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">docs</a>. </p>
",,2017-07-31 19:47:46,Why is Doc2Vec.scale_vocab(...)['memory']['vocab'] divided by 700 to obtain vocabulary size?,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13489,45380351,2017-07-28 18:42:22,,"<p>Suppose I have generated a latent Dirichlet allocation model of <code>Corpus1</code> using the basic command: </p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus1, num_topics=25, id2word = dictionary, passes=50, minimum_probability=0)
</code></pre>

<p>My question would be, how can I classify the new documents from say `Corpus2'? </p>

<p>I am trying to use the following command <code>print(ldamodel[Corpus2[1]])</code> to obtain the distribution for the first document but I get the following error:</p>

<pre><code>ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>

<p>I am very confused regarding the class that the object <code>Corpus2</code> should be. Any suggestions of where to find more information or a tutorial is more than welcome</p>
",,2017-10-16 16:29:18,"infer topic distributions on new, unseen documents with LDA and Gensim",<python><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13499,45454433,2017-08-02 07:38:26,,"<p>I'm new to doc2vec and I hope some one of you can help me with this issue.
I've asked a number of people about this issue, but nobody knows the solution.</p>

<p>What I wanto to do is cluster Doc2vec result into k-means. Please see below the code.</p>

<pre><code>mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[range([2000])                                                                                                 
MiniBatchKMeans.predict(mbk,mbk.labels_ )
</code></pre>

<p>I'm getting this Error.</p>

<pre><code>TypeErrorTraceback (most recent call last)
&lt;ipython-input-19-fbc57a13bf4b&gt; in &lt;module&gt;()
      6 
      7 
----&gt; 8 mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[:2000])
      9 
     10 #model_dm.docvecs.doctag_syn0[2000]

/usr/local/lib64/python2.7/site-packages/gensim/models/doc2vec.pyc in __getitem__(self, index)
    351             return self.doctag_syn0[self._int_index(index)]
    352 
--&gt; 353         return vstack([self[i] for i in index])
    354 
    355     def __len__(self):

TypeError: 'slice' object is not iterable
</code></pre>
",2017-08-02 09:09:16,2017-08-02 09:09:16,Gensim Doc2vec model clustering into K-means,<python><k-means><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13502,45420466,2017-07-31 15:59:08,,"<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>

<pre><code>b = ['let',
 'know',
 'buy',
 'someth',
 'featur',
 'mashabl',
 'might',
 'earn',
 'affili',
 'commiss',
 'fifti',
 'year',
 'ago',
 'graduat',
 '21yearold',
 'dustin',
 'hoffman',
 'pull',
 'asid',
 'given',
 'one',
 'piec',
 'unsolicit',
 'advic',
 'percent',
 'buy']
</code></pre>

<p><strong>Model</strong></p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)
print(model) 
### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####
</code></pre>

<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>

<blockquote>
  <p>KeyError: ""word 'buy' not in vocabulary""</p>
</blockquote>

<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>
",,2018-07-16 05:46:34,"Gensim: KeyError: ""word not in vocabulary""",<python><nlp><gensim><word2vec><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
13504,45424420,2017-07-31 20:04:41,,"<p>Related to: <a href=""https://stackoverflow.com/questions/28488714/getting-string-version-of-document-by-id-in-gensim"">Getting string version of document by id in Gensim</a></p>

<p>I am implementing <code>gensim</code> for document similarity. After retrieving the <code>gensim</code> Similarity object index and similarity score for a new document, I would like to trace that index back to the original document in the corpus. Based on the answer in the question above, it looks like there is no built-in way to achieve this. </p>

<p>The df that feeds into my corpus looks like this:</p>

<pre><code>id    text
1-23  She loathes apples
1-52  I like rocks
1-43  You like ice cream
1-67  He hates bananas
</code></pre>

<p>After building the <code>gensim</code> model (tweaking it for dataframes from this tutorial: <a href=""https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"" rel=""nofollow noreferrer"">https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python</a>), I run the following new document through:</p>

<pre><code>id    text
1-98  I like neutrons
</code></pre>

<p>I can get the max similarity score and the index of the Similarity object with this function:</p>

<pre><code>def search(row):
    query = [w.lower() for w in word_tokenize(row)]
    queryBOW = dictionary.doc2bow(query)
    queryTFIDF = tfidf[queryBOW]
    maxSim = max(sims[queryTFIDF])
    index = [i for i, j in enumerate(sims[queryTFIDF]) if j == maxSim]
    return maxSim, index
</code></pre>

<p>After unpacking the tuple into separate df columns:</p>

<pre><code>id    text              maxSim    index
1-98  I like neutrons   0.5678    4
</code></pre>

<p>The index of the Similarity object does not match the index of the original dataframe.</p>

<p>How would I carry the original dataframe index into the similarity object? Or, how would I carry the <code>id</code> field from the original dataframe object into the Similarity object? I understand I have to keep a separate mapping between Similarity object index and original dataframe index - what does that mapping look like?</p>

<p>Desired result:</p>

<pre><code>id    text              maxSim    index   originalTextID    originalText
1-98  I like neutrons   0.5678    4       1-52              I like rocks    
</code></pre>
",2017-07-31 20:19:49,2017-07-31 20:19:49,Maintain DataFrame Index Through Gensim,<python><python-3.x><pandas><indexing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13508,45476932,2017-08-03 06:54:34,,"<p>I want to use an LDA(Latent Dirichlet Allocation) model for an NLP purpose. 
To train a such a model from Wikipedia corpus takes about 5 to 6 hours and wiki corpus is about 8GB. Check the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Tutorial</a> </p>

<p>Rather than doing so, is there a place to download a built LDA model and use it directly with Gensim?</p>
",2017-08-04 03:31:18,2017-08-05 15:52:03,Where to download a trained LDA model for Gensim?,<nlp><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13517,45404027,2017-07-30 19:48:11,,"<p>So I am trying to run the demo from gensim for distributed LSI (You can find it <a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow noreferrer"">here</a>) Yet whenever I run the code I get the error</p>

<p><code>AttributeError: module 'Pyro4' has no attribute 'expose'</code></p>

<p>I have checked similar issues here on stackoverflow, and usually they are caused through misuse of the library.</p>

<p>However I am not using Pyro4 directly, I am using Distributed LSI introduced by gensim. So there is no room for mistakes on my side (or so I believe)</p>

<p>My code is really simple you can find it below</p>

<pre><code>from gensim import corpora, models, utils
import logging, os, Pyro4
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
os.environ[""PYRO_SERIALIZERS_ACCEPTED""] =  'pickle'
os.environ[""PYRO_SERIALIZER""] = 'pickle'

corpus = corpora.MmCorpus('wiki_corpus.mm') # load a corpus of nine documents, from the Tutorials
id2word = corpora.Dictionary.load('wiki_dict.dict')

lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True) # run distributed LSA on nine documents
</code></pre>
",,2017-08-05 10:33:07,AttributeError module 'Pyro4' has no attribute 'expose' while running gensim distributed LSI,<python-2.7><gensim><latent-semantic-indexing><pyro4>,,,CC BY-SA 3.0,False,False,True,False,False
13523,45458493,2017-08-02 10:37:59,,"<p>After creating word vectors in Gensim 2.2.0 from plain English text files with IMDB movie ratings:</p>

<pre><code>import gensim, logging
import smart_open, os
from nltk.tokenize import RegexpTokenizer

VEC_SIZE = 300 
MIN_COUNT = 5
WORKERS = 4
data_path = './data/'
vectors_path = 'vectors.bin.gz'

class AllSentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.read_err_cnt = 0
        self.tokenizer = RegexpTokenizer('[\'a-zA-Z]+', discard_empty=True)

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(fname)
            for line in open(os.path.join(self.dirname, fname)):
                words = []     
                try:
                    for word in self.tokenizer.tokenize(line):
                        words.append(word)
                    yield words
                except:
                    self.read_err_cnt += 1

sentences = AllSentences(data_path) 
</code></pre>

<p>Training and saving model:</p>

<pre><code>model = gensim.models.Word2Vec(sentences, size=VEC_SIZE, 
                               min_count=MIN_COUNT, workers=WORKERS)
word_vectors = model.wv
word_vectors.save(vectors_path)
</code></pre>

<p>And then trying to load it back:</p>

<pre><code>vectors = KeyedVectors.load_word2vec_format(vectors_path,
                                                    binary=True,
                                                    unicode_errors='ignore')
</code></pre>

<p>I get '<strong><em>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0</em></strong>' exception (see below). I tried different combinations of 'encoding' parameters including <strong><em>'ISO-8859-1'</em></strong> and <strong><em>'Latin1'</em></strong>. Also different combinations of <strong><em>'binary=True/False'</em></strong>. Nothing helps - the same exception, no matter what parameters are used. What is wrong? How to make loading vectors work?</p>

<p>Exception:</p>

<pre><code>UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-64-f353fa49685c&gt; in &lt;module&gt;()
----&gt; 1 w2v = get_w2v_vectors()

&lt;ipython-input-63-cbbe0a76e837&gt; in get_w2v_vectors()
      3     vectors = KeyedVectors.load_word2vec_format(word_vectors_path,
      4                                                     binary=True,
----&gt; 5                                                     unicode_errors='ignore')
      6 
      7                                                 #unicode_errors='ignore')

D:\usr\anaconda\lib\site-packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    204         logger.info(""loading projection weights from %s"", fname)
    205         with utils.smart_open(fname) as fin:
--&gt; 206             header = utils.to_unicode(fin.readline(), encoding=encoding)
    207             vocab_size, vector_size = map(int, header.split())  # throws for invalid file format
    208             if limit:

D:\usr\anaconda\lib\site-packages\gensim\utils.py in any2unicode(text, encoding, errors)
    233     if isinstance(text, unicode):
    234         return text
--&gt; 235     return unicode(text, encoding, errors=errors)
    236 to_unicode = any2unicode
    237 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
",2017-08-02 11:16:33,2017-08-02 15:48:40,Gensim: word vectors encoding problems,<python><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
13530,45571295,2017-08-08 14:38:45,,"<p>I am using word embeddings for finding similarity between two sentences. Using word2vec, I also get a similarity measure if one sentence is in English and the other one in Dutch (though not very good). </p>

<p>So I started wondering if it's possible to compute the similarity between two sentences in two different languages (without an explicit translation), especially if the languages have some similarities (Englis/Dutch)?</p>
",2017-08-08 15:08:50,2020-05-16 10:58:05,Semantic Similarity across multiple languages,<nlp><nltk><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
13534,45572515,2017-08-08 15:34:42,,"<p>I'm trying to read a large .log file (having more than sixty thousand lines) into memory. I want to apply Word2Vec algorithm implemented in gensim. I have tried number of solutions, but none of them seems to be working. Any help would be appreciated.</p>

<p>Code1:</p>

<pre><code>def file_reader(file_obj):
    return [word for line in open(file_obj, 'r') for word in line.split()]
</code></pre>

<p>Code2:</p>

<pre><code>for i,line in enumerate(open(file_obj,'r')):
       print(i,line)
       sentences += line
</code></pre>
",2017-08-09 06:27:49,2017-08-09 06:27:49,Reading large files into memory for word2vec conversion,<python><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13535,45573181,2017-08-08 16:08:35,,"<p>I'm using Gensim's excellent library to compute similarity queries on a corpus using LSI. However, I have a distinct feeling that the results could be better, and I'm trying to figure out whether I can adjust the corpus itself in order to improve the results. </p>

<p>I have a certain amount of control over how to split the documents. My original data has a lot of very short documents (mean length is 12 words in a document, but there exist documents that are 1-2 words long...), and there are a few logical ways to concatenate several documents into one. The problem is that I don't know whether it's worth doing this or not (and if so, to what extent). I can't find any material addressing this question, but only regarding the size of the corpus, and the size of the vocabulary. I assume this is because, at the end of the day, the size of a document is bounded by the size of the vocabulary. But I'm sure there are still some general guidelines that could help with this decision.</p>

<p>What is considered a document that is too short? What is too long? (I assume the latter is a function of <code>|V|</code>, but the former could easily be a constant value.)</p>

<p>Does anyone have experience with this? Can anyone point me in the direction of any papers/blog posts/research that address this question? Much appreciated!</p>

<p><em>Edited to add:</em>
Regarding the strategy for grouping documents - each document is a text message sent between two parties. The potential grouping is based on this, where I can also take into consideration the time at which the messages were sent. Meaning, I could group all the messages sent between A and B within a certain hour, or on a certain day, or simply group all the messages between the two. I can also decide on a minimum or maximum number of messages grouped together, but that is exactly what my question is about - how do I know what the ideal length is?</p>
",2017-08-08 17:29:40,2017-08-09 10:25:32,Optimal Document Size for LSI Similarity Model,<gensim><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
13536,45459496,2017-08-02 11:23:27,,"<p>Based on this article: <a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""nofollow noreferrer"">http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/</a> I am trying to implement a gensim word2vec model with the pretrained vectors of GloVe in a text classification task. However, I would like to do FeatureSelection also in my text data. I tried multiple sequences in the pipeline but i get fast a memory error which points to the transform part of TfidfEmbeddingVectorizer. </p>

<pre><code>   return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
</code></pre>

<p>If I replace the TfidfEmbeddingVectorizer class with a regular TfIdfVectorizer it works properly. Is there a way I could combine SelectFromModel and W2vec in the pipeline?</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
import itertools
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import precision_recall_fscore_support as score, f1_score
import pickle
from sklearn.externals import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import FeatureUnion
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.svm import LinearSVC
import gensim
import collections

class ItemSelector(BaseEstimator, TransformerMixin):

    def __init__(self, column):
        self.column = column

    def fit(self, X, y=None, **fit_params):
        return self

    def transform(self, X):
        return (X[self.column])




class TextStats(BaseEstimator, TransformerMixin):
    """"""Extract features from each document for DictVectorizer""""""

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        return [{'REPORT_M': text}
                for text in posts]


class TfidfEmbeddingVectorizer(object):
  def __init__(self, word2vec):
    self.word2vec = word2vec
    self.word2weight = None
    self.dim = len(word2vec.values())

  def fit(self, X, y):
    tfidf = TfidfVectorizer(analyzer=lambda x: x)
    tfidf.fit(X)
    # if a word was never seen - it must be at least as infrequent
    # as any of the known words - so the default idf is the max of 
    # known idf's
    max_idf = max(tfidf.idf_)
    self.word2weight = collections.defaultdict(
        lambda: max_idf,
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

    return self

  def transform(self, X):
    return np.array([
            np.mean([self.word2vec[w] * self.word2weight[w]
                     for w in words if w in self.word2vec] or
                    [np.zeros(self.dim)], axis=0)
            for words in X
        ])


# training model
 def train(data_train, data_val):

    with open(""glove.6B/glove.6B.50d.txt"", ""rb"") as lines:
        w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))
               for line in lines}
    classifier = Pipeline([
                    ('union', FeatureUnion([

                            ('text', Pipeline([
                                ('selector', ItemSelector(column='TEXT')),
                                (""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                                ('feature_selection', SelectFromModel(LinearSVC(penalty=""l1"", dual=False),threshold=0.01))
                            ])),

                            ('category', Pipeline([
                                ('selector', ItemSelector(column='category')),
                                ('stats', TextStats()),
                                ('vect', DictVectorizer())
                            ])) 
    ])),
                    ('clf',ExtraTreesClassifier(n_estimators=200, max_depth=500, min_samples_split=6, class_weight= 'balanced'))])

    classifier.fit(data_train,data_train.CLASSES)
    predicted = classifier.predict(data_val)
</code></pre>
",,2017-11-15 07:20:33,Combining w2vec and feature selection in pipeline,<python-3.x><gensim><text-classification><feature-selection>,,,CC BY-SA 3.0,False,False,True,False,True
13542,45444304,2017-08-01 17:29:52,,"<p>I am using gensim's <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">lda implementation</a> to create a document topic model distribution. I have a model <code>lda</code> trained</p>

<pre><code>dictionary = corpora.Dictionary(data)
corpus = [dictionary.doc2bow(doc) for doc in data]
num_cores = multiprocessing.cpu_count()
num_topics = 150
lda = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=num_cores, alpha=1e-4, eta=5e-1,
              minimum_probability=0.0)
</code></pre>

<p>I want to extract a distribution matrix of size N * M where N is the number of documents in the corpus, and M is the number of topics. Each row therefore would represent a document, and each column would represent a topic. These distributions are already available in the <code>lda</code> model variable e.g.</p>

<pre><code>lda
&gt;&gt;&gt; [[(0,0.01),(1,0.23),(2,1e-7)],
    [(0,0.91),(1,0.067),(2,0.38)]]
</code></pre>

<p>where the above represents a simple example if we had 2 documents and 3 topics (0,1,2). So <code>lda</code> is a list of list of tuples, where each list is a document, the first element in the tuple is the topic id and the second element is the topic contribution to that document.</p>

<p>What I have done so far</p>

<pre><code># approach 1
distributions = np.array( [[tup[1] for tup in lst] for lst in lda[corpus]] )
# approach 2 (about 3X slower than approach 1)
distributions = np.array(lda[corpus])[:,:,1]
</code></pre>

<p>The problem is, for a dataset with 300k documents and 150 topics, creating the <code>distributions</code> matrix using approach 1 takes over 10 minutes.</p>

<p>Ultimately, I want to create this matrix to feed into a function which calculates document similarity (comparing the first row / document of the matrix to all other rows / documents). If there is a way to directly feed the learned <code>lda</code> model values into the function, that would be great as it would save time on creating the redundant matrix. If not, is there a more efficient way to create the <code>distributions</code> matrix?</p>

<p>For reference, the document comparison function into which the <code>distributions</code> matrix is fed is</p>

<pre><code>from sklearn.preprocessing import normalize
from scipy.stats import entropy
def jsd(mat):
    mat = normalize(mat, axis=1, norm='l1')
    p = mat[0,None].T # just comparing first row to all other rows
    q = mat[0:].T
    m = 0.5*(p + q)
    return 0.5*(entropy(p,m) + entropy(q,m))
</code></pre>
",2017-08-01 17:56:21,2017-08-01 17:56:21,Extract LDA model values into distribution matrix efficiently,<python><matrix><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,True
13550,45499558,2017-08-04 06:22:59,,"<p>I re-install the gensim pkg and Cython but it continusly show this warning,
Does anybody know about this?
I am using Python 3.6,PyCharm Linux Mint.</p>

<p>UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
  warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""</p>

<p>And it also show this line when I create or load model.<br>
Slow version of gensim.models.doc2vec is being used</p>
",,2017-08-04 11:28:20,C extension not loaded for Word2Vec,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13552,45502464,2017-08-04 08:59:17,,"<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>

<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)
</code></pre>

<p>Reference: <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>

<p>Reference: 
<a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>

<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>

<pre><code>n_queries = len(query)
result = []
for qidx in range(n_queries):
    # Compute similarity for each query.
    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
    qresult = numpy.array(qresult)
    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.

    # Append single query result to list of all results.
    result.append(qresult)
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>
",2017-08-07 10:03:44,2017-08-07 16:59:10,What is the difference between wmd (word mover distance) and wmd based similarity?,<nlp><nltk><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,True,False,True,False,False
13559,45444964,2017-08-01 18:12:40,,"<p>I have been struggling to understand the use of <code>size</code> parameter in the <code>gensim.models.Word2Vec</code></p>

<p>From the Gensim documentation, <code>size</code> is the dimensionality of the vector. Now, as far as my knowledge goes, word2vec creates a vector of the probability of closeness with the other words in the sentence for each word. So, suppose if my <code>vocab</code> size is 30 then how does it create a vector with the dimension greater than 30? Can anyone please brief me on the optimal value of <code>Word2Vec</code> size? </p>

<p>Thank you.</p>
",,2017-08-02 06:28:45,"Python: What is the ""size"" parameter in Gensim Word2vec model class",<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13568,45631044,2017-08-11 08:59:24,,"<p>I hope you bear with my questions. please let me know if it is unclear. I tried a lot to say any detail. but it may be still unclear. if so please let me know.</p>

<p>We have LDA topic modeling which purpose is to generate a number of topics given a set of documents. 
so each document can belong to various topics.</p>

<p>Also, we can evaluate the model we have created. one of the approaches is using classification method like SVM.<strong>my goal is to evaluate created model.</strong></p>

<p>I have faced with two kinds of code for making the <code>LDA model</code>.</p>

<p><strong>1.</strong></p>

<pre><code>    # generate LDA model
id2word = corpora.Dictionary(texts)

# Creates the Bag of Word corpus.
mm = [id2word.doc2bow(text) for text in texts]

# Trains the LDA models.
lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10,
                               update_every=1, chunksize=10000, passes=1,gamma_threshold=0.00, minimum_probability=0.00)
</code></pre>

<p>this way I can not use <code>Fit_transform</code></p>

<p><strong>2.</strong></p>

<pre><code>tf_vectorizer = CountVectorizer(max_features=n_features,
                                stop_words='english')
tf = tf_vectorizer.fit_transform(data_samples)

lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,
                                learning_method='online',
                                learning_offset=50.,
                                random_state=0)

lda_x=lda.fit_transform(tf) 
</code></pre>

<p>in the first approach, there is no fit_transform method for LDA model, I do not know why as I don't understand the difference between them.</p>

<p>anyway, I need to pass that LDA model I have created with the first approach to SVM(the reason I put these two approach here I s that I know with the second approach there is no error probably becaz of fit_transform but for some reason I couldnt use that),
this is my final code:</p>

<pre><code>import os
from gensim.models import ldamodel
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC


tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = {'a'}

# Create p_stemmer of class PorterStemmer
lines=[]
p_stemmer = PorterStemmer()
lisOfFiles=[x[2] for x in os.walk(""data"")]

fullPath = [x[0] for x in os.walk(""data"")]
for j in lisOfFiles[2]:
    with open(os.path.join(fullPath[2],j)) as f:
                    a=f.read()
                    lines.append(a)


for j in lisOfFiles[3]:
    with open(os.path.join(fullPath[3],j)) as f:
                    a=f.read()
                    lines.append(a)

for j in lisOfFiles[4]:
    with open(os.path.join(fullPath[4],j)) as f:
                    a=f.read()
                    lines.append(a)

# compile sample documents into a list
doc_set = lines
# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)

# generate LDA model
id2word = corpora.Dictionary(texts)

# Creates the Bag of Word corpus.
mm = [id2word.doc2bow(text) for text in texts]

# Trains the LDA models.
lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10,
                               update_every=1, chunksize=10000, passes=1,gamma_threshold=0.00, minimum_probability=0.00)

# Assigns the topics to the documents in corpus

dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]


#creating the labels
lda_corpus = lda[mm]
label_y=[]
for i in lda_corpus:
    new_y = []
    for l in i:
        sorted_labels = sorted(i, key=lambda z: z[0], reverse=True)
        if l[1] &gt; 0.005:
            new_y.append(l[0])
        label_y.append(new_y)

classifier = Pipeline([
    ('vectorizer', CountVectorizer(max_df=2,min_df=1)),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
classifier.fit(lda, label_y)
</code></pre>

<p>As you see in my code I have used the first approach for some reasons,
but in the last line, it raises an error(<code>object of type int has no len()</code>). It seems it can not accept <code>lda</code> created in this way (I was thinking because in this way I didnt use fit_transform)
how can I fix this error with my code?</p>

<p>many thanks for your patience and help in advance.</p>

<p>this is the full stack error :</p>

<pre><code>/home/saria/tfwithpython3.6/bin/python /home/saria/PycharmProjects/TfidfLDA/test4.py
Using TensorFlow backend.
Traceback (most recent call last):
  File ""/home/saria/PycharmProjects/TfidfLDA/test4.py"", line 92, in &lt;module&gt;
    classifier.fit(lda, label_y)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/pipeline.py"", line 268, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/pipeline.py"", line 234, in _fit
    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/feature_extraction/text.py"", line 839, in fit_transform
    self.fixed_vocabulary_)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/feature_extraction/text.py"", line 760, in _count_vocab
    for doc in raw_documents:
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py"", line 1054, in __getitem__
    return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py"", line 922, in get_document_topics
    gamma, phis = self.inference([bow], collect_sstats=per_word_topics)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/gensim/models/ldamodel.py"", line 429, in inference
    if len(doc) &gt; 0 and not isinstance(doc[0][0], six.integer_types + (np.integer,)):
TypeError: object of type 'int' has no len()

Process finished with exit code 1
</code></pre>
",2017-08-11 09:09:22,2017-08-11 09:09:22,fitting classifier object of type 'int' has no len(),<python><svm><text-classification><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,True
13571,45478607,2017-08-03 08:19:04,,"<p>I would like to use Gensim's implemented Word2Vec with a list of context-word pairs as an input instead of sentences. I originally thought that entering the manually created context-word pairs as sentences would be equivalent to entering the raw sentences and setting the window parameter to 1, but the two approaches yield different results. How does Gensim's Word2Vec calculate the context-word pairs of sentences, and how should I enter my manually created pairs as an input to the function?</p>
",,2017-08-03 17:02:20,Using gensim's Word2Vec with custom word-context pairs,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13579,45565389,2017-08-08 10:05:15,,"<p>I am looking at various semantic similarity methods such as word2vec, word mover distance (WMD), and fastText. fastText is not better than Word2Vec as for as semantic similarity is concerned. WMD and Word2Vec have almost similar results. </p>

<p>I was wondering if there is an alternative which has outperformed the Word2Vec model for semantic accuracy? </p>

<p><strong>My use case:</strong>
<em>Finding word embeddings for two sentences, and then use cosine similarity to find their similarity.</em> </p>
",2017-08-08 12:22:25,2017-08-08 17:52:23,Is there a semantic similarity method that outperforms word2vec approach for semantic accuracy?,<nlp><nltk><gensim><word2vec><fasttext>,,,CC BY-SA 3.0,True,False,True,False,False
13599,45575127,2017-08-08 18:01:53,,"<p>I am using LDA over a simple collection of documents. my goal is to extract topics, then use the extracted topics as features to evaluate my model.</p>

<p>I decided to use multinomial SVM as the evaluater. not sure its good or not?</p>

<pre><code>import itertools
from gensim.models import ldamodel
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
from sklearn.naive_bayes import MultinomialNB

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = {'a'}

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# create sample documents
doc_a = ""Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.""
doc_b = ""My mother spends a lot of time driving my brother around to baseball practice.""
doc_c = ""Some health experts suggest that driving may cause increased tension and blood pressure.""
doc_d = ""I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.""
doc_e = ""Health professionals say that brocolli is good for your health.""

# compile sample documents into a list
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]


# generate LDA model
#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)

id2word = corpora.Dictionary(texts)
# Creates the Bag of Word corpus.
mm = [id2word.doc2bow(text) for text in texts]

# Trains the LDA models.
lda = ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=4,
                               update_every=1, chunksize=10000, passes=1)


# Assigns the topics to the documents in corpus
a=[]
lda_corpus = lda[mm]
for i in range(len(doc_set)):
    a.append(lda_corpus[i])
    print(lda_corpus[i])
merged_list = list(itertools.chain(*lda_corpus))
print(a)
    #my_list.append(my_list[i])


sv=MultinomialNB()

yvalues = [0,1,2,3]

sv.fit(a,yvalues)
predictclass = sv.predict(a)

testLables=[0,1,2,3]
from sklearn import metrics, tree
#yacc=metrics.accuracy_score(testLables,predictclass)
#print (yacc)
</code></pre>

<p>when I run this code it throws the error mentioned in the subject.</p>

<p>Also this is the output of LDA model(topic doc distribution) that I feed to SVM:</p>

<pre><code>[[(0, 0.95533888404477663), (1, 0.014775921798986477), (2, 0.015161897773308793), (3, 0.014723296382928375)], [(0, 0.019079556242721694), (1, 0.017932434792585779), (2, 0.94498655991579728), (3, 0.018001449048895311)], [(0, 0.017957955483631164), (1, 0.017900184473362918), (2, 0.018133572636989413), (3, 0.9460082874060165)], [(0, 0.96554611572184923), (1, 0.011407838337200715), (2, 0.011537900721487016), (3, 0.011508145219463113)], [(0, 0.023306931039431281), (1, 0.022823706054846005), (2, 0.93072240824085961), (3, 0.023146954664863096)]]
</code></pre>

<p>My labels here are 0,1,2,3 .</p>

<p>I found a response <a href=""https://stackoverflow.com/questions/34972142/sklearn-logistic-regression-valueerror-found-array-with-dim-3-estimator-expec"">here</a></p>

<p>but when I write down :</p>

<pre><code>nsamples, nx, ny = a.shape
d2_train_dataset = a.reshape((nsamples,nx*ny))
</code></pre>

<p>According to my case, it does not work. actually a does not have shape method.</p>

<p><strong>whole traceback error</strong></p>

<pre><code>Traceback (most recent call last):
  File ""/home/saria/PycharmProjects/TfidfLDA/test3.py"", line 87, in &lt;module&gt;
    sv.fit(a,yvalues)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/naive_bayes.py"", line 562, in fit
    X, y = check_X_y(X, y, 'csr')
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 521, in check_X_y
    ensure_min_features, warn_on_dtype, estimator)
  File ""/home/saria/tfwithpython3.6/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 405, in check_array
    % (array.ndim, estimator_name))
ValueError: Found array with dim 3. Estimator expected &lt;= 2.
</code></pre>
",2017-08-08 18:46:34,2017-08-10 20:32:41,Found array with dim 3. Estimator expected <= 2,<python><machine-learning><svm><text-classification><lda>,,,CC BY-SA 3.0,True,False,True,False,True
13622,45696948,2017-08-15 16:12:28,,"<p>I have a class wrapping the various objects required for calculating LSI similarity:</p>

<pre><code>class SimilarityFiles:

    def __init__(self, file_name, tokenized_corpus, stoplist=None):
        if stoplist is None:
            self.filtered_corpus = tokenized_corpus
        else:
            self.filtered_corpus = []
            for convo in tokenized_corpus:
                self.filtered_corpus.append([token for token in convo if token not in stoplist])
        self.dictionary = corpora.Dictionary(self.filtered_corpus)
        self.corpus = [self.dictionary.doc2bow(text) for text in self.filtered_corpus]
        self.lsi = models.LsiModel(self.corpus, id2word=self.dictionary, num_topics=100)
        self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])
</code></pre>

<p>I now want to add a function to the class to allow adding documents to the corpus and updating the model accordingly.
I've found <code>dictionary.add_documents</code>, and <code>model.add_documents</code>, but there are two things that aren't clear to me:</p>

<ol>
<li>When you originally create the LSI model, one of the parameters the function receives is <code>id2word=dictionary</code>. When updating the model, how do you tell it to use the updated dictionary? Is it actually unnecessary, or will it make a difference?</li>
<li>How do I update the index? It looks from the <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.Similarity.add_documents"" rel=""nofollow noreferrer"">documentation</a> that if I use the <code>Similarity</code> class, and not the <code>MatrixSimilarity</code> class, I can add documents to the index, but I don't see such functionality for <code>MatrixSimilarity</code>. If I understood correctly, the <code>MatrixSimilarity</code> is better if my input corpus contains dense vectors (which is does, because I'm using the LSI model). Do I have to change it to <code>Similarity</code> just so that I can update the index? Or, conversely, what's the complexity of creating this index? If it's insignificant, should I just create a new index with my updated corpus, as follows:</li>
</ol>

<p>Code:</p>

<pre><code>self.dictionary.add_documents(new_docs)    # new_docs is already after filtering stop words
new_corpus = [self.dictionary.doc2bow(text) for text in new_docs]
self.lsi.add_documents(new_corpus)
self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])
</code></pre>

<p>Thanks. :)</p>
",,2018-09-27 18:44:12,Adding documents to gensim model,<python-3.x><gensim><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
13643,45647333,2017-08-12 06:41:01,,"<p>I trained my word vector like i normally would. I cleaned the text before hand where each line is a sentence with tokens separated by a space</p>

<pre><code>class Sentences:
    def __init__(self):
        pass

    def __iter__(self):
        i = 0
        with codecs.open('./data/cleaned_corpus.txt', 'r', 'utf-8') as file:
            for line in file:
                i += 1
                if i % 5000 == 0:
                    print('processed ' + str(i))

                yield line.split()


w2v = Word2Vec(Sentences(), size=100, min_count=10)
w2v.wv.save('model')
</code></pre>

<p>The issue is that some of the vectors return arrays with values that are numpy infs </p>

<pre><code>array([-inf,  inf, -inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,
    inf, -inf, -inf,  inf,  inf, -inf, -inf,  inf,  inf,  inf, -inf,
    inf,  inf, -inf, -inf, -inf,  inf, -inf,  inf, -inf,  inf,  inf,
   -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,
   -inf, -inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf, -inf, -inf,
   -inf, -inf, -inf, -inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf,
    inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf, -inf,  inf,
   -inf, -inf,  inf,  inf,  inf,  inf, -inf,  inf, -inf, -inf,  inf,
   -inf, -inf,  inf,  inf,  inf, -inf, -inf,  inf, -inf, -inf, -inf,
    inf], dtype=float32) 
</code></pre>

<p>I've already retrained the vector 3 times, I don't know what's causing this</p>
",,2017-08-12 06:41:01,Some word vectors return values of infinite or -infinite,<python><numpy><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13644,45647439,2017-08-12 06:53:55,,"<p>I have built Word2Vecmodel using gensim library in python.I want to evaluate my word embedding as follows</p>

<blockquote>
  <p>If A is related to B and C is related to D, then A-C+B should be equal to D. For example, embedding vector arithmetic of ""India""-""Rupee""+""Japan"" should be equal to the embedding of ""Yen"".</p>
</blockquote>

<p>I have used in built functions of gensim like predict_output_word,most_similar but couldn't get desired results.</p>

<pre><code>new_model.predict_output_word(['india','rupee','japan'],topn=10)
new_model.most_similar(positive=['india', 'rupee'], negative=['japan'])
</code></pre>

<p>Kindly help me in evaluating my model as per the criteria above.</p>
",2017-08-12 07:40:49,2017-08-12 17:24:18,Evaluating Word2Vec model by finding linear algebraic structure of words,<nlp><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
13645,45569142,2017-08-08 13:03:45,,"<p>I want to compare word2vec and fasttext model based on this comparison tutorial. 
<a href=""https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb"" rel=""nofollow noreferrer"">https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a></p>

<p>According to this, the semantic accuracy of fastText model increase when we set the max length of char n-grams to zero, such that fastText starts to behave almost like to word2vec. It ignores the ngrams. </p>

<p>However, I can not find any formation on how to set this parameter while loading a fastText model. Any ideas on how to do this?</p>
",,2017-08-08 17:59:02,Setting max length of char n-grams for fastText,<nlp><nltk><gensim><word2vec><fasttext>,,,CC BY-SA 3.0,True,False,True,False,False
13650,45720880,2017-08-16 18:46:24,,"<p>I have a gensim LSI model that I want to persist in MongoDB. Specifically, I want to persist the following:</p>

<ul>
<li>Gensim dictionary (id &lt;-> word)</li>
<li>Gensim corpus</li>
<li>LSI model</li>
<li>MatrixSimilarity index</li>
</ul>

<p>I know that all of these objects can be saved to file, but I can't figure out how I can save these data to a MongoDB (specifically, using pyMongo). From what I've gathered, I don't need anything like GridFS, because the files are relatively small. But in any case, I want to save them all in a single document, and not separately.</p>

<p>Thanks :)</p>
",,2017-08-16 18:46:24,Persisting gensim LSI model in MongoDB,<mongodb><python-3.x><pymongo><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13663,45685055,2017-08-15 00:54:35,,"<p>This is my code:</p>

<pre><code>def build_trigram_model(corpus):
    corpus = lemmer(nlp(corpus))
    bigram = phrases.Phrases(corpus, min_count=2, threshold=40)
    bigram_phraser = phrases.Phraser(bigram)
    trigram = phrases.Phrases(bigram_phraser[corpus], min_count=2, threshold=50)
    trigram_phraser = phrases.Phraser(trigram)
    return bigram_phraser, trigram_phraser

def punct_space(token):
    """"""
    helper function to eliminate punctuation, spaces and numbers.
    """"""
    return token.is_punct or token.is_space or token.like_num

def lemmer(tokens):
    word_space = []
    stemmer = SnowballStemmer(""english"")
    for token in tokens:
        if not punct_space(token):
            word_space.append(stemmer.stem(str(token).lower()))
    return word_space


bigram_phraser, trigram_phraser = build_trigram_model(corpus)
</code></pre>

<p>This returns something like:</p>

<pre><code>bigram_phraser.phrasegrams

{(b'\xef\xa3\xaf', b'\xef\xa3\xb0'): (189, 49.569954185342624),
 (b'\xef\xa3\xba', b'\xef\xa3\xbb'): (189, 49.162979913552),
 (b'\xce\xbf', b'\xcf\x82'): (11, 52.7203947368421),
 (b'\xef\xa3\xb1', b'\xef\xa3\xb2'): (13, 78.54622410336378),
 (b'\xef\xa3\xb2', b'\xef\xa3\xb3'): (12, 74.75279850746269),
 (b'\xef\xa3\xbc', b'\xef\xa3\xbd'): (8, 73.94232987312573),
 (b'\xef\xa3\xbd', b'\xef\xa3\xbe'): (7, 65.46977124183007),
 (b'\xc9\xa1', b'\xcc\x8a'): (29, 64.73618071658315),
 (b'\xef\x9d\xb4', b'\xef\x9d\xb2'): (51, 105.61094674556212),
 (b'\xef\x9d\xa3', b'\xef\x9d\xac'): (26, 53.88736340711684)}
</code></pre>

<p>When running a new text through the model, no collocations are found.</p>

<p>However, when I use the following stemmer:</p>

<pre><code>def lemmer(tokens):
    """"""lemmatize words""""""
    word_space = []
    for sent in tokens.sents:
        sentence = []
        for token in sent:
            if not punct_space(token):
                if token.lemma_=='-PRON-':
                    sentence.append(token.lower_)
                else:
                    sentence.append(token.lemma_)
        word_space.append(sentence)
    return word_space
</code></pre>

<p>Everything works just as it is supposed to. The dtype returned by both stemmers is list of strings, so that cannot be the problem. Any ideas why this happens? Thanks!</p>

<ul>
<li><strong>Platform:</strong> Linux-4.4.0-1030-aws-x86_64-with-debian-stretch-sid</li>
<li><strong>Python version:</strong> 3.6.1</li>
</ul>
",2017-08-15 06:29:19,2017-08-15 06:29:19,Gensim.models.phrases.Phrases does not work when input comes from NLTK stemmed tokens,<python><nltk><gensim><spacy>,,,CC BY-SA 3.0,True,True,True,False,False
13669,45741156,2017-08-17 17:03:51,,"<p>I am very beginner to the scikit-learn .I am working on some classification problem for which I have to build some custom feature extraction class or method to find the features for the training data. 
I have made my custom feature extraction class as explain in this <a href=""https://www.dreisbach.us/blog/building-scikit-learn-compatible-transformers/"" rel=""nofollow noreferrer"">link</a>. When i run my code it shows me this error :- </p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 248, in &lt;module&gt;
    pred = pipe.predict(X_test)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/metaestimators.py"", line 54, in &lt;lambda&gt;
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/pipeline.py"", line 327, in predict
    return self.steps[-1][-1].predict(Xt)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 336, in predict
    scores = self.decision_function(X)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 317, in decision_function
    % (X.shape[1], n_features))
ValueError: X has 174 features per sample; expecting 443
</code></pre>

<p>Below is my code snippet , also i have given my full code. Please tell me where i am doing wrong and why , along with the suggestions so that my code will run without any error. </p>

<p>Code snippet :-
Here ""y"" is a list of all categories or labelled group .""corpus"" is the list of all documents (data) , where each doc. is represented like a string.""tfidf"" and ""lda"" are my two functions from which i am generating my feature vector </p>

<pre><code>y = [d[0] for d in doc_info_with_label] #length is no:ofsamples
corpus = [d[1] for d in doc_info_with_label]

class feature_extractor(TransformerMixin):
    def __init__(self,*featurizers):
        self.featurizers = featurizers
    def fit(self,X,y=None):
        return self
    def transform(self,X):
        collection_features=[]
        for f in self.featurizers:
            collection_features.append(f(X))
        feature_vect=np.array(collection_features[0])
        if len(collection_features)&gt;1:
            for i in range(1,len(collection_features)):
                feature_vect=np.concatenate((feature_vect,np.array(collection_features[i])),axis=1)
        #print feature_vect.shape
        return feature_vect


my_featurizer = feature_extractor(tfidf,lda)
X = my_featurizer.transform(corpus)
X_train , X_test , y_train , y_test = train_test_split(corpus,y,test_size=0.2,random_state=42)
pipe = make_pipeline(my_featurizer,svm.LinearSVC())
pipe.fit(X_train,y_train)
pred = pipe.predict(X_test)
print ""Expected output\n""
print y_test
print ""\n""
print ""Output\n""
print pred
print ""\n""
score = pipe.score(X_test,y_test)
print score
print ""\n""
print metrics.confusion_matrix(pred,y_test)
</code></pre>

<p>full code :- </p>

<pre><code># -*- coding: utf-8 -*-
#! /usr/bin/env python3
from gensim import corpora, models
import gensim
from operator import itemgetter
import numpy as np
import sys
import os
import re
import codecs
import io
import math
from scipy import sparse
from sklearn.cross_validation import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.base import TransformerMixin
from sklearn import svm
from sklearn import metrics
from sklearn.pipeline import make_pipeline , Pipeline


reload(sys)
sys.setdefaultencoding('utf8')
np.set_printoptions(threshold='nan')

suffixes = {
    1: ["""", """", """", """", """", """", """"],
    2: ["""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """"],
    3: ["""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """"],
    4: ["""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """"],
    5: ["""", """", """", """", """", """", """"],
}

categories=['A','C','D','E']
mappings={}
mappings['A']=1
mappings['C']=3
mappings['D']=4
mappings['E']=5

path='/home/priyansh/Downloads/ltrc/1055/'
train_data_path='/home/priyansh/Downloads/ltrc/extractor/clustering/four_class_devanagari/'
path1=train_data_path+""A/""
path2=train_data_path+""C/""
path3=train_data_path+""D/""
path4=train_data_path+""E/""
documents=[] #contains all doc filenames along with class labels
doc_info_with_label=[] #two tuple storage of doc info along with their respective labels

def hi_stem(word):
    for L in 5, 4, 3, 2, 1:
        if len(word) &gt; L + 1:
            for suf in suffixes[L]:
                if word.endswith(suf):
                    return word[:-L]
    return word

def store_data(dir_path_list):
    for dir_path in dir_path_list:
        class_name = dir_path.split(""/"")[8]
        for filename in os.listdir(dir_path):
            if filename not in documents:
                documents.append(filename+""+""+str(mappings[class_name]))
            infilename=os.path.join(dir_path,filename)
            with codecs.open(infilename,'r','utf-8') as fl:
                string=''
                for line in fl:
                    for word in line.split():
                        if word!="" "" or word!=""\n"":
                            string+=word+"" ""
            fl.close()
            temp=[]
            temp.append(class_name)
            temp.append(string)
            doc_info_with_label.append(tuple(temp))

path_list=[]
path_list.append(path1)
path_list.append(path2)
path_list.append(path3)
path_list.append(path4)
store_data(path_list)

y = [d[0] for d in doc_info_with_label] #length is no:ofsamples
corpus = [d[1] for d in doc_info_with_label]

class feature_extractor(TransformerMixin):
    def __init__(self,*featurizers):
        self.featurizers = featurizers
    def fit(self,X,y=None):
        return self
    def transform(self,X):
        collection_features=[]
        for f in self.featurizers:
            collection_features.append(f(X))
        feature_vect=np.array(collection_features[0])
        if len(collection_features)&gt;1:
            for i in range(1,len(collection_features)):
                feature_vect=np.concatenate((feature_vect,np.array(collection_features[i])),axis=1)
        #print feature_vect.shape
        return feature_vect

def tfidf_score(word,document_no,corpus_data):
    #print word
    my_word=word
    stopwords_path='/home/priyansh/Downloads/ltrc/extractor/'
    stop_words_filename='stopwords.txt'
    stopwords=[] #contain all stopwords
    with codecs.open(stopwords_path+stop_words_filename,'r','utf-8') as fl:
            for line in fl:
            for word in line.split():
                    stopwords.append(word)
    fl.close()
    document=corpus_data[document_no]
    #print document
    wordcount=0
    total=0
    temp = document.split()
    for i in temp:
        #print i
        if i not in stopwords:
            total+=1
            if i==my_word:
                #print my_word
                #print word
                wordcount+=1
    #print wordcount
    #print total
    tf = float(wordcount)/total
    #print tf
    #return tf(word,document)*idf(word,corpus_data)
    total_docs = len(corpus_data)
    count=0
    for doc in corpus_data:
        temp=[]
        temp = doc.split()
        for i in temp:
            if i==word:
                count+=1
                break
    total_docs_which_contains_the_words=count
    idf = math.log(total_docs/(1+total_docs_which_contains_the_words))
    return tf*idf


def tfidf(corpus_data):
    word_id_mapping={}
    cnt=0
    stopwords_path='/home/priyansh/Downloads/ltrc/extractor/'
    stop_words_filename='stopwords.txt'
    stopwords=[] #contain all stopwords
    with codecs.open(stopwords_path+stop_words_filename,'r','utf-8') as fl:
            for line in fl:
            for word in line.split():
                    stopwords.append(word)
    fl.close()
    unique_words_in_corpus={}
    count=0
    for data in corpus_data:
        corpus_id=count
        temp=[]
        temp=data.split()
        for word in temp:
            if word not in unique_words_in_corpus:
                unique_words_in_corpus[word]=corpus_id

        count+=1
    stopped_unique_words_in_corpus={}
    for word in unique_words_in_corpus:
        if word not in stopwords:
            stopped_unique_words_in_corpus[word]=unique_words_in_corpus[word]
            word_id_mapping[word]=cnt
            cnt+=1
    #print unique_words_in_corpus
    #print stopped_unique_words_in_corpus
    #print word_id_mapping
    feature_vect=[None]*len(corpus_data)
    #score_vect=[None]*cnt
    for i in range(0,len(corpus_data)):
        score_vect=[0]*cnt
        for word in stopped_unique_words_in_corpus:
            if i==stopped_unique_words_in_corpus[word]:
                #print word
                score=tfidf_score(word,i,corpus_data)
                #print score
                score_vect[word_id_mapping[word]]=score
        feature_vect[i]=score_vect
        return feature_vect

def lda(corpus_data):
    stopwords_path='/home/priyansh/Downloads/ltrc/extractor/'
    stop_words_filename='stopwords.txt'
    stopwords=[] #contain all stopwords
    with codecs.open(stopwords_path+stop_words_filename,'r','utf-8') as fl:
            for line in fl:
            for word in line.split():
                    stopwords.append(word)
    fl.close()
    texts=[]
    for data in corpus_data:
        #print data
        tokens=[]
        temp=[]
        stopped_tokens=[]
        temp = data.split()
        for word in temp:
            tokens.append(word)
        #print tokens
        for i in tokens:
            if i not in stopwords:
                stopped_tokens.append(i)
        stemmed_tokens=[]
        for token in stopped_tokens:
            stemmed_token = hi_stem(token)
            stemmed_tokens.append(stemmed_token)
        texts.append(stemmed_tokens)

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    num_topics=5
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=10)
    doc_topics=[]
    for doc_vector in corpus:
            doc_topics.append(ldamodel[doc_vector])


    for i in range(0,len(doc_topics)):
            doc_topics[i] = sorted(doc_topics[i],key=itemgetter(1),reverse=True)

    feature_vect=[]
    for i in doc_topics:
        prob_vect=[0]*num_topics
        #print i
        topic_num = i[0][0]
        topic_prob = i[0][1]
        prob_vect[topic_num]=topic_prob
        feature_vect.append(prob_vect)
        #print i
        #print feature_vect
        return feature_vect


my_featurizer = feature_extractor(tfidf,lda)
X = my_featurizer.transform(corpus)
X_train , X_test , y_train , y_test = train_test_split(corpus,y,test_size=0.2,random_state=42)
pipe = make_pipeline(my_featurizer,svm.LinearSVC())
pipe.fit(X_train,y_train)
pred = pipe.predict(X_test)
print ""Expected output\n""
print y_test
print ""\n""
print ""Output\n""
print pred
print ""\n""
score = pipe.score(X_test,y_test)
print score
print ""\n""
print metrics.confusion_matrix(pred,y_test)
</code></pre>
",,2017-08-17 17:03:51,Custom feature extraction class in scikit-learn,<machine-learning><scikit-learn><feature-extraction><text-classification><supervised-learning>,,,CC BY-SA 3.0,False,False,True,False,True
13675,45687700,2017-08-15 06:44:54,,"<p>When I use the python library <code>gensim</code> and train a Word2Vec model, I can call the function like this <code>word2vec_result.similarity('apple','banana')</code> to get the cosine similarity between apple and banana at local machine. <br>
But in <code>pyspark(version2.2)</code>, I can't find the same function in the document after the model built.<br></p>

<p>Code:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
from pyspark.mllib.feature import Word2Vec
from pyspark.mllib.feature import Word2VecModel
from pyspark import SparkConf, SparkContext
import logging
directory = ""data_path""  
inp = sc.textFile(directory).map(lambda row: row.split("" ""))
model = word2vec_run(inp)
model.save(sc, ""/data/word2vec_model"")
</code></pre>

<p>Are there any simple ways to achieve the goal?</p>
",,2017-08-15 06:44:54,How to compute cosine similarity between two words in Word2Vec model in pyspark,<python><pyspark><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13682,45758225,2017-08-18 13:45:36,,"<p>I am using gensim LdaMulticore to extract topics.It works perfectly fine from Jupyter/Ipython notebook, but when I run from Command prompt, the loop runs indefinitely.
Once the execution arrives at the LdaMulticore function, the execution starts from first. 
Please help me as I am novice</p>

<pre><code>if __name__ == '__main__': 
    model = models.LdaMulticore(corpus=corpus_train, id2word=dictionary, num_topics=20, chunksize=4000, passes=30, alpha=0.5, eta=0.05, decay=0.5, eval_every=10, workers=3, minimum_probability=0)

**RESULTS:-**
Moving to Topics Extraction Script---------------------------------
2017-08-18 18:59:36,448 : INFO : using serial LDA version on this node
2017-08-18 18:59:37,183 : INFO : running online LDA training, 20 topics, 1 passes over the supplied corpus of 400 documents, updating every 12000 documents, evaluating every ~400 documents, iterating 50x with a convergence threshold of 0.001000    
2017-08-18 18:59:37,183 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
2017-08-18 18:59:37,183 : INFO : training LDA model using 3 processes
2017-08-18 18:59:37,214 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #400/400, outstanding queue size 1
Importing required Packages
</code></pre>

<p>Importing required Packages <a href=""https://i.stack.imgur.com/aJaSc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJaSc.jpg"" alt=""enter image description here""></a></p>
",2017-08-18 13:59:12,2018-08-24 08:04:50,gensim LdaMulticore is not running from Command Prompt,<python><nlp><multicore><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13699,45711628,2017-08-16 10:53:19,,"<p>I am currently working on LDA logarithm in python. I want to covert the topics into just a list of the top 20 words in each topic. I tried below code but got different output. 
I want my output in following format: <code>topic=2,words=20</code>.</p>

<pre><code>['men', 'kill', 'soldier', 'order', 'patient', 'night', 'priest', 'becom', 'new', 'speech', 'friend', 'decid', 'young', 'ward', 'state', 'front', 'would', 'home', 'two', 'father']

[""n't"", 'go', 'fight', 'doe', 'home', 'famili', 'car', 'night', 'say', 'next', 'ask', 'day', 'want', 'show', 'goe', 'friend', 'two', 'polic', 'name', 'meet']
</code></pre>

<p>I got below output:</p>

<pre><code>[""(u'ngma', 0.034841332255132154)"", ""(u'video', 0.0073756817356584745)"", ""(u'youtube', 0.006524039676605746)"", ""(u'liked', 0.0065240394176856644)"",]
[""(u'ngma', 0.024537057880333127)"", ""(u'photography', 0.0068263432438681482)"", ""(u'tvallwhite', 0.0029535361359022566)"", ""(u'3', 0.0029252727655122079)""]
</code></pre>

<p>My code:   </p>

<pre><code>`ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = dictionary,passes=50)
lda=ldamodel.print_topics(num_topics=2, num_words=3)

f=open('LDA.txt','w')
f.write(str(lda))
f.close()

topics_matrix = ldamodel.show_topics(formatted=False,num_words=10)
topics_matrix = np.array((topics_matrix),dtype=list)
topic_words = topics_matrix[:, 1]
for i in topic_words:
    print([str(word) for word in i])
    print()`
</code></pre>

<p><strong>edit-1:</strong></p>

<pre><code>topic_words = []
for i in range(3):
    tt = ldamodel.get_topic_terms(i,10)
    topic_words.append([pair[0] for pair in tt])
    print topic_words
</code></pre>

<p>resulted in non expected output:</p>

<pre><code>[[1897, 135, 130, 127, 70, 162, 445, 656, 608, 1019], [1897, 364, 56, 1236, 181, 172, 449, 48, 15, 18], [1897, 163, 11, 70, 166, 345, 480, 9, 60, 351]]
</code></pre>
",2017-08-16 14:26:45,2017-08-16 18:05:16,how to convert the topics into just a list of the top 20 words in each topic in LDA in python,<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
13720,45783267,2017-08-20 14:36:55,,"<p>Please check my code of getting POS vectors.Instead of getting POS tag vectors I am just getting vectors of alphabets in POS.E.g instead of getting POS tags vectors CC,DT,PRP etc I am getting vectors of C,D and P.</p>

<pre><code>#get word and pos tagger
def get_pos_tagger(self, document):

    # tokenizer
    tokens = nltk.word_tokenize(document)
    # get pos tagger
    posTagger = nltk.pos_tag(tokens=tokens)
    tags = []
    for (word, tag) in posTagger:
        tags.append(tag)

    return tags


def get_tag_and_training_data(self):
    tags=[]
    documents=[]
    line_counter=1

    with open(self.filename) as csvfile:
        spamreader = csv.reader(csvfile, delimiter="","")
        for line in spamreader:
            #Initialize the token list for line
            tags.append(int(line[0]))
            documents.append(line[1].lower() + "" "" + line[2].lower())

    return tags,documents


# build pos model
def buildPosModel(self):
    tags, documents = self.get_tag_and_training_data()
    sentences = []

    for document in documents:
     sentences += self.get_pos_tagger(document)
     print(sentences)
    modelPos = gensim.models.Word2Vec(sentences=sentences, size=100, min_count=1, window=5, workers=cores)
    modelPos.wv.save_word2vec_format('word2vecposmodel.bin', binary=False)
    return modelPos
</code></pre>
",2017-08-20 18:57:31,2017-08-20 18:57:31,word2vec on POS tags,<python><nltk><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
13727,45825532,2017-08-22 19:28:32,,"<p>I am training a word2vec model using gensim on 800k browser useragent. My dictionary size is between 300 and 1000 depending on the word frequency limit.
I am looking at few embedding vectors and similarities to see if the algorithm has been converged.
here is my code:</p>

<pre><code>wv_sim_min_count_stat={}
window=7;min_count=50;worker=10;size=128
total_iterate=1000
from copy import copy
for min_count in [50,100,500]:
    print(min_count)

    wv_sim_min_count_stat[min_count]={}
    model=gensim.models.Word2Vec(size=size,window=window,min_count=min_count,iter=1,sg=1)
    model.build_vocab(ua_parsed)


    wv_sim_min_count_stat[min_count]['vocab_counts']=[len(ua_parsed),len(model.wv.vocab),len(model.wv.vocab)/len(ua_parsed)]
    wv_sim_min_count_stat[min_count]['test']=[]

    alphas=np.arange(0.025,0.001,(0.001-0.025)/(total_iterate+1))
    for i in range(total_iterate):
        model.train(ua_parsed,total_examples=model.corpus_count,
                    epochs=model.iter,start_alpha=alphas[i],end_alpha=alphas[i+1])

        wv_sim_min_count_stat[min_count]['test'].append(
        (copy(model.wv['iphone']),copy(model.wv['(windows']),copy(model.wv['mobile']),copy(model.wv['(ipad;']),copy(model.wv['ios']),
         model.similarity('(ipad;','ios')))
</code></pre>

<p>unfortunately even after 1000 epochs there is no sign of convergence in embedding vectors. for example I plot embedding of the first dimension of '(ipad''s embedding vector vs number of epochs below:</p>

<pre><code>for min_count in [50,100,500]:
    plt.plot(np.stack(list(zip(*wv_sim_min_count_stat[min_count]['test']))[3])[:,1],label=str(min_count))

plt.legend() 
</code></pre>

<p><a href=""https://i.stack.imgur.com/dfsJI.png"" rel=""nofollow noreferrer"">embedding of '(ipad' vs number of epochs</a></p>

<p>I looked at many blogs and papers and it seems nobody trained the word2vec beyond 100 epochs. What I am missing here?</p>
",,2017-08-23 06:29:04,embedded vectors doesn't converge in gensim,<python><gensim><convergence><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
13729,45802490,2017-08-21 17:11:02,,"<p>I am doing topic modelling on linguistics papers and I am using the Gensim Phrases to identify frequent collocations. I want to be able to mark terms as 'do-support' and 'it-clefts' as one single word, since they are specific linguistic terminology. However, if I make the Gensim model after taking out stopwords, these collocations will not be found (since they contain stopwords), if I make the model after taking out stopwords (or stopwords not including 'it' or 'do'), it identifies a whole lot of irrelevant collocations. Is there a way to manually add phrases that should be recognised as collocations by the Gensim Phrases? 
Thanks!</p>
",,2017-08-21 22:53:20,Manually add collocations to gensim phraser,<gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
13761,45783781,2017-08-20 15:25:44,,"<p>I am trying to save gensim Doc2vec model. The model is trained on 9M document vectors and vocabulary of around 1M words. But I am getting pickel error. ""top"" shows that the program uses around 13GB of RAM. Also I think since I need to re-train the model for new documents as and when required,  saving all parameters is necessary.</p>

<pre><code>Traceback (most recent call last):
 File ""doc_2_vec.py"", line 61, in &lt;module&gt;

model.save(""/data/model_wl_videos/model"",pickle_protocol=2)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1406, in save
super(Word2Vec, self).save(*args, **kwargs)
 File ""/home/meghana.negi/.local/lib/python2.7/site-packages/gensim/utils.py"", line 504, in save
pickle_protocol=pickle_protocol)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/utils.py"", line 376, in _smart_save
pickle(self, fname, protocol=pickle_protocol)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/utils.py"", line 930, in pickle
_pickle.dump(obj, fout, protocol=protocol)
</code></pre>

<p>MemoryError</p>
",,2017-08-20 15:25:44,Pickel Error while storing Doc2vec gensim model,<nlp><pickle><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13766,45904499,2017-08-27 11:32:49,,"<p>I am getting error while training word2vec with my own vocabulary. I am also not getting why its happening.</p>

<p>Code:</p>

<pre><code>from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

sentences = word2vec.LineSentence('test_data')

model = word2vec.Word2Vec(sentences, size=20)
model.build_vocab(sentences,update=True)
model.train(sentences)

print model.most_similar(['course'])
</code></pre>

<p>It throws an error</p>

<pre><code>2017-08-27 16:50:04,590 : INFO : precomputing L2-norms of word weight vectors
Traceback (most recent call last):
  File ""tryword2vec.py"", line 23, in &lt;module&gt;
    print model.most_similar(['course']) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 1285, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 97, in most_similar
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'course' not in vocabulary""
</code></pre>

<p>test_data contains:</p>

<blockquote>
  <p>Bachelor of Engg is a course. M.Tech is a course. ME is a course.
  B.Tech is a course. Bachelor of Arts is a course. Fashion Design is a
  course. Multimedia is a course. Mechanical engg is a course. Computer
  Science is a course. Electronics is a cource. Engineering is a course.
  MBA is a course. BBA is a course.</p>
</blockquote>

<p>Any help is appreciated?</p>
",,2017-08-27 13:53:23,How to train word2vec with your own vocab,<nlp><stanford-nlp><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,True,False
13778,45860212,2017-08-24 11:16:59,,"<p>I have pre trained continuous bag of words word2vec model.How do I load and which type of tasks could I perform to evaluate it? </p>
",,2017-08-24 11:16:59,How to load and evaluate pretrained word2vec model trained on CBOW algorithm,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13781,45869881,2017-08-24 19:53:46,,"<p>I want to calculate the similarity between two sentences using word2vectors, I am trying to get the vectors of a sentence so that i can calculate the average of a sentence vectors to find the cosine similarity. i have tried this code but its not working. the output it gives the sentence-vectors with ones. i want the actual vectors of sentences in sentence_1_avg_vector &amp; sentence_2_avg_vector.</p>

<p>Code:</p>

<pre><code>    #DataSet#
    sent1=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india'],['What', 'story', 'Kohinoor', 'KohiNoor', 'Diamond']]
    sent2=[['What', 'step', 'step', 'guide', 'invest', 'share', 'market'],['What', 'would', 'happen', 'Indian', 'government', 'stole', 'Kohinoor', 'KohiNoor', 'diamond', 'back']]
    sentences=sent1+sent2

    #''''Applying Word2vec''''#
    word2vec_model=gensim.models.Word2Vec(sentences, size=100, min_count=5)
    bin_file=""vecmodel.csv""
    word2vec_model.wv.save_word2vec_format(bin_file,binary=False)

    #''''Making Sentence Vectors''''#
    def avg_feature_vector(words, model, num_features, index2word_set):
        #function to average all words vectors in a given paragraph
        featureVec = np.ones((num_features,), dtype=""float32"")
        #print(featureVec)
        nwords = 0
        #list containing names of words in the vocabulary
        index2word_set = set(model.wv.index2word)# this is moved as input param for performance reasons
        for word in words:
            if word in index2word_set:
                nwords = nwords+1
                featureVec = np.add(featureVec, model[word])
                print(featureVec)
        if(nwords&gt;0):
            featureVec = np.divide(featureVec, nwords)
        return featureVec

    i=0
    while i&lt;len(sent1):
        sentence_1_avg_vector = avg_feature_vector(mylist1, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))
        print(sentence_1_avg_vector)

        sentence_2_avg_vector = avg_feature_vector(mylist2, model=word2vec_model, num_features=300, index2word_set=set(word2vec_model.wv.index2word))
        print(sentence_2_avg_vector)

        sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)
        print(sen1_sen2_similarity)

        i+=1
</code></pre>

<p>the output this code gives:</p>

<pre><code>[ 1.  1.  ....  1.  1.]
[ 1.  1.  ....  1.  1.]
0.999999898245
[ 1.  1.  ....  1.  1.]
[ 1.  1.  ....  1.  1.]
0.999999898245
</code></pre>
",2017-10-21 10:59:40,2020-06-19 00:16:07,Finding Similarity between 2 sentences using word2vec of sentence with python,<python><nlp>,,,CC BY-SA 3.0,False,False,True,False,False
13782,45810954,2017-08-22 07:03:01,,"<p>I have created two models using gensim word2vec. Now I want to merge these two models in a way that I get the union of these two models.</p>

<p>Eg: </p>

<ul>
1. Model one has following vocabulary
</ul>

<pre><code>{""Hi"", ""Hello"", ""World""}
</code></pre>

<ul>
2. Model two has the following vocabulary
</ul>

<pre><code>{""Hi"", ""King"", ""Hello"", ""Human""}
</code></pre>

<p>Now I want to use these two models and create a new model which will have the following vocabulary</p>

<pre><code>{""Hi"", ""Hello"", ""World"", ""King"", ""Human""}
</code></pre>
",2017-08-22 08:20:23,2017-10-12 12:46:28,How to create a model using trained models?,<machine-learning><nlp><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13783,45813264,2017-08-22 09:02:54,,"<p>I am training word vectors on particular text corpus using fast text.
Fasttext provides all the necessary mechanics and options for training word vectors and when looked with tsne, the vectors are amazing. I notice gensim has a wrapper for fasttext which is good for accessing vectors. </p>

<p>for my task, I have many text corpuses. I need to use the above trained vectors again with new corpus and use the trained vectors again on new discovered corpuses. fasttext doesnot provide this function. I donot see any package that achieves this or may be I am lost. I see in google <a href=""https://groups.google.com/forum/#!topic/gensim/Y_WmJST9xx8"" rel=""nofollow noreferrer"">forum</a> gensim provides intersect_word2vec_format, but cannot understand or find usage tutorial for this. There is another <a href=""https://stackoverflow.com/questions/41096821/word2vec-model-intersect-word2vec-format"">question</a> open similar to this with no answer.</p>

<p>So apart from gensim, is there any other way to train the models like above.</p>
",,2017-08-22 09:02:54,train word2vec with pretrained vectors,<nlp><gensim><word2vec><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
13786,45847370,2017-08-23 18:52:07,,"<pre><code>from gensim.parsing import PorterStemmer
from gensim.models import Word2Vec, Phrases

class SentenceClass(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            with open(os.path.join(self.dirname,fname), 'r') as myfile:
                doc = myfile.read().replace('\n', ' ')
                for sent in tokenize.sent_tokenize(doc.lower()):
                    yield [Stemming.stem(word)\
                    for word in word_tokenize(re.sub(""[^A-Za-z]"", "" "",sent))\
                    if word not in stopwords]
</code></pre>

<p>Now of the two approaches:<br/>
1)</p>

<pre><code>model = Word2Vec(SentenceClass(data_dir_path), size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>The above one runs really fine with no warning</p>

<p>2)</p>

<pre><code>bigram_transformer = Phrases(SentenceClass(data_dir_path), min_count=1)
model = Word2Vec(bigram_transformer[SentenceClass(data_dir_path)], size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>produces the warning:</p>

<pre><code>WARNING:gensim.models.word2vec:train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).
WARNING:gensim.models.word2vec:supplied example count (0) did not equal expected count (30)
</code></pre>

<p>Now I do understand the difference between generator and iterators, and I am passing iterators, which is verified by printing below commands multiple times:</p>

<pre><code>print(list(SentenceClass(data_dir_path)))
print(list(SentenceClass(data_dir_path)))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
</code></pre>

<p>And it prints thing fine, but I am still not sure why the warning of ""empty iterator"" for the second case, am I missing something here ? </p>
",2017-08-23 19:22:05,2017-08-23 20:00:37,Issue with gensim.models.Phrases,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13790,45906577,2017-08-27 15:29:57,,"<p>When I show the output of ldamodel learnt by gensim</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
ldamodel.print_topics(num_topics=2, num_words=4)
</code></pre>

<p>It will show all unicode character with \u</p>

<pre><code>[(0,
  u'0.128*""dddf"" + 0.128*""\u4f60\u597d"" + 0.046*""o"" + 0.046*""love""'),
 (1,
  u'0.057*""\u0646\u062f"" + 0.057*""\u0627\u06cc\u0646\u0647"" + 0.057*""\u0646\u0645\u06cc"" + 0.057*""\u0628\u06cc\u0637\u0631\u0641\u0647""')]
</code></pre>

<p>Those unicode characters are chinese or persian characters, how should I make it appear as those characters?</p>
",2017-08-27 15:35:14,2017-08-27 15:35:14,Make output of print_topics shows unicode character,<python><python-2.7><unicode><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
13798,45960671,2017-08-30 12:38:55,,"<p>I am using <code>gensim</code> <code>wmdistance</code> for calculating similarity between a reference sentence and 1000 other sentences. </p>

<pre><code>    model = gensim.models.KeyedVectors.load_word2vec_format(
     'GoogleNews-vectors-negative300.bin', binary=True)
    model.init_sims(replace=True)  

    reference_sentence = ""it is a reference sentence""
    other_sentences = [1000 sentences]
    index = 0
    for sentence in other_sentences: 
      distance [index] = model.wmdistance(refrence_sentence, other_sentences)
      index = index + 1
</code></pre>

<p>According to <code>gensim</code> <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">source code</a>, <code>model.wmdistance</code> returns the following:</p>

<pre><code>emd(d1, d2, distance_matrix)
</code></pre>

<p>where </p>

<pre><code>d1 =  # Compute nBOW representation of reference_setence.
d2 =  # Compute nBOW representation of other_sentence (one by one).
distance_matrix = see the source code as its a bit too much to paste it here.
</code></pre>

<p>This code is inefficient in two ways for my use case. </p>

<p>1) For the reference sentence, it is repeatedly calculating d1 (1000 times) for the distance function <code>emd(d1, d2, distance_matrix)</code>.</p>

<p>2) This distance function is called by multiple users from different points which repeat this whole process of <code>model.wmdistance(doc1, doc2)</code> for the same other_sentences and it is computationally expensive. For this 1000 comparisons, it takes around 7-8 seconds. </p>

<p>Therefore, I would like to isolate the two tasks. The final calculation of distance: <code>emd(d1, d2, distance_matrix)</code> and the preparation of these inputs: d1, d2, and distance matrix. As distance matrix depends on both so at least its input preparation should be isolated from the final matrix calculation. </p>

<p>My initial plan is to create three customized functions: </p>

<pre><code>d1 = prepared1(reference_sentence)
d2 = prepared2(other_sentence)
distance_matrix inputs = prepare inputs 
</code></pre>

<p>Is it possible to do this with this <code>gensim</code> function or should I just go my own customized version? Any ideas and solutions to deal with this problem in a better way?</p>
",2017-08-31 00:48:11,2018-12-30 06:14:30,Optimizing Gensim word mover's distance function for speed (wmdistance),<python><nlp><nltk><gensim><word2vec>,,,CC BY-SA 3.0,True,False,True,False,False
13814,45943832,2017-08-29 16:13:47,,"<p>I am trying to train a Doc2Vec model using gensim with 114M unique documents/labels and vocab size of around 3M unique words. I have 115GB Ram linux machine on Azure.
When I run build_vocab, the iterator parses all files and then throws memory error as listed below.</p>

<pre><code>    Traceback (most recent call last):
  File ""doc_2_vec.py"", line 63, in &lt;module&gt;
    model.build_vocab(sentences.to_array())
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 579, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 752, in finalize_vocab
    self.reset_weights()
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 662, in reset_weights
    self.docvecs.reset_weights(self)
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 390, in reset_weights
    self.doctag_syn0 = empty((length, model.vector_size), dtype=REAL)
MemoryError
</code></pre>

<p>My code-</p>

<pre><code>import parquet
import json
import collections
import multiprocessing


# gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources   
        flipped = {}

    def __iter__(self):
        for src in self.sources:
            with open(src) as fo:
               for row in parquet.DictReader(fo, columns=['Id','tokens']):
                    yield LabeledSentence(utils.to_unicode(row['tokens']).split('\x01'), [row['Id']])

## list of files to be open ##
sources =  glob.glob(""/data/meghana_home/data/*"")
sentences = LabeledLineSentence(sources)

#pre = Doc2Vec(min_count=0)
#pre.scan_vocab(sentences)
""""""
for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
    print(""done"")
""""""

NUM_WORKERS = multiprocessing.cpu_count()
NUM_VECTORS = 300
model = Doc2Vec(alpha=0.025, min_alpha=0.0001,min_count=15, window=3, size=NUM_VECTORS, sample=1e-4, negative=10, workers=NUM_WORKERS) 
model.build_vocab(sentences)
print(""built vocab......."")
model.train(sentences,total_examples=model.corpus_count, epochs=10)
</code></pre>

<p>Memory usage as per top is-</p>

<p><a href=""https://i.stack.imgur.com/xarz0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xarz0.png"" alt=""enter image description here""></a></p>

<p>Can someone please tell me how much is the expected memory? What is better option- Adding swap space and slow the process or add more memory so that cost of cluster might eventually be equivalent.
What vectors gensim stores in memory? Any flag that i am missing for memory efficient usage.</p>
",2017-08-30 05:23:15,2017-08-30 05:23:15,Gensim Doc2vec finalize_vocab Memory Error,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13824,45926917,2017-08-28 20:31:20,,"<p>I'm working on a document comparison engine / search engine. I'm currently using it as follows...</p>

<pre><code>search_doc = [""test search""]
documents = [""doc 1 text"", ""doc 2 text"", ""doc 3 text"", ""...""]
</code></pre>

<p>And then comparing the results.</p>

<p>What I would like to do (in the simplest terms possible) is have multi-dimensional documents (a document that has multiple dimensions, rather than just the ""document"")... for example..</p>

<pre><code>documents = {
                { ""doc 1 title"", ""doc 1 body"", ""doc 1 tags"" },
                { ""doc 2 title"", ""doc 2 body"", ""doc 2 tags"" },
                { ""doc 3 title"", ""doc 3 body"", ""doc 3 tags"" }
                { ... }
            }
</code></pre>

<p>And also be able to weight the results (for example, title is 0.6, body is 0.4, etc).</p>

<p>My question is... is there a way to do this within Gensim, or do I need to create a separate document for each meta item of the document (for example, comparing to each meta item (title, body, tags) as a separate document, and then combining weights after the fact using the document key/id? </p>

<p>I'm not sure i'm doing a good job of explaining this, but please let me know if I can improve my question.</p>

<p>Thank you.</p>
",,2017-08-28 23:59:21,Multi-dimensional documents with Gensim,<python><machine-learning><tensorflow><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,False
13827,45876711,2017-08-25 07:51:09,,"<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>

<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        
2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          
</code></pre>

<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>

<p><strong>EDIT:</strong> training and scoring phases in my code:</p>

<p>Training and saving to disk:</p>

<pre><code>w2v_size = 300
word2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is used
word2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
corpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)
corpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
</code></pre>

<p>Loading and scoring:</p>

<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
words = [t for t in proc_text if t in w2v.wv]
corpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
scores_w2v = np.array(corpus_w2v_wmd_index[words])  
</code></pre>
",2017-08-28 12:24:02,2017-08-29 00:50:55,Gensim word2vec WMD similarity dictionary,<python><nlp><word><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13844,45948533,2017-08-29 21:22:30,,"<p>Gensim implements a function called ""doesnt_match"" that return an outlier word from a list.</p>

<p>The function is called on a wordvector object.</p>

<p>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split())
'cereal'</p>

<p>The documentation is not specifying how this function really work (what is the computation background)</p>

<p>Anyone knows ?</p>
",,2017-08-30 00:12:00,How is the Gensim doesnt_match function working?,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13846,46001910,2017-09-01 14:01:30,,"<p>I've been doing LDA topic models of narrative reports in natural language for a research project (using Gensim with python). I have several smallish corpora (from 1400 to 200 docs each  I know, that's tiny!) that I'd like to compare, but I don't know how to do that beyond looking at each LDA model (for instance with pyLDAviz). My academic background is not in CS, and I'm still a bit new to NLP.</p>

<p>What are some good ways to compare topics across corpora/topic models? For instance, is it possible to estimate how much two LDA models overlap? Or are there other ways to assess the topic similarity of several corpora?</p>

<p>Thanks in advance for your help!</p>
",,2017-09-06 16:52:59,What's the best way to compare several corpora in natural language?,<python><nlp><nltk><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,False
13855,45917969,2017-08-28 11:31:30,,"<p>I am using doc2vec to convert the top 100 tweets of my followers in vector representation (say v1.....v100). After that I am using the vector representation to do the K-Means clusters. </p>

<pre><code>model = Doc2Vec(documents=t, size=100, alpha=.035, window=10, workers=4, min_count=2)
</code></pre>

<p>I can see that cluster 0 is dominated by some values (say v10, v12, v23, ....). My question is what does these v10, v12 ... etc represents. Can I deduce that these specific column clusters specific keywords of document.  </p>
",,2019-04-19 21:35:02,How to intrepret Clusters results after using Doc2vec?,<python><scikit-learn><cluster-analysis><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,True
13857,46064892,2017-09-06 00:00:34,,"<p>I'm trying to learn the skip-gram model within word2vec, however I'm confused by some of the basic concepts.  To start, here is my current understanding of the model motivated with an example.  I am using Python <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> as I go.</p>

<p>Here I have a corpus with three sentences.</p>

<pre><code>sentences = [
    ['i', 'like', 'cats', 'and', 'dogs'],
    ['i', 'like', 'dogs'],
    ['dogs', 'like', 'dogs']
]
</code></pre>

<p>From this, I can determine my vocabulary, <code>V = ['and', 'cats', 'dogs', 'i', 'like']</code>.</p>

<p>Following <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">this paper</a> by Tomas Mikolov (and others)</p>

<blockquote>
  <p>The basic Skip-gram formulation defines p(w_t+j |w_t) using the softmax
  function:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/hza7Q.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hza7Q.gif"" alt=""skipgram""></a></p>

<blockquote>
  <p>where v_w and v_w are the input and output vector representations
  of w, and W is the number of words in the vocabulary.</p>
</blockquote>

<p>To my understanding, the skip-gram model involves two matrices (I'll call them <em>I</em> and <em>O</em>) which are the vector representations of ""input/center"" words and the vector representation of ""output/context"" words.  Assuming d = 2 (vector dimension or 'size' as its called in genism), <em>I</em> should be a 2x5 matrix and <em>O</em> should be a 5x2 matrix.  At the start of the training procedure, these matrices are filled with random values (yes?).  So we might have</p>

<pre><code>import numpy as np
np.random.seed(2017)

I = np.random.rand(5,2).round(2)  # 5 rows by 2 cols
[[ 0.02  0.77] # and
 [ 0.45  0.12] # cats
 [ 0.93  0.65] # dogs
 [ 0.14  0.23] # i
 [ 0.23  0.26]] # like

O = np.random.rand(2,5).round(2)  # 2 rows by 5 cols
  #and  #cats #dogs  #i   #like 
[[ 0.11  0.63  0.39  0.32  0.63]
 [ 0.29  0.94  0.15  0.08  0.7 ]]
</code></pre>

<p>Now if I want to calculate the probability that the word ""dogs"" appears in the context of ""cats"" I should do</p>

<p>exp([0.39, 0.15] * [0.45  0.12])/(...) = (0.1125)/(...)</p>

<p>A few questions on this:</p>

<ol>
<li>Is my understanding of the algorithm correct thus far?</li>
<li>Using genism, I can train a model on this data using</li>
</ol>

<p>&nbsp;</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(sentences, sg = 1, size=2, window=1, min_count=1)
model.wv['dogs']  # array([ 0.06249372,  0.22618999], dtype=float32)
</code></pre>

<p>For the array given, is that the vector for ""dogs"" in the Input matrix or the Output matrix?  Is there a way to view both matrices in the final model?</p>

<ol start=""3"">
<li>Why does <code>model.wv.similarity('cats','cats')</code> = 1?  I thought this should be closer to 0, since the data would indicate that the word ""cats"" is unlikely to occur in the context of the word ""cats"".</li>
</ol>
",2017-09-06 00:31:04,2017-09-06 01:59:37,word2vec training procedure clarification,<gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13863,46047506,2017-09-05 05:23:46,,"<p>I have applied Doc2vec to convert documents into vectors.After that, I used the vectors in clustering and figured out the 5 nearest/most similar document to the centroid of each cluster. Now I need to find the most dominant or important terms of these documents so that I can figure out the characteristics of each cluster. 
My question is is there any way to figure out the most dominat or simlar terms/word of a document in Doc2vec . I am using python's gensim package for the Doc2vec implementaton </p>
",,2018-05-23 20:53:55,How to find most similar terms/words of a document in doc2vec?,<python><cluster-analysis><gensim><word2vec><doc2vec>,2017-09-06 23:04:01,,CC BY-SA 3.0,False,False,True,False,False
13873,46065514,2017-09-06 01:35:34,,"<p>I am trying to get most probable sequence of word using gensim word2vec model. I have found a pretrained model which provides these files:</p>

<pre><code>word2vec.bin
word2vec.bin.syn0.npy
word2vec.bin.syn1neg.npy
</code></pre>

<p>This is my code trying to get the probability of the sentence with this model:</p>

<pre><code>model = model.wv.load(word_embedding_model_path)
model.hs = 1
model.negative = 0
print model.score(sentence.split("" ""))
</code></pre>

<p>While running this code I am getting this error:</p>

<pre><code>AttributeError: 'Word2Vec' object has no attribute 'syn1'
</code></pre>

<p>Can anyone help me figure out how to solve the problem. In general, I want to use some pretrained model to get the probability of sequence of word appearing together.</p>
",,2017-09-06 17:42:38,Getting probability of the text given word embedding model in gensim word2vec model,<python><nlp><gensim><word2vec><language-model>,,,CC BY-SA 3.0,False,False,True,False,False
13874,46065773,2017-09-06 02:12:56,,"<p>In word2vec, after training, we get two weight matrixes:1.input-hidden weight matrix; 2.hidden-output weight matrix. and people will use the input-hidden weight matrix as the word vectors(each row corresponds to a word, namely, the word vectors).Here comes to my confusions:</p>

<ol>
<li>why people use input-hidden weight matrix as the word vectors instead of the hidden-output weight matrix.</li>
<li>why don't we just add softmax activation function to the hidden layers rather than output layers, thus preventing time-consuming.</li>
</ol>

<p>Plus, clarifying remarks on the intuition of how word vectors can be obtained like this will be appreciated.</p>
",,2020-10-17 12:31:19,why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?,<nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13912,46146241,2017-09-10 22:45:20,,"<pre><code>from gensim import corpora, models, similarities

documents = [""This is a book about cars, dinosaurs, and fences""]

# remove common words and tokenize
stoplist = set('for a of the and to in - , is'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# Remove commas
texts[0] = [text.replace(',','') for text in texts[0]]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = ""I like cars and birds""
vec_bow = dictionary.doc2bow(doc.lower().split())

vec_lsi = lsi[vec_bow] 
index = similarities.MatrixSimilarity(lsi[corpus]) 

sims = index[vec_lsi] # perform a similarity query against the corpus
print(sims)
</code></pre>

<p>In the above code I am comparing how much ""This is a book about cars, dinosaurs, and fences"" is similar to ""I like cars and birds"" using the cosine similarity technique.</p>

<p>The two sentences have effectively 1 words in common, which is ""cars"", however when I run the code I get that they are 100% similar.  This does not make sense to me.</p>

<p>Can someone suggest how to improve my code so that I get a reasonable number?</p>
",,2017-09-11 01:30:24,Text similarity with gensim and cosine similarity,<python><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
13913,46147013,2017-09-11 01:14:35,,"<p>I want to extract all bigrams and trigrams of the given sentences. </p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""Human Computer Interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
trigram = Phrases(bigram(sentence_stream, min_count=1, threshold=2, delimiter=b' '))

for sent in sentence_stream:
    #print(sent)
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>The code works fine for bigrams and capture 'new york' and 'machine learning' ad bigrams.</p>

<p>However, I get the following error when I try to insert trigrams.</p>

<pre><code>TypeError: 'Phrases' object is not callable
</code></pre>

<p>Please let me know, how to correct my code.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example documentation</a> of gensim.</p>
",2017-09-27 04:11:06,2018-07-30 13:45:23,Error getting trigrams using gensim's Phrases,<python><nlp><data-mining><text-mining><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13915,46148182,2017-09-11 04:28:12,,"<p>I want to get bigrams and trigrams from the example sentences I have mentioned.</p>

<p>My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)</p>

<p><strong>Approach 1</strong> Mentioned below is my code using Phrases in Gensim.</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p><strong>Approach 2</strong> I even tried to use Phraser and Phrases both, but it didn't work.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>Please help me to fix this issue of getting trigrams.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">example documentation</a> of Gensim.</p>
",2017-09-11 07:33:33,2017-09-11 15:19:31,Issues in getting trigrams using Gensim,<python><data-mining><text-mining><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13923,46013294,2017-09-02 11:26:06,,"<p>Just reading through the doc2vec commands on the gensim page. </p>

<p>I am curious about  the command""intersect_word2vec_format"" . </p>

<p>My understanding of this command is it lets me inject vector values from a pretrained word2vec model into my doc2vec model and then train my doc2vec model using the pretrained word2vec values rather than generating the word vector values from my document corpus. The result is that I get a more accurate doc2vec model because I am using pretrained w2v values which was generated from a much larger corpus of data compared to my relatively small document corpus. </p>

<p>Is my understanding of this command correct or not even close?  ;-) </p>
",,2017-09-03 20:54:19,"gensim doc2vec ""intersect_word2vec_format"" command",<nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13926,46168239,2017-09-12 05:00:47,,"<p>I'm trying to solve a problem of sentence comparison using naive approach of summing up word vectors and comparing the results. My goal is to match people by interest, so the dataset consists of names and short sentences describing their hobbies. The batches are fairly small, few hundreds of people, so i wanted to give it a try before digging into doc2vec.</p>

<p>I prepare the data by cleaning it completely, removing stop words, tokenizing and lemmatizing. I use pre-trained model for word vectors which returns adequate results when finding similarities for some test words. Also tried summing up the sentence words to find similarities in the original model - the matches do make sense. The similarities would be around general sense of the phrase.</p>

<p>For sentence matching I'm trying the following: create an empty model</p>

<pre><code>b = gs.models.Word2Vec(min_count=1, size=300, sample=0, hs=0)
</code></pre>

<p>Build vocab out of names (or person id's), no training</p>

<pre><code>#first create vocab with an empty vector
test = [['test']]
b.build_vocab(test)
b.wv.syn0[b.wv.vocab['test'].index] = b.wv.syn0[b.wv.vocab['test'].index]*0

#populate vocab from an array
b.build_vocab([personIds], update=True)
</code></pre>

<p>Summ each sentence's word vectors and store the results into the model for a corresponding id</p>

<pre><code>#sentences are pulled from pandas dataset df. 'a' is a pre-trained model i use to get vectors for each word

def summ(phrase, start_model):
    '''
    vector addition function
    '''
    #starting with a vector of 0's
    sum_vec = start_model.word_vec(""cat_NOUN"")*0
    for word in phrase:
        sum_vec += start_model.word_vec(word)
    return sum_vec

for i, row in df.iterrows():
    try:
        personId = row[""ID""]
        summVec = summ(df.iloc[i,1],a)
        #updating syn0 for each name/id in vocabulary
        b.wv.syn0[b.wv.vocab[personId].index] = summVec
    except:
        pass
</code></pre>

<p>I understand that i shouldn't be expecting much accuracy here, but the t-SNE print doesn't show any clustering whatsoever. Finding similarities method also fails to find matches (&lt;0.2 similarity coefficient basically for everything). [<img src=""https://i.stack.imgur.com/w81nX.png"" alt=""]plot of the entire model[1]""></p>

<p>Wondering if anyone has an idea of where did i go wrong? Is my approach valid at all?</p>
",,2017-09-12 18:22:55,Sentence matching with gensim word2vec: manually populated model doesn't work,<python><nlp><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13928,46072991,2017-09-06 10:32:37,,"<p>How does word2vec create vectors for words? I trained two word2vec models using two different files (from commoncrawl website) but I am getting same word vectors for a given word from both models. </p>

<p>Actually, I have created multiple word2vec models using different text files from the commoncrawl website. Now I want to check which model is better among all. How can select the best model out of all these models and why I am getting same word vectors for different models?</p>

<p>Sorry, If the question is not clear. </p>
",,2017-09-06 17:32:11,How does word embedding/ word vectors work/created?,<neural-network><nlp><deep-learning><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13932,46129335,2017-09-09 09:49:15,,"<p>I am currently using uni-grams in my word2vec model as follows.</p>

<pre><code>def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    #Returns a list of sentences, where each sentence is a list of words
    #
    #NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) &gt; 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences
</code></pre>

<p>However, then I will miss important bigrams and trigrams in my dataset.</p>

<pre><code>E.g.,
""team work"" -&gt; I am currently getting it as ""team"", ""work""
""New York"" -&gt; I am currently getting it as ""New"", ""York""
</code></pre>

<p>Hence, I want to capture the important bigrams, trigrams etc. in my dataset and input into my word2vec model.</p>

<p>I am new to wordvec and struggling how to do it. Please help me.</p>
",2017-09-10 18:49:11,2019-05-09 06:01:54,Get bigrams and trigrams in word2vec Gensim,<python><tokenize><word2vec><gensim><n-gram>,,,CC BY-SA 3.0,True,False,True,False,False
13940,46086858,2017-09-07 02:16:18,,"<p>I am new in 'Word2Vec' in Gensim. I want to build a Word2Vec model for the text (Extracted from Wikipedia: Machine Learning) and find <strong>most similar words</strong> to 'Machine Learning'.</p>

<p>My current code is as follows.</p>

<pre><code># import modules &amp; set up logging
from gensim.models import Word2Vec

sentences = ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4]  such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.""
# train word2vec on the sentences
model = Word2Vec(sentences, min_count=1)
vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, for vocab I get one character output.</p>

<pre><code>['M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'r']
</code></pre>

<p>Please help me to get the most_similar_words by using using <strong>model.most_similar</strong></p>
",2017-09-07 02:26:59,2017-09-07 02:34:23,Word2Vec in Gensim using model.most_similar,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13947,46168600,2017-09-12 05:33:26,,"<p>I trying to import gensim.</p>

<p>I have the following code</p>

<pre><code>import gensim
model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-  
vectors-negative300.bin', binary=True)  
</code></pre>

<p>I got the following error.</p>

<pre><code>ImportError                               Traceback (most recent call  
last)
&lt;ipython-input-5-50007be813d4&gt; in &lt;module&gt;()
----&gt; 1 import gensim
  2 model = gensim.models.Word2Vec.load_word2vec_format('./model  
/GoogleNews-vectors-negative300.bin', binary=True)

ImportError: No module named 'gensim'
</code></pre>

<p>I installed gensim in python. I use genssim for word2vec.</p>
",,2020-07-22 22:03:11,gensim error : no module named gensim,<python><linux><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13950,46173339,2017-09-12 09:55:39,,"<p>I have problem installing gensim module. I installed numpy and scipy dependent modules successfully but was getting error while installing gensim.
I tried solutions given in <a href=""https://stackoverflow.com/questions/35991403/python-pip-install-gives-command-python-setup-py-egg-info-failed-with-error-c"">Python pip install gives &quot;Command &quot;python setup.py egg_info&quot; failed with error code 1&quot;</a>
but none of them worked.</p>

<p>Here is the error:</p>

<pre><code>    &gt;pip install --target=""D:\python\packages"" gensim
Collecting gensim
  Using cached gensim-2.3.0-cp36-cp36m-win32.whl
Collecting scipy&gt;=0.18.1 (from gensim)
  Using cached scipy-0.19.1.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\setup.py"", line 416, in &lt;module&gt;
        setup_package()
      File ""C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\setup.py"", line 412, in setup_package
        setup(**metadata)
      File ""c:\program files (x86)\python36-32\lib\distutils\core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 315, in __init__
        self.fetch_build_eggs(attrs['setup_requires'])
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 361, in fetch_build_eggs
        replace_conflicting=True,
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 850, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 1122, in best_match
        return self.obtain(req, installer)
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 1134, in obtain
        return installer(requirement)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 429, in fetch_build_egg
        return cmd.easy_install(req)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\command\easy_install.py"", line 653, in easy_install
        not self.always_copy, self.local_index
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 636, in fetch_distribution
        dist = find(requirement)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 617, in find
        dist.download_location = self.download(dist.location, tmpdir)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 566, in download
        found = self._download_url(scheme.group(1), spec, tmpdir)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 805, in _download_url
        return self._attempt_download(url, filename)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 811, in _attempt_download
        headers = self._download_to(url, filename)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 726, in _download_to
        block = fp.read(bs)
      File ""c:\program files (x86)\python36-32\lib\http\client.py"", line 449, in read
        n = self.readinto(b)
      File ""c:\program files (x86)\python36-32\lib\http\client.py"", line 493, in readinto
        n = self.fp.readinto(b)
      File ""c:\program files (x86)\python36-32\lib\socket.py"", line 586, in readinto
        return self._sock.recv_into(b)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 1002, in recv_into
        return self.read(nbytes, buffer)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 865, in read
        return self._sslobj.read(len, buffer)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 625, in read
        v = self._sslobj.read(len, buffer)
    ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\
</code></pre>
",,2018-03-19 11:32:54,Unable to install gensim in python,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13953,46083322,2017-09-06 19:52:17,,"<p>Is it possible to to train a doc2vec model where a single document has multiple tags?
For example, in movie reviews,</p>

<pre><code>doc0 = doc2vec.TaggedDocument(words=review0,tags=['UID_0','horror','action'])
doc1 = doc2vec.TaggedDocument(words=review1,tags=['UID_1','drama','action','romance'])
</code></pre>

<p>In such case where each document has a unique tag (UID) and multiple categorical tags, how do I access the vector after the training? For example, what would be the most proper syntax to call</p>

<pre><code>model['UID_1']
</code></pre>
",,2017-09-08 05:41:33,Multiple tags for single document in doc2vec. TaggedDocument,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13963,46157805,2017-09-11 14:12:49,,"<p>I have created a doc2vec model of size of 100 dimensions. From what I understand from my reading that these dimensions are features of my model. How can I identify what these dimensions are exactly.</p>
",,2017-09-11 17:51:30,Identify the dimensions in doc2vec model,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
13964,46157937,2017-09-11 14:19:26,,"<p>I am running the gensim word2vec code on a corpus of resumes(stopwords removed) to identify similar context words in the corpus from a list of pre-defined keywords.</p>

<p>Despite several iterations with input parameters,stopword removal etc the similar context words are not at all making sense(in terms of distance or context)
Eg. correlation and matrix occurs in the same window several times yet matrix doesnt fall in the most_similar results for correlation </p>

<p>Following are the details of the system and codes
gensim 2.3.0 ,Running on Python 2.7 Anaconda
Training Resumes :55,418 sentences
Average words per sentence : 3-4 words(post stopwords removal)
Code :</p>

<pre><code>    wordvec_min_count=int()
    size = 50
    window=10
    min_count=5
    iter=50
    sample=0.001
    workers=multiprocessing.cpu_count()
    sg=1
    bigram = gensim.models.Phrases(sentences, min_count=10, threshold=5.0)
    trigram = gensim.models.Phrases(bigram[sentences], min_count=10, threshold=5.0)
    model=gensim.models.Word2Vec(sentences = trigram[sentences], size=size, alpha=0.005, window=window, min_count=min_count,max_vocab_size=None,sample=sample, seed=1, workers=workers, min_alpha=0.0001, sg=sg, hs=1, negative=0, cbow_mean=1,iter=iter)

model.wv.most_similar('correlation')
Out[20]: 
[(u'rankings', 0.5009744167327881),
 (u'salesmen', 0.4948525130748749),
 (u'hackathon', 0.47931140661239624),
 (u'sachin', 0.46358123421669006),
 (u'surveys', 0.4472047984600067),
 (u'anova', 0.44710394740104675),
 (u'bass', 0.4449636936187744),
 (u'goethe', 0.4413239061832428),
 (u'sold', 0.43735259771347046),
 (u'exceptional', 0.4313117265701294)]
</code></pre>

<p>I am lost as to why the results are so random ? Is there anyway to check the accuracy for word2vec ?</p>

<p>Also is there an alternative of word2vec for most_similar() function ? I read about gloVE but was not able to install the package.</p>

<p>Any information in this regard would be helpful</p>
",,2017-09-11 17:59:21,Why Word2Vec's most_similar() function is giving senseless results on training?,<python-2.7><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
13980,46137572,2017-09-10 05:27:14,,"<p>I am trying to get the bigrams in the sentences using Phrases in Gensim as follows.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
#print(sentence_stream)
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)

for sent in sentence_stream:
    tokens_ = bigram_phraser[sent]
    print(tokens_)
</code></pre>

<p>Even though it catches ""new"", ""york"" as ""new york"", it does not catch ""machine"", learning as ""machine learning""</p>

<p>However, in the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example shown in Gensim Website</a> they were able to catch the words ""machine"", ""learning"" as ""machine learning"".</p>

<p>Please let me know how to get ""machine learning"" as a bigram in the above example</p>
",,2017-09-11 01:13:28,Error in extracting phrases using Gensim,<python><data-mining><text-mining><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13983,46141998,2017-09-10 14:51:35,,"<p>I trying to install gensim using the following command:</p>

<pre><code>sudo pip install gensim
</code></pre>

<p>I got the following error messages:</p>

<pre><code>The directory '/home/woojung/.cache/pip/http' or its parent directory is
not owned by the current user and the cache has been disabled. Please  
check the permissions and owner of that directory. If executing pip with 
sudo, you may want sudo's -H flag.


The directory '/home/woojung/.cache/pip' or its parent directory is not 
owned by the current user and caching wheels has been disabled. check the   
permissions and owner of that directory. If executing pip with sudo, you  
may want sudo's -H flag.
Collecting gensim
Downloading gensim-2.3.0.tar.gz (17.2MB)
99% || 17.2MB 422kB/s eta  
0:00:01Exception:
Traceback (most recent call last):
File ""/home/woojung/.local/lib/python2.7/site-packages 
/pip/basecommand.py"", line 215, in main
status = self.run(options, args)
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/commands 
/install.py"", line 324, in run
requirement_set.prepare_files(finder)

....


File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/filewrapper.py"", line 50, in _close
self.__callback(self.__buf.getvalue())
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/controller.py"", line 275, in cache_response
self.serializer.dumps(request, response, body=body),
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/serialize.py"", line 87, in dumps
).encode(""utf8""),
MemoryError
</code></pre>

<p>I installed numpy and scipy. How can I fix this problem?</p>
",2017-09-10 15:15:35,2017-09-10 15:21:15,install gensim error in ubuntu,<python><ubuntu><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
13992,46250396,2017-09-16 05:26:40,,"<p>I have a large number of strings in a list:
A small example of the list  contents is :</p>

<blockquote>
  <p>[""machine learning"",""Apple"",""Finance"",""AI"",""Funding""]</p>
</blockquote>

<p>I wish to convert these into vectors and use them for clustering purpose.
Is the context of these strings in the sentences considered while finding out their respective vectors?</p>

<p>How should I go about with getting the vectors of these strings if i have just this list containing the strings?</p>

<p>I have done this code so far..</p>

<pre><code> from gensim.models import Word2Vec 
    vec = Word2Vec(mylist)
</code></pre>

<p>P.S. Also, can I get a good reference/tutorial on <strong>Word2Vec</strong>?</p>
",2017-09-16 05:36:12,2017-10-15 15:43:37,Need of context while using Word2Vec,<python><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14000,46197493,2017-09-13 12:22:52,,"<p>Im currently trying to implement a convolutional lstm network using keras. Instead of using keras' embedding layer, I used Gensim's doc2vec embeddings and created input data from it.</p>

<p><strong>preprocessing</strong></p>

<pre><code>preprocessed_train = utils.preprocess_text(train_vect)
preprocessed_test = utils.preprocess_text(test_vect)

print preprocessed_train[0]

result: [u'snes_classic', u'preorders_open', u'later_month', u'ever_since', u'nintendo', u'announce', u'snes_classic', u'edition', u'earlier', u'fan', u'desperate', u'register', u'interest', u'ensure', u'come', u'launch', u'however', u'although', u'system', u'pre-orders', u'make', u'available', u'retailers', u'every', u'store', u'plan', u'sell', u'console', u'allow', u'people', u'place', u'pre-orders', u'yet', u'today', u'though', u'nintendo', u'confirm', u'snes_classic', u'edition', u'pre-orders', u'soon', u'available', u'fan', u'post_official', u'facebook', u'company', u'console', u'make', u'available_pre-order', u'various_retailers', u'late', u'month', u'nintendo', u'appreciate', u'incredible', u'anticipation', u'hardware', u'reference', u'fact', u'snes_classic', u'edition', u'already', u'sell', u'many', u'place', u'across_globe', u'unfortunately', u'nintendo', u'clarify', u'exactly', u'retailers', u'open', u'snes_classic', u'pre-orders', u'provide', u'exact_date', u'however', u'stand_reason', u'wal-mart', u'retailers', u'force', u'cancel_pre-orders', u'hardware', u'website', u'error', u'saw', u'go_live', u'prematurely', u'currently_unclear', u'wal-mart', u'help', u'cancel', u'reservations', u'sign-up', u'pre-orders', u'go_live', u'properly', u'month', u'appreciate', u'incredible', u'anticipation', u'exist', u'super_nintendo', u'entertainment_system', u'super_nes', u'classic', u'post', u'nintendo', u'tuesday_august', u'1', u'2017', u'post', u'nintendo', u'mention', u'ship', u'significant_amount', u'snes_classic', u'edition', u'units', u'retailers', u'launch', u'company', u'make', u'units', u'available', u'throughout', u'balance', u'calendar', u'snes_classic', u'edition', u'first', u'announce', u'nintendo', u'explain', u'make', u'units', u'nes_classic', u'constantly', u'sell', u'leave', u'many', u'glad', u'nintendo', u'offer_clarification', u'others', u'however', u'remain_unconvinced', u'nintendo', u'able', u'keep', u'demand', u'console', u'incredibly_hard', u'fan', u'place', u'legitimate', u'order', u'snes_classic', u'edition', u'end', u'even_harder', u'find', u'throughout', u'scalpers', u'place', u'pre-orders', u'pick', u'console', u'post-launch', u'order', u'sell', u'higher_price', u'later_date', u'retailers', u'like', u'ebay', u'enforce_rule', u'scalpers', u'unclear_whether', u'enough', u'snes_classic', u'edition', u'launch', u'september_29', u'2017_source', u'nintendo', u'facebook']
</code></pre>

<p><strong>data labels</strong></p>

<pre><code>y_test = [x for x in test_data['slabel']]
y_train = [x for x in train_data['slabel']]

y_test = keras.utils.to_categorical(y_test)
y_train = keras.utils.to_categorical(y_train)

result:
array([[ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.]])
</code></pre>

<p><strong>load doc2vec model</strong></p>

<pre><code>doc2vec_model = gensim.models.Doc2Vec.load('./doc2vec-models/dmbbv_300_epoch_500_size_model')
</code></pre>

<p><strong>infer data and create input vectors</strong>. The <em>infer_vector</em> function creates the document embeddings based on the doc2vec model that I created. </p>

<pre><code>X_train = []
for text in preprocessed_train:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_train.append(inferred_vec)

X_test = []
for text in preprocessed_test:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_test.append(inferred_vec)
</code></pre>

<p><strong>reshape data</strong></p>

<pre><code>X_train = np.array(X_train)
X_test = np.array(X_test)
X_train = X_train.reshape((X_train.shape[0],1,X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0],1,X_test.shape[1]))
X_train.shape,X_test.shape

result: ((1476, 1, 500), (370, 1, 500))
</code></pre>

<p><strong>building model</strong></p>

<pre><code>model = Sequential()
model.add(Conv1D(filters = 128,
                 kernel_size = 5,
                 input_shape = (X_train.shape[1],X_train.shape[2]), 
                 padding = 'valid',
                 activation = 'relu'))
model.add(MaxPooling1D(2))
model.add(LSTM(X_train.shape[1],return_sequences = True, 
               implementation=2, 
               kernel_regularizer=regularizers.l1_l2(0.001),
               activity_regularizer=regularizers.l1(0.01)
              ))
model.add(Dropout(0.7))
model.add(Activation('relu'))
model.add(LSTM(256,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(128))
model.add(Activation('relu'))
model.add(LSTM(64,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(32,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(16))
model.add(Activation('relu'))
model.add(Dense(5, activation = 'sigmoid'))
model.compile(loss=""categorical_crossentropy"", optimizer='adamax',metrics=['categorical_accuracy', 'accuracy'])
</code></pre>

<p>then I get this error </p>

<hr>

<pre><code>-----------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-488-b29db30c3ee7&gt; in &lt;module&gt;()
      5 #                  use_bias=True,
      6                  padding = 'valid',
----&gt; 7                  activation = 'relu'))
      8 model.add(MaxPooling1D(2))
      9 model.add(LSTM(X_train.shape[1],return_sequences = True, 

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self, layer)
    434                 # and create the node connecting the current layer
    435                 # to the input layer we just created.
--&gt; 436                 layer(x)
    437 
    438             if len(layer.inbound_nodes) != 1:

/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in __call__(self, inputs, **kwargs)
    594 
    595             # Actually call the layer, collecting output(s), mask(s), and shape(s).
--&gt; 596             output = self.call(inputs, **kwargs)
    597             output_mask = self.compute_mask(inputs, previous_mask)
    598 

/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.pyc in call(self, inputs)
    154                 padding=self.padding,
    155                 data_format=self.data_format,
--&gt; 156                 dilation_rate=self.dilation_rate[0])
    157         if self.rank == 2:
    158             outputs = K.conv2d(

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in conv1d(x, kernel, strides, padding, data_format, dilation_rate)
   3114         strides=(strides,),
   3115         padding=padding,
-&gt; 3116         data_format=tf_data_format)
   3117     return x
   3118 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in convolution(input, filter, padding, strides, dilation_rate, name, data_format)
    670         dilation_rate=dilation_rate,
    671         padding=padding,
--&gt; 672         op=op)
    673 
    674 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in with_space_to_batch(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)
    336       raise ValueError(""dilation_rate must be positive"")
    337     if np.all(const_rate == 1):
--&gt; 338       return op(input, num_spatial_dims, padding)
    339 
    340   # We have two padding contributions. The first is used for converting ""SAME""

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in op(input_converted, _, padding)
    662           data_format=data_format,
    663           strides=strides,
--&gt; 664           name=name)
    665 
    666     return with_space_to_batch(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in _non_atrous_convolution(input, filter, padding, data_format, strides, name)
    114           padding=padding,
    115           data_format=data_format_2d,
--&gt; 116           name=scope)
    117     elif conv_dims == 2:
    118       if data_format is None or data_format == ""NHWC"":

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in conv1d(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name)
   2011     result = gen_nn_ops.conv2d(value, filters, strides, padding,
   2012                                use_cudnn_on_gpu=use_cudnn_on_gpu,
-&gt; 2013                                data_format=data_format)
   2014     return array_ops.squeeze(result, [spatial_start_dim])
   2015 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)
    395                                 strides=strides, padding=padding,
    396                                 use_cudnn_on_gpu=use_cudnn_on_gpu,
--&gt; 397                                 data_format=data_format, name=name)
    398   return result
    399 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    766                          input_types=input_types, attrs=attr_protos,
--&gt; 767                          op_def=op_def)
    768         if output_structure:
    769           outputs = op.outputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2630                     original_op=self._default_original_op, op_def=op_def)
   2631     if compute_shapes:
-&gt; 2632       set_shapes_for_outputs(ret)
   2633     self._add_op(ret)
   2634     self._record_op_seen_by_control_dependencies(ret)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1909       shape_func = _call_cpp_shape_fn_and_require_op
   1910 
-&gt; 1911   shapes = shape_func(op)
   1912   if shapes is None:
   1913     raise RuntimeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in call_with_requiring(op)
   1859 
   1860   def call_with_requiring(op):
-&gt; 1861     return call_cpp_shape_fn(op, require_shape_fn=True)
   1862 
   1863   _call_cpp_shape_fn_and_require_op = call_with_requiring

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in call_cpp_shape_fn(op, require_shape_fn)
    593     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,
    594                                   input_tensors_as_shapes_needed,
--&gt; 595                                   require_shape_fn)
    596     if not isinstance(res, dict):
    597       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)
    657       missing_shape_fn = True
    658     else:
--&gt; 659       raise ValueError(err.message)
    660 
    661   if missing_shape_fn:

ValueError: Negative dimension size caused by subtracting 5 from 1 for 'conv1d_55/convolution/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,500], [1,5,500,128].
</code></pre>
",2017-09-13 12:28:03,2017-09-13 13:04:51,Using gensim doc2vec with Keras Conv1d. ValueError,<python><machine-learning><keras><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14001,46201029,2017-09-13 15:09:18,,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X  Rdn for a finite size vocabulary of n words. The 
ith column, xi  Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d  Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
",2017-09-15 09:47:58,2017-09-15 09:47:58,How Word Mover's Distance (WMD) uses word2vec embedding space?,<nlp><nltk><gensim><word2vec><word-embedding>,,,CC BY-SA 3.0,True,False,True,False,False
14006,46238185,2017-09-15 11:09:31,,"<p>I am a little new to doc2vec algorithm and using gensim for its implementation in python.</p>

<p>Following the gensim tutorial <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">""Gensim Doc2vec Tutorial on the IMDB Sentiment Dataset""</a> I have built vocab and trained a doc2vec model, and stored it on the disc using :</p>

<pre><code>model = Doc2Vec(dm=0, dbow_words=1, size=300, window=8, min_count=2, iter=10, workers=cores, alpha=0.025, min_alpha=0.025)
model.build_vocab(art_shuffle, progress_per=10000)
model.train(art_shuffle, total_examples=len(art_shuffle), epochs=10)
model.save('doc2vec_model')
</code></pre>

<p>It creates the following four files in my directory:</p>

<pre><code>doc2vec_model
doc2vec_model.docvecs.doctag_syn0.npy
doc2vec_model.syn1neg.npy
doc2vec_model.wv.syn0.npy
</code></pre>

<p>I load the model back, using the same filename I used to save it i.e.</p>

<pre><code>model = Doc2Vec.load('doc2vec_model')
</code></pre>

<p>After that, if I use this model to create a vector for my document I get an error</p>

<pre><code>model.infer_vector(tokenize(doc_text))

Traceback (most recent call last):
  File ""C:\Users\vipul\Documents\NLP_testing\python-nlp\doc2vec_trials\story_prediction_doc2vec.py"", line 394, in &lt;module&gt;
    inferred_vector = model.infer_vector(tokenize(doc_text))
  File ""C:\Python27\lib\site-packages\gensim\models\doc2vec.py"", line 743, in infer_vector
    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
  File ""gensim\models\doc2vec_inner.pyx"", line 272, in gensim.models.doc2vec_inner.train_document_dbow (./gensim/models/doc2vec_inner.c:3535)
    _word_vectors = &lt;REAL_t *&gt;(np.PyArray_DATA(word_vectors))
TypeError: Cannot convert list to numpy.ndarray
</code></pre>

<p>Where am I going wrong?</p>

<p>Note : tokenize() function is returning a list of words using the nltk wordpunct_tokenizer</p>
",,2017-09-15 11:09:31,TypeError while using infer_vector on a gensim Doc2Vec model loaded from memory,<python><gensim><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
14029,46125018,2017-09-08 21:55:58,,"<p>I've been trying to understand the sample code with <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/recurrent</a>
which you can find at <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py</a></p>

<p>(Using tensorflow 1.3.0.)</p>

<p>I've summarized (what I think are) the key parts, for my question, below:</p>

<pre><code> size = 200
 vocab_size = 10000
 layers = 2
 # input_.input_data is a 2D tensor [batch_size, num_steps] of
 #    word ids, from 1 to 10000

 cell = tf.contrib.rnn.MultiRNNCell(
    [tf.contrib.rnn.BasicLSTMCell(size) for _ in range(2)]
    )

 embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=tf.float32)
 inputs = tf.nn.embedding_lookup(embedding, input_.input_data)

inputs = tf.unstack(inputs, num=num_steps, axis=1)
outputs, state = tf.contrib.rnn.static_rnn(
    cell, inputs, initial_state=self._initial_state)

output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
softmax_w = tf.get_variable(
    ""softmax_w"", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())
logits = tf.matmul(output, softmax_w) + softmax_b

# Then calculate loss, do gradient descent, etc.
</code></pre>

<p>My biggest question is <em>how do I use the produced model to actually generate a next word suggestion, given the first few words of a sentence</em>? Concretely, I imagine the flow is like this, but I cannot get my head around what the code for the commented lines would be:</p>

<pre><code>prefix = [""What"", ""is"", ""your""]
state = #Zeroes
# Call static_rnn(cell) once for each word in prefix to initialize state
# Use final output to set a string, next_word
print(next_word)
</code></pre>

<p>My sub-questions are:</p>

<ul>
<li>Why use a random (uninitialized, untrained) word-embedding?</li>
<li>Why use softmax?</li>
<li>Does the hidden layer have to match the dimension of the input (i.e. the dimension of the word2vec embeddings)</li>
<li>How/Can I bring in a pre-trained word2vec model, instead of that uninitialized one?</li>
</ul>

<p>(I'm asking them all as one question, as I suspect they are all connected, and connected to some gap in my understanding.)</p>

<p>What I was expecting to see here was loading an existing word2vec set of word embeddings (e.g. using gensim's <code>KeyedVectors.load_word2vec_format()</code>), convert each word in the input corpus to that representation when loading in each sentence, and then afterwards the LSTM would spit out a vector of the same dimension, and we would try and find the most similar word (e.g. using gensim's <code>similar_by_vector(y, topn=1)</code>).</p>

<p>Is using softmax saving us from the relatively slow <code>similar_by_vector(y, topn=1)</code> call?</p>

<hr>

<p>BTW, for the pre-existing word2vec part of my question <a href=""https://stackoverflow.com/q/42064690/841830"">Using pre-trained word2vec with LSTM for word generation</a> is similar. However the answers there, currently, are not what I'm looking for. What I'm hoping for is a plain English explanation that switches the light on for me, and plugs whatever the gap in my understanding is.<a href=""https://stackoverflow.com/questions/44614097/use-pre-trained-word2vec-in-lstm-language-model"">Use pre-trained word2vec in lstm language model?</a> is another similar question.</p>

<p><strong>UPDATE:</strong> <a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> and <a href=""https://stackoverflow.com/q/36286594/841830"">Predicting the next word using the LSTM ptb model tensorflow example</a> are similar questions. However, neither shows the code to actually take the first few words of a sentence, and print out its prediction of the next word. I tried pasting in code from the 2nd question, and from <a href=""https://stackoverflow.com/a/39282697/841830"">https://stackoverflow.com/a/39282697/841830</a> (which comes with a github branch), but cannot get either to run without errors. I think they may be for an earlier version of TensorFlow?</p>

<p><strong>ANOTHER UPDATE:</strong> Yet another question asking basically the same thing: <a href=""https://stackoverflow.com/q/42333101/841830"">Predicting Next Word of LSTM Model from Tensorflow Example</a>
It links to 
<a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> (and, again, the answers there are not quite what I am looking for).</p>

<p>In case it still isn't clear, what I am trying to write a high-level function called <code>getNextWord(model, sentencePrefix)</code>, where <code>model</code> is a previously built LSTM that I've loaded from disk, and <code>sentencePrefix</code> is a string, such as ""Open the"", and it might return ""pod"".  I then might call it with ""Open the pod"" and it will return ""bay"", and so on.</p>

<p>An example (with a character RNN, and using mxnet) is the <code>sample()</code> function shown near the end of <a href=""https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb"" rel=""noreferrer"">https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb</a>
You can call <code>sample()</code> during training, but you can also call it after training, and with any sentence you want.</p>
",2017-09-13 13:35:00,2018-06-08 18:27:52,Use LSTM tutorial code to predict next word in a sentence?,<python><tensorflow><lstm><word2vec><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14030,46270606,2017-09-18 02:00:01,,"<p>I'm trying to calculate the cosine similarity between all the values.</p>

<p>The time for 1000*20000 calculations cost me more than 10 mins.</p>

<p>Code:</p>

<pre><code>from gensim import matutils
# array_A contains 1,000 TF-IDF values
# array_B contains 20,000 TF-IDF values 
for x in array_A:
   for y in array_B:
      matutils.cossim(x,y)
</code></pre>

<p>It's necessary to using gensim package to get the tf-idf value and similarity calculation.</p>

<p>Can someone please give me some advice and guidance to speed up time? </p>
",2017-09-18 02:06:24,2019-10-24 11:26:04,How to speed up time when calculate cosine similarity using nested loops in python,<python><gensim><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
14034,46297740,2017-09-19 10:10:57,,"<p>I have a DataFrame in which the index are words and I have 100 columns with float number such that for each word I have its embedding as a 100d vector. I would like to convert my DataFrame object into a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim model object</a> so that I can use its methods; specially <code>gensim.models.keyedvectors.most_similar()</code> so that I can search for similar words within my subset.</p>

<p>Which is the preferred way of doing that?</p>

<p>Thanks</p>
",,2017-09-19 12:17:40,How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?,<python><pandas><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14041,46244286,2017-09-15 16:48:47,,"<p>I am currently using the Word2Vec model trained on Google News Corpus (from <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">here</a>)
Since this is trained on news only until 2013, I need to updated the vectors and also add new words in the vocabulary based on the news coming after 2013. </p>

<p>Suppose I have a new corpus of news after 2013. Can I re-train or fine tune or update the Google News Word2Vec model? Can it be done using Gensim? Can it be done using FastText?</p>
",,2019-05-02 15:11:14,fine tuning pre-trained word2vec Google News,<python><gensim><word2vec><google-news><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
14049,46258266,2017-09-16 20:42:10,,"<p>I'm trying to the Q6 recipe shown <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ"" rel=""nofollow noreferrer"">here</a> but my corpus keep getting returned as [] even though I have checked and it does seem to be reading the document correctly.</p>

<p>So my code is:</p>

<pre><code>def iter_documents(top_directory):
    """"""Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.""""""
    for root, dirs, files in os.walk(top_directory):
        for file in filter(lambda file: file.endswith('.txt'), files):
            document = open(os.path.join(root, file)).read() # read the entire document, as one big string
            yield utils.tokenize(document, lower=True) # or whatever tokenization suits you
class MyCorpus(object):
    # Used to create the object
    def __init__(self, top_dir):
        self.top_dir = top_dir
        self.dictionary = corpora.Dictionary(iter_documents(top_dir))
        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params
# Used if you ever need to iterate through the values
def __iter__(self):
    for tokens in iter_documents(self.top_dir):
        yield self.dictionary.doc2bow(tokens)
</code></pre>

<p>and the text file I'm using to test is <a href=""https://radimrehurek.com/gensim/mycorpus.txt"" rel=""nofollow noreferrer"">this</a>.</p>
",,2017-09-16 22:23:05,Gensim - iterating over multiple documents,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14061,46326173,2017-09-20 15:30:07,,"<p>I'm new to topic modelling / Latent Dirichlet Allocation and have trouble understanding how I can apply the concept to my dataset (or whether it's the correct approach).</p>

<p>I have a small number of literary texts (novels) and would like to extract some general topics using LDA.</p>

<p>I'm using the <code>gensim</code> module in Python along with some <code>nltk</code> features. For a test I've split up my original texts (just 6) into 30 chunks with 1000 words each. Then I converted the chunks into document-term matrices and ran the algorithm. This is the code (although I think it doesn't matter for the question) :</p>

<pre><code># chunks is a 30x1000 words matrix

dictionary = gensim.corpora.dictionary.Dictionary(chunks)
corpus = [ dictionary.doc2bow(chunk) for chunk in chunks ]
lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,
    num_topics = 10)
topics = lda.show_topics(5, 5)
</code></pre>

<p>However the result is completely different from any example I've seen in that the topics are full of meaningless words that can be found in <em>all</em> source documents, e.g. ""I"", ""he"", ""said"", ""like"", ... example:</p>

<pre><code>[(2, '0.009*""I"" + 0.007*""\'s"" + 0.007*""The"" + 0.005*""would"" + 0.004*""He""'), 
(8, '0.012*""I"" + 0.010*""He"" + 0.008*""\'s"" + 0.006*""n\'t"" + 0.005*""The""'), 
(9, '0.022*""I"" + 0.014*""\'s"" + 0.009*""``"" + 0.007*""\'\'"" + 0.007*""like""'), 
(7, '0.010*""\'s"" + 0.009*""I"" + 0.006*""He"" + 0.005*""The"" + 0.005*""said""'), 
(1, '0.009*""I"" + 0.009*""\'s"" + 0.007*""n\'t"" + 0.007*""The"" + 0.006*""He""')]
</code></pre>

<p>I don't quite understand why that happens, or why it doesn't happen with the examples I've seen. How do I get the LDA model to find more distinctive topics with less overlap? Is it a matter of filtering out more common words first? How can I adjust how many times the model runs? Is the number of original texts too small?</p>
",,2017-09-20 17:42:59,Understanding LDA / topic modelling -- too much topic overlap,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
14068,46300155,2017-09-19 12:11:55,,"<p>I have written tensorflow code based on:</p>

<p><a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow noreferrer"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>but using precomputed word embeddings from the GoogleNews word2vec 300 dimension model.</p>

<p>I created my own data from the UCML News Aggregator Dataset in which I parsed the content of the news articles and have created my own labels.</p>

<p>Due to the size of the articles I use TF-IDF to filter out the top 120 words per article and embed those into 300 dimensions.</p>

<p>When I run the CNN I created regardless of the hyper parameters it converges to a small general accuracy, around 38%.</p>

<p>Hyper parameters changed: </p>

<p>Various filter sizes: </p>

<p>I've tried a single filter of 1,2,3
Combinations of filters [3,4,5], [1,3,4]</p>

<p>Learning Rate:</p>

<p>I've varied this from very low to very high, very low doesn't converge to 38% but anything between 0.0001 and 0.4 does.</p>

<p>Batch Size:</p>

<p>Tried many ranges between 5 and 100.</p>

<p>Weight and Bias Initialization:</p>

<p>Set stddev of weights between 0.4 and 0.01.
Set bias initial values between 0 and 0.1.
Tried using the xavier initializer for the conv2d weights. </p>

<p>Dataset Size:</p>

<p>I have only tried on two partial data sets, one with 15 000 training data, and the other on the 5000 test data. In total I have 263 000 data to train on. There is no accuracy difference whether trained and evaluated on the 15 000 training data or by using the 5000 test data as the training data (to save testing time).</p>

<p>I've run successful classifications on the 15 000 / 5000 split using a feed forward network with a BoW input (93% accurate), TF-IDF with SVM (92%), and TF-IDF with Native Bayes (91.5%). So I don't think it is the data.</p>

<p>What does this imply? Is the model just a poor model for this task? Is there an error in my work?</p>

<p>I feel like my do_eval function is incorrect to evaluate the accuracy / loss over an epoch of the data:</p>

<pre><code>        def do_eval(data_set,
                label_set,
                batch_size):
            """"""
            Runs one evaluation against the full epoch of data.
            data_set: The set of embeddings to eval
            label_set: the set of labels to eval
            """"""
            # And run one epoch of eval.

            true_count = 0  # Counts the number of correct predictions.
            steps_per_epoch = len(label_set) // batch_size
            num_examples = steps_per_epoch * batch_size
            totalLoss = 0
            # Need to compute eval accuracy
            for evalStep in xrange(steps_per_epoch):
                input_batch, label_batch = nextBatch(data_set, labels_set, batchSize)
                evalAcc, evalLoss = eval_step(input_batch, label_batch)
                true_count += evalAcc * batchSize
                totalLoss += evalLoss
            precision = float(true_count) / num_examples
            print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))
            print(""Eval Loss: "" + str(totalLoss))
</code></pre>

<p>The entire model is as follows:</p>

<pre><code>class TextCNN(object):
""""""
A CNN for text classification
Uses a convolutional, max-pooling and softmax layer.
""""""

    def __init__(
            self, batchSize, numWords, num_classes,
            embedding_size, filter_sizes, num_filters):

        # Set place holders
        self.input_placeholder = tf.placeholder(tf.float32,[batchSize,numWords,embedding_size,1])
        self.labels = tf.placeholder(tf.int32, [batchSize,num_classes])
        self.pKeep = tf.placeholder(tf.float32)

        # Inference
        '''
        Ready to build conv layers followed by max pooling layers
        Each conv layer produces a different shaped output so need to loop over
        them and create a layer for each and then merge the results
        '''
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, embedding_size, 1, num_filters]

                # W: Filter matrix
                W = tf.Variable(tf.truncated_normal(filter_shape,stddev=0.01), name='W')
                b = tf.Variable(tf.constant(0.0,shape=[num_filters]),name=""b"")


                # Valid padding: Narrow convolution (no edge padded so filter slides over everything)
                # Output size = (input_size (numWords in this case) + 2 * padding (0 in this case) - filter_size) + 1
                conv = tf.nn.conv2d(
                    self.input_placeholder,
                    W,
                    strides=[1, 1, 1, 1],
                    padding=""VALID"",
                    name=""conv"")

                # Apply nonlinearity i.e add the bias to Wx + b
                # Where Wx is the conv layer above
                # Then run it through the activation function
                h = tf.nn.relu(tf.nn.bias_add(conv, b),name='relu')

                # Max-pooling over the outputs
                # Max-pool to control the output size
                # By taking only the best features determined by the filter
                # Ksize is the size of the window of the input tensor
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, numWords - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name=""pool"")

                # Each pooled outputs a tensor of size
                # [batchSize, 1, 1, num_filters] where num_filters represents the
                # Number of features we wanted pooled
                pooled_outputs.append(pooled)

        # Combine all pooled features
        num_filters_total = num_filters * len(filter_sizes)
        # Concat the pool output along the 3rd (num_filters / feature size) dimension
        self.h_pool = tf.concat(pooled_outputs, 3)
        # Flatten
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

        # Add drop out to regularize the learning curve / accuracy
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat,self.pKeep)

        # Fully connected output layer
        with tf.name_scope(""output""):
            W = tf.Variable(tf.truncated_normal([num_filters_total,num_classes],stddev=0.01),name=""W"")
            b = tf.Variable(tf.constant(0.0,shape=[num_classes]), name='b')
            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name='logits')
            self.predictions = tf.argmax(self.logits, 1, name='predictions')

        # Loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.labels,logits=self.logits, name=""xentropy"")
            self.loss = tf.reduce_mean(losses)

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.labels,1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")

     ##################################################################################################################
# Running the training
# Define various parameters for network

batchSize = 100
numWords = 120
embedding_size = 300
num_classes = 4
filter_sizes = [3,4,5] # slide over a the number of words, i.e 3 words, 4     words etc...
num_filters = 126
maxSteps = 5000
initial_learning_rate = 0.001
dropoutRate = 1


data_set = np.load(""/home/kevin/Documents/NSERC_2017/articles/classifyDataSet/TestSmaller_CNN_inputMat_0.npy"")
labels_set = np.load(""Test_NN_target_smaller.npy"")


with tf.Graph().as_default():

    sess = tf.Session()

    with sess.as_default():
    cnn = TextCNN(batchSize=batchSize,
                  numWords=numWords,
                  num_classes=num_classes,
                  num_filters=num_filters,
                  embedding_size=embedding_size,
                  filter_sizes=filter_sizes)

        # Define training operation
        # Pick an optimizer, set it's learning rate, and tell it what to minimize

        global_step = tf.Variable(0,name='global_step', trainable=False)
        optimizer = tf.train.AdamOptimizer(initial_learning_rate)
        grads_and_vars = optimizer.compute_gradients(cnn.loss)
        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

        # Summaries to save for tensor board

        # Set directory
        out_dir = ""/home/kevin/Documents/NSERC_2017/articles/classifyDataSet/tf_logs/CNN_Embedding/""

        # Loss and accuracy summaries
        loss_summary = tf.summary.scalar(""loss"",cnn.loss)
        acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)

        # Train summaries
        train_summary_op = tf.summary.merge([loss_summary,acc_summary])
        train_summary_dir = out_dir + ""train/""
        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

        # Test summaries
        test_summary_op = tf.summary.merge([loss_summary, acc_summary])
        test_summary_dir = out_dir + ""test/""
        test_summary_write = tf.summary.FileWriter(test_summary_dir, sess.graph)

        # Init all variables

        init = tf.global_variables_initializer()
        sess.run(init)

    ############################################################################################

        def train_step(input_data, labels_data):
            '''
            Single training step
            :param input_data: input
            :param labels_data: labels to train to
            '''
            feed_dict = {
                cnn.input_placeholder: input_data,
                cnn.labels: labels_data,
                cnn.pKeep: dropoutRate
            }
            _, step, summaries, loss, accuracy = sess.run(
                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
            feed_dict=feed_dict)
            train_summary_writer.add_summary(summaries, step)


    ###############################################################################################

        def eval_step(input_data, labels_data, writer=None):
            """"""
            Evaluates model on a test set
            Single step
            """"""
            feed_dict = {
            cnn.input_placeholder: input_data,
            cnn.labels: labels_data,
            cnn.pKeep: 1.0
            }

            step, summaries, loss, accuracy = sess.run(
            [global_step, test_summary_op, cnn.loss, cnn.accuracy],
            feed_dict)
            if writer:
                writer.add_summary(summaries, step)
        return accuracy, loss

    ###############################################################################

        def nextBatch(data_set, labels_set, batchSize):
            '''
            Get the next batch of data
            :param data_set: entire training or test data set
            :param labels_set: entire training or test label set
            :param batchSize: batch size
            :return: a batch of the data and it's corresponding labels
            '''
            # Generate random row indices for the documents
            rand_index = np.random.choice(data_set.shape[0], size=batchSize)

            # Grab the data to give to the feed dicts
            data_batch, labels_batch = data_set[rand_index, :, :], labels_set[rand_index, :]

            # Resize for tensorflow
            data_batch = data_batch.reshape([data_batch.shape[0],data_batch.shape[1],data_batch.shape[2],1])
            return data_batch, labels_batch
 ################################################################################

        def do_eval(data_set,
                label_set,
                batch_size):
            """"""
            Runs one evaluation against the full epoch of data.
            data_set: The set of embeddings to eval
            label_set: the set of labels to eval
            """"""
            # And run one epoch of eval.

            true_count = 0  # Counts the number of correct predictions.
            steps_per_epoch = len(label_set) // batch_size
            num_examples = steps_per_epoch * batch_size
            totalLoss = 0
            # Need to compute eval accuracy
            for evalStep in xrange(steps_per_epoch):
                input_batch, label_batch = nextBatch(data_set, labels_set, batchSize)
                evalAcc, evalLoss = eval_step(input_batch, label_batch)
                true_count += evalAcc * batchSize
                totalLoss += evalLoss
            precision = float(true_count) / num_examples
            print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))
            print(""Eval Loss: "" + str(totalLoss))

    ######################################################################################################
        # Training Loop

        for step in range(maxSteps):
            input_batch, label_batch = nextBatch(data_set,labels_set,batchSize)
            train_step(input_batch,label_batch)

        # Evaluate over the entire data set on last eval
            if step  % 100 == 0:
                print ""On Step : "" + str(step) + "" of "" + str(maxSteps)
                do_eval(data_set, labels_set,batchSize)
</code></pre>

<p>The embedding is done before the model:</p>

<pre><code>def createInputEmbeddedMatrix(corpusPath, maxWords, svName):
    # Create a [docNum, Words per Art, Embedding Size] matrix to fill

    genDocsPath = ""gen_docs_classifyData_smallerTest_TFIDF.npy""
    # corpus = ""newsCorpus_word2vec_All_Corpus.mm""
    dictPath = 'news_word2vec_smallerDict.dict'
    tf_idf_path = ""news_tfIdf_word2vec_All.tfidf_model""

    gen_docs = np.load(genDocsPath)
    dictionary = gensim.corpora.dictionary.Dictionary.load(dictPath)
    tf_idf = gensim.models.tfidfmodel.TfidfModel.load(tf_idf_path)

    corpus = corpora.MmCorpus(corpusPath)
    numOfDocs = len(corpus)
    embedding_size = 300

    id2embedding = np.load(""smallerID2embedding.npy"").item()

    # Need to process in batches as takes up a ton of memory

    step = 5000
    totalSteps = int(np.ceil(numOfDocs / step))

    for i in range(totalSteps):
        # inputMatrix = scipy.sparse.csr_matrix([step,maxWords,embedding_size])
        inputMatrix = np.zeros([step, maxWords, embedding_size])
        start = i * step
        end = start + step
        for docNum in range(start, end):
            print ""On docNum "" + str(docNum) + "" of "" + str(numOfDocs)
            # Extract the top N words
            topWords, wordVal = tf_idfTopWords(docNum, gen_docs, dictionary, tf_idf, maxWords)
            # doc = corpus[docNum]
            # Need to track word dex and doc dex seperate
            # Doc dex because of the batch processing
            wordDex = 0
            docDex = 0
            for wordID in wordVal:
                inputMatrix[docDex, wordDex, :] = id2embedding[wordID]
                wordDex += 1
            docDex += 1

        # Save the batch of input data
        # scipy.sparse.save_npz(svName + ""_%d""  % i, inputMatrix)
        np.save(svName + ""_%d.npy"" % i, inputMatrix)


#####################################################################################
</code></pre>
",2017-09-19 12:54:51,2017-09-20 00:19:20,"CNN converges to same accuracy regardless of hyperparameters, what does this indicate?",<tensorflow><conv-neural-network><text-classification>,,,CC BY-SA 3.0,False,False,True,False,False
14076,46362028,2017-09-22 10:02:21,,"<p>I am using PTVS in VS 2015 to write python code. When I write import gensim library and start debugging using F5, Debugger takes around 5 mins to load the library and move to next line.</p>

<pre><code>from gensim import utils
</code></pre>

<p>a. How do I fix this problem? 
b. If I start without debug(ctrl+F5) its fast to load. What code I should write to pause execution, attach the debugger then continue.</p>
",2017-09-22 10:07:51,2017-09-28 14:37:27,python tools for Visual studio 2015 debugging slow when importing gensim package,<python><visual-studio><gensim><ptvs>,,,CC BY-SA 3.0,False,False,True,False,False
14078,46379763,2017-09-23 12:49:41,,"<p>I have a document Term matrix with nine documents:</p>

<p><a href=""https://i.stack.imgur.com/oKl7a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oKl7a.png"" alt=""enter image description here""></a></p>

<p>I am running the code as below:</p>

<pre><code>import pyLDAvis.gensim
topicData = pyLDAvis.gensim.prepare(ldamodel, docTermMatrix, dictionary)
pyLDAvis.display(topicData)  
</code></pre>

<p>I am getting the below error when executing pyLDAvis.display function:</p>

<blockquote>
  <p>TypeError: Object of type 'complex' is not JSON serializable</p>
</blockquote>

<p>Can someone guide here? What could be the reason?</p>
",2017-09-23 16:43:41,2018-11-21 07:42:59,TypeError: Object of type 'complex' is not JSON serializable while using pyLDAvis.display() function,<json><gensim><serializable>,,,CC BY-SA 3.0,False,False,True,False,False
14083,46282473,2017-09-18 14:58:51,,"<p>I try to run the LDA model n pass LDA object to the get_coherence() it showing the error </p>

<pre><code>x.get_coherence()
</code></pre>

<p>*** TypeError: diags() takes at least 2 arguments (2 given)</p>

<p>My code :-</p>

<pre><code>iModel = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, 
num_topics=i, passes=10)

ldalist.append(iModel)

x = CoherenceModel(model=iModel, texts=tokenizedTexts, dictionary=dictionary, 
coherence=coherence)

cohValue = x.get_coherence()
</code></pre>
",2018-07-17 14:16:38,2019-08-14 18:09:19,error while Identify the coherence value from LDA model,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
14104,46421771,2017-09-26 08:47:19,,"<p>I want to make a word2vec model with more n-grams that usual. As I found, Phrase class in gensim.models.phrase can find phrases that I want and it's possible to use phrases on corpus and use it's result model for word2vec train function.</p>

<p>So first of all I do something like below, exactly like sample codes in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim documentation</a>.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

sentences = MySentences('sentences_directory')

bigram = gensim.models.Phrases(sentences)

model = gensim.models.Word2Vec(bigram['sentences'], size=300, window=5, workers=8)
</code></pre>

<p>model has been created but without any good result in evaluation and a warning :</p>

<pre><code>WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable)
</code></pre>

<p>I searched for it and I found <a href=""https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0</a> and changed my code:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

class PhraseItertor(object):
    def __init__(self, my_phraser, data):
        self.my_phraser, self.data = my_phraser, data

    def __iter__(self):
        yield self.my_phraser[self.data]


sentences = MySentences('sentences_directory')

bigram_transformer = gensim.models.Phrases(sentences)

bigram = gensim.models.phrases.Phraser(bigram_transformer)

corpus = PhraseItertor(bigram, sentences)

model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
</code></pre>

<p>I get error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/fatemeh/Desktop/Thesis/bigramModeler.py"", line 36, in &lt;module&gt;
    model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 478, in init
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 553, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 575, in scan_vocab
    vocab[word] += 1
TypeError: unhashable type: 'list'
</code></pre>

<p>Now I want to know that what is wrong in my codes.</p>
",,2017-10-02 07:08:45,Text Processing - Word2Vec training after phrase detection (bigram model),<python><text-processing><gensim><word2vec><python-textprocessing>,,,CC BY-SA 3.0,False,False,True,False,False
14124,46441876,2017-09-27 07:27:33,,"<p>I am building a word2vec model as follows.</p>

<pre><code>from gensim.models import word2vec, Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]

    print(bigrams_)
    print(trigrams_)


# Set values for various parameters
num_features = 10    # Word vector dimensionality                      
min_word_count = 1   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 5          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words


model = word2vec.Word2Vec(trigrams_, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, the output I get for the model's vocabulary is single character as follows. </p>

<pre><code>['h', 'u', 'm', 'a', 'n', ' ', 'c', 'o', 'p', 't']
</code></pre>

<p>I am getting the bigrams and trigrams correctly. Hence, I am just confused where I make the code wrong. Please let me know what is the problem?</p>
",2017-09-27 07:37:28,2017-09-27 07:49:52,Why do I get single letter vocabulary in Gensim word2vec?,<python><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14130,46368720,2017-09-22 15:55:37,,"<p>TypeError: 'TfidfModel' object is not callable</p>

<p><strong>Why can I not compute the TFIDF Matrix for each Doc after initializing?</strong></p>

<p>I started with 999 <em>documents</em>: 999 paragraphs with about 5-15 sentences each.
After spaCy tokenizing everything, I created the <em>dictionary</em> (~16k unique tokens) and <em>corpus</em> (a list of lists of tuples)</p>

<p>Now I'm ready to create the tfidf matrix (and later LDA and w2V matricies) for some ML; however, after initializing the tfidf model with my corpus (for calculation of the 'IDF')
<code>tfidf = models.TfidfModel(corpus)</code> I get the following error message when trying to see the tfidf of each doc <code>tfidf(corpus[5])</code>
<strong>TypeError: 'TfidfModel' object is not callable</strong></p>

<p>I am able to create this model using a differnt corpus where i have four docs each comprised of only a sentence.
There I can confirm that the expected corpus fomat is a list of lists of tuples: 
[doc1[(word1, count),(word2, count),...], doc2[(word3, count),(word4,count),...]...]</p>

<pre><code>from gensim import corpora, models, similarities

texts = [['teenager', 'martha', 'moxley'...], ['ok','like','kris','usual',...]...]
dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; Dictionary(15937 unique tokens: ['teenager', 'martha', 'moxley']...)

corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; [[(0, 2),(1, 2),(2, 1)...],[(3, 1),(4, 1)...]...]

tfidf = models.TfidfModel(corpus)
&gt;&gt;&gt; TfidfModel(num_docs=999, num_nnz=86642)

tfidf(corpus[0])
&gt;&gt;&gt; TypeError: 'TfidfModel' object is not callable

corpus[0]
&gt;&gt;&gt; [(0, 2),(1, 2),(2, 1)...]

print(type(corpus),type(corpus[1]),type(corpus[1][3]))
&gt;&gt;&gt; &lt;class 'list'&gt; &lt;class 'list'&gt; &lt;class 'tuple'&gt;
</code></pre>
",,2017-09-24 05:55:14,TFIDIF Model Creation TypeError in Gensim,<python><nlp><gensim><tf-idf><language-features>,,,CC BY-SA 3.0,False,True,True,False,False
14147,46505573,2017-09-30 18:01:30,,"<p>I recently started to play with the dataset from the Quora Question Pairs Challenge.</p>

<p><a href=""https://www.kaggle.com/c/quora-question-pairs/data"" rel=""nofollow noreferrer"">Quora Question Pairs Challenge Dataset</a></p>

<p>So i did some basic stuff like visualizing the data a bit,cleaning it(Lematization,stop word reomval,punctuation removal etc.).And i have also generated embeddings from the question pairs using word2vec model from the gensim package.I am a bit confused as to what do i have to now?</p>

<p>How do i fit the model and enable it to make a prediction.Any help in this regard is welcome.</p>

<p>Here is the code which i have written for the same.</p>

<pre><code>   '''Opening directory path'''

path=os.path.normpath('M:\PycharmProjects\AI+DL+CP\QQP')

train_df=0
test_dataset=0

for subdir,dir,files in os.walk(path):
    for file in files:
        #print(file)
        if file =='QQPT.csv':
            train_df=pd.read_csv(os.path.join(subdir,file),encoding='utf-8')

        elif file=='QQPTest.csv':
            test_dataset=pd.read_csv(os.path.join(subdir,file),encoding='utf-8')

eng_stopwords = set(stopwords.words('english'))

def remove_special_characters_after_tokenization(tokens):
    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))
    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])
    #print('Without special')
    return list(filtered_tokens)


def remove_stop_words(tokens):
    filtered_tokens=[word for word in tokens if word not in eng_stopwords]
    return list(filtered_tokens)

def remove_repeated_characters(tokens):
    repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
    match_substitution = r'\1\2\3'

    def replace(old_word):
        if wordnet.synsets(old_word):
            return old_word
        new_word = repeat_pattern.sub(match_substitution, old_word)
        return replace(new_word) if new_word != old_word else new_word
    correct_tokens = [replace(word) for word in tokens]
    return list(correct_tokens)

'''Stopword removal and extra character removal'''
tokenizer=nltk.word_tokenize

print(train_df['question2'])
#print(train_df.dtypes.index)
lemmatizer=nltk.stem.WordNetLemmatizer()

print('Processing question2')
train_df['question2']=train_df['question2'].apply(lambda x:str(x))
train_df['question2']=[word.lower() for word in train_df['question2']]
train_df['question2']=train_df['question2'].apply(tokenizer)

print('Processing question1')
train_df['question1']=[word.lower() for word in train_df['question1']]
train_df['question1']=train_df['question1'].apply(tokenizer)

for something in train_df['question1']:
    for item in something:
        item=lemmatizer.lemmatize(item)

for something in train_df['question2']:
    for item in something:
        item=lemmatizer.lemmatize(item)

train_df['question2']=train_df['question2'].apply(lambda x:remove_repeated_characters(x))
train_df['question2']=train_df['question2'].apply(lambda z:remove_special_characters_after_tokenization(z))
train_df['question2']=train_df['question2'].apply(lambda x:remove_stop_words(x))

train_df['question1']=train_df['question1'].apply(lambda x:remove_repeated_characters(x))
train_df['question1']=train_df['question1'].apply(lambda z:remove_special_characters_after_tokenization(z))
train_df['question1']=train_df['question1'].apply(lambda x:remove_stop_words(x))



'''Creating Embeddings[For both question together]'''

print('Creating Embeddings')
embeddings=gensim.models.Word2Vec(train_df['question1']+train_df['question2'])

print(embeddings)
</code></pre>
",,2017-09-30 18:01:30,Working with text in Quora Pairs Kaggle Challenge,<python-3.x><machine-learning><nlp>,,,CC BY-SA 3.0,True,False,True,False,False
14167,46433778,2017-09-26 18:51:02,,"<p>I am working on code using the gensim and having a tough time troubleshooting a ValueError within my code. I finally was able to zip GoogleNews-vectors-negative300.bin.gz file so I could implement it in my model. I also tried gzip which the results were unsuccessful. The error in the code occurs in the last line. I would like to know what can be done to fix the error. Is there any workarounds? Finally, is there a website that I could reference? </p>

<p>Thank you respectfully for your assistance! </p>

<pre><code>import gensim
from keras import backend
from keras.layers import Dense, Input, Lambda, LSTM, TimeDistributed
from keras.layers.merge import concatenate
from keras.layers.embeddings import Embedding
from keras.models import Mode

pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-23bd96c1d6ab&gt; in &lt;module&gt;()
  1 pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
----&gt; 2 word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

C:\Users\green\Anaconda3\envs\py35\lib\site-
packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, 
fvocab, binary, encoding, unicode_errors, limit, datatype)
244                             word.append(ch)
245                     word = utils.to_unicode(b''.join(word), 
encoding=encoding, errors=unicode_errors)
--&gt; 246                     weights = fromstring(fin.read(binary_len), 
dtype=REAL)
247                     add_word(word, weights)
248             else:

ValueError: string size must be a multiple of element size
</code></pre>
",,2019-12-02 11:05:08,Import GoogleNews-vectors-negative300.bin,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14168,46435220,2017-09-26 20:25:36,,"<p>I am have generated a tf-idf model on ~20,000,000 documents using the following code, which works well. The problem is when I try to calculate similarity scores when using linear_kernel the memory usage blows up:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

train_file = ""docs.txt""
train_docs = DocReader(train_file) #DocReader is a generator for individual documents

vectorizer = TfidfVectorizer(stop_words='english',max_df=0.2,min_df=5)
X = vectorizer.fit_transform(train_docs)

#predicting a new vector, this works well when I check the predictions
indoc = ""This is an example of a new doc to be predicted""
invec = vectorizer.transform([indoc])

#This is where the memory blows up
similarities = linear_kernel(invec, X).flatten()
</code></pre>

<p>Seems like this shouldn't take up much memory, doing a comparison of a 1-row-CSR to a 20mil-row-CSR should output a 1x20mil ndarray.</p>

<p>Justy FYI: X is a CSR matrix ~12 GB in memory (my computer only has 16). I have tried looking into gensim to replace this but I can't find a great example.</p>

<p>Any thoughts on what I am missing?</p>
",,2017-09-26 21:49:09,Calculating similarity between Tfidf matrix and predicted vector causes memory overflow,<python><scikit-learn><gensim><tf-idf><csr>,,,CC BY-SA 3.0,False,False,True,False,True
14179,46308283,2017-09-19 19:21:35,,"<p>I'm using python gensim package for word2vec.</p>

<p>I want to run the model on tokenize words and 2-words phrase. I have 10,000~ documents and I used the nltk Regextoknizer to get the single word tokens from all the documents.
How can I tokenizer the document to get also the 2-words phrase.</p>

<p>For example:</p>

<p>document: ""I have a green apple""</p>

<p>and the 2 word phrase: {I_have}, {green_apple}, ... etc.</p>
",2017-09-20 04:36:35,2017-12-29 19:13:58,python tokenizer 2 words phrases to word2vec model,<python><nltk><tokenize>,,,CC BY-SA 3.0,True,False,True,False,False
14185,46455511,2017-09-27 19:08:58,,"<p>I am trying to use PySpark to identify a ""good"" number of topics in some dataset (e.g., tweets), and several ways exist to do this task (see <a href=""https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling"" rel=""nofollow noreferrer"">here</a> for examples).</p>

<p>My question though is about the values reported by PySpark's logPerplexity and logLikelihood functions accompanying pyspark.ml.clustering.LDA. My understanding says that, as I increase topic counts, I should see perplexity decrease and the log likelihood value increase. I see this behavior in gensim using the same model parameters.</p>

<p>In PySpark, however, as I increase the topics, values returned by log-perplexity <em>increase</em>, and log-likelihood <em>decreases</em>. These results seem counter to my understanding of what these values mean. </p>

<p>Am I missing some fundamental aspect of PySpark's LDA model? Any help would be greatly appreciated! I've included a copy of the Jupyter notebook I'm using to run these tests, so you can see my results.</p>

<p><a href=""http://www.cs.umd.edu/~cbuntain/FindTopicK-pyspark-regex.html"" rel=""nofollow noreferrer"">http://www.cs.umd.edu/~cbuntain/FindTopicK-pyspark-regex.html</a></p>

<p>Thanks in advance!</p>
",2017-09-28 16:42:59,2017-09-28 16:42:59,Optimizing Perplexity or Log-Likelihood for Topic Counts in PySpark's LDA,<apache-spark><pyspark><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
14205,46574720,2017-10-04 21:48:23,,"<p>I'm using LDA from gensim to perform a topic modeling. I know how to convert the raw text data to corpus and get the topics. But, after I get the topics, can I label or add back the topics results to the raw document? </p>

<p><strong>Here are my codes:</strong></p>

<pre><code>movie_reviews = pd.read_csv(data_path + 'movie_review.tsv',header=0,delimiter='\t',quoting=3) 

reviews = []
for i in range(len(movie_reviews['review'])):
reviews.append(review_to_words(movie_reviews['review']
              [i],stops=stopwords.words('english')))
from gensim import corpora
dictionary = corpora.Dictionary(reviews)
corpus = [dictionary.doc2bow(review) for review in reviews]
from gensim import models
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10)
corpus_lda = lda[corpus_tfidf]
lda.print_topics(10)
[(0,
  u'0.001*ben + 0.001*sinatra + 0.001*santa + 0.001*henry + 0.001*band + 0.001*william + 0.001*fool + 0.001*tragic + 0.001*favourite + 0.001*bed'),
 (1,
  u'0.002*dentist + 0.002*homeless + 0.002*connery + 0.002*hawn + 0.002*judas + 0.002*bogus + 0.001*dickens + 0.001*hilarity + 0.001*snuff + 0.001*chong'),
 (2,
  u'0.002*freddy + 0.002*mst + 0.001*summary + 0.001*aliens + 0.001*fred + 0.001*broke + 0.001*express + 0.001*cube + 0.001*perfection + 0.001*struck'),
 (3,
  u'0.004*ned + 0.003*kidman + 0.002*nicole + 0.002*chuck + 0.002*hart + 0.002*sabrina + 0.002*miyazaki + 0.002*roberts + 0.002*amitabh + 0.001*educational'),
 (4,
  u'0.002*seagal + 0.002*buffy + 0.002*caprica + 0.002*stargate + 0.002*clown + 0.002*travolta + 0.001*bsg + 0.001*goat + 0.001*insomnia + 0.001*update'),
 (5,
  u'0.003*cinderella + 0.002*envy + 0.002*homicide + 0.002*sucker + 0.002*quantum + 0.002*stallone + 0.002*elvira + 0.002*walt + 0.002*lundgren + 0.001*boobs'),
 (6,
  u'0.002*pickford + 0.002*guaranteed + 0.002*swearing + 0.002*eleniak + 0.002*biko + 0.002*tremendously + 0.001*characterisation + 0.001*arnie + 0.001*radical + 0.001*generate'),
 (7,
  u'0.003*sandler + 0.002*dont + 0.002*buff + 0.002*ustinov + 0.002*brosnan + 0.001*amazon + 0.001*perry + 0.001*link + 0.001*maker + 0.001*adam'),
 (8,
  u'0.002*gandhi + 0.002*scarecrow + 0.002*frankie + 0.002*boxing + 0.002*creep + 0.002*worms + 0.002*mcqueen + 0.002*sellers + 0.002*duchovny + 0.002*appearances'),
 (9,
  u'0.002*sentinel + 0.002*scrooge + 0.002*che + 0.002*robots + 0.002*betty + 0.002*wtf + 0.002*redneck + 0.002*unexplained + 0.002*stiller + 0.002*groups') 

print corpus_lda[0]]
[(1, 0.032862717742657352), (2, 0.061544456899498043), (3, 0.17498689066920223), (5, 0.034931340026756269), (6, 0.01142214861116901), (7, 0.01368447078032208), (8, 0.014051012107502465), (9, 0.58954345105937356)]
</code></pre>

<p>The last code shows the distribution of each topic in document 1. Now, my question is: how can I convert it into a numeric variable for each topic with its weight for each document? </p>

<p>Desired output in Dataframe:</p>

<pre><code>Document ID  Topic1   Topic2   Topic3.... 
0           0.032    0.062   0.175  
</code></pre>

<p>As you can see, this is a DataFrame with the topic as column name and weight as the value. </p>

<p>Also, can I link this topic variable back to the raw document, which is movie_review here? </p>
",2017-10-04 21:49:23,2020-04-23 22:01:18,Python gensim LDA: add the topic to the document after getting the topics,<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
14210,46536132,2017-10-03 01:58:24,,"<p>I built LDA model using Gensim and I want to get the topic words only How can I get the words of the topics only no probabilities and no IDs.words only </p>

<p>I tried print_topics() and show_topics() functions in gensim but I can't get clean words ! </p>

<p>This is the code I used</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)
x = ldamodel.print_topics(num_topics=12, num_words=5)
for i in x:
    print(i[1])
    #print('\n' + str(i))

0.045* + 0.045* + 0.045* + 0.045* + 0.045*
0.021* + 0.021* + 0.021* + 0.021* + 0.021*
0.068* + 0.068* + 0.068* + 0.068* + 0.005*
0.033* + 0.033* + 0.033* + 0.033* + 0.033*
0.007* + 0.007* + 0.007* + 0.007* + 0.007*
0.116* + 0.116* + 0.060* + 0.060* + 0.005*
0.064* + 0.064* + 0.064* + 0.064* + 0.064*
0.036* + 0.036* + 0.036* + 0.036* + 0.036*
0.052* + 0.052* + 0.052* + 0.052* + 0.052*
0.034* + 0.034* + 0.034* + 0.034* + 0.034*
0.035* + 0.035* + 0.035* + 0.035* + 0.035*
0.064* + 0.064* + 0.064* + 0.064* + 0.064*
</code></pre>

<p>I tried show_topics and it gave the same output</p>

<pre><code>y = np.array(ldamodel.show_topics(num_topics=12, num_words=5))
for i in y[:,1]:
    #if i != '%d':
    #print([str(word) for word in i])
    print(i)
</code></pre>

<p>If I have the topic ID how can I access its words and other informations </p>

<p>Thanks in Advance</p>
",,2019-09-24 11:44:26,How to access topic words only in gensim,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
14213,46605194,2017-10-06 11:58:12,,"<p>i have implemented LDA in python.now i want to label the topics whichever i got from LDA.</p>

<pre><code>[(0, u'0.023*""alternate"" + 0.023*""transfervisions"" + 0.013*""tvcommunity""'), (1, u'0.026*""minimalism"" + 0.026*""minimalist"" + 0.018*""honking""'), (2, u'0.027*""videomaking"" + 0.019*""python"" + 0.019*""httpstcoc2ythrctki""')]
</code></pre>
",,2018-05-16 00:59:41,how to label topics automatically after applying LDA,<python><nltk><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,False
14221,46647945,2017-10-09 13:41:42,,"<p>I have a Word2Vec model with a lot of word vectors. I can access a word vector as so. </p>

<pre><code>word_vectors = gensim.models.Word2Vec.load(wordspace_path)
print(word_vectors['boy'])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[ -5.48055351e-01   1.08748421e-01  -3.50534245e-02  -9.02988110e-03...]
</code></pre>

<p>Now I have a proper vector representation that I want to replace the word_vectors['boy'] with.</p>

<pre><code>word_vectors['boy'] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>But the following error is thrown</p>

<pre><code>TypeError: 'Word2Vec' object does not support item assignment
</code></pre>

<p>Is there any fashion or workaround to do this? That is manipulate word vectors manually once the model is trained? Is it possible in other platforms except Gensim? </p>
",,2017-10-09 23:49:48,How to manually change the vector dimensions of a word in Gensim Word2Vec,<python><vector><gensim><word2vec><vector-space>,,,CC BY-SA 3.0,False,False,True,False,False
14223,46609507,2017-10-06 15:50:21,,"<p>I am trying to build an embedding for a corpus using Python's gensim's word2vec implementation. The catch is that I wish to have in the same embedding all of the unigrams and bigrams for the corpus.
Is there a way to embed in the same space both unigrams and bigrams?</p>
",,2018-10-18 05:58:39,Mixed Unigram Bigram word2vec embedding,<python><text-mining><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14243,46576437,2017-10-05 01:17:40,,"<p>I have a dataframe it looks like this :</p>

<pre><code>id  created_at  text    month

0   911721027587231746  2017-09-23 22:36:46       ...   9
1   911719688257851397  2017-09-23 22:31:27      \n https:/...   9
2   911715658395725826  2017-09-23 22:15:26 "" ""      ...   9
3   911715466166587392  2017-09-23 22:14:40  ..     (2) https://t.c...   9
</code></pre>

<p>month column has values that range from 1 to 11 and I want to build a model on the text data based on the number of the month and I'm trying to get the output and save it to a txt file but when I open the files I find it only contains one line each. </p>

<p>what I want is to get 11 text file each named per index and each one should contain 12 lines .</p>

<p>this is my code</p>

<pre><code>def model(final_text):

    sentences = [clean(raw_sentence) for raw_sentence in final_text]
    doc_clean = [i.split() for i in sentences]
    dictionary = corpora.Dictionary(doc_clean)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)
    x = ldamodel.print_topics(num_topics=12, num_words=5)

    y = ldamodel.show_topics(num_topics=12, num_words=5, formatted=False)
    topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in y]
    for topic,words in topics_words:
        #print("" "".join(words).encode('utf-8'))
        #print(words)

        f = open(str(i)+'.txt', 'wb')
        f.write("" "".join(words).encode('utf-8'))
        #f.write(words.encode('utf-8'))
    f.close()

#clean is just a function for cleaning data and it returns text

for i in range(1,12):
    df = parsed[parsed['month'] == i]
    text = df.text
    model(text)
</code></pre>

<p>what am I doing wrong here?</p>

<p>Thanks in advance</p>
",,2017-10-05 03:16:10,how to write output data into a text file iteratively?,<python><string><pandas><encoding><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14247,46610746,2017-10-06 17:10:48,,"<p>I use filter like this:</p>

<pre><code>text_to_word_sequence(line,filters='!#$%&amp;()*+,-./:;&lt;=&gt;?
@[\\]^_`{|}~\t\n\r')
</code></pre>

<p>to eliminate punctuation 
but when I check the performance of gensim word2Vec</p>

<pre><code>model.most_similar('country')
</code></pre>

<p>I got </p>

<pre><code>[('nation', 0.8125789761543274),
 ('country,', 0.7982310056686401),
 ('country.', 0.6794269680976868),
 ('region', 0.667543351650238),
 ('continent', 0.6630430221557617),
 ('countries', 0.6313810348510742),
 ('territory', 0.6307457685470581),
 ('nation,', 0.6306755542755127),
 ('britain', 0.6112251281738281),
 ('india', 0.6077225208282471)]
</code></pre>

<p>It seems that not all the punctuation is eliminated,why? </p>
",2017-10-06 17:16:06,2017-10-06 17:16:06,Gensim & Keras preprocessing text:how to eliminate punctuation when using text_to_word_sequence function,<python><nlp><keras><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14249,46612949,2017-10-06 19:45:38,,"<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
",,2018-03-09 06:31:00,Can we build word2vec model in a distributed way?,<nlp><deep-learning><distributed-computing><gensim><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14251,46684810,2017-10-11 09:37:55,,"<p><strong>Does gensim.corpora.Dictionary have term frequency saved?</strong> </p>

<p>From <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""noreferrer""><code>gensim.corpora.Dictionary</code></a>, it's possible to get the document frequency of the words (i.e. how many document did a particular word occur in):</p>

<pre><code>from nltk.corpus import brown
from gensim.corpora import Dictionary

documents = brown.sents()
brown_dict = Dictionary(documents)

# The 100th word in the dictionary: 'these'
print('The word ""' + brown_dict[100] + '"" appears in', brown_dict.dfs[100],'documents')
</code></pre>

<p>[out]:</p>

<pre><code>The word ""these"" appears in 1213 documents
</code></pre>

<p>And there is the <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_n_most_frequent"" rel=""noreferrer""><code>filter_n_most_frequent(remove_n)</code></a> function that can remove the n-th most frequent tokens: </p>

<blockquote>
  <p><code>filter_n_most_frequent(remove_n)</code>
  Filter out the remove_n most frequent tokens that appear in the documents.</p>
  
  <p>After the pruning, shrink resulting gaps in word ids.</p>
  
  <p>Note: Due to the gap shrinking, the same word may have a different word id before and after the call to this function!</p>
</blockquote>

<p><strong>Is the <code>filter_n_most_frequent</code> function removing the n-th most frequent based on the document frequency or term frequency?</strong> </p>

<p>If it's the latter, <strong>is there some way to access the term frequency of the words in the <code>gensim.corpora.Dictionary</code> object?</strong></p>
",,2020-02-04 22:14:16,Does gensim.corpora.Dictionary have term frequency saved?,<python><dictionary><frequency><gensim><tf-idf>,,,CC BY-SA 3.0,True,False,True,False,False
14258,46630171,2017-10-08 10:41:59,,"<p>I try to build a Word-RNN equivalent of Char-RNN, the net should generate next word in a sentence.</p>

<p>As input I use pre-trained word2vec 100-dim vectors, hidden layer size is 200. My main problem is output layer, how it should be designed?</p>

<p>In char-rnn, output it is vocabulary size(number of unique chars) vector with char probabilities distribution (softmax). So generating next char is simply sampling form this distribution.
But using word2vec when my word vocabulary is over 300k this approach is not feasible.</p>

<p>Should my output generates 100-dim vector and then I should find nearest similar word with use of <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similar_by_vector"" rel=""nofollow noreferrer"">gensim similar_by_vector function</a></p>

<p>Could you provide some good and easy to understand python and tensorflow  implementation, some link to github or publication.</p>

<p>I have found a <a href=""https://datascience.stackexchange.com/questions/15149/output-a-word-instead-of-a-vector-after-word-embedding"">similar question</a>, but it doesn't answer my question:</p>
",2018-01-14 12:48:55,2018-01-14 12:48:55,How to design the output layer of word-RNN model with use word2vec embedding,<python><tensorflow><neural-network><recurrent-neural-network><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14263,46674609,2017-10-10 19:37:00,,"<p>Im trying to understand doc2vec and can I use it to solve my scenario. I want to label sentences with 1 or more tags using TaggedSentences([words], [tags]), but im unsure If my understanding is correct.</p>

<p>so basically, i need this to happen(or am I totally off the mark)</p>

<p>I create 2 TaggedDocuments</p>

<pre><code>TaggedDocument(words=[""the"", ""bird"", ""flew"", ""over"", ""the"", ""coocoos"", ""nest"", labels=[""animal"",""tree""])
TaggedDocument(words=[""this"", ""car"", ""is"", ""over"", ""one"", ""million"", ""dollars"", labels=[""motor"",""money""])
</code></pre>

<p>I build my model</p>

<pre><code>model = gensim.models.Doc2Vec(documents, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)
</code></pre>

<p>Then I train my model</p>

<pre><code>model.train(documents, total_examples=len(documents), epochs=1)
</code></pre>

<p>So when I have all that done, what I expect is when I execute</p>

<pre><code>model.most_similar(positive=[""bird"", ""flew"", ""over"", ""nest])
</code></pre>

<p>is  [animal,tree], but I get</p>

<pre><code>[('the', 0.4732949137687683), 
('million', 0.34103643894195557),
('dollars', 0.26223617792129517),
('one', 0.16558100283145905),
('this', 0.07230066508054733),
('is', 0.012532509863376617),
('cocos', -0.1093338280916214),
('car', -0.13764989376068115)]
</code></pre>

<p>UPDATE:
when I infer</p>

<pre><code>vec_model = model.Word2Vec.load(os.path.join(""save"",""vec.w2v""))
infer = vec_model.infer_vector([""bird"", ""flew"", ""over"", ""nest""])
print(vec_model.most_similar(positive=[infer], topn=10))
</code></pre>

<p>I get</p>

<pre><code>[('bird', 0.5196993350982666),
('car', 0.3320297598838806), 
('the',  0.1573483943939209), 
('one', 0.1546170711517334), 
('million',  0.05099521577358246),
('over', -0.0021460093557834625), 
('is',  -0.02949431538581848),
('dollars', -0.03168443590402603), 
('flew', -0.08121247589588165),
('nest', -0.30139490962028503)]
</code></pre>

<p>So the elephant in the room, Is doc2vec what I need to accomplish the above scenario, or should I go back to bed and have a proper think about what Im trying to achieve in life :)</p>

<p>Any help greatly appreciated</p>
",2017-10-11 07:49:36,2017-10-11 07:49:36,Gensim doc2vec sentence tagging,<python><machine-learning><data-science><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14265,46704738,2017-10-12 08:20:21,,"<p>I'm using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim library for word2vec</a>. I want to train the model on text examples that are unrelated, for example: ""The cat is brown. What time is it?""</p>

<p>I have created the following input to the model:</p>

<p><code>[[""The"", ""cat"", ""is"", ""brown""], [""What"", ""time"", ""is"", ""it""]]</code>, however I'm wondering whether the model assumes that ""brown"" and ""What"" are in the same context.</p>

<p>Tried to find the answer in the api, but could not find it.</p>
",2017-10-12 08:35:52,2017-10-12 10:16:26,gensim with different context,<machine-learning><nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14270,46656290,2017-10-09 23:06:46,,"<p>I have a set of words (n-grams) where I need to calculate tf-idf values. These words are;</p>

<pre><code>myvocabulary = ['tim tam', 'jam', 'fresh milk', 'chocolates', 'biscuit pudding']
</code></pre>

<p>My corpus looks as follows.</p>

<pre><code>corpus = {1: ""making chocolates biscuit pudding easy first get your favourite biscuit chocolates"", 2: ""tim tam drink new recipe that yummy and tasty more thicker than typical milkshake that uses normal chocolates"", 3: ""making chocolates drink different way using fresh milk egg""}
</code></pre>

<p>I am currently getting tf-idf values for my n-grams in <code>myvocabulary</code> using sklearn as follows.</p>

<pre><code>tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,3))
tfs = tfidf.fit_transform(corpus.values())
</code></pre>

<p>However, I am interested in doing the same in Gensim. Forall the examples I came across in Gensim;</p>

<ol>
<li>uses only unigrams ( iwant it for bigrams and trigrams as well)</li>
<li>calculated for all the words (I only want to calculate for the words in <code>myvocabulary</code>)</li>
</ol>

<p>Hence, please help me to find out how to do the above two things in Gensim.</p>
",,2017-10-11 04:48:41,Calculate tf-idf in Gensim for my vocabulary,<python><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,True
14280,46559980,2017-10-04 08:09:53,,"<p>I am trying to get the doc2vec function to work in python 3.
I Have the following code:</p>

<pre><code>tekstdata = [[ index, str(row[""StatementOfTargetFiguresAndPoliciesForTheUnderrepresentedGender""])] for index, row in data.iterrows()]
def prep (x):
    low = x.lower()
    return word_tokenize(low)

def cleanMuch(data, clean):
    output = []
    for x, y in data:
        z = clean(y)
        output.append([str(x), z])
    return output

tekstdata = cleanMuch(tekstdata, prep)

def tagdocs(docs):
    output = []    
    for x,y in docs:
        output.append(gensim.models.doc2vec.TaggedDocument(y, x))
    return output
    tekstdata = tagdocs(tekstdata)

    print(tekstdata[100])

vectorModel = gensim.models.doc2vec.Doc2Vec(tekstdata, size = 100, window = 4,min_count = 3, iter = 2)


ranks = []
second_ranks = []
for x, y in tekstdata:
 print (x)
 print (y)
 inferred_vector = vectorModel.infer_vector(y)
 sims = vectorModel.docvecs.most_similar([inferred_vector], topn=1001,   restrict_vocab = None)
rank = [docid for docid, sim in sims].index(y)
ranks.append(rank)
</code></pre>

<p>All works as far as I can understand until the rank function. 
The error I get is that there is no zero in my list e.g. the documents I am putting in does not have 10 in list:</p>

<pre><code>  File ""C:/Users/Niels Hels/Documents/github/Speciale/Test/Data prep.py"", line 59, in &lt;module&gt;
rank = [docid for docid, sim in sims].index(y)

ValueError: '10' is not in list
</code></pre>

<p>It seems to me that it is the similar function that does not work. 
the model trains on my data (1000 documents) and build a vocab which is tagged.
The documentation I mainly have used is this: 
<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Gensim dokumentation</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">Torturial</a></p>

<p>I hope that some one can help. If any additional info is need please let me know. 
best
Niels</p>
",2017-10-04 08:17:03,2017-10-04 21:56:56,applying the Similar function in Gensim.Doc2Vec,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14291,46731926,2017-10-13 14:13:15,,"<p>I'm playing with gensim's wordvec and try to build a model using the terms from a large medical thesaurus as sentences. There are about 1 million terms (most of the multiword terms which I treat as sentences) and the hope is, that if <code>word2vec</code> sees terms like ""breast cancer"" and ""breast tumor"" etc. it will be able to conclude that ""cancer"" and ""tumor"" are somewhat similar. </p>

<p>I run experiments in which I track how similar terms like that are when using different numbers of iterations but it seems that the results don't correlate. I'd expect that when considering word pairs like (wound, lesion), (thorax, lung), (cancer, tumor) etc, when going from 5 to 100 iterations there'd be a tendency (even if small) that the one word in the pair is ""more similar"" to the the other as the number of iterations grows. But no, results appear pretty random or even getting worse.  </p>

<p>Specifically: I loop with 1,5,10,20,50,100 iterations and train a w2v model and then for my word pairs above check the rank of the 2nd word in the list (say ""lung"") of similar words (as returned by w2v) for the first word (say ""thorax""), then sum up and build the average. And the average rank is growing (!) not decreasing, meaning as training proceeds, the vectors for ""lung"" and ""thorax"" move further and further away from each other. </p>

<p>I didn't expect gensim to detect the clean synonyms and also perhaps 'only' 1 million terms (sentences) is not enough, but still I am puzzled by this effect. </p>

<p>Does anyone have a suspicion? </p>

<p>====================================================</p>

<p>Added after comments and feedback came in: </p>

<p>Thanks for the detailed feedback, gojomo. I had checked many of these issues before: </p>

<ul>
<li><p>yes, the thesaurus terms (""sentences"") come in the right format, e.g. ['breast', 'cancer'] </p></li>
<li><p>yes, of the ~1mio terms more than 850.000 are multiword. Clear that 1-word terms won't provide any context. But there should be ample evidence from the multiword terms</p></li>
<li><p>the examples I gave ('clinic', 'cancer', 'lung', ...) occur in many hundreds of terms, often many thousands. This is what I find odd: That not even for words this frequent really good similar words are suggested. </p></li>
<li><p>you ask for the code: Here it is <a href=""https://www.dropbox.com/s/fo3fazl6frj99ut/w2vexperiment.py?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/fo3fazl6frj99ut/w2vexperiment.py?dl=0</a> It expects to be called (python3) with the name of the model and then the SKOS-XML files of a large thesaurus like Snomed</p>

<p>python w2vexperiment.py snomed-w2v.model SKOS/*.skos</p></li>
<li><p>I the code you see that I create a new model with each new experiment (with a different number of iterations) So there should be no effect that one run pollutes the other (wrong learning rate etc...) </p></li>
<li><p>I have set min_count to 10 </p></li>
</ul>

<p>Still: the models don't get better but often worse as number of iterations grows. And even the better ones (5 or 10 iterations) give me strange results for my test words... </p>
",2017-10-15 14:29:28,2017-10-15 14:29:28,Gensim: quality of word2vec model seems not to correlate with num of iterations in training,<python><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14319,46769705,2017-10-16 11:53:39,,"<p>Is there any way to load doc2vec model saved using gensim into deeplearning4j's ParagraphVectors?</p>

<p>My gensim model is valid - I am able to load it using gensim with no problems.</p>

<p>When I call WordVectorSerializer.readParagraphVectors on my model from Java it throws exception:</p>

<pre><code>Exception in thread ""main"" java.util.zip.ZipException: error in opening zip file
    at java.util.zip.ZipFile.open(Native Method)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:163)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readWord2Vec(WordVectorSerializer.java:889)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readParagraphVectors(WordVectorSerializer.java:825)
    at pl.org.opi.main.main(main.java:17)
</code></pre>

<p>Upon debugging the code, I noticed that deeplearning4j expects a zip file with multiple txt files and a single json file inside of it. Is there a way to convert the gensim model to the zip expected by deeplearning4j or is there a dedicated method for this in dl4j API (couldn't find any using examples and javadoc)?</p>
",,2017-10-16 11:53:39,Doc2Vec from gensim to deeplearning4j,<java><python><gensim><deeplearning4j><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14321,46697503,2017-10-11 21:02:23,,"<p>I have found successful weighting theme for adding word vectors which seems to work for sentence comparison in my case:</p>

<pre><code>query1 = vectorize_query(""human cat interaction"")
query2 = vectorize_query(""people and cats talk"")
query3 = vectorize_query(""monks predicted frost"")
query4 = vectorize_query(""man found his feline in the woods"")

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query2))
&gt;&gt;&gt; 0.7154500319

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query3))
&gt;&gt;&gt; 0.415183904078  

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query4))
&gt;&gt;&gt; 0.690741014142 
</code></pre>

<p>When I add additional information to the sentence which acts as noise I get decrease:</p>

<pre><code>&gt;&gt;&gt; query4 = vectorize_query(""man found his feline in the dark woods while picking white mushrooms and watching unicorns"")
&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query4))
&gt;&gt;&gt; 0.618269123349
</code></pre>

<p>Are there any ways to deal with additional information when comparing using word vectors? When I know that some subset of the text can provide better match.</p>

<hr>

<p>UPD: edited the code above to make it more clear.</p>

<p><code>vectorize_query</code> in my case does so called smooth inverse frequency weighting, when word vectors from GloVe model (that can be word2vec as well, etc.) are added with weights <code>a/(a+w)</code>, where w should be the word frequency. I use there word's inverse tfidf score, i.e. <code>w = 1/tfidf(word)</code>. Coefficient <code>a</code> is typically taken 1e-3 in this approach. Taking just tfidf score as weight instead of that fraction gives almost similar result, I also played with normalization, etc. </p>

<p>But I wanted to have just ""vectorize sentence"" in my example to not overload the question as I think it does not depend on how I add word vectors using weighting theme - the problem is only that comparison works best when sentences have approximately the same number of meaning words.</p>

<p>I am aware of another approach when distance between sentence and text is being computed using the sum or mean of minimal pairwise word distances, e.g.
""Obama speaks to the media in Illinois"" &lt;-> ""The President greets the press in Chicago"" where we have <code>dist = d(Obama, president) + d(speaks, greets) + d(media, press) + d(Chicago, Illinois).</code> But this approach does not take into account that adjective can change the meaning of noun significantly, etc - which is more or less incorporated in vector models. Words like adjectives  'good', 'bad', 'nice', etc. become noise there, as they match in two texts and contribute as zero or low distances, thus decreasing the distance between sentence and text. </p>

<p>I played a bit with doc2vec models, it seems it was gensim doc2vec implementation and <code>skip-thoughts</code> embedding, but in my case (matching short query with much bigger amount of text) I had unsatisfactory results.</p>
",2017-10-13 13:06:51,2017-10-13 20:06:35,Is it possible to search for part the of text using word embeddings?,<gensim><word2vec><word-embedding><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14322,46701173,2017-10-12 03:59:07,,"<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
",,2017-10-12 09:33:25,How to create gensim word2vec model using pre trained word vectors?,<nlp><gensim><word2vec><text-analysis><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14324,46807010,2017-10-18 09:33:27,,"<p>I am new to doc2vec. I was initially trying to understand doc2vec and mentioned below is my code that uses Gensim. As I want I get a trained model and document vectors for the two documents.</p>

<p>However, I would like to know the benefits of retraining the model in several epoches and how to do it in Gensim? Can we do it using <code>iter</code> or <code>alpha</code> parameter or do we have to train it in a seperate <code>for loop</code>? Please let me know how I should change the following code to train the model for 20 epoches.</p>

<p>Also, I am interested in knowing is the multiple training iterations are needed for word2vec model as well.</p>

<pre><code># Import libraries
from gensim.models import doc2vec
from collections import namedtuple

# Load data
doc1 = [""This is a sentence"", ""This is another sentence""]

# Transform data
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(doc1):
    words = text.lower().split()
    tags = [i]
    docs.append(analyzedDocument(words, tags))

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)

# Get the vectors
model.docvecs[0]
model.docvecs[1]
</code></pre>
",2017-10-18 09:47:15,2018-05-17 13:23:11,What are doc2vec training iterations?,<python><deep-learning><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14353,46889727,2017-10-23 12:44:40,,"<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
",2017-10-24 07:38:32,2020-04-10 02:42:38,"word2vec - what is best? add, concatenate or average word vectors?",<python><word2vec><gensim><word-embedding><language-model>,,,CC BY-SA 3.0,False,False,True,False,False
14360,46754881,2017-10-15 12:10:44,,"<p>I use Python 3.6.3rc1. I get following message after executing my python script:</p>

<pre><code>Traceback (most recent call last):
  File ""main.py"", line 6, in &lt;module&gt;
    from train import train
  File ""C:\path\train.py"", line 2, in &lt;module&gt;
    import dataUtils
  File ""C:\path\dataUtils.py"", line 5, in &lt;module&gt;
    import gensim
ImportError: No module named gensim
</code></pre>

<p>But gensim is already installed. Command <code>pip3 freeze</code> gives following list:</p>

<pre><code>bleach==1.5.0
boto==2.48.0
bz2file==0.98
certifi==2017.7.27.1
chardet==3.0.4
gensim==3.0.1
html5lib==0.9999999
idna==2.6
jieba==0.39
Markdown==2.6.9
numpy==1.13.3+mkl
protobuf==3.4.0
requests==2.18.4
scipy==0.19.1
six==1.11.0
smart-open==1.5.3
tensorflow==1.3.0
tensorflow-tensorboard==0.1.7
urllib3==1.22
Werkzeug==0.12.2
</code></pre>

<p>When I imported KeyedVectors from gensim.models (<code>from gensim.models import KeyedVectors</code>) in another script, it worked.</p>

<p>Any suggestions?</p>

<p>Thanks for any reply.</p>
",,2017-10-15 12:38:53,Python3.6 - Cannot import gensim in Windows,<python><python-3.x><pip><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14377,46762366,2017-10-16 03:06:11,,"<p>I load a KeyedVectors model and the word frequency seems like word index</p>

<p>And I miss something?</p>

<p><a href=""https://i.stack.imgur.com/Fy0vC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fy0vC.png"" alt=""enter image description here""></a></p>
",,2017-10-16 23:18:30,gensim KeyedVectors object word count,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14393,46796227,2017-10-17 17:51:12,,"<p>I am trying to create a custom version of word2vec. I want to be able to define that some words that meet certain criteria should stay always in the window either of cbow or skip gram. </p>

<p>I create a new .pyx file based on the word2vec_inner.pyx source code.
For instance I modify the skip-gram training as below. You can find between ##### my modifications. The compilation of the cython code to C succeeds but still the my new fast version is not used. When I checked through ipython if I can import the new pyx with pyximport it failed because of not finding some numpy core .h files under /Users//.pyxbld/temp.macosx-10.10-x86_64-2.7/pyrex/numpy/  </p>

<pre><code>def train_batch_sg(model, sentences, alpha, _work, compute_loss):
    cdef int hs = model.hs
    cdef int negative = model.negative
    cdef int sample = (model.sample != 0)

    cdef int _compute_loss = (1 if compute_loss == True else 0)
    cdef REAL_t _running_training_loss = model.running_training_loss

    cdef REAL_t *syn0 = &lt;REAL_t *&gt;(np.PyArray_DATA(model.wv.syn0))
    cdef REAL_t *word_locks = &lt;REAL_t *&gt;(np.PyArray_DATA(model.syn0_lockf))
    cdef REAL_t *work
    cdef REAL_t _alpha = alpha
    cdef int size = model.layer1_size

    cdef int codelens[MAX_SENTENCE_LEN]
    #########
    cdef int always_codelens[MAX_SENTENCE_LEN]
    #########
    cdef np.uint32_t indexes[MAX_SENTENCE_LEN]
    ###########
    cdef np.uint32_t always_indexes[MAX_SENTENCE_LEN]
    ###########
    cdef np.uint32_t reduced_windows[MAX_SENTENCE_LEN]
    cdef int sentence_idx[MAX_SENTENCE_LEN + 1]
    #########
    cdef int sentence_always_idx[MAX_SENTENCE_LEN + 1]
    #########
    cdef int window = model.window

    cdef int i, j, k
    cdef int effective_words = 0, effective_sentences = 0
    #########
    cdef int always_effective_words = 0, always_effective_sentences = 0
    #########
    cdef int sent_idx, idx_start, idx_end

    # For hierarchical softmax
    cdef REAL_t *syn1
    cdef np.uint32_t *points[MAX_SENTENCE_LEN]
    cdef np.uint8_t *codes[MAX_SENTENCE_LEN]
    cdef np.uint32_t *always_points[MAX_SENTENCE_LEN]
    cdef np.uint8_t *always_codes[MAX_SENTENCE_LEN]

    # For negative sampling
    cdef REAL_t *syn1neg
    cdef np.uint32_t *cum_table
    cdef unsigned long long cum_table_len
    # for sampling (negative and frequent-word downsampling)
    cdef unsigned long long next_random

    if hs:
        syn1 = &lt;REAL_t *&gt;(np.PyArray_DATA(model.syn1))

    if negative:
        syn1neg = &lt;REAL_t *&gt;(np.PyArray_DATA(model.syn1neg))
        cum_table = &lt;np.uint32_t *&gt;(np.PyArray_DATA(model.cum_table))
        cum_table_len = len(model.cum_table)
    if negative or sample:
        next_random = (2**24) * model.random.randint(0, 2**24) + model.random.randint(0, 2**24)

    # convert Python structures to primitive types, so we can release the GIL
    work = &lt;REAL_t *&gt;np.PyArray_DATA(_work)

    # prepare C structures so we can go ""full C"" and release the Python GIL
    vlookup = model.wv.vocab
    sentence_idx[0] = 0  # indices of the first sentence always start at 0
    sentences_with_always_idx_cnt=0
    for sent in sentences:
        if not sent:
            continue  # ignore empty sentences; leave effective_sentences unchanged
        for token in sent:
            #########
            if token in vlookup:
                if token.startswith('.'):
                    word = vlookup[token] if token in vlookup else None
                    if word is None:
                        continue  # leaving `effective_words` unchanged = shortening the sentence = expanding the window
                    if sample and word.sample_int &lt; random_int32(&amp;next_random):
                        continue
                    always_indexes[always_effective_words] = word.index
                    if hs:
                        always_codelens[always_effective_words] = &lt;int&gt;len(word.code)
                        always_codes[always_effective_words] = &lt;np.uint8_t *&gt;np.PyArray_DATA(word.code)
                        always_points[always_effective_words] = &lt;np.uint32_t *&gt;np.PyArray_DATA(word.point)
                    always_effective_words += 1
                    if always_effective_words == MAX_SENTENCE_LEN:
                        break  # TODO: log warning, tally overflow?
            #########
            word = vlookup[token] if token in vlookup else None
            if word is None:
                continue  # leaving `effective_words` unchanged = shortening the sentence = expanding the window
            if sample and word.sample_int &lt; random_int32(&amp;next_random):
                continue
            indexes[effective_words] = word.index
            if hs:
                codelens[effective_words] = &lt;int&gt;len(word.code)
                codes[effective_words] = &lt;np.uint8_t *&gt;np.PyArray_DATA(word.code)
                points[effective_words] = &lt;np.uint32_t *&gt;np.PyArray_DATA(word.point)
            effective_words += 1
            if effective_words == MAX_SENTENCE_LEN:
                break  # TODO: log warning, tally overflow?

        # keep track of which words go into which sentence, so we don't train
        # across sentence boundaries.
        # indices of sentence number X are between &lt;sentence_idx[X], sentence_idx[X])
        effective_sentences += 1
        sentence_idx[effective_sentences] = effective_words

        if effective_words == MAX_SENTENCE_LEN:
            break  # TODO: log warning, tally overflow?

    # precompute ""reduced window"" offsets in a single randint() call
    for i, item in enumerate(model.random.randint(0, window, effective_words)):
        reduced_windows[i] = item

    # release GIL &amp; train on all sentences
    with nogil:
        for sent_idx in range(effective_sentences):
            idx_start = sentence_idx[sent_idx]
            idx_end = sentence_idx[sent_idx + 1]
            for i in range(idx_start, idx_end):
                j = i - window + reduced_windows[i]
                if j &lt; idx_start:
                    j = idx_start
                k = i + window + 1 - reduced_windows[i]
                if k &gt; idx_end:
                    k = idx_end
                for j in range(j, k):
                    if j == i:
                        continue
                    if hs:
                        fast_sentence_sg_hs(points[i], codes[i], codelens[i], syn0, syn1, size, indexes[j], _alpha, work, word_locks, _compute_loss, &amp;_running_training_loss)
                    if negative:
                        next_random = fast_sentence_sg_neg(negative, cum_table, cum_table_len, syn0, syn1neg, size, indexes[i], indexes[j], _alpha, work, next_random, word_locks, _compute_loss, &amp;_running_training_loss)
                #########
                if hs:
                    fast_sentence_sg_hs(always_points[i], always_codes[i], always_codelens[i], syn0, syn1, size, always_indexes[j], _alpha, work, word_locks, _compute_loss, &amp;_running_training_loss)
                if negative:
                    next_random = fast_sentence_sg_neg(negative, cum_table, cum_table_len, syn0, syn1neg, size, always_indexes[i], always_indexes[j], _alpha, work, next_random, word_locks, _compute_loss, &amp;_running_training_loss)
                #########
    model.running_training_loss = _running_training_loss
    return effective_words
</code></pre>
",2017-10-17 17:58:15,2017-10-17 17:58:15,Gensim Word2vec customized,<python><cython><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14400,46786311,2017-10-17 08:59:39,,"<p>Does anyone know which function should I use if I want to use the pre-trained doc2vec models in this website <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a>?</p>

<p>I know we can use the <code>Keyvectors.load_word2vec_format()</code>to laod the word vectors from pre-trained word2vec models, but do we have a similar function to load pre-trained doc2vec models as well in gensim?</p>

<p>Thanks a lot.</p>
",,2019-02-09 04:39:17,How to load the pre-trained doc2vec model and use it's vectors,<python><numpy><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14408,46899062,2017-10-23 21:48:12,,"<p>I want to train a word2vec model on a tokenized file of size 400MB. I have been trying to run this python code :
</p>

<pre><code>import operator
import gensim, logging, os
from gensim.models import Word2Vec
from gensim.models import *

class Sentences(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for line in open(self.filename):
            yield line.split()

def runTraining(input_file,output_file):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    sentences = Sentences(input_file)
    model = gensim.models.Word2Vec(sentences, size=200)
    model.save(output_file)
</code></pre>

<p></p>

<p>When I call this function on my file, I get this :</p>

<pre><code>2017-10-23 17:57:00,211 : INFO : collecting all words and their counts
2017-10-23 17:57:04,071 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
2017-10-23 17:57:16,781 : INFO : Loading a fresh vocabulary
2017-10-23 17:57:18,873 : INFO : min_count=5 retains 290537 unique words (6% of original 4735816, drops 4445279)
2017-10-23 17:57:18,873 : INFO : min_count=5 leaves 42158450 word corpus (89% of original 47054017, drops 4895567)
2017-10-23 17:57:19,563 : INFO : deleting the raw counts dictionary of 4735816 items
2017-10-23 17:57:20,217 : INFO : sample=0.001 downsamples 34 most-common words
2017-10-23 17:57:20,217 : INFO : downsampling leaves estimated 35587188 word corpus (84.4% of prior 42158450)
2017-10-23 17:57:20,218 : INFO : estimated required memory for 290537 words and 200 dimensions: 610127700 bytes
2017-10-23 17:57:21,182 : INFO : resetting layer weights
2017-10-23 17:57:24,493 : INFO : training model with 3 workers on 290537 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-10-23 17:57:28,216 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:32,107 : INFO : PROGRESS: at 20.00% examples, 1314 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:36,071 : INFO : PROGRESS: at 40.00% examples, 1728 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:41,059 : INFO : PROGRESS: at 60.00% examples, 1811 words/s, in_qsize 0, out_qsize 0
Killed
</code></pre>

<p>I know that word2vec needs a lot of space, but I still think there is a problem here. As you see the estimated memory for this model is of 600MB, while my computer has 16GB of RAM. Yet monitoring the process while the code runs shows that it occupies all of my memory and then gets killed.</p>

<p>As other posts advise I have tried to increase min_count and decrease size. But even with ridiculous values (min_count=50, size=10) the process stops at 60%.</p>

<p>I also tried to make python an exception to OOM so that the process doesn't get killed. When I do that, I have a MemoryError instead of the killing.</p>

<p>What is going on ?</p>

<p>(I use a recent laptop with Ubuntu 17.04, 16GB RAM and a Nvidia GTX 960M. I run python 3.6 from Anaconda and gensim 3.0, but it does'nt do better with gensim 2.3)</p>
",2017-10-23 21:59:42,2017-10-24 05:04:45,Gensim Word2Vec uses too much memory,<python-3.x><memory><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14410,46914400,2017-10-24 15:25:38,,"<p>I want to construct word embeddings for documents using GloVe. I know how to obtain vector embeddings for single words (unigrams) as follows (for their example text document).</p>

<pre><code>$ git clone http://github.com/stanfordnlp/glove
$ cd glove &amp;&amp; make
$ ./demo.sh
</code></pre>

<p>Now, I want to obtain vector embeddings for bigrams. For example;</p>

<ol>
<li>""New york"" -> instead of ""New"", and ""york""</li>
<li>""machine learning"" -> instead of ""machine"", and ""learning""</li>
</ol>

<p>Is it possible to do in GloVe? If yes, how?</p>
",,2017-10-25 13:01:55,N-grams in GloVe,<nlp><stanford-nlp><data-mining><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,True,False
14411,46914513,2017-10-24 15:31:58,,"<p>I'm using <code>gensim 3.0.1</code>. </p>

<p>I have a list of <code>TaggedDocument</code> with unique labels of the form <code>""label_17""</code>, but when I train Doc2Vec model, it somehow splits the labels to symbols, so the output for <code>model.docvecs.doctags</code> is the following:
</p>

<pre><code>{'0': Doctag(offset=5, word_count=378, doc_count=40),
 '1': Doctag(offset=6, word_count=1330, doc_count=141),
 '2': Doctag(offset=7, word_count=413, doc_count=50),
 '3': Doctag(offset=8, word_count=365, doc_count=41),
 '4': Doctag(offset=9, word_count=395, doc_count=41),
 '5': Doctag(offset=10, word_count=420, doc_count=41),
 '6': Doctag(offset=11, word_count=408, doc_count=41),
 '7': Doctag(offset=12, word_count=426, doc_count=41),
 '8': Doctag(offset=13, word_count=385, doc_count=41),
 '9': Doctag(offset=14, word_count=376, doc_count=40),
 '_': Doctag(offset=4, word_count=2009, doc_count=209),
 'a': Doctag(offset=1, word_count=2009, doc_count=209),
 'b': Doctag(offset=2, word_count=2009, doc_count=209),
 'e': Doctag(offset=3, word_count=2009, doc_count=209),
 'l': Doctag(offset=0, word_count=4018, doc_count=418)}
</code></pre>

<p>but in the initial list of tagged document each document has its own unique label.</p>

<p>The code for model training is the following:
</p>

<pre><code>model = Doc2Vec(size=300, sample=1e-4, workers=2)
print('Building Vocabulary')
model.build_vocab(data)
print('Training...')
model.train(data, total_words=total_words_count, epochs=20)
</code></pre>

<p>Therefore I can't index my documents like <code>model.docvecs['label_17']</code> and get <code>KeyError</code>.</p>

<p>The same thing if I pass data to the constructor instead of building the vocabulary.</p>

<p>Why is this happening? Thanks.</p>
",,2017-10-24 18:39:14,Doc2Vec model splits documents tags in symbols,<python-3.x><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14412,46915589,2017-10-24 16:27:57,,"<p>I am trying to summarise some text using Gensim in python and want exactly 3 sentences in my summary. There doesn't seem to be an option to do this so I have done the following workaround:</p>

<pre><code>with open ('speeches//'+speech, ""r"") as myfile:
    speech=myfile.read()
    sentences = speech.count('.')
    x = gensim.summarization.summarize(speech, ratio=3.0/sentences)
</code></pre>

<p>However this code is only giving me two sentences. Furthermore, as I incrementally increase 3 to 5 still nothing happens.</p>

<p>Any help would be most appreciated.</p>
",2018-01-09 14:25:05,2018-04-04 13:20:15,NLP: How to get an exact number of sentences for a text summary using Gensim,<nlp><text-processing><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14418,46860197,2017-10-21 04:58:22,,"<p>My current doc2vec code is as follows.</p>

<pre><code># Train doc2vec model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>I also have a word2vec code as below.</p>

<pre><code> # Train word2vec model
model = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)
</code></pre>

<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>

<p>In Gensim I found the below mentioned sentence:
<strong>""Produce word vectors with deep learning via word2vecs skip-gram and CBOW models, using either hierarchical softmax or negative sampling""</strong></p>

<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>

<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>

<p>P.s. my application is a recommendation system :)</p>
",,2017-10-23 01:25:42,Doc2vec and word2vec with negative sampling,<python><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14432,46885454,2017-10-23 09:00:26,,"<p>I tried to follow this documentation:
nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb
Where I have the following code snippet:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in food2vec.vocab.iteritems()]

ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)

ordered_terms, term_indices, term_counts = zip(*ordered_vocab)

word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],
                        index=ordered_terms
</code></pre>

<p>To get it to run i have change it to following:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in word2vecda.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab)
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectorsda = pd.DataFrame(word2vecda.wv.syn0norm[term_indices,],index=ordered_terms)
word_vectorsda [:20]
</code></pre>

<p>But the last line before I print the DataFrame give me an error I cannot get my head around. It keeps return that the noneType object cannot be in this line. To me, it looks like it is Term_indices there tracking it, but I do not get why? </p>

<pre><code> TypeError: 'NoneType' object is not subscriptable
</code></pre>

<p>Can any help me with this? Any inputs are most welcome
Best Niels</p>
",2017-10-23 09:36:02,2017-10-23 09:47:57,"How to create a DataFrame with the word2ve vectors as data, and the terms as row labels?",<python-3.x><pandas><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14437,46917675,2017-10-24 18:33:27,,"<p>I have downloaded dump of Wikipedia files (13.40 GB). It is downloaded in the format <code>enwiki-latest-pages-articles.xml.bz2</code>.</p>

<p>How to load the file in Python &amp; then convert the articles into a plain text file inorder to perform LDA on it?</p>

<p>Was following the instructions fromm <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> but data loading into Python is not mentioned.</p>
",2017-10-24 18:35:53,2017-10-24 19:12:38,How to load the Wikipedia dump?,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14450,46888984,2017-10-23 12:07:55,,"<p>The code found test.py runs correctly. But when importing test.py in the test2.py file the line that creates the LdaMulticore model seems to be stuck.</p>

<p>I have added the example code to illustrate the problem. Is there a solution for this? </p>

<p>test.py:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora, models
from gensim.matutils import Sparse2Corpus
import time

data_clean = [""This is the first example document."",""This is the second example document."",""This is the third and last exaple document""]

vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0.0001, max_df=0.8,stop_words = 'english')
matrix =  vectorizer.fit_transform(data_clean)

id2words = dict()
for k, v in vectorizer.vocabulary_.iteritems():
    id2words[v] = k

train_corpus = Sparse2Corpus(matrix, documents_columns=False)

if __name__ == 'test':
    print ""The file is being imported""
    model = models.LdaMulticore(train_corpus ,id2word=id2words,num_topics=10, workers=4)
else:
    print ""The file is directly executed""
    model = models.LdaMulticore(train_corpus ,id2word=id2words,num_topics=10, workers=4)
</code></pre>

<p>test2.py:</p>

<pre><code>import test
</code></pre>

<p>Terminal output : 
<a href=""https://i.stack.imgur.com/m8x2V.png"" rel=""nofollow noreferrer"">terminal output when running test2.py</a></p>
",,2017-10-23 12:07:55,Gensim models.LdaMulticore() not executing when imported trough other file,<python><machine-learning><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,True
14454,46960119,2017-10-26 17:11:55,,"<p>I want to load pre-trained word embeddings from google news</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print (model.wv.vocab)
</code></pre>

<p>But the error is showing:</p>

<pre><code>UnicodeEncodeError: 'ascii' codec can't encode character '\u2022' in position 62425: ordinal not in range(128)
</code></pre>

<p>How do I fix this? as I want to list all the words in the word embeddings and do the average for the sentence embedding.</p>
",,2018-02-01 00:19:28,load pre-trained word embeddings,<python><encoding><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14457,47018088,2017-10-30 14:44:50,,"<p>Using gensim word2vec, built a CBOW model with a bunch of litigation files for representation of word as vector in a Named-Entity-recognition problem, but I want to known how to evaluate my representation of words. If I use any other datasets like wordsim353(NLTK) or other online datasets of google, it doesn't work because I built the model specific to my domain dataset of files. How do I evaluate my word2vec's representation of word vectors .I want words belonging to similar context to be closer in vector space.How do I ensure that the build model is doing it ?</p>

<p>I started by using a techniques called <strong>odd one out</strong>. Eg:</p>

 

<pre class=""lang-python prettyprint-override""><code>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split()) --&gt; 'cereal'
</code></pre>

<p>I created my own dataset(for validating) using the words in the training of word2vec .Started evaluating with taking three words of similar context and an odd word out of context.But the accuracy of my model is only 30 % .</p>

<p>Will the above method really helps in evaluating my w2v model ? Or Is there a better way ?</p>

<p>I want to go with word_similarity measure but I need a reference score(Human assessed) to evaluate my model or is there any techniques to do it? Please ,do suggest any ideas or techniques .</p>
",2017-10-30 15:34:04,2017-10-30 16:01:24,How to evaluate word2vec build on a specific context files,<machine-learning><nltk><word2vec><gensim><feature-engineering>,,,CC BY-SA 3.0,True,False,True,False,False
14469,47037276,2017-10-31 14:01:09,,"<p>I wants to optimize gensim to run doc2vec in Window7</p>
<p>[1] C compiler</p>
<p>I installed gensim by following this instruction: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/install.html</a></p>
<pre><code>pip install --upgrade gensim
</code></pre>
<p>However, in this page(<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a>), it is saying that C compiler is needed before installing gensim.</p>
<blockquote>
<p>Make sure you have a C compiler before installing gensim, to use optimized (compiled) doc2vec training (70x speedup [blog]).</p>
</blockquote>
<ol>
<li>Should I do something before using pip?</li>
</ol>
<p>[2] BLAS</p>
<p>In the tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a> it is saying that</p>
<blockquote>
<p>Time to Train</p>
<p>If the BLAS library is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time.</p>
</blockquote>
<p>So it seems like I have to install BLAS for optimization,
but I have no idea what BLAS is and there are little and complex BLAS installation guides for window.</p>
<ol start=""2"">
<li>Which BLAS library should I install for running gensim in Window?</li>
<li>If I install BLAS library, will it be automatically linked to python code when I am running gensim doc2vec? or should I do something to link it to doc2vec code?</li>
</ol>
",2020-06-20 09:12:55,2017-11-01 16:21:32,Optimizing gensim(C compilier and BLAS) in Window 7,<python-2.7><word2vec><gensim><blas><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14476,46998858,2017-10-29 09:56:08,,"<p>i am trying to install gensim using </p>

<pre><code>sudo -H pip install --upgrade gensim
</code></pre>

<p>but it is giving me this error :</p>

<pre><code>  File ""setup.py"", line 301, in &lt;module&gt;
    include_package_data=True,
  File ""/usr/lib/python2.7/distutils/core.py"", line 151, in setup
    dist.run_commands()
  File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command                      /install.py"", line 67, in run
    self.do_egg_install()
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command /install.py"", line 98, in do_egg_install
    easy_install = self.distribution.get_command_class('easy_install')
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/dist.py"", line 576, in get_command_class
    self.cmdclass[command] = cmdclass = ep.load()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2229, in load
    return self.resolve()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2235, in resolve
module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 47, in &lt;module&gt;
from setuptools.sandbox import run_setup
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 15, in &lt;module&gt;
    import pkg_resources.py31compat
ImportError: No module named py31compat
</code></pre>

<p>please help me, in installing gensim. i googled it, but i am not able to find the solution.</p>
",,2020-01-21 11:01:35,ImportError: No module named py31compat,<python><python-2.7><importerror><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14482,46985320,2017-10-28 01:08:17,,"<p>I am trying to use a pre-trained model and add additional vocabulary to it. I have a csv file with 1 column of sentences in it.</p>

<pre><code>import gensim

existing_model_fr = gensim.models.Word2Vec.load('./fr/fr.bin')

new_sentences = gensim.models.word2vec.LineSentence('./data/french.csv')
existing_model_fr.build_vocab(new_sentences, update=True)

existing_model_fr.train(new_sentences, total_examples=existing_model_fr.corpus_count, epochs=5)
existing_model_fr.save('new_model_fr')
</code></pre>

<p>I get following error on existing_model_fr.train() line. What am I missing?</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last) in ()</p>
  
  <p>/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py in
  train(self, sentences, total_examples, total_words, epochs,
  start_alpha, end_alpha, word_count, queue_factor, report_delay,
  compute_loss) 863 is only called once, the model's cached iter value
  should be supplied as epochs value. 864 """""" --> 865 if
  self.model_trimmed_post_training: 866 raise RuntimeError(""Parameters
  for training were discarded using model_trimmed_post_training method"")
  867 if FAST_VERSION &lt; 0:</p>
  
  <p>AttributeError: 'Word2Vec' object has no attribute
  'model_trimmed_post_training'</p>
</blockquote>
",,2017-10-28 07:06:33,gensim - Word2vec online training - AttributeError: 'Word2Vec' object has no attribute 'model_trimmed_post_training,<nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14493,47060859,2017-11-01 17:51:04,,"<p>I have a corpus file which was made using <code>dictionary.doc2bow</code> function of gensim but I have lost the dictionary file. Is there any way I can get the dictionary file by doing something on the corpus file using gensim</p>
",2017-11-02 11:08:36,2017-11-02 11:08:36,I have a corpus file which was made by doc2bow function of gensim but I have lost the dictionary file. How do I get the dictionary file back,<machine-learning><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14499,47022246,2017-10-30 18:46:21,,"<p>When I tried to import gensim module in Windows, I end up with below error.</p>

<blockquote>
  <p>c:\python27\lib\site-packages\gensim-3.0.1-py2.7-win-amd64.egg\gensim\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
    <strong>warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>Is there any possibility to overcome this warning?</p>
",2018-06-12 21:46:39,2018-06-12 21:46:39,Warning message after importing gensim module in Windows,<python><nlp><warnings><semantics><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
14501,47025885,2017-10-30 23:48:37,,"<p>Does gensim <code>Word2Vec</code> have an option that is the equivalent of ""training steps"" in the TensorFlow word2vec example here: <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"" title=""Word2Vec Basic"">Word2Vec Basic</a>? If not, what default value does gensim use? Is the gensim parameter <code>iter</code> related to training steps?</p>

<p>The TensorFlow script includes this section.</p>

<pre><code>with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    print('Initialized')

    average_loss = 0
    for step in xrange(num_steps):
        batch_inputs, batch_labels = generate_batch(
            batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

    # We perform one update step by evaluating the optimizer op (including it
    # in the list of returned values for session.run()
    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
        if step &gt; 0:
            average_loss /= 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step ', step, ': ', average_loss)
        average_loss = 0

    # Note that this is expensive (~20% slowdown if computed every 500 steps)
    if step % 10000 == 0:
        sim = similarity.eval()
        for i in xrange(valid_size):
            valid_word = reverse_dictionary[valid_examples[i]]
            top_k = 8  # number of nearest neighbors
            nearest = (-sim[i, :]).argsort()[1:top_k + 1]
            log_str = 'Nearest to %s:' % valid_word
            for k in xrange(top_k):
                close_word = reverse_dictionary[nearest[k]]
                log_str = '%s %s,' % (log_str, close_word)
            print(log_str)
  final_embeddings = normalized_embeddings.eval()
</code></pre>

<p>In the TensorFlow example, if I perform T-SNE on the embeddings and plot with matplotlib, the plot looks more reasonable to me when the number of steps is high. 
I am using a small corpus of 1,200 emails. One way it looks more reasonable is that numbers are clustered together. I would like to attain the same apparent level of quality using gensim.</p>
",2017-12-26 20:20:36,2017-12-26 20:20:36,Gensim equivalent of training steps,<python><tensorflow><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14511,46970376,2017-10-27 08:11:23,,"<p>I am using Gensim wrapper to obtain wordRank embeddings (I am following their <a href=""https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/Wordrank_comparisons.ipynb"" rel=""nofollow noreferrer"">tutorial</a> to do this) as follows.</p>

<pre><code>from gensim.models.wrappers import Wordrank

model = Wordrank.train(wr_path = ""models"", corpus_file=""proc_brown_corp.txt"", 
out_name= ""wr_model"")

model.save(""wordrank"")
model.save_word2vec_format(""wordrank_in_word2vec.vec"")
</code></pre>

<p>However, I am getting the following error <code>FileNotFoundError: [WinError 2] The system cannot find the file specified</code>. I am just wondering what I have made wrong as everything looks correct to me. Please help me.</p>

<p>Moreover, I want to know if the way I am saving the model is correct. I saw that Gensim offers the method <code>save_word2vec_format</code>. What is the advantage of using it without directly using the original wordRank model?</p>
",2017-10-27 08:29:47,2018-09-24 00:07:54,Issues in Gensim WordRank Embeddings,<python><nlp><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14517,47028943,2017-10-31 06:12:27,,"<p>I am running the below code, but gensim word2vec is throwing a word not in vocabulary error. Can you let me know the solution?</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

sentences = [[""The quick brown fox jumped over the lazy dog""], 
         [""The sun is shining bright""]]

from gensim.models import word2vec
model = word2vec.Word2Vec(sentences, iter=10, min_count=1, size=300, workers=4)

print(model['quick'])
</code></pre>

<p>Output:</p>

<pre><code>KeyError: ""word 'quick' not in vocabulary""
</code></pre>

<p>But If I use this</p>

<pre><code>print(model['The quick brown fox jumped over the lazy dog'])
</code></pre>

<p>it prints a list</p>

<pre><code>[  1.60348183e-03  -9.17983416e-04  -8.30831763e-04   9.46367683e-04
</code></pre>
",2017-10-31 06:23:23,2017-10-31 06:46:53,"gensim: KeyError: ""word 'quick' not in vocabulary""",<python-3.x><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14526,47080842,2017-11-02 17:03:00,,"<p>Im using Jython 2.7.1. It is working fine. I need to install gensim. Does this library works in jython?</p>

<p>Thank you</p>
",2017-11-02 18:30:07,2017-12-08 01:58:13,gensim with jython,<gensim><jython-2.7>,,,CC BY-SA 3.0,False,False,True,False,False
14557,47163477,2017-11-07 16:59:29,,"<p>I am trying to build a word-level language model in TensorFlow. My inputs are batches with word id's of shape <code>(batch_size, seq_length)</code>, my targets are the inputs shifted one time step to the left (so for each word, the target is the next word in the sequence).</p>

<p>The model receives word embeddings as an input (word embeddings were pre-trained using gensim word2vec). I manually checked that the word embeddings are read in correctly and that they correspond to the right word id's.</p>

<p>Although I have tried out a lot of things, my model is not improving. Even when training for 100 epochs over the full training set, the accuracy remains the same.</p>

<p>What I have tried (without any success):</p>

<ul>
<li>Removing dropout. My first goal is to get rid of underfitting</li>
<li>Different vocabulary size (100, 1000, 10000)</li>
<li>Using gradient clipping/ not using gradient clipping</li>
<li>Changing the initialization of the weights </li>
<li>Data shuffling</li>
<li>different optimizer (RSMProp, Adam and Gradient Descent)</li>
<li>larger/smaller model (2-4 hidden layers with 128-256 hidden units)</li>
<li>different batch size (10, 20, 128)</li>
<li>different learning rate (0.01, 0.001, 0.1)</li>
<li>different loss function (sparse_softmax_cross_entropy_with_logits or tf.contrib.seq2seq.sequence_loss)</li>
<li>refeeding/not refeeding the final state of the LSTM during training*</li>
</ul>

<p>In the beginning, both loss and accuracy are improving. Also, the model is adapting its predictions. But then, after some epochs over the full training set, loss and accuracy stay constant. Also, the model predictions aren't changing anymore and it gets stuck.
Here is an example that shows the development of loss and accuracy for the same input sequence. After epoch 30, nothing is changing anymore:</p>

<pre><code>2017-11-08 06:59:24,298 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 06:59:24,299 - DEBUG - Predicted sequence: [0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0]
 2017-11-08 06:59:24,299 - INFO - Current epoch: 1
 2017-11-08 06:59:24,299 - INFO - Current training step: 2000
 2017-11-08 06:59:24,299 - INFO - Current loss: 107.67147064208984
 2017-11-08 06:59:24,299 - INFO - Current accuracy: 0.1599999964237213


 2017-11-08 07:04:09,559 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:04:09,560 - DEBUG - Predicted sequence: [ 4  4  6  6 16  0  0  3  2  1  9  2  1  0  0  4  0  0  4  8]
 2017-11-08 07:04:09,560 - INFO - Current epoch: 5
 2017-11-08 07:04:09,560 - INFO - Current training step: 2000
 2017-11-08 07:04:09,560 - INFO - Current loss: 97.8116455078125
 2017-11-08 07:04:09,560 - INFO - Current accuracy: 0.2150000035762787


2017-11-08 07:43:03,875 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:43:03,875 - DEBUG - Predicted sequence: [ 6  4  9 55 47  0  5  3  2  1  9  2  1  0 55 24  0  0  3  6]
 2017-11-08 07:43:03,876 - INFO - Current epoch: 30
 2017-11-08 07:43:03,876 - INFO - Current training step: 2000
 2017-11-08 07:43:03,876 - INFO - Current loss: 84.75357055664062
 2017-11-08 07:43:03,876 - INFO - Current accuracy: 0.2549999952316284
</code></pre>

<p>I have been working on this for a week already and I don't know what I can try out anymore. I would be super grateful for any tips or ideas.</p>

<p>The important parts of the code are here:</p>

<pre><code>    def build_graph(self, graph):
    with graph.as_default():
        tf.set_random_seed(self.random_seed)

        with tf.variable_scope('embedding'):
            embedding_matrix = tf.get_variable(name='embedding_matrix', shape=self.embds.shape, initializer=tf.constant_initializer(self.embds), trainable=False)

        with tf.name_scope('input'):
            self.input_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))
            self.inputs = tf.nn.embedding_lookup(embedding_matrix, self.input_batch)
            self.label_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))

        with tf.name_scope('rnn'):
            # Set up the RNN architecture
            cells = []

            for i in range(self.n_layers):
                cell = tf.contrib.rnn.LSTMCell(self.n_hidden, initializer=tf.contrib.layers.xavier_initializer())#use_peepholes=True,

                # Add dropout (only used during training)
                # cell = tf.contrib.rnn.DropoutWrapper(
                #     cell,
                #     output_keep_prob=(1.0 if not self.config['train'] else
                #                       self.dropout_keep_prob))
                cells.append(cell)


            cell = tf.contrib.rnn.MultiRNNCell(
                cells, state_is_tuple=True)

            # Create a zero-filled state tensor as an initial state
            self.init_state = cell.zero_state(self.batch_size, tf.float32)

            # Create a recurrent neural network
            output, self.final_state = tf.nn.dynamic_rnn(
                cell,
                inputs=self.inputs,
                initial_state=self.init_state)

            # OLD VERSION
            # self.logits = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)

            # NEW VERSION
            # Try out part of tensorflow tutorial

            self.output_flat = tf.reshape(output, [-1, cell.output_size])
            softmax_w = tf.get_variable(""softmax_w"", [self.n_hidden, self.vocab_size], dtype=tf.float32)

            softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size], dtype=tf.float32)
            logits = tf.nn.xw_plus_b(self.output_flat, softmax_w, softmax_b)
            # Reshape logits to be a 3-D tensor for sequence loss
            self.logits = tf.reshape(logits, [self.batch_size, self.seq_length, self.vocab_size])

            # Use the contrib sequence loss and average over the batches
            loss = tf.contrib.seq2seq.sequence_loss(
                self.logits,
                self.label_batch,
                tf.ones([self.batch_size, self.seq_length], dtype=tf.float32),
                average_across_timesteps=False, average_across_batch=True)

            self.loss = tf.reduce_sum(loss)


        with tf.name_scope('prediction'):

            # Compute real-valued predictions of the network
            self.predictions = tf.argmax(self.logits, axis=2)

            # Compute the softmax                
            # softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_batch, logits=self.logits)

        #with tf.name_scope(""loss""):
            # Compute the loss (cross-entropy)
            # self.loss = tf.reduce_mean(softmax_ce)

        with tf.name_scope(""metrics""):
            # Compute accuracy and perplexity for evaluation

            correct_predictions = tf.to_float(tf.equal(self.label_batch, self.predictions))

            self.perplexity = tf.reduce_mean(tf.exp(softmax_ce))
            self.accuracy = tf.reduce_mean(correct_predictions)

        with tf.name_scope('train'):
            # Create a global step variable
            self.global_step = tf.Variable(
                0,
                trainable=False,
                name=""global_step"",
                collections=[ tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES ])

            # Get all variables created with trainable=True
            parameters = tf.trainable_variables()
            # Compute the gradient of the loss w.r.t to the params
            gradients = tf.gradients(self.loss, parameters)
            # Clip the gradients. How this works: Given a tensor t, and a maximum
            # clip value clip_norm the op normalizes t so that its L2-norm is less
            # than or equal to clip_norm
            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.clip_norm)

            self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.lr, epsilon=0.1)
            # Apply the optimizer              
            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, parameters), global_step=self.global_step)

            # If not clipping the gradients, minimize the loss directly
            # self.train_step = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)
            # self.train_step = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)

        self._create_summaries()

    return graph


def train(self, save_every=20):
    with self.graph.as_default():

        # Initialize the state of the network
        feed2 = np.zeros((self.batch_size, self.n_hidden))
        t = tuple((feed2, feed2))
        _current_state = np.array([t, t])
        training_step = 0

        for epoch_id in range(0, self.n_epochs):     
            m, n = self.x_train.shape
            self.n_batches = int(m//self.batch_size)

            for batch_number in range(0, self.n_batches):
                training_step += 1
                from_index = batch_number*self.batch_size
                to_index = (batch_number+1)*self.batch_size
                _inputs = self.x_train[from_index:to_index,:]
                _labels = self.y_train[from_index:to_index,:]

                # Run training step
                # The final state of the net is fed back into the net 
                _logits, _predictions, _train_step, _current_state, _loss, _acc, summary = self.sess.run(
                        [self.logits,
                        self.predictions,
                        self.train_step,
                        self.final_state,
                        self.loss,
                        self.accuracy,
                        #self.perplexity,
                        self.merged],
                        feed_dict={
                            self.input_batch: _inputs,
                            self.label_batch: _labels,
                            self.init_state[0][0]: _current_state[0][0],
                            self.init_state[0][1]: _current_state[0][1],
                            self.init_state[1][0]: _current_state[1][0],
                            self.init_state[1][1]: _current_state[1][1],
                           })

                pred = _predictions[0]

                if batch_number % 2000 == 0:
                    self.sw.add_summary(summary, training_step)
                    tf.logging.debug(""Targets: {}"".format(_labels[0]))
                    tf.logging.debug(""Predicted sequence: {}"".format(pred))
                    tf.logging.info(""Current epoch: {}"".format(epoch_id))
                    tf.logging.info(""Current training step: {}"".format(batch_number))
                    tf.logging.info(""Current loss: {}"".format(_loss))
                    tf.logging.info(""Current accuracy: {}"".format(_acc))
                    tf.logging.info(""Current perplexity: {}"".format(_perpl))

            self.save(epoch_id)
</code></pre>
",2017-11-08 07:31:46,2017-11-08 07:31:46,"language modeling - model loss and accuracy not improving, model is underfitting",<python><tensorflow><nlp><language-model>,,,CC BY-SA 3.0,False,False,True,False,False
14569,47105869,2017-11-03 23:47:34,,"<p>Here is the code:</p>

<pre><code>from pyemd import emd

print(""sentence 1:"")
print(input_document_lower[0])
print(""sentence 2:"")
print(input_document_lower[1])
print(""similarity:"")
model_w2v.wmdistance(input_document_lower[0], input_document_lower[1])
</code></pre>

<p>Here's the error:</p>

<pre><code>sentence 1:
incorrect batch number printed primary label pbn
sentence 2:
unconfirmed oos met vial washing qualification sample per 
similarity:

ImportErrorTraceback (most recent call last)
&lt;ipython-input-201-50af089a2354&gt; in &lt;module&gt;()
      4 print(input_document_lower[1])
      5 print(""similarity:"")
----&gt; 6 model_w2v.wmdistance(input_document_lower[0], input_document_lower[1])

C:\ProgramData\Anaconda2\lib\site-packages\gensim\models\word2vec.pyc in wmdistance(self, document1, document2)
   1308         Refer to the documentation for `gensim.models.KeyedVectors.wmdistance`
   1309         """"""
-&gt; 1310         return self.wv.wmdistance(document1, document2)
   1311 
   1312     def most_similar_cosmul(self, positive=None, negative=None, topn=10):

C:\ProgramData\Anaconda2\lib\site-packages\gensim\models\keyedvectors.pyc in wmdistance(self, document1, document2)
    386 
    387         if not PYEMD_EXT:
--&gt; 388             raise ImportError(""Please install pyemd Python package to compute WMD."")
    389 
    390         # Remove out-of-vocabulary words.

ImportError: Please install pyemd Python package to compute WMD.
</code></pre>

<p>It is being installed properly so I really have no clue as to what is going wrong. Have any of your encountered this?</p>
",2017-11-04 08:27:33,2019-06-05 08:34:51,Getting an error to install pyemd even though I just installed it,<python><installation><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14585,47148615,2017-11-07 01:36:06,,"<p>This is my first time using Doc2Vec
I'm trying to classify works of an author. I have trained a model with Labeled Sentences (paragraphs, or strings of specified length), with words = the list of words in the paragraph, and tags = author's name. In my case I only have two authors.
I tried accessing the docvecs attribute from the trained model but it only contains two elements, corresponding to the two tags I have when I trained the model. I'm trying to get the doc2vec numpy representations of each paragraph I fed in to the training so I can use that as training data later on. How can I do this?
Thanks.</p>
",,2017-11-08 05:37:06,Getting numpy vector from a trained Doc2Vec model for each document,<python-3.x><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14603,47155414,2017-11-07 10:21:23,,"<p>I obtain word vectors from my code. e.g., </p>

<pre><code>array([ -3.09521449e-04,   2.73033947e-06,   2.15601496e-04, ...,
         5.12349070e-04,   5.04256517e-04,   8.16784304e-05], dtype=float32)
</code></pre>

<p>Now, I want to identify what is the word that represents this word vector in wor2vec genism.</p>

<p>I tried it using the below code. However it did not work.</p>

<pre><code>print(model.wv.index2word(kmeans_clustering.cluster_centers_))
</code></pre>

<p>Please help me.</p>
",,2017-11-08 05:32:32,Given a word vector get the word of it in word2vec,<python><word2vec><gensim><word-embedding><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14605,47157286,2017-11-07 11:51:49,,"<p>I am using the following python code to generate similarity matrix of word vectors (My vocabulary size is <code>77</code>).</p>

<pre><code>similarity_matrix = []
index = gensim.similarities.MatrixSimilarity(gensim.matutils.Dense2Corpus(model.wv.syn0))

for sims in index:
    similarity_matrix.append(sims)
similarity_array = np.array(similarity_matrix)
</code></pre>

<p>The dimensionality of the <code>similarity_array</code> is <code>300 X 300</code>. However as I understand the dimensionality should be <code>77 x 77</code> (as my vocabulary size is 77).</p>

<pre><code>i.e.,
      word1, word2, ......, word77
word1 0.2,     0.8,    ...,  0.9
word2 0.1,     0.2,   ....,  1.0
...  ....,    ....., .....,   ....
word77 0.9,  0.8,    ...,    0.1
</code></pre>

<p>Please let me know what is wrong in my code.</p>

<p>Moreover, I want to know what is the order of the vocabulary <code>(word1, word2, ..., word77)</code> used to calculate this similarity matrix? Can I obtain this <code>order</code> from <code>model.wv.index2word</code>?</p>

<p>Please help me!</p>
",,2019-04-29 14:10:11,Get a similarity matrix from word2vec in python (Gensim),<python><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14612,47117569,2017-11-05 02:21:00,,"<p>By doc we can use this to read a word2vec model with genism</p>

<pre><code>model = KeyedVectors.load_word2vec_format('word2vec.50d.txt', binary=False)
</code></pre>

<p>This is an index-to-word mapping, that is, e.g., <code>model.index2word[2]</code>, how to derive an inverted mapping (word-to-index) based on this?</p>
",2020-02-08 05:09:00,2020-02-08 05:09:00,How to get word2index from gensim,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
14625,47235153,2017-11-11 06:39:27,,"<p>When I execute the below code</p>

<pre><code>sim_model = gensim.similarities.MatrixSimilarity(corp)
sim_model.save(""sim_model.pkl"")
</code></pre>

<p>Instead of getting ""sim_model.pkl"" I get two files ""sim_model.pkl.index.npy"" and ""sim_model.pkl"" why is this behavior.</p>
",2017-11-13 16:24:06,2017-11-13 16:24:06,Suffixes being added to extra model files during save,<python-2.7><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14626,47171777,2017-11-08 04:47:30,,"<p>I have a set of document vectors generated using gensim doc2vec (~500K vectors of 150 dimensions). I wish to cluster similar documents for which i want to generate a n*n similarity matrix over which i can run my clustering algorithm.</p>

<p>I tried instructions of this link <a href=""https://github.com/RaRe-Technologies/gensim/issues/140"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/140</a> using the gensim.similarities but the output for 500k records was 500k*150 matrix. I dont understand the output. Shouldn't it be 500k * 500k ? am i missing something?</p>
",,2017-11-08 07:45:16,doc2vec clustering n*n similarity between documents,<cluster-analysis><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14627,47173538,2017-11-08 07:07:15,,"<p>When I try to create a word2vec model (skipgram with negative sampling) I received 3 files as output as follows.</p>

<pre><code>word2vec (File)
word2vec.syn1nef.npy (NPY file)
word2vec.wv.syn0.npy (NPY file)
</code></pre>

<p>I am just worried why this happens as for my previous test examples in word2vec I only received one model(no npy files).</p>

<p>Please help me.</p>
",2017-11-13 16:24:46,2017-11-13 16:24:46,Why are multiple model files created in gensim word2vec?,<python><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14632,47134108,2017-11-06 09:54:21,,"<p>I am new in NLP subject. 
I am working on Turkish so it is hard to find a proper corpus. I read a lot and found this <a href=""https://radimrehurek.com/gensim/wiki.html#preparing-the-corpus"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html#preparing-the-corpus</a> it tells to use Wikipedia data but even if I use it will not give me the stems of words because of the structure of the language(agglutinative language). </p>

<p>My aim is to put my texts into some categories. </p>

<p>Result must be like 
text1 related to subject1 20% and subject2 50% and subject3 30% </p>

<p>I thought that what if I try to do it without a corpus but first wanted to ask here if my idea is possible or utopic. </p>

<p>Also, I will need categories and words related with them. 
If someone can show me a path to go on that will be great I am drifting inside this topic. </p>

<p>Thanks. </p>
",2017-11-06 23:51:01,2018-04-06 14:57:46,"How to work with languages without explicit tokens, e.g. Turkish?",<python><nlp>,,,CC BY-SA 3.0,False,False,True,False,False
14633,47138149,2017-11-06 13:30:44,,"<p>The LDA code generates topics say from 0 to 5 . Is there a standard way (a norm) used to link the generated topics and the documents themselves. Eg: doc1 is of Topic0 , doc5 is of topic Topic1 etc. 
One way i can think of is to string search each of geenrated key words in each topic on the docs , is there a generic way or practice followed for this?</p>

<p>Ex LDA code - <a href=""https://github.com/manhcompany/lda/blob/master/lda.py"" rel=""nofollow noreferrer"">https://github.com/manhcompany/lda/blob/master/lda.py</a> </p>
",,2018-06-04 17:07:20,How do you link back topics generated by LDA model to actual document,<machine-learning><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
14655,47270934,2017-11-13 18:16:27,,"<p>I am trying to classify a set of text documents using multiple sets of features. I am using <a href=""http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py"" rel=""nofollow noreferrer"">sklearn's Feature Union</a> to combine different features for fitting into a single model. One of the features includes word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim's word2vec</a>.</p>

<pre><code>import numpy as np
from gensim.models.word2vec import Word2Vec
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
data = fetch_20newsgroups(subset='train', categories=categories)#dummy dataset

w2v_model= Word2Vec(data .data, size=100, window=5, min_count=5, workers=2)
word2vec={w: vec for w, vec in zip(w2v_model.wv.index2word, w2v_model.wv.syn0)} #dictionary of word embeddings
feat_select = SelectKBest(score_func=chi2, k=10) #other features
TSVD = TruncatedSVD(n_components=50, algorithm = ""randomized"", n_iter = 5)
#other features
</code></pre>

<p>In order to include transformers/estimators not already available in sklearn, I am attempting to wrap my word2vec results into a custom transformer class that returns the vector averages. </p>

<pre><code>class w2vTransformer(TransformerMixin):
    """"""
    Wrapper class for running word2vec into pipelines and FeatureUnions
    """"""
    def __init__(self,word2vec,**kwargs):
        self.word2vec=word2vec
        self.kwargs=kwargs
        self.dim = len(word2vec.values())
    def fit(self,x, y=None):
        return self

    def transform(self, X):
        return np.array([
        np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
            or [np.zeros(self.dim)], axis=0)
       for words in X
])
</code></pre>

<p>However when it comes time to fit the model I receive an error.</p>

<pre><code>combined_features = FeatureUnion([(""w2v_class"",w2vTransformer(word2vec)),
     (""feat"",feat_select),(""TSVD"",TSVD)])#join features into combined_features
#combined_features = FeatureUnion([(""feat"",feat_select),(""TSVD"",TSVD)])#runs when word embeddings are not included    
text_clf_svm = Pipeline([('vect', CountVectorizer()),
         ('tfidf', TfidfTransformer()),
         ('feature_selection', combined_features),
          ('clf-svm',  SGDClassifier( loss=""modified_huber"")),
 ]) 

text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data
Traceback (most recent call last):

  File ""&lt;ipython-input-8-a085b7d40f8f&gt;"", line 1, in &lt;module&gt;
    text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 248, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 213, in _fit
    **fit_params_steps[name])

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\memory.py"", line 362, in __call__
    return self.func(*args, **kwargs)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 739, in fit_transform
    for name, trans, weight in self._iter())

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 332, in __init__
    self.results = batch()

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\base.py"", line 520, in fit_transform
    return self.fit(X, y, **fit_params).transform(X)

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in transform
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in &lt;listcomp&gt;
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 14, in &lt;listcomp&gt;
    np.mean([self.word2vec[w] for w in words if w in self.word2vec]

TypeError: unhashable type: 'csr_matrix'

Traceback (most recent call last):

  File ""&lt;ipython-input-8-a085b7d40f8f&gt;"", line 1, in &lt;module&gt;
    text_clf_svm_1 = text_clf_svm.fit(data.data,data.target) # fits data

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 248, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 213, in _fit
    **fit_params_steps[name])

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\memory.py"", line 362, in __call__
    return self.func(*args, **kwargs)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 739, in fit_transform
    for name, trans, weight in self._iter())

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 332, in __init__
    self.results = batch()

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\pipeline.py"", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)

  File ""C:\Users\rlusk\AppData\Local\Continuum\Anaconda3\lib\site-packages\sklearn\base.py"", line 520, in fit_transform
    return self.fit(X, y, **fit_params).transform(X)

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in transform
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 16, in &lt;listcomp&gt;
    for words in X

  File ""&lt;ipython-input-6-cbc52cd420cd&gt;"", line 14, in &lt;listcomp&gt;
    np.mean([self.word2vec[w] for w in words if w in self.word2vec]

TypeError: unhashable type: 'csr_matrix'
</code></pre>

<p>I understand that the error is because the variable ""words"" is a csr_matrix, but it needs to be an iterable such as a list. My question is how do I modify the transformer class or data so I can use the word embeddings as features to feed into FeatureUnion? This is my first SO post, please be gentle. </p>
",2017-11-14 12:18:48,2018-05-15 23:28:15,Custom Transformer and FeatureUnion for word2vec,<python><scikit-learn><nlp><pipeline><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
14685,47332205,2017-11-16 14:28:15,,"<p>I am using gensim doc2vec as below.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple
import re

my_d = {'recipe__001__1': 'recipe 1 details should come here',
 'recipe__001__2': 'Ingredients of recipe 2 need to be added'}
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key
    docs.append(analyzedDocument(words, tags))
model = doc2vec.Doc2Vec(docs, size = 300, window = 10, dm=1, negative=5, hs=0, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>However, when I check <code>model.docvecs.offset2doctag</code> I get <code>['r', 'e', 'c', 'i', 'p', '_', '0', '1', '2']</code> as the output. The real output should be `'recipe__001__1' and 'recipe__001__2'.</p>

<p>When I use <code>len(model.docvecs.doctag_syn0)</code> I get <code>9</code> as the output. But the real value should be <code>2</code> because I only have 2 recipes in my test dictionary.</p>

<p>Please let me know, why this happens?</p>
",2017-11-18 03:59:46,2017-11-18 03:59:46,Issues in doc2vec tags in Gensim,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14693,47319033,2017-11-15 23:25:04,,"<p>I have trained an topic model using a symmetric alpha in my lda distibution:</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(bows, num_topics = 20, id2word = dictionary, passes = 100)
</code></pre>

<p>I can see that:</p>

<pre><code>model.alpha
array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05])
</code></pre>

<p>where </p>

<pre><code>numpy.sum(model.alpha)
1.0000000000000002
</code></pre>

<p>I can't quite understand how gensim allows for lowering alpha parameter to allow each document to be a mixture of fewer topics?</p>
",,2017-11-15 23:46:08,How to adjust alpha parameter in gensim LdaModel,<python><alpha><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
14701,47353341,2017-11-17 14:48:50,,"<p>I am calculating tf-idf as follows.</p>

<pre><code>texts=['human interface computer',
 'survey user computer system response time',
 'eps user interface system',
 'system human system eps',
 'user response time']

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
analyzedDocument = namedtuple('AnalyzedDocument', 'word tfidf_score')
d=[]
for doc in corpus_tfidf:
    for id, value in doc:
        word = dictionary.get(id)
        score = value
        d.append(analyzedDocument(word, score))
</code></pre>

<p>However, now I want to identify the most 3 important words in my corpus using the words that has the highest <code>idf</code> values. Please let me know how to do it?</p>
",2017-11-17 14:54:45,2017-11-18 00:35:33,Get the most important words in the corpus using tf-idf (Gensim),<python><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,False
14707,47300015,2017-11-15 05:28:44,,"<p>I am using Gensim to calculate tf-idf scores for my corpus mentioned below.</p>

<pre><code>corpus=['human interface computer',
 'survey user computer system response time',
 'eps user interface system',
 'system human system eps',
 'user response time']
</code></pre>

<p>My current code is as follows.</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in corpus)

class MyCorpus(object):
    def __iter__(self):
        for line in corpus:
            yield dictionary.doc2bow(line.lower().split())

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus)

corpus_tfidf = tfidf[corpus]
</code></pre>

<p>However, I get the error <code>RecursionError: maximum recursion depth exceeded while calling a Python object</code> (PS: if my code is wrong I am happy to have a different code). Please help me to calculate tf-idf values for my current corpus. Moreover, I want to get the 3 terms that has the highest tf-idf score in my corpus.</p>

<p>Please help me!</p>
",2017-11-15 08:13:48,2017-11-15 08:13:48,Issues in calculating tf-idf in gensim,<python><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,False
14708,47300490,2017-11-15 06:09:06,,"<p>I know to obtain a document vector for a given tag in doc2vec using <code>print(model.docvecs['recipe__11'])</code>.</p>

<p>My document vectors are either recipes (tags start with <code>recipe__</code>), newspapers (tags start with <code>news__</code>) or ingredients (tags start with <code>ingre__</code>)</p>

<p>Now I want to retrieve all the document vectors of recipes. The pattern of my recipe documents is <code>recipe__&lt;some number&gt;</code> (e.g., recipe__23, recipe__34). I am interested in knowing if it possible to obtain multiple document vectors using a pattern (e.g., tags starting with <code>recipe__</code>)</p>

<p>Please help me!</p>
",2017-11-15 06:27:25,2017-11-15 17:42:38,How to obtain document vectors in doc2vec in gensim,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14721,47325484,2017-11-16 09:04:42,,"<p>I have an assignment that's something like this:</p>

<pre><code>import gensim
from sklearn.feature_extraction.text import CountVectorizer

newsgroup_data = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

vect = CountVectorizer(stop_words='english', 
                       token_pattern='(?u)\\b\\w\\w\\w+\\b')
X = vect.fit_transform(newsgroup_data)
corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)
id_map = dict((v, k) for k, v in vect.vocabulary_.items())
</code></pre>

<p>My task is to estimate LDA model parameters on the corpus, find a list of the 10 topics and the most significant 10 words in each topic, which I do as such:</p>

<pre><code>top10 = ldamodel.print_topics(num_topics=10, num_words=10)
ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, 
              id2word=id_map, num_topics=10, minimum_probability=0)
</code></pre>

<p>Which passes the autograder fine.  The next task is to find the topic distribution of a new doc which I attempt to do as follows:</p>

<pre><code>new_doc = [""\n\nIt's my understanding that the freezing will start to occur because \
of the\ngrowing distance of Pluto and Charon from the Sun, due to it's\nelliptical orbit. \
It is not due to shadowing effects. \n\n\nPluto can shadow Charon, and vice-versa.\n\nGeorge \
Krumins\n-- ""]
newX = vect.transform(new_doc)
newC = gensim.matutils.Sparse2Corpus(newX, documents_columns=False)
print(ldamodel.get_document_topics(newC))
</code></pre>

<p>This however simply returns </p>

<p><code>gensim.interfaces.TransformedCorpus</code></p>

<p>I also see from the docs the statement: ""You can then infer topic distributions on new, unseen documents, with >>> doc_lda = lda[doc_bow]"" but have no success here either.  Any help appreciated.</p>
",,2017-11-16 20:27:33,topic distristribution in gensim ldamodel trained with countvectorizer,<python-3.x><gensim><topic-modeling><countvectorizer>,,,CC BY-SA 3.0,False,False,True,False,True
14723,47303842,2017-11-15 09:36:41,,"<p>when I use model.infer_vector  to compute the vectors, differ order of 
        document results different.</p>

<pre><code>size=200;negative=15; min_count=1;iterNum=20;
windows = 5
modelName = ""datasets/dm-sum.bin_""+str(windows)+""_"" 
+str(size)+""_""+str(negative)
model = loadDoc2vecModel(modelName)
vecNum = 200
</code></pre>

<p>call infer_vector</p>

<pre><code>test_docs = [ x.strip().split() for x in 
codecs.open(""datasets/test_keyword_f1"", ""r"", ""utf-8"").readlines() ]
for item in test_docs:

    print(""%s"" %(resStr.strip()))
    vecTmp = model.infer_vector(item,  alpha=0.05, steps=20)
    print(vecTmp)
</code></pre>

<p>When I executed call infer_vector twice, the results were as follows.</p>

<p>I don't know why did this happen.</p>

<p><a href=""https://i.stack.imgur.com/mN98V.png"" rel=""nofollow noreferrer"">this link is the result</a></p>
",2017-11-15 10:04:22,2019-08-02 17:09:11,"gensim doc2vec, why the order of the sentences affects the doc2vec vector",<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14737,47325820,2017-11-16 09:23:24,,"<p>I am using Gensim to load my fasttext <code>.vec</code> file as follows.</p>

<pre><code>m=load_word2vec_format(filename, binary=False)
</code></pre>

<p>However, I am just confused if I need to load <code>.bin</code> file to perform commands like <code>m.most_similar(""dog"")</code>, <code>m.wv.syn0</code>, <code>m.wv.vocab.keys()</code> etc.?  If so, how to do it?</p>

<p>Or <code>.bin</code> file is not important to perform this cosine similarity matching?</p>

<p>Please help me!</p>
",2017-11-16 11:05:13,2018-06-18 13:03:33,FastText in Gensim,<python><word2vec><gensim><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
14760,47381841,2017-11-19 20:47:53,,"<p>I am building a chat-bot where every message a user sends needs to be converted to a vector(for other ML related work). I am using a pre-trained Word2Vec model to do this. The Word2Vec model was created using the Gensim library and is saved to disk as a 600MB file and is being used in a Django/Python web-application.</p>

<p>Every time a new message is received as an API request, a function loads the word2Vec model and uses that object to generate a vector of the message. This needs to happen on a real time basis. I am worried that every time a new message is received, the application loads an instance of the Word2Vec model and this would cause a memory problem if there are too many requests coming at the same time(because there will be multiple instance of the Word2Vec model present in the RAM at that time). How do I handle the memory efficiently such that it does not use too much memory?</p>
",2017-11-21 04:04:07,2017-11-21 04:04:07,Handling a large number of requests that use an ML model,<django><memory-management><machine-learning><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14761,47386166,2017-11-20 06:28:05,,"<p>I have been given a doc2vec model using gensim which was trained on 20 Million documents. The 20 Million documents it was trained are also given to me but I have no idea how or which order the documents were trained in from the folder. I am supposed to use the test data to find the top 10 match from the training set. The code I use is - </p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec.load(""doc2vec_sample.model"")

test_docs=[""This is the test set I want to test on.""]

def read_corpus(documents, tokens_only=False):
    count=0
    count=count+1
    for line in documents:
        if tokens_only:
            yield gensim.utils.simple_preprocess(line)
        else:
            # For training data, add tags
            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [count])


test_corpus = list(read_corpus(test_docs, tokens_only=True))

doc_id=0

inferred_vector = model.infer_vector(test_corpus[doc_id])
maxx=10
sims = model.docvecs.most_similar([inferred_vector], topn=maxx)

for match in sims:
    print match
</code></pre>

<p>`
The output I get is -</p>

<pre><code>(1913, 0.4589531719684601)
(3250, 0.4300411343574524)
(1741, 0.42669129371643066)
(1, 0.4023148715496063)
(1740, 0.3929900527000427)
(1509, 0.39229822158813477)
(3189, 0.387174129486084)
(3145, 0.3842133581638336)
(1707, 0.3813004493713379)
(3200, 0.3754497170448303)
</code></pre>

<p>How do I get to know which document does document id ""1913"" refer to? How can I access the documents of the trained data set from these 10 job ids? </p>
",,2018-04-20 18:54:21,How to access document details from Doc2Vec similarity scores in gensim model?,<python><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
14772,47365480,2017-11-18 11:25:19,,"<p>I am new to GloVe. I successfully ran their <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">demo.sh</a> as given in their website. After running demo I got several files created such as <code>vocab</code>, <code>vectors</code> etc. But they haven't any documentation or anything that describes what files we need to use and how to use to find most similar words.</p>

<p>Hence, please help me to find the most similar words given a word in GloVe (using cosine similarity)? (e.g., like <code>most.similar</code> in Gensim word2vec)</p>

<p>Please help me!</p>
",,2017-11-21 08:54:23,Get most similar words using GloVe,<nlp><stanford-nlp><word-embedding>,,,CC BY-SA 3.0,False,False,True,True,False
14773,47366918,2017-11-18 14:00:21,,"<p>I have about 200-600k documents of user descriptions.</p>

<p>and I'm using <code>gensim</code> Doc2Vec model.</p>

<p>I wanted to ask what would be the best fit model configuration so I can do a contextual search on my documents? I want to enter a free text query and get the best similar results for this query. Some of my queries including unique words that are important.</p>

<p>For example: ""I need an English speaker for ...""</p>

<p>with my current configuration:</p>

<p><code>Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, sample=1e-4,  workers=cores)</code></p>

<p>I'm getting really good results but, for my example it will find a description with a speaker inside but not an English one, it can be German speaker.</p>

<p>Is there a better configuration of those examples?</p>
",,2017-11-18 14:00:21,Doc2Vec configuration,<python><similarity><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14784,47424335,2017-11-21 23:19:15,,"<p>I've trained an LDA model using gensim. I am under the impression that Lda reduces the data to two lower level matrices (ref: <a href=""https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/</a>) but I cannot seem to figure out how to access the term-topic matrix. The only reference I could find in gensim's documentation is for the .get_topics() attribute, however the format it provides makes no sense to me.</p>

<p>It is easy enough to apply a transformation to retrieve the Document-topic matrix, like so:</p>

<pre><code>doc_topic_matrix = lda_model[doc_term_matrix]
</code></pre>

<p>so I am hoping that there is a similarly functional method to generate the topic-term matrix.</p>

<p>Ideally, output should look like this:</p>

<pre><code>         word1  word2  word3  word4  word5
topic_a   .12    .38    .07    .24    .19
topic_b   .41    .11    .04    .14    .30
</code></pre>

<p>Any thoughts on whether or not this is possible?</p>
",,2018-02-28 09:55:23,Access Term Topic Matrix generated by Gensim LDA,<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
14785,47441798,2017-11-22 18:40:07,,"<p>I am trying to use Gensim with Glove instead of word2vec. To make the shape of Glove compatible with Gensim and use it, I am using the following lines of code:</p>

<pre><code>import gensim
from gensim.scripts.glove2word2vec import glove2word2vec
glove_in = 'glove.840B.300d.txt'
word2vec_format_out = 'glove.840B.300d.txt.word2vec'
glove2word2vec(glove_in, word2vec_format_out)
model =
gensim.models.KeyedVectors.load_word2vec_format(word2vec_format_out,
encoding='utf-8', binary=True)
</code></pre>

<p>However, this last line of code gives the following error: </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 0:
invalid start byte
</code></pre>

<p>I have tried to open Glove first and then writing as a csv file, then re-open specifying encoding='utf-8'. I also tried several other things mentioned here, but the error keeps coming back. Does anyone know a solution for this?</p>
",2019-04-09 17:12:23,2019-04-09 17:12:23,Why does using Gensim with Glove continue to give a 'utf-8' UnicodeDecodeError?,<python-3.x><utf-8><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
14801,47427986,2017-11-22 06:15:32,,"<p>I followed the example in this link and ran the following script to process the latest english wikipedia articles:</p>

<p><a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a></p>

<p>$ python -m gensim.scripts.make_wiki</p>

<p>The result of running the script after 9 hours is that I now have .mm and .txt files. I want to train a word2vec model but all the examples found start from the .bz2 file. </p>

<p>How do I train a word2vec model using the .mm files as input instead of the raw bz2 file? The link below shows how to train an LDA model. Can someone pls share syntax?</p>

<p><a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a></p>

<p>Thanks! </p>
",,2017-11-22 06:15:32,Wikipedia word2vec,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14811,47507091,2017-11-27 09:01:16,,"<p>I have two different word vector models created using word2vec algorithm . Now issue i am facing is few words from first model is not there in second model . I want to create a third model from two different word vectors models where i can use word vectors from both models without loosing meaning and the context of word vectors. </p>

<p>Can I do this, and if so, how?</p>
",2017-11-27 16:01:44,2017-11-27 16:01:44,Creating a wordvector model combining words from other models,<machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14822,47598369,2017-12-01 17:19:25,,"<p>I found <a href=""https://stackoverflow.com/questions/36790867"">this question</a> which provides evidence that sentence order probably matters (but effect can be also a result of different random initialization).</p>

<p>I want to process <a href=""https://files.pushshift.io/reddit/comments/"" rel=""nofollow noreferrer"">Reddit comment dumps</a> for my project, but the strings extracted from json would be unsorted and belong to very different subreddits and topics, so I don't want to mess up contexts:</p>

<pre><code>{""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}
</code></pre>

<p>So does the neighbor sentences matter for gensim word2vec and should I recover whole comment tree structure, or I can simply extract ""bag of sentences"" and train the model on it?</p>
",,2017-12-01 18:44:09,Does word2vec realization from gensim go beyond sentence level when examining context?,<word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
14827,47598646,2017-12-01 17:36:25,,"<p>There are many different ways in which tf and idf can be calculated. I want to know which formula is used by gensim in its LSA model. I have been going through its source code <code>lsimodel.py</code>, but it is not obvious to me where the document-term matrix is created (probably because of memory optimizations).</p>

<p>In <a href=""http://www.ling.ohio-state.edu/~reidy.16/LSAtutorial.pdf"" rel=""nofollow noreferrer"">one LSA paper</a>, I read that each cell of the document-term matrix is the log-frequency of that word in that document, divided by the entropy of that word:</p>

<pre><code>tf(w, d) = log(1 + frequency(w, d))
idf(w, D) = 1 / (-_D p(w) log p(w))
</code></pre>

<p>However, this seems to be a very unusual formulation of tf-idf. A more familiar form of tf-idf is:</p>

<pre><code>tf(w, d) = frequency(w, d)
idf(w, D) = log(|D| / |{d  D: w  d}|)
</code></pre>

<p>I also notice that there is a <a href=""https://stackoverflow.com/questions/9470479/how-is-tf-idf-implemented-in-gensim-tool-in-python"">question on how the <code>TfIdfModel</code> itself is implemented in gensim</a>. However, I didn't see <code>lsimodel.py</code> importing <code>TfIdfModel</code>, and therefore can only assume that <code>lsimodel.py</code> has its own implementation of tf-idf.</p>
",,2017-12-14 03:00:16,Which formula of tf-idf does the LSA model of gensim use?,<gensim><tf-idf><latent-semantic-indexing><latent-semantic-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
14848,47563821,2017-11-29 23:58:19,,"<p>The code is used to generate word2vec and use it to train the naive Bayes classifier.
I am able to generate word2vec and use the similarity functions successfully.As a next step I would want to use the word2vec to train the naive bayes classifier. Currently the code given an error when I am trying to slit the data in test and training. How do i convert word2vec model into the array so that it can be used as training data.</p>

<p># Importing the libraries
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    import gensim</p>

<pre><code># Importing the dataset
dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)

# Cleaning the texts
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, 1000):
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
#    for word2vec we want an array of vectors

    corpus.append(review)

#print(corpus)
X = gensim.models.Word2Vec(corpus, min_count=1,size=1000)
#print (X.most_similar(""love""))


#embedding_matrix = np.zeros(len(X.wv.vocab), dtype='float32')
#for i in range(len(X.wv.vocab)):
#    embedding_vector = X.wv[X.wv.index2word[i]]
#    if embedding_vector is not None:
#        embedding_matrix[i] = embedding_vector

# Creating the Bag of Words model
#from sklearn.feature_extraction.text import CountVectorizer
#cv = CountVectorizer(max_features = 1500)
#X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

It gives an error on line -
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
TypeError: Expected sequence or array-like, got &lt;class 'gensim.models.word2vec.Word2Vec'&gt;
</code></pre>
",2017-12-16 12:52:51,2017-12-16 12:52:51,How can I use word2vec to train a classifier?,<python><word2vec><naivebayes>,,,CC BY-SA 3.0,True,False,True,False,True
14860,47604717,2017-12-02 04:50:50,,"<p>I'm using the <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser.save"" rel=""nofollow noreferrer"">save method</a> on the Gensim Phrases class to store a model for future use but if I update my version of Gensim, I have problems loading that model back in. For example, I get the following error when loading a model in Gensim 2.3.0 that was made in 2.2.0:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;()

~/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py in __init__(self, phrases_model)
    395         self.min_count = phrases_model.min_count
    396         self.delimiter = phrases_model.delimiter
--&gt; 397         self.scoring = phrases_model.scoring
    398         self.phrasegrams = {}
    399         corpus = pseudocorpus(phrases_model.vocab, phrases_model.delimiter)

AttributeError: 'Phrases' object has no attribute 'scoring'
</code></pre>

<p>Is there a better way to ensure forwards compatibility?</p>
",,2017-12-02 05:18:39,How do I save a Gensim model while ensuring forwards compatibility?,<python><machine-learning><nlp><data-science><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14868,47624644,2017-12-03 23:53:47,,"<p>I do not understand why words are not present in gensim model vocabulary after training</p>

<pre><code>model = gensim.models.Word2Vec(sentences, min_count=1, size=200, iter=1)
print ""AMBER"" in sentences
vec = model.wv[""AMBER""]
print vec
</code></pre>

<p>gives the following</p>

<p>True</p>

<pre><code>Traceback (most recent call last):
  File ""model.py"", line 38, in &lt;module&gt;
    vec = model.wv[""AMBER""]
  File ""/Users/nadiia/miniconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 601, in __getitem__
    return self.word_vec(words)
  File ""/Users/nadiia/miniconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 288, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'AMBER' not in vocabulary""
</code></pre>

<p>I do not understand why...
I am specifically running this model to learn embeddings of every word in list <em>sentences</em>, but then I cannot find any word in model's vocabulary. I cannot get any embeddings. </p>

<p>What's wrong with it?</p>

<p>Thanks</p>

<p>EDIT:
This helped to solve the problem</p>

<pre><code>text = [nltk.word_tokenize(sent.decode(""utf-8"")) for sent in sentences]
</code></pre>
",2017-12-04 04:11:43,2017-12-04 04:11:43,gensim model KeyError,<python><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
14884,47533772,2017-11-28 14:18:29,,"<p>I would like to call <code>model.wv.most_similar_cosmul</code>, on the same copy of <code>model</code> object, using <code>multiple cores</code>, on <code>batches of input pairs</code>.</p>

<p>The <code>multiprocessing</code> module requires multiple copies of <code>model</code>, which will require too much RAM because my <code>model</code> is 30+ GB in RAM.</p>

<p>I have tried to evaluate my query pairs. It took me ~12 hours for the first round. There may be more rounds coming. That's why I am looking for a threading solution. I understand Python has <code>Global Interpreter Lock</code> issue.</p>

<p>Any suggestions?</p>
",,2017-11-30 20:26:22,Gensim word2vec / doc2vec multi-threading parallel queries,<python><multithreading><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14887,47681257,2017-12-06 18:37:35,,"<p>On <a href=""https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel.save"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel.save</a>, function <code>save</code> has the prototype:</p>

<pre><code>save(fname, *args, **kwargs)
</code></pre>

<p>I'd like to understand what <code>args</code> and <code>kwargs</code> are, and how to pass them to <code>save</code>. However, the document doesn't explain these.</p>

<p>Could anyone help?</p>

<p>Thanks!</p>
",,2018-07-17 02:26:53,How to call Gensim's LsiModel save?,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14898,47572091,2017-11-30 11:13:39,,"<p>I am trying to find the duplicated words in google word2vec, for example, in word2vec, there are two word embeddings for 'Hello' and 'hello'. Here is my code, it's simple but not efficient.</p>

<pre><code>def load_w2v():
openfile = '../Pretrained/word2vec/GoogleNews-vectors-negative300.bin'
model = gensim.models.KeyedVectors.load_word2vec_format(openfile, binary=True)
return model.vocab.keys()

if __name__ == '__main__':
    pre_trained_words = load_w2v()
    ready_have = []
    duplicated_words = []
    for word in pre_trained_words:
        if word.lower() not in ready_have:
            ready_have.append(word.lower())
        else:
            duplicated_words.append(word)
            continue
</code></pre>

<p>However, as the pre-trained google word2vec has 3 million words, my computer has been runing for 18 hours and not finish, so I was wondering is there some effiencient way to get the duplicated words?</p>
",,2017-11-30 11:34:07,efficient way to iterate list?,<python><list><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
14914,47610985,2017-12-02 18:13:02,,"<p>I'm trying to learn dynamic topic modeling(to capture the semantic changes in the words) from data scrapped from PUBMED. I was able to get the data in the form of xml and was able to extract the ""abstract"" text and the date information off of it and saved that in the csv format. (But this is just a part of the data.)</p>

<p>Format obtained</p>

<p>Year|month|day|abstractText</p>

<p>I'm planning on using gensim lda for my model</p>

<p>I've never really done topic modeling before and need your help with guiding me through this process one step at a time.</p>

<p>Questions:</p>

<ol>
<li>Is csv a preferred format to feed into gensim lda?</li>
<li>for dynamic modeling, how should the time aspect of the data be captured and used in the model?</li>
<li>is there a better way to organize the data than in csv files?</li>
<li>Should i use the bodytext instead of the abstract for this?</li>
</ol>

<p>Hope I learn a lot from this. Thanks in advance.</p>
",2017-12-03 10:25:43,2019-02-16 21:36:36,Setup data for dynamic topic modelling,<python><text-mining><gensim><topic-modeling><pubmed>,,,CC BY-SA 3.0,False,False,True,False,False
14917,47666699,2017-12-06 04:16:35,,"<p><strong>BACKGROUND</strong></p>

<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>

<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'
['yellow', 'red','green'] -&gt; 'Colors'
['tokyo','bejing','washington','mumbai'] -&gt; 'Places'
</code></pre>

<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is ""purple"" then I should be able to predict 'Colors' as the correct category. If the new input is ""Calgary"" it should predict 'Places' as the correct category.</p>

<p><strong>APPROACH</strong></p>

<p>I did some research and came across <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Word2vec</a>. This library has a ""similarity"" and ""mostsimilarity"" function which i can use. So one brute force approach I thought of is the following:</p>

<ol>
<li>Take new input.</li>
<li>Calculate it's similarity with each word in each vector and take an average.</li>
</ol>

<p>So for instance for input ""pink"" I can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>

<p><strong>ISSUE</strong></p>

<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>
",2017-12-11 22:23:36,2018-05-14 16:43:38,Using word2vec to classify words in categories,<python><machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14939,47725012,2017-12-09 03:25:02,,"<p>SOLVED: It seems that referencing my toUse variable does not work as an acceptable file path input. Changing it to the true path fixed the problem. </p>

<p>I'm relatively new to Python and am biting off more than I can chew but I don't understand how I keep getting this error in my code. It's just a simple I/O operation on closed file yet I don't know how my file is a closed file, error is derived from the corpora.MmCorpus.serialize() statement shown after this block of code.</p>

<pre><code>from gensim import corpora
temporary=open('C:\\Users\\A\\Horror and Suspense\\data\\inUse.txt','r')
toUse=open('C:\\Users\\A\\Horror and Suspense\\data\\parsing.txt','r+')
for line in temporary:
    toUse.write(line)
temporary.close()
corpus=corpora.textcorpus.TextCorpus(input=toUse)
corpora.MmCorpus.serialize('C:\\Users\\A\\Horror and 
Suspense\\data\\corpora.mm',corpus)
</code></pre>

<p>Here's the error:</p>

<pre><code>runfile('C:/Users/A/Horror and Suspense/System1/useFiles.py',         wdir='C:/Users/A/Horror and Suspense/System1')
Traceback (most recent call last):

  File ""&lt;ipython-input-37-ccfa33041487&gt;"", line 1, in &lt;module&gt;
runfile('C:/Users/A/Horror and Suspense/System1/useFiles.py', wdir='C:/Users/A/Horror and Suspense/System1')

  File ""C:\Users\A\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 710, in runfile
execfile(filename, namespace)

  File ""C:\Users\A\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 101, in execfile
exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/A/Horror and Suspense/System1/useFiles.py"", line 10, in &lt;module&gt;
corpora.MmCorpus.serialize('C:\\Users\\A\\Horror and Suspense\\data\\corpora.mm',corpus)

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\indexedcorpus.py"", line 93, in serialize
offsets = serializer.save_corpus(fname, corpus, id2word, **kwargs)

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\mmcorpus.py"", line 51, in save_corpus
fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\matutils.py"", line 723, in write_corpus
    for docno, doc in enumerate(corpus):

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 184, in __iter__
    for text in self.get_texts():

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 250, in get_texts
for line in lines:

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 193, in getstream
with utils.file_or_filename(self.input) as f:

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\utils.py"", line 148, in file_or_filename
input.seek(0)

ValueError: I/O operation on closed file.
</code></pre>
",2017-12-10 15:54:55,2017-12-10 15:54:55,Gensim ValueError: I/O operation on closed file,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
14964,47785599,2017-12-13 04:41:03,,"<p>I am reading the paper</p>

<p>Distributed Representations of Words and Phrases and their Compositionality.</p>

<p>It is very interesting but I am really curious the relationship between the parameter 'negative' and the final performance. I personally think the final performance may become better as the increase of negative until some value. Because the more negative samples, which we are using to make the comparison, we should get better results theoretical. Of course, the performance will not become better until some points. Am I right?</p>
",,2017-12-13 19:09:08,How the negative affect model performance in gensim?,<nlp><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15002,47775557,2017-12-12 14:56:40,,"<p>I have an existing gensim Doc2Vec model, and I'm trying to do iterative updates to the training set, and by extension, the model.</p>

<p>I take the new documents, and perform preproecssing as normal:</p>

<pre><code>stoplist = nltk.corpus.stopwords.words('english')
train_corpus= []
for i, document in enumerate(corpus_update['body'].values.tolist()):
     train_corpus.append(gensim.models.doc2vec.TaggedDocument([word for word in gensim.utils.simple_preprocess(document) if word not in stoplist], [i]))
</code></pre>

<p>I then load the original model, update the vocabulary, and retrain:</p>

<pre><code>#### Original model
## model = gensim.models.doc2vec.Doc2Vec(dm=0, size=300, hs=1, min_count=10, dbow_words= 1, negative=5, workers=cores)

model = Doc2Vec.load('pvdbow_model_6_06_12_17.doc2vec')

model.build_vocab(train_corpus, update=True)

model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I then update the training set Pandas dataframe by appending the new data, and reset the index.</p>

<pre><code>corpus = corpus.append(corpus_update)
corpus = corpus.reset_index(drop=True)
</code></pre>

<p>However, when I try to use infer_vector() with the <em>updated</em> model:</p>

<pre><code>inferred_vector = model1.infer_vector(tokens)
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>the result quality is poor, suggesting that the indices from the model and the training set dataframe no longer match. </p>

<p>When I compare it against the <em>non-updated</em> training set dataframe (again using the updated model) the results are fine - though, obviously I'm missing the new documents.</p>

<p>Is there anyway to have both updated, as I want to be able to make frequent updates to the model without a full retrain of the model?</p>
",,2017-12-13 18:04:31,Updating training documents for gensim Doc2Vec model,<gensim><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
15005,47735393,2017-12-10 02:49:16,,"<p>I am using Gensim Phrases to identify important n-grams in my text as follows.</p>

<pre><code>bigram = Phrases(documents, min_count=5)
trigram = Phrases(bigram[documents], min_count=5)

for sent in documents:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
</code></pre>

<p>However, this detects uninteresting n-grams such as <code>special issue</code>, <code>important matter</code>, <code>high risk</code> etc. I am particularly, interested in detecting concepts in the text such as <code>machine learning</code>, <code>human computer interaction</code> etc.</p>

<p>Is there a way to stop phrases detecting uninteresting  n-grams as I have mentioned above in my example?</p>
",,2017-12-11 02:37:00,Gensim Phrases usage to filter n-grams,<python><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15009,47812930,2017-12-14 12:02:09,,"<p>I would like to know if there is any scientific explanation why word2vec models like CBOW perform poorly on small data. Here's what I tested;</p>

<pre><code>data=[[context1], [context2], [context3]......[contextn]]

model=trained word2vec model

model.most_similar('word')
output=[word not in even in top-10]
</code></pre>

<p>I retrained the model with 10 times the dataset.</p>

<pre><code>model.most_similar(word)
output=[word in the 10 most similar words]
</code></pre>

<p>Is there any scientific reason for the improvement in performance as the data size increased other than the increase in the word count with increase in data?</p>
",2017-12-14 12:03:51,2017-12-14 18:09:56,Scientific explanation why word2vec models perform poorly on small data,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15027,47827130,2017-12-15 06:44:04,,"<p>I am trying to do textual analysis on a bunch (about 140 ) of textual documents. Each document, after preprocessing and removing unnecessary words and stopwords, has about 7000 sentences (as determined by nlkt's sentence tokenizer) and each sentence has about 17 words on average. My job is to find hidden themes in those documents. </p>

<p>I have thought about doing topic modeling. However, I cannot decide if the data I have is enough to obtain meaningful results via LDA or is there anything else that I can do. </p>

<p>Also, how do I divide the texts into different documents? Is 140 documents (each with roughly 7000 x 17 words) enough ? or should I consider each sentence as a document. But then each document will have only 17 words on average; much like tweets. </p>

<p>Any suggestions would be helpful. 
Thanks in advance.  </p>
",,2017-12-17 06:18:40,Suggestion on LDA,<python-3.x><nlp><gensim><text-analysis>,,,CC BY-SA 3.0,False,False,True,False,False
15061,47929028,2017-12-21 16:27:43,,"<p>I am trying doc2vec for 600000 rows of sentences and my code is below:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
model.build_vocab(res) 
model.train(res, total_examples=model.corpus_count, epochs=model.iter)

#len(res) = 663406

#length of unique words 15581
print(len(model.wv.vocab))

#length of doc vectors is 10
len(model.docvecs)

# each of length 100
len(model.docvecs[1])
</code></pre>

<p>How do I interpret this result? why is the length of vector only 10 with each of size 100? when the length of 'res' is 663406, it does not make sense. I know something is wrong here.</p>

<p>In <a href=""https://stackoverflow.com/questions/37196520/understanding-the-output-of-doc2vec-from-gensim-package?rq=1"">Understanding the output of Doc2Vec from Gensim package</a>, they mention that the length of docvec is determined by 'size' which is not clear. </p>
",,2018-01-05 19:42:01,Doc2vec: model.docvecs is only of length 10,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15063,47838719,2017-12-15 19:25:11,,"<p>I loaded google's news vector -300 dataset.  Each word is represented with a 300 point vector.  I want to use this in my neural network for classification.  But 300 for one word seems to be too big.  How can i reduce the vector from 300 to say 100 without compromising on the quality.  </p>
",,2018-07-29 14:14:05,reducing word2vec dimension from Google News Vector Dataset,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15068,47799657,2017-12-13 18:13:04,,"<p>my task is to assign tags (descriptive words) to documents or posts from the list of available tags. I'm working with Doc2vec available in Gensim. I read that doc2vec can be used for document tagging. But i could not get the suitable parameter values for this task. Till now, i have tested it by changing value of parameters named 'size' and 'window'. The results i'm getting are too nonsense and also by changing values of these parameters i haven't find any trend in results i.e. at some values results got little bit improved and at some values results fall down. Can anyone suggest what should be suitable parameter values for this task? I found that 'size'(defines size if feature vector) should be large if we have enough training data. But about the rest of parameters, i am not getting sure! </p>
",,2017-12-14 05:08:30,Parameter values of Doc2vec for Document Tagging - Gensim,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15071,47890052,2017-12-19 15:20:20,,"<p>I tried to apply doc2vec on 600000 rows of sentences: Code as below:</p>

<pre><code>from gensim import models
model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1, workers = 5)
model.build_vocab(res)
token_count = sum([len(sentence) for sentence in res])
token_count

%%time
for epoch in range(100):
    #print ('iteration:'+str(epoch+1))
    #model.train(sentences)
    model.train(res, total_examples = token_count,epochs = model.iter)
    model.alpha -= 0.0001  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</code></pre>

<p>I am getting very poor results with the above implementation. 
the change I made apart from what was suggested in the tutorial was change the below line:</p>

<pre><code>  model.train(sentences)
</code></pre>

<p>As:</p>

<pre><code> token_count = sum([len(sentence) for sentence in res])
model.train(res, total_examples = token_count,epochs = model.iter)
</code></pre>
",2017-12-21 05:02:27,2017-12-21 05:02:27,Improving Gensim Doc2vec results,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15077,47859739,2017-12-17 21:40:27,,"<p>I am using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">gensim</a> to analyze document similarity in a large corpus. Each document has a ""title"", or more specifically, a unique ID string, along with the content text.</p>

<p>After looking through several <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">tutorials</a> about <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">top modeling</a>, <a href=""https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html"" rel=""nofollow noreferrer"">indexing and retrieval</a>, and Wikipedia, what is still not clear to me is how to get interpretable results getting building the LSI model, and querying the index for some search vector. After I see the top N most similar document indexes and their similarity scores, how do I lookup the <strong>titles</strong> of those documents?</p>

<p>For example, in this <a href=""https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html"" rel=""nofollow noreferrer"">code</a>:</p>

<pre><code>index.num_best = 10
print(index[query_lsi])
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.0
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.1
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.2

[(4028, 0.82495784759521484), (52384, 0.82495784759521484), (13582, 0.8166358470916748), (61938, 0.8166358470916748), (0, 0.80658835172653198), (48356, 0.80658835172653198), (85, 0.8048851490020752), (48441, 0.8048851490020752), (115, 0.79446637630462646), (48471, 0.79446637630462646)]
</code></pre>

<p>How would I lookup the title of, for example, document #61938 that came back in the most similar results?</p>

<p>In the <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">previous part to that tutorial</a>, the <code>iter_wiki()</code> function yielded a tuple of the (title, tokens). That <code>title</code> is what I want.</p>
",,2017-12-19 16:25:28,gensim document similarity: how to get document titles from most similar results?,<python><nlp><similarity><gensim><lsa>,,,CC BY-SA 3.0,False,False,True,False,False
15079,47930809,2017-12-21 18:34:44,,"<p>In the doc2vec model, Can we cluster on the vectors themselves? Should we cluster each resulting <code>model.docvecs[1]</code>vector? How to implement the clustering model?</p>

<pre><code> model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
    model.build_vocab(res) 
    model.train(res, total_examples=model.corpus_count, epochs=model.iter)


    # each of length 100
    len(model.docvecs[1])
</code></pre>
",,2018-02-12 13:29:33,Doc2vec: clustering resulting vectors,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15086,47873938,2017-12-18 17:57:17,,"<p>I have trained a word2vec model in tensorflow. But when I save the session, it only outputted <code>model.ckpt.data</code> / <code>.index</code> / <code>.meta</code> files.</p>

<p>I was thinking of implementing KNN method in retrieving nearest words. I saw answers of using gensim, but how can I save my tensorflow word2vec model into <code>.txt</code> first?</p>
",2017-12-19 19:23:35,2018-07-28 13:12:50,How to save the tensorflow's word2vec in text/binary file for later use of kNN output?,<machine-learning><tensorflow><nlp><word2vec><embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15094,47914953,2017-12-20 21:59:54,,"<p>please I have a university project called (automatic generation of OCL</p>

<p>constraints) and my supervisor asked me to choose a tool from this list of tools </p>

<p>for natural language processing:</p>

<p>Apache OpenNLP, Deeplearning4j, ChatScript, DELPH-IN, DKPro Core,
general architecture text engineering GATE, Gensim, LinguaStream, 
Mallet (software project), Modular Audio Recognition Framework, MontyLingua,
Natural Language Toolkit, SpaCy, UIMA.</p>

<p>what would be the easiest to implement and which one would be most suitable for my future work?</p>

<p>else any propositions!</p>
",2017-12-21 01:24:20,2017-12-21 07:21:26,Natural language processing tools for generating OCL,<nltk><opennlp><deeplearning4j><ocl><dkpro-core>,2020-06-21 16:38:11,,CC BY-SA 3.0,True,True,True,False,False
15101,47882600,2017-12-19 08:16:30,,"<p>I have a class <code>TfidfRecommendations</code> with several methods and inputs. Some of the inputs are the trained model objects of <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">Gensims TfIDF model</a> (the function <code>train_tfidf</code> below):</p>

<pre><code>import gensim
from gensim import models, corpora, similarities
def train_tfidf(data):
    dictionary = corpora.Dictionary(data)
    corpus = [dictionary.doc2bow(doc) for doc in data]
    tfidf = models.TfidfModel(corpus)
    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary))
    return tfidf_model, tfidf_dictionary, tfidf_index
</code></pre>

<p>Where <code>data</code> is a pandas dataframe of text documents (one document per row).</p>

<p>I use the output from <code>train_tfidf</code> above, as input into <code>TfidfRecommendations</code>:</p>

<pre><code>class TfidfRecommendations:

    def __init__(self, data, tfidf_dictionary, tfidf_model, tfidf_index):
        self.data               = data
        self.tfidf_dictionary   = tfidf_dictionary
        self.tfidf_model        = tfidf_model
        self.tfidf_index        = tfidf_index

    ...

    def get_sims(self, query):
        # query is a list of strings to be compared to the corpus data
        vec_bow = self.tfidf_dictionary.doc2bow(query)
        sims = self.tfidf_index[self.tfidf_model[vec_bow]]
        return sims
</code></pre>

<p>the problem with the class <code>TfidfRecommendations</code> is that it returns a list of tuples for <code>sims</code> which is incorrect:</p>

<pre><code>tfidf_model, tfidf_dictionary, tfidf_index = train_tfidf(data)
TFIDF = TfidfRecommendations(data, tfidf_dictionary, tfidf_model, tfidf_index)
sims = TFIDF.get_sims(query_text) # query_text is a list of string tokens
print(sims)
&gt;&gt;&gt;[(4, 0.004360197614450217),
   (19, 0.044387503503385946),
   (46, 0.10344463256852278),
   (82, 0.01845695743910715),
   (125, 0.024611722270581393),
   (133, 0.045794061264144204)]
</code></pre>

<p>Whereas it should return a numpy array of length <code>len(data)</code> with each entry being a cosine similarity between <code>query_text</code> and each row in <code>data</code>. This works fine if <code>get_sims</code> is its own independent function outside of the class <code>TfidfRecommendations</code></p>

<pre><code>def get_sims(query, tfidf_dictionary, tfidf_index, tfidf_model):
    # query is a list of strings to be compared
    # to the corpus data
    vec_bow = tfidf_dictionary.doc2bow(query)
    sims = tfidf_index[tfidf_model[vec_bow]]
    return sims

get_sims(query, tfidf_dictionary, tfidf_index, tfidf_model)
&gt;&gt;&gt; array([ 0.00123292,  0.0080641 ,  0.00420302, ...,  0.        ,
    0.0101376 ,  0.00987199], dtype=float32)
</code></pre>

<p><strong>What is going wrong here?</strong> Why can't gensim model objects be used with <code>self.</code> inside a class? Any help would be much appreciated.</p>
",2017-12-19 08:35:06,2017-12-19 08:35:06,Using self. in a class with Gensim TfIDF,<python><class><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,False
15108,47936578,2017-12-22 06:02:29,,"<p>I am trying to use word2vec in text classification algorithm.
I want t create vectorizer using word2vec, I have used below script. But I am not able to get one row for each document instead I am getting matrix of different dimension for every document. 
For example for 1st document matrix of 31X100,  2nd 163X100 and 3rd 73X100 and so on.
Actually I need dimension of every document as 1X100 , so that i can use these as input feature for training model</p>

<p>Can anyone help me here.</p>

<pre><code>import os
import pandas as pd       
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # Import the stop word list
import gensim
import numpy as np

train = pd.read_csv(""Data.csv"",encoding='cp1252')
wordnet_lemmatizer = WordNetLemmatizer()

def Description_to_words(raw_Description):
    Description_text = BeautifulSoup(raw_Description).get_text() 
    letters_only = re.sub(""[^a-zA-Z]"", "" "", Description_text)
    words = word_tokenize(letters_only.lower())    
    stops = set(stopwords.words(""english"")) 
    meaningful_words = [w for w in words if not w in stops]
    return( "" "".join(wordnet_lemmatizer.lemmatize(w) for w in meaningful_words))

num_Descriptions = train[""Summary""].size
clean_train_Descriptions = []
print(""Cleaning and parsing the training set ticket Descriptions...\n"")
clean_train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    clean_train_Descriptions.append(Description_to_words( train[""Summary""][i] ))

model = gensim.models.Word2Vec(clean_train_Descriptions, size=100)
w2v = dict(zip(model.wv.index2word, model.wv.syn0))

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        #self.dim = len(word2vec.itervalues().next())
        self.dim = 100

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

a=MeanEmbeddingVectorizer(w2v)
clean_train_Descriptions[1]
a.transform(clean_train_Descriptions[1])

train_Descriptions = []
for i in range( 0, num_Descriptions ):
    if( (i+1)%1000 == 0 ):
        print(""Description %d of %d\n"" % ( i+1, num_Descriptions ))
    train_Descriptions.append(a.transform("" "".join(clean_train_Descriptions[i])))
</code></pre>
",2017-12-22 08:14:54,2019-04-13 19:58:23,word2Vec vector representation for text classification algorithm,<python><word2vec><word-embedding>,,,CC BY-SA 3.0,True,False,True,False,False
15109,47893976,2017-12-19 19:30:31,,"<p>Below is the code I used to preprocess the text and apply text rank(I followed the gensim textrank tutorial). Please help me with a method to get better results. My text data is a column from a csv with more than 2000 rows. (each row, a sentence). </p>

<p>Output I get is 18 lines (Each different line, not a paragraph) of text as 
summary, and 20 words as keywords. Will the output be a paragraph of text as summary? Can we control the number of keywords to be displayed </p>

<pre><code>reg_ex = r'[^a-zA-Z]'
replace = ' '
wordnet_lemmatizer = WordNetLemmatizer()
#stop = stopwords.words('english')

comp_df = df['COMMENT'].str.replace(reg_ex, replace).apply(lambda t: ' '.join([wordnet_lemmatizer.lemmatize(w)for w in t.split()])).str.lower()

aa = comp_df.to_string()

import requests

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

from gensim.summarization import summarize
from gensim.summarization import keywords


print ('Summary:')
print (summarize(aa,ratio=0.01))

print ('\nKeywords:')
print (keywords(aa, ratio=0.01))
</code></pre>
",,2018-07-18 09:44:25,Gensim text rank:,<python><nlp><gensim><summarization>,,,CC BY-SA 3.0,False,False,True,False,False
15111,47898159,2017-12-20 02:57:57,,"<p>I am trying to build a word2vec  similarity dictionary. I was able to build one dictionary but the similarities are not being populated correctly. Am I missing anything in my code?</p>

<p><strong>Input sample data  Text</strong></p>

<pre><code>TAK PO LUN UNIT 3 15/F WAYSON COMMERCIAL G 28 CONNAUGHT RD WEST SHEUNG WAN
- EDDY SUSANTO YAHYA ROOM 1503-05 WESTERN CENTRE 40-50 DES VOEUX W. SHEUNG WAN
DNA FINANCIAL SYSTEMS INC UNIT 10 19F WAYSON COMMERCIAL 28 CONNAUGHT RD SHEUNG WAN
G/F 60 PO HING FONG SHEUNG WAN
10B CENTRAL MANSION 270 QUEENS RD CENTRAL SHEUNG WAN
AKAMAI INTERNATIONAL BV C/O IADVANTAGE 28/F OF MEGA I-ADVANTAGE 399 CHAI WAN RD CHAI WAN HONG KO HONG KONG
VICTORIA CHAN F/5E 1-3 FLEMING RD WANCHI WAN CHAI
HISTREND 365 5/F FOO TAK BUILDING 365 HENNESSY RD WAN CHAI H WAN CHAI
ROOM 1201 12F CHINACHEM JOHNSO PLAZA 178 186 JOHNSTON RD WAN CHAI
LUEN WO BUILDING 339 HENNESSY RD 9 FLOOR WAN CHAI HONG KONG
</code></pre>

<p><strong>My code:</strong> </p>

<pre><code>import gensim
from gensim import corpora,similarities,models
class AccCorpus(object):

   def __init__(self):
       self.path = ''

   def __iter__(self):
       for sentence in data[""Adj_Addr""]:
           yield [word.lower() for word in sentence.split()]

   def build_corpus():
       model = gensim.models.word2vec.Word2Vec(alpha=0.05, min_alpha=0.05,window=2,sg=1)
       sentences = AccCorpus()
       model.build_vocab(sentences)
       for epoch in range(1):
           model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)
           model.alpha -= 0.002  # decrease the learning rate
           model.min_alpha = model.alpha  # fix the learning rate, no decay

       model_name = ""word2vec_model""
       model.save(model_name)
       return model

model=build_corpus()
</code></pre>

<p><strong>My results:</strong></p>

<pre><code>model.most_similar(""wan"")
[('want', 0.6867533922195435),
 ('puiwan', 0.6323356032371521),
 ('wan.', 0.6132887005805969),
 ('wanstreet', 0.5945449471473694),
 ('aupuiwan', 0.594132661819458),
 ('futan', 0.5883135199546814),
 ('fotan', 0.5817855000495911),
 ('shanmei', 0.5807071924209595),
 ('30-33', 0.5789132118225098),
 ('61-63au', 0.5711270570755005)]
</code></pre>

<p>Here are my expected outputs for the similarity: <strong>sheungwan, wanchai, chaiwan</strong>. I am guessing my skipgrams are not working properly. How can I fix this?</p>
",2017-12-20 15:56:38,2017-12-20 15:56:38,Skip-gram with Word2Vec not working properly,<scikit-learn><neural-network><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,True
15128,47901979,2017-12-20 08:47:26,,"<p>I am using the Doc2Vec model in gensim python library.</p>

<p>Every time I feeds the model with the same sentences data and set the parameter:seed of Doc2Vec to a fixed number, the model gives different vectors after the model is built.</p>

<p>For tests purpose, I need a determined result every time I gave a unchanged input data. I searched a lot and does not find a way to keep the gensim's result unchanged. </p>

<p>Is there anything wrong in the way I use it? thanks for replying in advance.</p>

<p>Here is my code:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec(sentences, dm=1, dm_concat=1, size=100, window=5, hs=0, min_count=10, seed=64)
result = model.docvecs
</code></pre>
",2017-12-20 13:38:00,2017-12-21 05:08:19,gensim doc2vec give non-determined result,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15129,47974626,2017-12-26 06:19:39,,"<p>I recently installed gensim and glove in my mac and am trying to get word embedding for textual data I have. However, I'm having trouble finding the right function for it. I've only come across methods to get similarity metrics between two words. How do I train a glove object with data present in the library and use it to obtain embeddings for words in my dataset? Or is there any other library in python to do this? Thanks!</p>
",,2018-01-03 06:35:30,Getting word embeddings for your dataset using training data in glove,<python><macos><nlp>,,,CC BY-SA 3.0,False,False,True,False,False
15131,47976170,2017-12-26 09:02:50,,"<p>I have taken a code online to do sentiment analysis on twitter database. I tried running it and it gave me at the beginning error for printing, which I figured out that the newer version of python has changed its way to do print. I am getting error that shows my data is not filled in the array, if anyone has worked with python and has eagle eye to see where I am going wrong please help. </p>

<pre><code>    import numpy as np 
    from copy import deepcopy
    from string import punctuation
    from random import shuffle
    import chardet
    from sklearn.manifold import TSNE
    from sklearn.preprocessing import scale


    import bokeh.plotting as bp
    from bokeh.models import HoverTool, BoxSelectTool
    from bokeh.plotting import figure, show, output_notebook

    import gensim
    from gensim.models.word2vec import Word2Vec 
    LabeledSentence = gensim.models.doc2vec.LabeledSentence 

    import pandas as pd 
    pd.options.mode.chained_assignment = None

    from tqdm import tqdm
    tqdm.pandas(desc=""progress-bar"")

    from nltk.tokenize import TweetTokenizer 
    tokenizer = TweetTokenizer()

    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer

    def ingest(filename):
        with open(filename, 'rb') as f:
            result = chardet.detect(f.read())
        data = pd.read_csv(filename, encoding=result['encoding'])
        data.drop(['ItemID', 'Date', 'Blank', 'SentimentSource'], axis=1, inplace=True)
        data = data[data.Sentiment.isnull() == False]
        data['Sentiment'] = data['Sentiment'].map({4:1, 0:0})
        data = data[data['SentimentText'].isnull() == False]
        data.reset_index(inplace=True)
        data.drop('index', axis=1, inplace=True)
        print('dataset loaded with shape {}', format(data.shape)) 

        return data

    def tokenize(tweet):
        try:
            tweet = unicode(tweet.decode('utf-8').lower())
            tokens = tokenizer.tokenize(tweet)
            tokens = filter(lambda t: not t.startswith('@'), tokens)
            tokens = filter(lambda t: not t.startswith('#'), tokens)
            tokens = filter(lambda t: not t.startswith('http'), tokens)
            return tokens
        except:
            return 'NC'

    def postprocess(data, n=100):
        data = data.head(n)
        data['tokens'] = data['SentimentText'].progress_map(tokenize)  
        data = data[data.tokens != 'NC']
        data.reset_index(inplace=True)
        data.drop('index', inplace=True, axis=1)
        return data


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in  enumerate(tweets):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
            print("":::::::::::::::::::::::::"")
        return labelized


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in tqdm(enumerate(tweets)):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
        return labelized


    def buildWordVector(tokens, size):
        vec = np.zeros(size).reshape((1, size))
        count = 0.
        for word in tokens:
            try:
                vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]
                count += 1.
            except KeyError: 

                continue
        if count != 0:
            vec /= count
        return vec



    if __name__ == '__main__':

        filename = './training.csv'

        #n = 1000000
        n = 100
        n_dim = 200

        data = ingest(filename)
        #data = data.head(5)
        data = postprocess(data, n)

        x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens), np.array(data.head(n).Sentiment), test_size=0.2)


        print(""training length X"", len(x_train))

        print(""training length Y"", len(y_train))


        x_train = labelizeTweets(x_train, 'TRAIN')
        x_test = labelizeTweets(x_test, 'TEST')

        print(""jljkjkjlkjlj"", len(x_train))

        tweet_w2v = Word2Vec(size=n_dim, min_count=10)
        #tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])
        tweet_w2v.build_vocab([x.words for x in x_train])

        #tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
        tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)




        print(tweet_w2v.most_similar('good'))

        if True:
            print('building tf-idf matrix ...')
            vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
            matrix = vectorizer.fit_transform([x.words for x in x_train])
            tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
            print('vocab size :', len(tfidf))

            train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])
            train_vecs_w2v = scale(train_vecs_w2v)

            test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])
            test_vecs_w2v = scale(test_vecs_w2v)

            model = Sequential()
            model.add(Dense(32, activation='relu', input_dim=200))
            model.add(Dense(1, activation='sigmoid'))
            model.compile(optimizer='rmsprop',
                                        loss='binary_crossentropy',
                                        metrics=['accuracy'])

            model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=32, verbose=2)

            score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)
            print (score[1])

    output_notebook()
    plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=""A map of 10000 word vectors"",
        tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",
        x_axis_type=None, y_axis_type=None, min_border=1)

    word_vectors = [tweet_w2v[w] for w in tweet_w2v.wv.vocab.keys()[:5000]]

    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
    tsne_w2v = tsne_model.fit_transform(word_vectors)

    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])
    tsne_df['words'] = tweet_w2v.wv.vocab.keys()[:5000]

    plot_tfidf.scatter(x='x', y='y', source=tsne_df)
    hover = plot_tfidf.select(dict(type=HoverTool))
    hover.tooltips={""word"": ""@words""}
    show(plot_tfidf)
</code></pre>

<p>This is the error I am getting </p>

<pre><code>    C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
dataset loaded with shape {} (505, 2)
progress-bar: 100%|##########################################################################| 505/505 [00:00&lt;?, ?it/s]
training length X 0
training length Y 0
0it [00:00, ?it/s]
0it [00:00, ?it/s]
jljkjkjlkjlj 0
Traceback (most recent call last):
  File ""Sentiment_Analysis.py"", line 127, in &lt;module&gt;
    tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\models\word2vec.py"", line 951, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>
",2017-12-26 12:39:45,2018-01-06 07:43:20,Sentiment Analysis Code (word2vec) not properly working in my python version (vocabulary not built),<python><twitter><nltk><sentiment-analysis><word2vec>,,,CC BY-SA 3.0,True,False,True,False,True
15134,47978579,2017-12-26 12:20:35,,"<p>I am using the following function to load my word2vec model.</p>

<pre><code>def __init__(self, filename):
        print filename
        try:
            self.model = gensim.models.Word2Vec.load(filename)
        except cPickle.UnpicklingError:
            load = gensim.models.Word2Vec.load_word2vec_format
            self.model = load(filename, binary=True)
</code></pre>

<p>However, I am getting the following error when I try to do it.</p>

<pre><code>Traceback (most recent call last):
  File ""./explore"", line 70, in &lt;module&gt;
    api_controller.model = Model(sys.argv[1])
  File ""/home/volka/Documents/projects/word2vec-explorer/explorer.py"", line 77, in __init__
    self.model = gensim.models.Word2Vec.load(filename)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 1458, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 256, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 920, in unpickle
    return _pickle.loads(f.read())
AttributeError: 'module' object has no attribute 'call_on_class_only'
</code></pre>

<p>The genism version I am using in both the versions are 0.12.3.</p>

<p>Please let me know where I am making it wrong?</p>

<hr>

<p>This is how I tried to remove call_on_class_only.</p>

<pre><code>model = word2vec.Word2Vec(text, sg=0, negative=5, hs=0)
model.save(""test_project"")

#load, delete and save
model_1 = word2vec.Word2Vec.load(""test_project"")
del model_1.call_on_class_only
model.save(model_name_2)
</code></pre>

<p>It gives me the following error: <code>AttributeError: call_on_class_only</code></p>

<p>Please help me.</p>
",2017-12-28 03:49:45,2017-12-28 03:49:45,Error when loading the word2vec model,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15141,47996444,2017-12-27 17:43:11,,"<p>I am trying to get a PyLDAvis graph that looks like the 2 shown in this link, that you can see right away (Intertopic Distance Map and Top 30 Most Salient Terms): </p>

<p><a href=""http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb"" rel=""nofollow noreferrer"">http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb</a></p>

<p>My code does display it but only partially, I can only see 1 cluster on the left and like 5-6 terms on the right, the rest gets cut off (there should be many clusters and 30 words shown). This is the code I have:</p>

<pre><code>import warnings
warnings.filterwarnings('ignore')

import os 
%matplotlib inline
import pyLDAvis.gensim
import gensim
pyLDAvis.enable_notebook()

d = gensim.corpora.Dictionary.load('dictionary.dict')
c = gensim.corpora.MmCorpus('corpus.mm')


pd.options.display.max_colwidth = 5000
pd.options.display.show_dimensions
data = pyLDAvis.gensim.prepare(ldamodel, c, d, mds='tsne')
</code></pre>

<p>Any ideas as to why its only showing me part of the graph?</p>

<p>EDIT update:</p>

<p>It seems the very similar code below worked:</p>

<pre><code>import warnings
warnings.filterwarnings('ignore')

ldamodel.save('topic.model')
import os 
%matplotlib inline
import pyLDAvis.gensim
import gensim
pyLDAvis.enable_notebook()

d = gensim.corpora.Dictionary.load('dictionary.dict')
c = gensim.corpora.MmCorpus('corpus.mm')
lda = gensim.models.LdaModel.load('topic.model')

pd.options.display.max_colwidth = 5000
data = pyLDAvis.gensim.prepare(lda, c, d, mds='tsne')
pyLDAvis.display(data)
</code></pre>
",2018-05-13 18:38:35,2018-05-13 18:38:35,Graph only partially displaying in Jupyter Notebook output,<python><python-3.x><lda><topic-modeling><graph-visualization>,,,CC BY-SA 4.0,False,False,True,False,False
15147,47959639,2017-12-24 09:58:48,,"<p>I have a set of embeddings trained with a neural network that has nothing to do with gensim's word2vec.</p>

<p>I want to use these embeddings as the initial weights in <code>gensim.Word2vec</code>.</p>

<p>Now what I did see is that I can <code>model.load(SOME_MODEL)</code> and then continue training, but it requires a gensim modle as input. Also <code>reset_from()</code> seems to only accept other gensim model.
But in my case, I don't have a gensim model to start from, but a text file in word2vec format of embeddings.</p>

<p>So how do I start transfer learning from an word2vec text file to <code>gensim.Word2vec</code>?</p>
",,2017-12-24 15:02:56,gensim Word2vec transfer learning (from a non-gensim model),<python><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15153,47905576,2017-12-20 11:59:12,,"<p>I have created document vectors for a large corpus using Gensim's doc2vec.</p>

<p><code>sentences=gensim.models.doc2vec.TaggedLineDocument('file.csv')</code></p>

<p><code>model = gensim.models.doc2vec.Doc2Vec(sentences,size = 10, window = 800, min_count = 1, workers=40, iter=10, dm=0)</code></p>

<p>Now I am using Gensim's infer_vector() using those document vectors to create document vectors for another sample corpus</p>

<p><code>Eg: model.infer_vector('This is a string')</code></p>

<p>Is there a way to pass the entire DataFrame through infer_vector and get the output vectors for each line in the DataFrame?</p>
",,2017-12-21 13:11:43,How to use Gensim Doc2vec infer_vector() for large DataFrame?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15155,48017343,2017-12-29 04:18:46,,"<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
",,2018-05-17 09:53:40,How to convert gensim Word2Vec model to FastText model?,<nlp><word2vec><gensim><word-embedding><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
15157,48019843,2017-12-29 08:45:29,,"<p>I am trying to reproduce the results of this paper: <a href=""https://arxiv.org/pdf/1607.06520.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.06520.pdf</a></p>

<p>Specifically this part:</p>

<blockquote>
  <p>To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/EOJJK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EOJJK.png"" alt=""enter image description here""></a></p>

<p>I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec. </p>

<p>The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:</p>

<p><a href=""https://i.stack.imgur.com/7b6Dj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7b6Dj.png"" alt=""enter image description here""></a></p>

<p>I've computed the differences between each normalized vector in the following way:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-
negative300.bin', binary = True)
model.init_sims()

pairs = [('she', 'he'),
('her', 'his'),
('woman', 'man'),
('Mary', 'John'),
('herself', 'himself'),
('daughter', 'son'),
('mother', 'father'),
('gal', 'guy'),
('girl', 'boy'),
('female', 'male')]

difference_matrix = np.array([model.word_vec(a[0], use_norm=True) - model.word_vec(a[1], use_norm=True) for a in pairs])
</code></pre>

<p>I then perform PCA on the resulting matrix, with 10 components, as per the paper:</p>

<pre><code>from sklearn.decomposition import PCA
pca = PCA(n_components=10)
pca.fit(difference_matrix)
</code></pre>

<p>However I get very different results when I look at <code>pca.explained_variance_ratio_</code> :</p>

<pre><code>array([  2.83391436e-01,   2.48616155e-01,   1.90642492e-01,
         9.98411858e-02,   5.61260498e-02,   5.29706681e-02,
         2.75670634e-02,   2.21957722e-02,   1.86491774e-02,
         1.99108478e-32])
</code></pre>

<p>or with a chart:</p>

<p><a href=""https://i.stack.imgur.com/RuNEi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RuNEi.png"" alt=""enter image description here""></a></p>

<p>The first component accounts for less than 30% of the variance when it should be above 60%! </p>

<p>The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what.</p>

<p>Note: I've tried without normalizing the vectors, but I get the same results.</p>
",,2018-11-09 21:37:06,PCA on word2vec embeddings,<python><scikit-learn><nlp><pca><word2vec>,,,CC BY-SA 3.0,False,False,True,False,True
15158,47998685,2017-12-27 21:10:25,,"<p>I tried generating topics using gensim for 300000 records. On trying to visualize the topics, I get a validation error. I can print the topics after model training, but it fails on using pyLDAvis</p>

<pre><code># Running and Training LDA model on the document term matrix.
ldamodel1 = Lda(doc_term_matrix1, num_topics=10, id2word = dictionary1, passes=50, workers = 4)

(ldamodel1.print_topics(num_topics=10, num_words = 10))
 #pyLDAvis
d = gensim.corpora.Dictionary.load('dictionary1.dict')
c = gensim.corpora.MmCorpus('corpus.mm')
lda = gensim.models.LdaModel.load('topic.model')

#error on executing this line
data = pyLDAvis.gensim.prepare(lda, c, d)
</code></pre>

<p>I got the below error on trying to after running above pyLDAvis </p>

<pre><code>---------------------------------------------------------------------------
ValidationError                           Traceback (most recent call last)
&lt;ipython-input-53-33fd88b65056&gt; in &lt;module&gt;()
----&gt; 1 data = pyLDAvis.gensim.prepare(lda, c, d)
      2 data

C:\ProgramData\Anaconda3\lib\site-packages\pyLDAvis\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)
    110     """"""
    111     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)
--&gt; 112     return vis_prepare(**opts)

C:\ProgramData\Anaconda3\lib\site-packages\pyLDAvis\_prepare.py in prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)
    372    doc_lengths      = _series_with_name(doc_lengths, 'doc_length')
    373    vocab            = _series_with_name(vocab, 'vocab')
--&gt; 374    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)
    375    R = min(R, len(vocab))
    376 

C:\ProgramData\Anaconda3\lib\site-packages\pyLDAvis\_prepare.py in _input_validate(*args)
     63    res = _input_check(*args)
     64    if res:
---&gt; 65       raise ValidationError('\n' + '\n'.join([' * ' + s for s in res]))
     66 
     67 

ValidationError: 
 * Not all rows (distributions) in topic_term_dists sum to 1.
</code></pre>
",,2019-09-11 03:06:52,pyLDAvis: Validation error on trying to visualize topics,<python><nlp><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
15165,48051767,2018-01-01 17:30:03,,"<p>I am trying to built a predictive model using address data. I have transformed the address data into a bigram model using <strong>Gensim Phrases</strong>  but I am facing  issues while transforming the address data into the corresponding bigrams and attaching as a separate column which can be further used for countvectorization.</p>

<p>My code</p>

<p><strong>Gensim Bigram Phrases model</strong></p>

<pre><code>from gensim.models import Phrases
adj_addr=data['Clean_addr'].values.tolist()
sentence_stream = [doc.split("" "") for doc in adj_addr]
bigram = Phrases(sentence_stream, min_count=100, threshold=2)
</code></pre>

<hr>

<p><strong>Sample input from column of dataframe</strong></p>

<pre><code>SUITE 7001 70/F INTERNATIONAL FINANCE CENTRE TWO  8 FINANCE ST CENTRAL HONG HONG KONG
QUALCOMM INTERNATIONAL INC. 9 QUEENS RD CENTRAL 27TH FLOOR HONG KONG
SAMUEL CHEN COMPANY LIMITED 25 CHIU LUNG ST CHIU LUNG BUILDING 4TH FLOOR CENTRAL HONG KONG
</code></pre>

<p><strong>Expected output(New data after passing the gensim phrase model)</strong></p>

<pre><code>SUITE 7001 70/F INTERNATIONAL_FINANCE CENTRE TWO 8_FINANCE ST CENTRAL_HONG HONG_KONG
QUALCOMM INTERNATIONAL INC. 9 QUEENS_RD CENTRAL 27TH FLOOR HONG_KONG
SAMUEL CHEN COMPANY_LIMITED 25 CHIU LUNG_ST CHIU LUNG BUILDING 4TH_FLOOR CENTRAL_HONG KONG
</code></pre>

<p><strong>I am not able to replace the corresponding addresses with the respective bigrams from gensim 
 phraseses model iteratively  .</strong> <strong>My expected output is to replace all the old addresses with newly generated bigram phrases .  So that i can pass it to a countvectorizer</strong> </p>

<p>Any help is appreciated.</p>
",2018-01-02 03:04:35,2018-01-02 03:04:35,Creating Bigrams Phrases of a Column in Pandas using Gensim and attaching it to the to same dataframe,<pandas><nlp><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15171,47925377,2017-12-21 12:49:07,,"<p>I need to extract only those words whose tags match with pos-tags variable of program and pass those words to LSI model but when i print nouns i get an empty list. </p>

<p>Here is my sample input of noun file:</p>

<pre><code>['All,DT', 'praise,NN', 'is,VBZ', 'due,JJ', 'to,TO', 'God,NNP', 'alone,RB', ',,,', 'the,DT', 'Sustainer,NNP', 'of,IN', 'all,PDT', 'the,DT', 'worlds,NNS', ',,,', '\n']
['The,DT', 'Most,JJS', 'Gracious,JJ', ',,,', 'the,DT', 'Dispenser,NNP', 'of,IN', 'Grace,NNP', ',,,', '\n']
['Lord,NNP', 'of,IN', 'the,DT', 'Day,NNP', 'of,IN', 'Judgment,NN', '!,.', '\n']
['Thee,NNP', 'alone,RB', 'do,VBP', 'we,PRP', 'worship,NN', ';,:', 'and,CC', 'unto,JJ', 'Thee,NNP', 'alone,RB', 'do,VBP', 'we,PRP', 'turn,VB', 'for,IN', 'aid,NN', '.,.', '\n']
['Guide,NNP', 'us,PRP', 'the,DT', 'straight,JJ', 'way,NN', '.,.', '\n']  
</code></pre>

<p>Here is my sample code:</p>

<pre><code>import nltk
import os.path
import re
import gensim
from gensim import corpora, models
from gensim import corpora, models, similarities

thefile='E://noun.txt'
file3 = open(thefile,'r',encoding='utf8')
nouns=[]
arr=[]
arr1=[]
pos_tags = ('NN','NNS', 'NNPS')
for line in file3.readlines():
    nouns.append([j.split(',')[0] for i in line for j in i if any(j.endswith(p) for p in pos_tags)])
print(nouns)
dictionary = corpora.Dictionary(nouns)
corpus = [dictionary.doc2bow(inp) for inp in nouns]
# extract 400 LSI topics; use the default one-pass algorithm
lsi = models.lsimodel.LsiModel(corpus=corpus, id2word=dictionary, num_topics=10)
# print the most contributing words (both positively and negatively) for each of the first ten topics
arr1.append(lsi.print_topics(10))
print(arr1)
</code></pre>
",,2017-12-21 12:49:07,How to retrieve only nouns from a file and pass them as an array to LSA?,<python><arrays><pos-tagger><lsa>,,,CC BY-SA 3.0,True,False,True,False,False
15182,48073718,2018-01-03 08:28:06,,"<p>I have created a word2vec  dictionary using Gensim .I want replace my text corpus with the root word .
Is there a way by which we can replace the text data corpus by the root word .</p>

<p><strong>Eg. Building is my root word and i have similarity of it in my dictionary. I want replace all similar words to building that have similarity above.6 in my orginal text corpus</strong>.</p>

<p><strong>Sample data from column of the dataframe</strong></p>

<pre><code>canara bank aon china bldng queens rd centeal central
des voeux rd west hk unit f kwan yick bldng phase central western
formula growth asia limited suite chinachem tower connaught rd central
bangkok bank public company limited central district branch des voeux rd central cenrta
</code></pre>

<p><strong>Similarities</strong></p>

<pre><code>  model.most_similar(""building"")
    [('bu', 0.762892484664917),
     ('bldg', 0.7351159453392029),
     ('bl', 0.7237456440925598),
     ('building.', 0.7153196334838867),
     ('buliding', 0.6988817453384399),
     ('bld', 0.6966143846511841),
     ('bldng', 0.663501501083374),
     ('bdg', 0.6504702568054199),
     ('bd', 0.6480772495269775),
     ('blog', 0.6432161331176758)]

model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p><strong>Dictionary</strong></p>

<pre><code>import gensim
from gensim import corpora,similarities,models
class AccCorpus(object):

    def __init__(self):
        self.path = ''


    def __iter__(self):
        for sentence in data[""Adj_Addr""]:
            yield [word.lower() for word in sentence.split()]


def build_corpus():
    model = gensim.models.word2vec.Word2Vec(alpha=0.025, min_alpha=0.025,window=2,sg=2)
    sentences = AccCorpus()
    model.build_vocab(sentences)
    for epoch in range(1):
        model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)
        model.alpha -= 0.002  # decrease the learning rate
        model.min_alpha = model.alpha  # fix the learning rate, no decay

    model_name = ""word2vec_model""
    model.save(model_name)
    return model
model=build_corpus()
</code></pre>
",2018-01-03 17:43:41,2018-01-17 13:15:33,Word Replacement in text corpus using word2vec similarity dictionary in a pandas dataframe,<python><pandas><string-matching><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15196,48096066,2018-01-04 13:09:28,,"<p>I am trying to use pre-trained word embeddings taking into account phrases. Popular pre-trained embeddings like <code>GoogleNews-vectors-negative300.bin.gz</code> have separate embeddings for phrases as well as unigrams e.g., embeddings for <code>New_York</code> and the two unigrams <code>New</code> and <code>York</code>. Naive word tokenization and dictionary look-up ignore the bigram embedding.     </p>

<p>Gensim provides a nice <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">Phrase</a> model, where given a text sequence it can learn compact phrases e.g., <code>New_York</code> instead of two unigrams <code>New</code> and <code>York</code>. This is done by aggregating and comparing count statistics between the  unigrams and the bigram. 1. Is it possible to use <code>Phrase</code> with pre-trained embeddings without estimating the count statistics elsewhere? </p>

<ol>
<li>Is it possible to use <code>Phrase</code> with pre-trained embeddings without estimating the count statistics elsewhere? </li>
<li>If not, is there an efficient way to use these bigrams? I can imagine a way using a loop, but I believe it is ugly (Below).</li>
</ol>

<p>Here is the ugly code.</p>

<pre><code>from ntlk import word_tokenize
last_added = False
sentence = 'I love New York.'
tokens =  [""&lt;s&gt;""]+ word_tokenize(sentence) +""&lt;\s&gt;""]
vectors = []
for index, token in enumerate(tokens):
    if last_added:
        last_added=False
        continue
    if ""%s_%s""%(tokens[index-1], token) in model:
        vectors.append(""%s_%s""%(tokens[index-1], token))
        last_added = True
    else:
        vectors.append(tokens[index-1])
        lase_added = False
</code></pre>
",,2018-01-04 13:09:28,Using gensim's Phraser with pre-trained vectors,<python><machine-learning><gensim><phrase>,,,CC BY-SA 3.0,False,False,True,False,False
15206,48059145,2018-01-02 10:19:07,,"<p>I have been using <strong>gensim's</strong> libraries to train a doc2Vec model. After experimenting with different datasets for training, I am fairly confused about what should be an ideal training data size for doc2Vec model?</p>

<p>I will be sharing my understanding here. Please feel free to correct me/suggest changes-</p>

<ol>
<li><strong>Training on a general purpose dataset-</strong> If I want to use a model trained on a general purpose dataset, in a specific use case, I need to train on a lot of data.</li>
<li><strong>Training on the context related dataset-</strong> If I want to train it on the data having the same context as my use case, usually the training data size can have a smaller size.</li>
</ol>

<p><em>But what are the number of words used for training, in both these cases?</em></p>

<p>On a general note, we stop training a ML model, when the error graph reaches an ""elbow point"", where further training won't help significantly in decreasing error. Has any study being done in this direction- where doc2Vec model's training is stopped after reaching an elbow ?</p>
",,2018-01-03 15:40:45,How much data is actually required to train a doc2Vec model?,<neural-network><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15212,48060401,2018-01-02 11:46:40,,"<p>as an output for creating a Word2Vec model on ~1GB of corpus I got 3 files as an output:</p>

<ul>
<li>word2vec_model</li>
<li>word2vec_model.syn1neg.npy</li>
<li>word2vec_model.wv.syn0.npy</li>
</ul>

<p>I used to have only the first file on (when training a smaller corpus).</p>

<p>how should I treat the last 2 files when loading the model?
Should I load only the first one and run queries on it as usual?</p>
",2018-01-02 11:49:25,2018-01-03 15:30:29,syn1neg & syn0 created as output,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15217,48009532,2017-12-28 14:47:23,,"<p>I have generated word vectors from a corpus, but I am facing out of vocabulary issues for many words. How can I generate word vectors for OOV words on the fly using existing word embedding?</p>
",,2019-05-08 14:45:29,Word embedding for OOV words,<machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15223,48044670,2017-12-31 17:55:45,,"<p>I am trying to get started with <code>word2vec</code> and <code>doc2vec</code> using the excellent tutorials, <a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104"" rel=""nofollow noreferrer"">here</a> and trying to use the code samples. I only added in a <code>line_clean()</code> method to remove punctuation, stopwords etc. </p>

<p>But I am having trouble with the <code>line_clean()</code> method called in the training iterations.  I understand the call to the global method is messing it up, but I am not sure how to get past this problem.</p>

<pre><code>Iteration 1
Traceback (most recent call last):
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 96, in &lt;module&gt;
    train()
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 91, in train
    model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 61, in sentences_perm
    shuffled = list(self.sentences)
AttributeError: 'TaggedLineSentence' object has no attribute 'sentences'
</code></pre>

<p>My code is below:</p>

<pre><code>import gensim
from gensim import utils
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import os
import random
import numpy
from sklearn.linear_model import LogisticRegression
import logging
import sys
from nltk import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stopword_set = set(stopwords.words('english'))


def clean_line(line):
    new_str = unicode(line, errors='replace').lower() #encoding issues
    dlist = tokenizer.tokenize(new_str)
    dlist = list(set(dlist).difference(stopword_set))
    new_line = ' '.join(dlist)
    return new_line


class TaggedLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no]))
        return(self.sentences)

    def sentences_perm(self):
        shuffled = list(self.sentences)
        random.shuffle(shuffled)
        return(shuffled)


def train():
    #create a list data that stores the content of all text files in order of their names in docLabels
    doc_files = [f for f in os.listdir('./data/') if f.endswith('.csv')]

    sources = {}
    for doc in doc_files:
        doc2 = os.path.join('./data',doc)
        sources[doc2] = doc.replace('.csv','')

    sentences = TaggedLineSentence(sources)


    # #iterator returned over all documents
    model = gensim.models.Doc2Vec(size=300, min_count=2, alpha=0.025, min_alpha=0.025)
    model.build_vocab(sentences)

    #training of model
    for epoch in range(10):
        #random.shuffle(sentences)
        print 'iteration '+str(epoch+1)
        #model.train(it)
        model.alpha -= 0.002
        model.min_alpha = model.alpha
        model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
    #saving the created model
    model.save('reddit.doc2vec')
    print ""model saved"" 

train()
</code></pre>
",,2018-01-03 16:00:15,doc2vec/gensim - issue with shuffling sentences in the epochs,<python><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,True
15228,48115965,2018-01-05 14:50:58,,"<p>I want to compute Cosine Similarity between LDA topics. In fact, gensim function .matutils.cossim can do it but I dont know  which parameter (vector ) I can use for this function?</p>

<p>Here is a snap of  code :</p>

<pre><code>import numpy as np
import lda
from sklearn.feature_extraction.text import CountVectorizer

cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')
cvz = cvectorizer.fit_transform(tweet_texts_processed)

n_topics = 8
n_iter = 500
lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
X_topics = lda_model.fit_transform(cvz)

n_top_words = 6
topic_summaries = []

topic_word = lda_model.topic_word_  # get the topic words
vocab = cvectorizer.get_feature_names()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    topic_summaries.append(' '.join(topic_words))
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))

doc_topic = lda_model.doc_topic_
lda_keys = []
for i, tweet in enumerate(tweets):
    lda_keys += [X_topics[i].argmax()]

import gensim
from gensim import corpora, models, similarities
#Cosine Similarity between LDA topics
 **sim = gensim.matutils.cossim(LDA_topic[1], LDA_topic[2])** 
</code></pre>
",2018-05-23 08:51:56,2018-05-23 08:51:56,Cosine Similarity and LDA topics,<python><nlp><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,True
15238,48064378,2018-01-02 16:20:45,,"<p>I have a pyspark dataframe with a corpus of ~300k unique rows each with a ""doc"" that contains a few sentences of text in each.</p>

<p>After processing, I have a 200 dimension vectorized representation of each row/doc. My NLP Process: </p>

<ol>
<li>Remove Punctuation with regex udf </li>
<li>Word Stemming with nltk snowball udf)</li>
<li>Pyspark Tokenizer</li>
<li>Word2Vec (ml.feature.Word2Vec, vectorSize=200, windowSize=5)</li>
</ol>

<p>I understand how this implementation uses the skipgram model to create embeddings for each word based on the full corpus used. My question is: <strong>How does this implementation go from a vector for each word in the corpus to a vector for each document/row?</strong></p>

<p>Is it the same processes as in the gensim doc2vec implementation where it simply concatenates the word vectors in each doc together?: <a href=""https://stackoverflow.com/questions/40413866/how-does-gensim-calculate-doc2vec-paragraph-vectors"">How does gensim calculate doc2vec paragraph vectors</a>. If so, how does it cut the vector down to the specified size of 200 (Does it use just the first 200 words? Average?)? </p>

<p>I was unable to find the information from the sourcecode: <a href=""https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec"" rel=""noreferrer"">https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/feature.html#Word2Vec</a> </p>

<p>Any help or reference material to look at is super appreciated!</p>
",,2019-03-28 02:48:39,How does Pyspark Calculate Doc2Vec from word2vec word embeddings?,<apache-spark><nlp><pyspark><word2vec><doc2vec>,,,CC BY-SA 3.0,True,False,True,False,False
15239,48137617,2018-01-07 13:31:36,,"<p>I have a word2vec model trained on twitter. I imported it into gensim using</p>



<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('./twitter.txt', binary=False)  
</code></pre>

<p>I would like to use a function similar to this one:</p>

<pre class=""lang-py prettyprint-override""><code>word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>

<p>to show the most similar words, but I want to restrict the results to words that start with a hashtag.
Can somebody please give explain how I can accomplish this?</p>
",2018-01-07 16:24:38,2018-01-07 16:24:38,Gensim word2vec most_similar filtering by # prefix,<python><machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15246,48082018,2018-01-03 17:11:12,,"<p>I have a word2vec dictionary which gives a top similar words  to given word.</p>

<p>I want to pass the list of words for which similarity  needs to calculated  from  a file or list</p>

<p><strong>Input</strong> </p>

<pre><code>word_list =['wan,'floor','street']
</code></pre>

<p>Similarity of these words should be checked against the word2vec dictionary and  similar words to the input word_list must found and written to a dataframe in the below shown format.</p>

<pre><code>model.most_similar(""wan"")

[('wan.', 0.7509685754776001),
 ('want', 0.7326164245605469),
 ('aupuiwan', 0.7161564230918884),
 ('puiwan', 0.7119397521018982),
 ('wanstreet', 0.7096157073974609),
 ('woshing', 0.7046518921852112),
 ('futan', 0.6979573369026184),
 ('won', 0.696295440196991),
 ('fota', 0.6961145401000977),
 ('pul', 0.6921802759170532)]
</code></pre>

<p>I want create a dataframe with  two columns Word and Similar words. </p>

<p><strong>Output Dataframe</strong></p>

<pre><code>Word    Similar Words
wan     ('wan.', 'want','aupuiwan','puiwan','wanstreet')
floor   ('fl','flooor','flor','flr','gf')
street  ('st','rosestreet','stret','strt','str')
</code></pre>

<p>Any help is appreciated.</p>
",2018-01-03 17:58:41,2018-01-03 17:58:41,How to create dataframe of top 5 close words to a particular word lists from a dictionary in pandas,<python><string><pandas><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15250,48049619,2018-01-01 12:22:42,,"<p>I am trying to solve a nlp problem where i have a dict of words like :</p>

<pre><code>list_1={'phone':'android','chair':'netflit','charger':'macbook','laptop','sony'}
</code></pre>

<p>Now if input is 'phone' i can easily use 'in' operator to get the description of phone and its data by key but problem is if input is something like  'phones' or 'Phones' .</p>

<p>I want if i input  'phone' then i get words like </p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>

<p>I don't know which word2vec i can use and which nlp module can provide solution like this.</p>

<p>second issue is if i give a word 'Dog' can i get words like 'Puppy','Kitty','Dog','dog' etc ?</p>

<p>I tried something like this but its giving synonyms :</p>

<pre><code>from nltk.corpus import wordnet as wn
for ss in wn.synsets('phone'): # Each synset represents a diff concept.
    print(ss)
</code></pre>

<p>but its returning :</p>

<pre><code>Synset('telephone.n.01')
Synset('phone.n.02')
Synset('earphone.n.01')
Synset('call.v.03')
</code></pre>

<p>Instead i wanted :</p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>
",2018-01-01 14:14:28,2018-01-01 14:14:28,How to get similar words related to one word?,<python><nlp><nltk><gensim><spacy>,,,CC BY-SA 3.0,True,True,True,False,False
15263,48089141,2018-01-04 05:24:03,,"<p>I have a word2vec dictionary which has a list of similar words to given word.</p>

<p><strong>Example</strong></p>

<pre><code>model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p>I want to create  dataframe containing root and similar_words(having similarity above .6)</p>

<p>Currently I am able to write all the  similar words corresponding to root word</p>

<pre><code>words = y
similar = [[item[0] for item in model.most_similar(word)[:6]] for word in words]
similarity_matrix = pd.DataFrame({'Root_Word': words, 'Similar_Words': similar})
</code></pre>

<p><strong>Current Output</strong></p>

<pre><code>Root_Word    Similar_word
[st]         [st., sreet, rd;, yop, tseun, tsven] 
[limited]    [ltd, lt, wt, serial, (h.k., dk] 
[centre]     [cent, ct, cte, entre, ctr., ce]
</code></pre>

<p><strong>Expected output is have only Similar words which having similarity above .6.</strong></p>

<p>How can this be done</p>
",2018-01-04 06:05:32,2018-01-04 06:05:32,How to write words having similarity above .6 to a specific word from a dictionary to a dataframe in pandas,<python><string><pandas><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15276,48090426,2018-01-04 07:15:54,,"<p>While training using <code>doc2vec</code>, I got this error:</p>

<pre><code>AttributeError: 'list' object has no attribute 'words' in python gensim module
</code></pre>

<p>This is my code:</p>

<pre><code># Extracting titles from csv to list
with open(query+'_titles.csv', 'rb') as f:
    reader = csv.reader(f)
    titlelist = list(reader)
# build
model = doc2vec.Doc2Vec(size=30, window=1, alpha=0.01, min_count=2, sample=1e-5, workers=100)
model.build_vocab(titlelist)
titlearray = np.asarray(titlelist)
print 'Training Model...'
</code></pre>

<p>I use python <em>2.7.11</em> and gensim version is <em>3.2.0</em> if that helps. There must be something I am really missing. </p>
",2018-06-09 04:56:25,2018-06-09 04:56:25,AttributeError: 'list' object has no attribute 'words' in python gensim module,<python><machine-learning><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
15282,48164954,2018-01-09 09:25:03,,"<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
",2018-01-09 17:36:21,2018-01-09 17:36:21,Gensim word embedding training with initial values,<machine-learning><nlp><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15318,48205412,2018-01-11 10:56:23,,"<p>The gensim Word2Vec allows us to train a model using text. As far as I know, it creates an interim <strong>nx|V|</strong> (<strong>n</strong> is number of relevant concepts, <strong>V</strong> is Vector size, user specified) matrix that stores the information which is later used to create the wordspace. How to access and work with this particular data structure?</p>
",,2018-01-11 10:56:23,How can I access the interim model created by CBOW of gensim Word2Vec?,<python><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15323,48281026,2018-01-16 12:14:07,,"<p>I want to cluster 3.5M 300-dimensional word2vec vectors from my custom gensim model to determine whether I can use those clustering to find topic-related words. It is not the same as <code>model.most_similar_...</code>, as I hope to attach quite distant, but still related words.</p>

<p>The overall size of the model (after normalization of vectors, i.e. <code>model.init_sims(replace=True)</code>) in memory is 4GB:</p>

<pre><code>words = sorted(model.wv.vocab.keys())
vectors = np.array([model.wv[w] for w in words]) 
sys.getsizeof(vectors)
4456416112
</code></pre>

<p>I tried both scikit's DBSCAN and some other implementations from GitHub, but they seem to consume more and more RAM during processing and crash with <code>std::bad_alloc</code> after some time. I have 32 GB of RAM and 130GB swap.</p>

<p>Metric is euclidean, <a href=""https://stackoverflow.com/questions/32745541/dbscan-error-with-cosine-metric-in-python/40368885#40368885"">I convert my cosine distance threshold cos=0.48 as eps=sqrt(2-2*0.48)</a>, so all the optimizations should be applied. </p>

<p>The problem is that I don't know the number of clusters and want to determine them by setting the threshold for closely related words (let it be <code>cos&lt;0.48</code> or <code>d_l2 &lt; sqrt(2-2*0.48)</code>). DBSCAN seems working on small subsets, but I can't pass the computation on the full data.</p>

<p>Is there any algorithm or workaround in Python which can help with that?</p>

<p>EDIT: Distance matrix seem to be for a size(float)=4bytes: 3.5M*3.5M*4/1024(KB)/1024(MB)/1024(GB)/1024(TB) = 44.5 TB, so it's impossible to precompute it.</p>

<p>EDIT2: Currently trying ELKI, but cannot make it to cluster data on toy subset properly. </p>
",2018-01-16 16:12:23,2019-02-18 11:32:29,Fixed RAM DBSCAN or another clustering algorithm without predefined number of clusters?,<python><optimization><cluster-analysis><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15324,48234595,2018-01-12 22:00:38,,"<p>I have a working app using <code>doc2vec</code> from <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">gensim</a>. I know the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVector</code></a> is now the recommended approach, and trying to port over however I am not sure what is the equivalent method for the <code>infer_vector</code> method in <code>Doc2Vec</code>?</p>

<p>Or better put, how do I obtain a document vector for an entire document using the <code>KeyedVector</code> model to write to my Annoy model?</p>
",2018-01-14 11:27:02,2018-01-14 11:27:02,Gensim Doc2Vec.infer_vector() equivalent in KeyedVector,<machine-learning><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15344,48313840,2018-01-18 04:25:36,,"<p>I want to train doc2vec model by gensim but my corpus is too large. 
Is there any method to train every batch of sentences corpus? For example, iterately load some corpus and train the model on it, and reload another batch of corpus....</p>

<p>I don't know if there is any api or method for doing this. Any hints ?</p>
",,2018-01-18 22:47:38,Training gensim doc2vec occures memory error?,<word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15349,48297322,2018-01-17 09:10:02,,"<p><strong>Some context:</strong>
I am preparing a large input array for use in a machine learning algorithm. I am using word embeddings (using gensim and Word2Vec) to represent a 5-word sentence as a vector input. After calling the Word2Vec model on my sentence of 5 words, I will have a 500-length list of floats.</p>

<p><strong>The problem:</strong>
I iterate through my sentences, and apply the word embedding function to them, forming a new array with the new word embedded representation. 1 word = 100 length vector of floats. I've got 1.2million sentences to encode, and when I hit around the million row mark, my 16gb RAM gets maxed out. Is there a way in which I can avoid this from happening? I have read that generators can help with memory efficiency? My first solution used numpy arrays - I changed to lists to help with memory efficiency, but I still can't complete the task. Would I have to use a cloud service to achieve this? Or can some magic be applied to the code to be able to do this on a laptop with 16Gb ram?</p>

<p><strong>The code:</strong></p>

<pre><code>prev_time = time.time()
l_encodings = []
for k, sentence in enumerate(l_vocab):  # l_vocab is a list of lists where the inner list is the 5-word-sentence

    #print k, sentence
    if k % 100000 == 0:
        print k
        print time.time() - prev_time
        prev_time = time.time()

    # form list of 5-word-embedding to append to l_encodings
    for i, word in enumerate(sentence):

        if i == 0:
            l_array = model[word].tolist()  # model[word] generates a 1d numpy array of length 100
        elif i &lt; 5:
            l_array2 = model[word].tolist()
            l_array += l_array2

    # normalise sentence lengths
    if len(l_array) != 500:
        diff = 500 - len(l_array)
        l_array += [0 for _ in range(diff)]

    print l_array
    print len(l_array)

    l_encodings.append(l_array)
</code></pre>

<p>Any help appreciated - </p>
",,2018-01-17 09:10:02,Large Python List Maxing out Memory (Word2Vec),<python><memory><memory-management><machine-learning><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15350,48298619,2018-01-17 10:17:33,,"<p>I use <a href=""https://pypi.python.org/pypi/lda"" rel=""nofollow noreferrer"">LDA</a> package to model topics with a large set of text documents. A simplified(!) example (I removed all other cleaning steps, lemmatization, biograms etc.) of my code is below and I'm happy with the results so far. But now I struggle to write a code to predict a new text. I can't find any reference in LDA's documentation about save/loading/predict options. I can add a new text to my set and fit it again but it is an expensive way of doing it.</p>

<p>I know I can do it with gensim. But somehow the results from the gensim model are less impressive so I'd stick to my initial LDA model.</p>

<p>Will appreciate any suggestions!</p>

<p>My code:</p>

<pre><code>import lda
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import nltk
from nltk.corpus import stopwords
stops = set(stopwords.words('english'))  # nltk stopwords list

documents = [""Liz Dawn: Coronation Street's Vera Duckworth dies at 77"",
            'Game of Thrones stars Kit Harington and Rose Leslie to wed',
            'Tony Booth: Till Death Us Do Part actor dies at 85',
            'The Child in Time: Mixed reaction to Benedict Cumberbatch drama',
            ""Alanna Baker: The Cirque du Soleil star who 'ran off with the circus'"",
            'How long can The Apprentice keep going?',
            'Strictly Come Dancing beats X Factor for Saturday viewers',
            ""Joe Sugg: 8 things to know about one of YouTube's biggest stars"",
            'Sir Terry Wogan named greatest BBC radio presenter',
            ""DJs celebrate 50 years of Radio 1 and 2'""]

clean_docs = []
for doc in documents:
    # set all to lower case and tokenize
    tokens = nltk.tokenize.word_tokenize(doc.lower())
    # remove stop words
    texts = [i for i in tokens if i not in stops]
    clean_docs.append(texts)

# join back all tokens to create a list of docs
docs_vect = [' '.join(txt) for txt in clean_docs]

cvectorizer = CountVectorizer(max_features=10000, stop_words=stops)
cvz = cvectorizer.fit_transform(docs_vect)

n_topics = 3
n_iter = 2000
lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
X_topics = lda_model.fit_transform(cvz)

n_top_words = 3
topic_summaries = []

topic_word = lda_model.topic_word_  # get the topic words
vocab = cvectorizer.get_feature_names()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    topic_summaries.append(' '.join(topic_words))
    print('Topic {}: {}'.format(i+1, ' '.join(topic_words)))

# How to predict a new document?
new_text = '50 facts about Radio 1 &amp; 2 as they turn 50'
</code></pre>
",2018-01-17 10:28:06,2018-03-16 06:05:23,Predict new text using Python Latent Dirichlet Allocation (LDA) model,<python><predict><lda><topic-modeling>,,,CC BY-SA 3.0,True,False,True,False,True
15351,48225845,2018-01-12 11:56:34,,"<p>I have the following problem:</p>

<p>In English language my code generates successful word embeddings with Gensim, and similar phrases are close to each other considering cosine distance:</p>

<p>The angle between ""Response time and error measurement"" and ""Relation of user perceived response time to error measurement"" is very small, thus they are the most similar phrases in the set.</p>

<p><a href=""https://i.stack.imgur.com/Seuzy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Seuzy.png"" alt=""enter image description here""></a></p>

<p>However, when I use the same phrases in Portuguese, it doesn't work:</p>

<p><a href=""https://i.stack.imgur.com/0l2Sz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0l2Sz.png"" alt=""enter image description here""></a></p>

<p>My code as follows:</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import matplotlib.pyplot as plt
from gensim import corpora
documents = [""Interface mquina humana para aplicaes computacionais de laboratrio abc"",
          ""Um levantamento da opinio do usurio sobre o tempo de resposta do sistema informtico"",
           ""O sistema de gerenciamento de interface do usurio EPS"",
           ""Sistema e testes de engenharia de sistemas humanos de EPS"",
           ""Relao do tempo de resposta percebido pelo usurio para a medio de erro"",
           ""A gerao de rvores no ordenadas binrias aleatrias"",
           ""O grfico de interseo dos caminhos nas rvores"",
           ""Grfico de menores IV Largura de rvores e bem quase encomendado"",
           ""Grficos menores Uma pesquisa""]

stoplist = set('for a of the and to in on'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]
texts

from collections import defaultdict
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1
frequency

from nltk import tokenize  
texts=[tokenize.word_tokenize(documents[i], language='portuguese') for i in range(0,len(documents))]

from pprint import pprint
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save('/tmp/deerwester.dict')
print(dictionary)

print(dictionary.token2id)


# VECTOR
new_doc = ""Tempo de resposta e medio de erro""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)

## VETOR OF PHRASES
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  
print(corpus)

from gensim import corpora, models, similarities
tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

### PHRASE COORDINATES
frase=tfidf[new_vec]
print(frase)

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus_tfidf]

lsi.print_topics(2)

## TEXT COORDINATES
todas=[]
for doc in corpus_lsi:
    todas.append(doc)
todas

from gensim import corpora, models, similarities
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(corpus)

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = new_doc
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
print(vec_lsi)

p=[]
for i in range(0,len(documents)):
    doc1 = documents[i]
    vec_bow2 = dictionary.doc2bow(doc1.lower().split())
    vec_lsi2 = lsi[vec_bow2]
    p.append(vec_lsi2)

p

index = similarities.MatrixSimilarity(lsi[corpus])

index.save('/tmp/deerwester.index')
index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

sims = index[vec_lsi]
print(list(enumerate(sims)))

sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) 

#################

import gensim
import numpy as np
import matplotlib.colors as colors
import matplotlib.cm as cmx
import matplotlib as mpl

matrix1 = gensim.matutils.corpus2dense(p, num_terms=2)
matrix3=matrix1.T
matrix3[0]
ss=[]
for i in range(0,9):
    ss.append(np.insert(matrix3[i],0,[0,0]))
matrix4=ss
matrix4

matrix2 = gensim.matutils.corpus2dense([vec_lsi], num_terms=2)
matrix2=np.insert(matrix2,0,[0,0])
matrix2

DATA=np.insert(matrix4,0,matrix2)
DATA=DATA.reshape(10,4)
DATA

names=np.array(documents)
names=np.insert(names,0,new_doc)
new_doc
cmap = plt.cm.jet

cNorm  = colors.Normalize(vmin=np.min(DATA[:,3])+.2, vmax=np.max(DATA[:,3]))

scalarMap = cmx.ScalarMappable(norm=cNorm,cmap=cmap)
len(DATA[:,1])

plt.subplots()
plt.figure(figsize=(12,9))
plt.scatter(matrix1[0],matrix1[1],s=60)
plt.scatter(matrix2[2],matrix2[3],color='r',s=95)
for idx in range(0,len(DATA[:,1])):
    colorVal = scalarMap.to_rgba(DATA[idx,3])
    plt.arrow(DATA[idx,0],
          DATA[idx,1], 
          DATA[idx,2], 
          DATA[idx,3], 
          color=colorVal,head_width=0.002, head_length=0.001)
for i,names in enumerate (names):
    plt.annotate(names, (DATA[i][2],DATA[i][3]),va='top')
plt.title(""PHRASE SIMILARITY - WORD2VEC with GENSIM library"")
plt.xlim(min(DATA[:,2]-.2),max(DATA[:,2]+1))
plt.ylim(min(DATA[:,3]-.2),max(DATA[:,3]+.3))
plt.show()
</code></pre>

<p>My question is: is there any additional set up for Gensim to generate proper word embeddings in Portuguese language or Gensim does not support this language?</p>
",2018-01-12 12:11:19,2019-11-22 19:38:39,How to generate word embeddings in Portuguese using Gensim?,<python><nlp><nltk><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
15354,48263122,2018-01-15 12:32:11,,"<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
",2018-01-15 16:01:58,2018-01-15 16:01:58,How can I get a vector after each training iter in word2vec?,<python-3.x><nlp><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15357,48267720,2018-01-15 17:10:08,,"<p>The use-case I have is to have a collection of ""upvoted"" documents and ""downvoted"" documents and using those to re-order a set of results in a search.</p>

<p>I am using gensim <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec</a> and am able to run the <code>most_similar</code> queries for word(s) and fetch matching words. But how would I be able to fetch the matching keywords given a vector fetched by a vector sum of the above doc vectors? </p>
",,2018-01-15 17:58:30,doc2vec: any way to fetch closest matching terms for a given vector?,<word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15372,48253833,2018-01-14 20:25:59,,"<p>I need to print only the topic word (only one word). But it contains some number, But I can not get only the topic name like ""Happy"". My String word is ""Happy"", why it shows ""Happi""</p>

<pre><code>    import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models

import gensim
import string
tokenizer = RegexpTokenizer(r'\w+')
en_stop = get_stop_words('en')
p_stemmer = PorterStemmer()
fr = open('Happy DespicableMe.txt','r')
doc_a = fr.read()
fr.close()
doc_set = [doc_a]
texts = []
for i in doc_set:


    raw = i.lower()
    tokens = tokenizer.tokenize(raw)


    stopped_tokens = [i for i in tokens if not i in en_stop]


    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]


    texts.append(stemmed_tokens)


dictionary = corpora.Dictionary(texts)


corpus = [dictionary.doc2bow(text) for text in texts]


ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)



rafa = ldamodel.show_topics(num_topics=1, num_words=1, log=False , formatted=False)


print(rafa)
</code></pre>

<p>It only shows [(0, '0.142*""happi""')]. But I want to print only the word.</p>

<p><a href=""https://i.stack.imgur.com/SOHXi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SOHXi.jpg"" alt=""Strings on the File""></a></p>
",2018-01-15 06:03:10,2018-02-05 00:08:46,Print only topic name using LDA with python,<python-3.x><nltk><gensim><lda><stemming>,,,CC BY-SA 3.0,True,False,True,False,False
15373,48358161,2018-01-20 16:03:38,,"<p>Is there a more direct or efficient method for getting the topic probabilities data from a gensim.interfaces.TransformedCorpus object into a numpy array (or alternatively, pandas dataframe) than the by-row method below?</p>

<pre><code>from gensim import models
import numpy as np

num_topics = 5
model = models.LdaMulticore(corpus, num_topics=num_topics, minimum_probability=0.0)

all_topics = model.get_document_topics(corpus)
num_docs = len(all_topics)

lda_scores = np.empty([num_docs, num_topics])

for i in range(0, num_docs):
    lda_scores[i] = np.array(all_topics[i]).transpose()[1]
</code></pre>
",,2018-06-21 21:39:43,Efficient transformation of gensim TransformedCorpus data to array,<python><numpy><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
15389,48290403,2018-01-16 21:51:28,,"<p>I am working on node2vec. When I am using small dataset the code works well. But as soon as I try to run the same code on large dataset, the code crashes. </p>

<p>Error:  Process finished with exit code 134 (interrupted by signal 6: SIGABRT).</p>

<p>The line which is giving error is </p>

<pre><code>model = Word2Vec(walks, size=args.dimensions, window=args.window_size, min_count=0, sg=1, workers=args.workers,
                 iter=args.iter)
</code></pre>

<p>I am using pycharm and python 3.5.</p>

<p>Any idea what is happening? I could not found any post which could solve my problem.</p>
",,2019-12-31 01:25:35,Process finished with exit code 134 (interrupted by signal 6: SIGABRT),<python><pycharm><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15400,48402364,2018-01-23 12:57:48,,"<p>I am trying to extract the documents vector to feed into a regression model for prediction.</p>

<p>I have fed around 1 400 000 of labelled sentences into doc2vec for training, however I was only able to retrieve only 10 vectors using model.docvecs.</p>

<p>This is a snapshot of the labelled sentences I used to trained the doc2vec model:</p>

<pre><code>In : documents[0]

Out: TaggedDocument(words=['descript', 'yet'], tags='0')

In : documents[-1]

Out: TaggedDocument(words=['new', 'tag', 'red', 'sparkl', 'firm', 'price', 'free', 'ship'], tags='1482534')
</code></pre>

<p>These are the code used to train the doc2vec model</p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=5, size=100, sample=1e-4, negative=5, workers=4)
model.build_vocab(documents)
model.train(documents, total_examples =len(documents), epochs=1)
</code></pre>

<p>This is the dimension of the documents vectors:</p>

<pre><code>In : model.docvecs.doctag_syn0.shape
Out: (10, 100)
</code></pre>

<p>On which part of the code did I mess up?</p>

<p><strong>Update:</strong></p>

<p>Adding on to the comment from <a href=""https://stackoverflow.com/users/6573902/sophros"">sophros</a>, it appear that i have made a mistake when I am creating the TaggedDocument prior to training which resulted in 1.4 mil Documents appearing as 10 Documents.</p>

<p>Courtesy of <a href=""https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/"" rel=""nofollow noreferrer"">Irene Li</a> on your tutorial on Doc2vec, I have made some slightly edit to the class she used to generate TaggedDocument</p>

<pre><code>def get_doc(data):

tokenizer = RegexpTokenizer(r'\w+')
en_stop = stopwords.words('english')
p_stemmer = PorterStemmer()

taggeddoc = []

texts = []
for index,i in enumerate(data):
    # for tagged doc
    wordslist = []
    tagslist = []
    i = str(i)
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    # remove numbers
    number_tokens = [re.sub(r'[\d]', ' ', i) for i in stopped_tokens]
    number_tokens = ' '.join(number_tokens).split()
    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in number_tokens]
    # remove empty
    length_tokens = [i for i in stemmed_tokens if len(i) &gt; 1]
    # add tokens to list
    texts.append(length_tokens)

    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),str(index))

    taggeddoc.append(td)

return taggeddoc
</code></pre>

<p>The mistake was fixed when I made the change from </p>

<pre><code>td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),str(index))
</code></pre>

<p>to this</p>

<pre><code>td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),[str(index)])
</code></pre>

<p>It appear that the index of the TaggedDocument must be in the form of the list for TaggedDocument to work properly. For more details as to why, please refer to this answer by <a href=""https://stackoverflow.com/questions/47929028/doc2vec-model-docvecs-is-only-of-length-10"">gojomo</a>.</p>
",2018-01-24 06:38:06,2018-01-25 00:31:16,Extracting vectors from Doc2Vec,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15401,48362530,2018-01-21 00:31:48,,"<p>I am trying to follow the official Doc2Vec Gensim tutorial mentioned here - <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>

<p>I modified the code in line 10 to determine best matching document for the given query and everytime I run, I get a completely different resultset. My new code iin line 10 of the notebook is:</p>

<p><code>
inferred_vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
rank = [docid for docid, sim in sims]
print(rank)
</code></p>

<p>Everytime I run the piece of code, I get different set of documents that are matching with this query: ""only you can prevent forest fires"". The difference is stark and just does not seem to match.</p>

<p>Is Doc2Vec not a suitable match for querying and information extraction? Or are there bugs?</p>
",,2019-11-21 04:10:08,Doc2Vec.infer_vector keeps giving different result everytime on a particular trained model,<nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15407,48307812,2018-01-17 18:34:23,,"<p>I'm trying to use google's word2vec model (<a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a> - <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a>) based on google news in Python 3.6.
I load the model using gensim package and specially <code>KeyedVectors</code> class (<a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a>). After, I use <code>similarity()</code> method, but I have some problems with named entities.</p>

<p>For example, I would compare and measure ""Roger Federer"" with ""Michael Jordan"" (and not ""federer"" with ""jordan"", because I wouldn't want to tokenize the single named entity), and with many named entities I got an error <code>word 'named_entities' is not in vocabulary</code>.</p>

<p>Is there any other pre-trained word2vec model which can be used to measure distance between named entities.</p>
",,2018-01-17 18:34:23,Named Entity Recognition Cosine Similarity,<python><nlp><word2vec><named-entity-recognition>,,,CC BY-SA 3.0,False,False,True,False,False
15413,48382663,2018-01-22 13:26:24,,"<p>I'm working in Python and I  have a column in data frame that is a string and looks like that : </p>

<pre><code>df['set'] 

0  [911,3040]
1  [130055, 99832, 62131]
2  [19397, 3987, 5330, 14781]
3  [76514, 70178, 70301, 76545]
4  [79185, 38367, 131155, 79433]
</code></pre>

<p>I would like it to be: </p>

<pre><code>['911','3040'],['130055','99832','62131'],['19397','3987','5330','14781'],['76514',70178','70301','76545'],['79185','38367','131155','79433']
</code></pre>

<p>in order to be able to run Word2Vec:</p>

<pre><code>model = gensim.models.Word2Vec(df['set'] , size=100)
</code></pre>

<p>Thanks ! </p>
",2018-01-22 13:29:46,2018-01-22 14:09:11,string vector to list python,<python><pandas><dataframe><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15420,48455703,2018-01-26 04:12:40,,"<p>I am using Gensim to train Word2Vec. I know word similarities are deteremined by if the words can replace each other and make sense in a sentence. But can word similarities be used to extract relationships between entities?</p>

<p>Example:
I have a bunch of interview documents and in each interview, the interviewee always says the name of their manager. If I wanted to extract the name of the manager from these interview transcripts could I just get a list of all human name's in the document (using nlp), and the name that is the most similar to the word ""manager"" using Word2Vec, is most likely the manager.</p>

<p>Does this thought process make any sense with Word2Vec? If it doesn't, would the ML solution to this problem then be to input my word embeddings into a sequence to sequence model?</p>
",,2018-01-26 18:30:59,Can Word2Vec be used for information extraction?,<machine-learning><word2vec><gensim><rnn><information-extraction>,,,CC BY-SA 3.0,False,False,True,False,False
15421,48371824,2018-01-21 21:19:23,,"<p>I have the following code (only relevant parts shown):</p>

<pre><code>def load_model(model_file):
    return Doc2Vec.load(model_file)

# infer 
def infer_docs(input_string, model_file, inferred_docs=5):
    model = load_model(model_file)
    processed_str = simple_preprocess(input_string, min_len=2, max_len=35)    
    inferred_vector = model.infer_vector(processed_str)
    return model.docvecs.most_similar([inferred_vector], topn=inferred_docs)
</code></pre>

<p>The code runs as a lambda on aws. It works fine when my model is small (I think that is the reason) but when I have a decent size model (~200mb) I get the following error</p>

<pre><code>[INFO]  2018-01-21T20:44:59.613Z    f2689816-feeb-11e7-b397-b7ff2947dcec    testing keys in event dict
[INFO]  2018-01-21T20:44:59.614Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading model from s3://data-d2v/trained_models/model_law
[INFO]  2018-01-21T20:44:59.614Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading Doc2Vec object from s3://data-d2v/trained_models/model_law
[INFO]  2018-01-21T20:44:59.650Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Found credentials in environment variables.
[INFO]  2018-01-21T20:44:59.707Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Starting new HTTPS connection (1): s3.eu-west-1.amazonaws.com
[INFO]  2018-01-21T20:44:59.801Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Starting new HTTPS connection (2): s3.eu-west-1.amazonaws.com
[INFO]  2018-01-21T20:45:35.830Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading wv recursively from s3://data-d2v/trained_models/model_law.wv.* with mmap=None
[INFO]  2018-01-21T20:45:35.830Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading syn0 from s3://data-d2v/trained_models/model_law.wv.syn0.npy with mmap=None
[Errno 2] No such file or directory: 's3://data-d2v/trained_models/model_law.wv.syn0.npy': FileNotFoundError
Traceback (most recent call last):
  File ""/var/task/handler.py"", line 20, in infer_handler
    event['input_text'], event['model_file'], inferred_docs=10)
  File ""/var/task/infer_doc.py"", line 26, in infer_docs
    model = load_model(model_file)
  File ""/var/task/infer_doc.py"", line 21, in load_model
    return Doc2Vec.load(model_file)
  File ""/var/task/gensim/models/word2vec.py"", line 1569, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/var/task/gensim/utils.py"", line 282, in load
    obj._load_specials(fname, mmap, compress, subname)
  File ""/var/task/gensim/models/word2vec.py"", line 1593, in _load_specials
    super(Word2Vec, self)._load_specials(*args, **kwargs)
  File ""/var/task/gensim/utils.py"", line 301, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File ""/var/task/gensim/utils.py"", line 312, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File ""/var/task/numpy/lib/npyio.py"", line 372, in load
    fid = open(file, ""rb"")
FileNotFoundError: [Errno 2] No such file or directory: 's3://data-d2v/trained_models/model_law.wv.syn0.npy'
</code></pre>

<p>First of all the file <code>s3://data-d2v/trained_models/model_law.wv.syn0.npy</code> exists and secondly to me it seems that the main model file <code>s3://data-d2v/trained_models/model_law</code> is loaded. </p>

<p>To validate access and existence of the file I added:</p>

<pre><code>import smart_open
with smart_open.smart_open('s3://data-d2v/trained_models/model_law.wv.syn0.npy') as prut:
    for line in prut:
        print(line)
</code></pre>

<p>which works just fine printing.</p>

<p>Can you help?  </p>
",2018-01-22 12:02:42,2018-01-23 12:38:54,gensim loading from s3 does not work,<amazon-web-services><amazon-s3><aws-lambda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15426,48507404,2018-01-29 18:05:36,,"<p>I just came across <a href=""https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec"">this StackOverflow post</a> on word counts in a doc2vec model vocabulary. I wonder if there is another method to retrieve the word frequency, other than </p>

<pre><code>for word, vocab_obj in model.wv.vocab.items():
    print(str(word) + str(vocab_obj.count))
</code></pre>

<p>Maybe there is a more elegant way via the gensim library (i.e. output words and frequencies in a txt file)?</p>
",,2018-01-30 21:37:18,gensim: Retrieving word frequency in doc2vec vocabulary,<dictionary><word2vec><gensim><doc2vec><vocabulary>,,,CC BY-SA 3.0,False,False,True,False,False
15428,48403942,2018-01-23 14:18:59,,"<p>I am trying to create a gensim corpus and save it to arbitrary HDFS or regular FS path. I am using pyspark (2.2.1) and running a zeppelin notebook on a hadoop cluster. Here is my minimal example:</p>

<pre><code>from gensim import corpora
import os

path = ""/my/existing/hadoop/path""
corpus = [[(0,0), (1,2)]]
corpora.MmCorpus.serialize(os.path.join(path,""corpus.mm""), corpus)
</code></pre>

<p>This leads to error:</p>

<pre><code>[Errno 2] No such file or directory: '/my/existing/hadoop/path/corpus.mm' 
</code></pre>

<p>Although the path exists.</p>

<p>Running the following works.  </p>

<pre><code>corpora.MmCorpus.serialize(""corpus.mm"", corpus)
corpora.MmCorpus.serialize(os.path.join(""/tmp"",""corpus.mm""), corpus)
</code></pre>

<p>However, I can't find it. I checked <code>/tmp</code> and <code>hadoop fs -ls /tmp</code>
What kind of path is required when working with pyspark? </p>
",2018-01-24 09:05:34,2018-01-24 13:55:53,How to serialize gensim corpus in pyspark using apache-zeppelin notebook?,<hadoop><serialization><pyspark><gensim><apache-zeppelin>,,,CC BY-SA 3.0,False,False,True,False,False
15457,48411780,2018-01-23 22:19:37,,"<p>A little background about this project. I have copies with an identifier and the text, e.g. <code>{name: ""sports-football"", text: ""Content related to football sports""}</code>.</p>

<p>I need to find the right match for the given text input within this corpus.
However, I was able to achieve somewhat using Gensim. Similarity with LDA and LSI Model.</p>

<p>How to update the <code>Genism.Similarity</code> Index with new a document. The idea here is to keep training the model at live stage.</p>

<p>Here is the step I followed.</p>

<p>QueryText = ""Guardiola moved Lionel Messi to the No 9 role so that he didn't have to come deep and I think Aguero drops back into deeper positions too often.""</p>

<p>Note: some codes are just layman </p>

<p>The index is created using </p>

<pre><code>`similarities.Similarity(indexpath, model,topics)`
</code></pre>

<ol>
<li><p>Create A dictionary </p>

<p><code>dictionary = Dictionary(QueryText )</code></p></li>
<li><p>Create a corpus   </p>

<p><code>corpus = Corpus(QueryText, dictionary)</code></p></li>
<li><p>Create an LDA Model</p>

<p><code>LDAModel =  ldaModel(corpus,dictionary)</code></p></li>
</ol>

<p>Update existing dictionary, model, and index</p>

<p>Update existing dictionary</p>

<pre><code>existing_dictionary.add_document(dictionary)
</code></pre>

<p>Update existing LDA Model</p>

<pre><code>existing_lda_model.update(corpus)
</code></pre>

<p>Update existing Similarity index</p>

<pre><code>existing_index.add_dcoument(LDAModel[corpus])
</code></pre>

<p>Other than below warning update seems to be worked.</p>

<pre><code>gensim\models\ldamodel.py:535: RuntimeWarning: overflow encountered in exp2 perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words
</code></pre>

<p>Let's run the similarity for the query text</p>

<pre><code>vec_bow = dictionary.doc2bow(QueryText) 
vec_model = existing_lda_model[vec_bow] 
sims = existing_index[vec_model]
</code></pre>

<p>However, it failed with below error.</p>

<pre><code>Similarity index with 723 documents in 1 shards (stored under \Files\models\lda_model)
Similarity index with 725 documents in 0 shards (stored under \Files\models\lda_model)
\lib\site-packages\gensim\models\ldamodel.py:535: RuntimeWarning: overflow encountered in exp2
  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-3-8fe711724367&gt; in &lt;module&gt;()
     45 trigram = Trigram.apply_trigram_model(queryText, bigram, trigram)
     46 vec_bow = dictionry.doc2bow(trigram)
---&gt; 47 vec_model =  lda_model[vec_bow]
     48 print(vec_model)
     49 

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1103             `(topic_id, topic_probability)` 2-tuples.
   1104         """"""
-&gt; 1105         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1106 
   1107     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    944             return self._apply(corpus, **kwargs)
    945 
--&gt; 946         gamma, phis = self.inference([bow], collect_sstats=per_word_topics)
    947         topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution
    948 

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in inference(self, chunk, collect_sstats)
    442             Elogthetad = Elogtheta[d, :]
    443             expElogthetad = expElogtheta[d, :]
--&gt; 444             expElogbetad = self.expElogbeta[:, ids]
    445 
    446             # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.

IndexError: index 718 is out of bounds for axis 1 with size 713
</code></pre>

<p>I really appreciate, helping me with this.
Looking forward to awesome replies. </p>
",2018-02-02 12:04:15,2018-02-02 12:04:15,Gensim.Similarity Add document or Live training,<python><nlp><similarity><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15488,48479915,2018-01-27 19:50:24,,"<p>When using for example <strong>gensim</strong>, <strong>word2vec</strong> or a similar method for training your embedding vectors I was wonder what is a good ratio or is there a preferred ratio between the embedding dimension to vocabulary size ?
Also how does that change with more data coming along ? </p>

<p>As I am still on the topic how would one chose a good window size when training your embedding vectors ?</p>

<p>I am asking this because I am not training my network with a real-life language dictionary, but rather the sentences would describe relationships between processes and files and other processes and so on.
For example a sentence in my text corpus would look like:</p>

<blockquote>
  <p>smss.exe irp_mj_create systemdrive windows system32 ntdll dll DesiredAccess: Execute/Traverse, Synchronize, Disposition: Open, Options: ,
  Attributes: n/a, ShareMode: Read, AllocationSize: n/a, OpenResult:
  Opened""</p>
</blockquote>

<p>As you may imagine the variations are numberous but the question still remains how can I fine tune these hyperparameters the best way so that the embedding space will not over-fit but also have enough meaningful features for each word. </p>

<p>Thanks,</p>

<p>Gabriel</p>
",2018-01-28 12:15:42,2019-03-29 07:33:54,What is the preferred ratio between the vocabulary size and embedding dimension?,<machine-learning><keras><nltk><word-embedding><nltk-trainer>,,,CC BY-SA 3.0,True,False,True,False,False
15493,48438304,2018-01-25 08:12:47,,"<p>I run the sourcecode of TWE model. I need to compile the C extension of python. I have installed the Microsoft Visual C++ Compiler for Python 2.7 and Cython. </p>

<p>First, I need to run the TWE/train.py:</p>

<pre><code>import gensim
sentence_word = gensim.models.word2vec.LineSentence(""tmp/word.file"")
print ""Training the word vector...""
w = gensim.models.Word2Vec(sentence_word,size=400, workers=20)
sentence = gensim.models.word2vec.CombinedSentence(""tmp/word.file"",""tmp/topic.file"")
print ""Training the topic vector...""
w.train_topic(topic_number, sentence)
print ""Saving the topic vectors...""
w.save_topic(""output/topic_vector.txt"")
print ""Saving the word vectors...""
w.save_wordvector(""output/word_vector.txt"")`
</code></pre>

<p>Second, TWE/gensim/models/wor2vec.py:</p>

<pre><code>try:
    raise ImportError  # ignore for now
    from gensim_addons.models.word2vec_inner import train_sentence_sg,train_sentence_cbow, FAST_VERSION, train_sentence_topic
except ImportError:
    try:
        import pyximport
        print 'import pyximport'
        models_dir = os.path.dirname(__file__) or os.getcwd()
        print 'models_dir'
        pyximport.install(setup_args={""include_dirs"": [models_dir, get_include()]})
        print 'pyximport'   # is the follow code's problem
        from word2vec_inner import train_sentence_sg, train_sentence_cbow, 
        FAST_VERSION, train_sentence_topic
        print 'from word2vec'
    except:
        FAST_VERSION = -1
        def train_sentence_sg(model, sentence, alpha, work=None):
                   ...
        def train_sentence_cbow(model, sentence, alpha, work=None, neu1=None):
                   ...
class Word2Vec(utils.SaveLoad):
                   ...
    def train(self, sentences, total_words=None, word_count=0, chunksize=100):
        if FAST_VERSION &lt; 0:
        import warnings
        warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
        logger.info(""training model with %i workers on %i vocabulary and %i features, ""
        ""using 'skipgram'=%s 'hierarchical softmax'=%s 'subsample'=%s and 'negative sampling'=%s"" %
        (self.workers, len(self.vocab), self.layer1_size, self.sg, self.hs, self.sample, self.negative))
         def worker_train():
              ...
             if self.sg:
                 job_words = sum(train_sentence_topic(self, sentence, alpha, work) for sentence in job)
             else:
                 ob_words = sum(train_sentence_cbow(self, sentence, alpha, work, neu1) for sentence in job)`
              ...
</code></pre>

<p>Thrid, I haved compiled the TWE/gensim/models/word2vec_inner.pyx with a setup.py:</p>

<pre><code>from distutils.core import setup  
from distutils.extension import Extension  
from Cython.Build import cythonize  
import numpy  
extensions = [  
    Extension(""word2vec_inner"", [""word2vec_inner.pyx""],  
              include_dirs=[numpy.get_include()])  
]  
setup(  
    name=""word2vec_inner"",  
    ext_modules=cythonize(extensions),  
)
</code></pre>

<p>by using the command 'python setup.py install', I have complied the word2vec_inner.pyx. But it appears the follow errors:</p>

<pre><code>E:\Python27\python.exe D:/pycharm/TWE/TWE1/train.py wordmap.txt model-final.tassign 100
import pyximport
models_dir
pyximport
word2vec_inner.c
e:\python27\lib\site-packages\numpy\core\include\numpy\npy_1_7_deprecated_api.h(12) : Warning Msg : Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
C:\Users\hp\.pyxbld\temp.win32-2.7\Release\pyrex\gensim\models\word2vec_inner.c(15079) : warning C4244:
'initializing' : conversion from 'double' to 'float', possible loss of data
C:\Users\hp\.pyxbld\temp.win32-2.7\Release\pyrex\gensim\models\word2vec_inner.c(15085) : warning C4244 : 'initializing' : conversion from 'double' to 'float', possible loss of data
LINK : fatal error LNK1104: cannot open file 'C:\Users\hp\.pyxbld\lib.win32-2.7\gensim\models\word2vec_inner.pyd'
Training the word vector...
D:\pycharm\TWE\TWE1\gensim\models\word2vec.py:410: UserWarning: Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`
warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
PROGRESS: at 100.00% words, alpha 0.02500, 2556 words/s
Training the topic vector...
D:\pycharm\TWE\TWE1\gensim\models\word2vec.py:882: UserWarning: Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`
warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
Exception in thread Thread-23:
Traceback (most recent call last):
  File ""E:\Python27\lib\threading.py"", line 801, in __bootstrap_inner
      self.run()
  File ""E:\Python27\lib\threading.py"", line 754, in run
      self.__target(*self.__args, **self.__kwargs)
  File ""D:\pycharm\TWE\TWE1\gensim\models\word2vec.py"", line 909, in worker_train
     job_words = sum(train_sentence_topic(self, sentence, alpha, work) for sentence in job)
  File ""D:\pycharm\TWE\TWE1\gensim\models\word2vec.py"", line 909, in &lt;genexpr&gt;
     job_words = sum(train_sentence_topic(self, sentence, alpha, work) for sentence in job)
NameError: global name 'train_sentence_topic' is not defined

Saving the topic vectors...
Saving the word vectors...

Process finished with exit code 0
</code></pre>

<p>I have checked that the .pyx file was compiled correctly and also installed the cython. in conclusion, it can't import train_sentence_sg,train_sentence_cbow, FAST_VERSION, train_sentence_topic from gensim/models/word2vec_inner or gensim_addons/models/word2vec_inner. So it appears these problems. But why? I have compiled the .pyx file correctly in both two directionarys. Anyone can help me? This problem haved troubled me several days. Please help me, thank you!</p>
",,2018-05-19 08:52:26,LINK : fatal error LNK1104: cannot open file 'C:\Users\hp\.pyxbld\lib.win32-2.7\gensim\models\word2vec_inner.pyd',<c><python-2.7><cython><word2vec><cythonize>,,,CC BY-SA 3.0,False,False,True,False,False
15509,48590383,2018-02-02 20:25:44,,"<p>I would like to apply ""graph2vec"" code to a my own dataset. However I can not figure out how to properly format the input data nor understand the input data format of examples available on ""github"" page of the authors. A network in my dataset has integer nodes and binary label, so it is a dataframe with three columns. I appreciate if anyone can point me to the right direction.</p>

<p>""graph2vec"" on github:   <a href=""https://github.com/MLDroid/graph2vec_tf"" rel=""nofollow noreferrer"">https://github.com/MLDroid/graph2vec_tf</a></p>

<p>""graph2vec"" on arxiv:   <a href=""https://arxiv.org/pdf/1707.05005.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1707.05005.pdf</a> </p>
",2018-02-02 20:39:09,2018-05-16 10:18:05,"""graph2vec"" input data format",<python-3.x><tensorflow><graph><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15523,48443999,2018-01-25 13:28:06,,"<p>I am trying to continue training on an existing model,</p>

<pre><code>model = gensim.models.Word2Vec.load('model/corpus.zhwiki.word.model')
more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]    
model.build_vocab(more_sentences, update=True)
model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>but I got an error with the last line:</p>

<p>AttributeError: 'Word2Vec' object has no attribute 'compute_loss'</p>

<p>Some posts said it's caused by using a earlier version of gensim, and I have tried to add this after loading the existing model and before train(). </p>

<pre><code>model.compute_loss = False
</code></pre>

<p>After that, it didn't give me the AttributeError, but the output of model.train() is 0, and model didn't trained with new sentences.</p>

<p><a href=""https://i.stack.imgur.com/ECtw0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ECtw0.png"" alt=""enter image description here""></a></p>

<p>How to solve this problem?</p>
",2019-10-24 13:23:11,2019-10-24 13:23:11,gensim - Word2vec continue training on existing model - AttributeError: 'Word2Vec' object has no attribute 'compute_loss',<python><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
15524,48444393,2018-01-25 13:45:10,,"<p>I recently came across following problem: When applying a topic model on a bunch of parsed PDF files, I discovered that content of the references unfortunately also counts for the model. I.e. words within the references appear in the tokenized list of words.</p>

<p><strong>Is there any known ""best-practice"" to solve this problem?</strong></p>

<p>I thought about a search strategy where the python code automatically removes all content after the last mention of ""references"" or ""bibliography"". If I would go by the first, or a random mention of ""references"" or ""bibliography"" within the full text, the parser might not capture the true full content.</p>

<p>The input PDF are all from different journals and thus have a different page structure.</p>
",,2018-01-30 04:42:47,NLP Challenge: Automatically removing bibliography/references?,<nlp><gensim><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
15534,48504933,2018-01-29 15:42:02,,"<p>I trained my model and got the accuracy of 79% for sentiment classification. For embedding layer, I used word2vec. 
Now that I have my model trained and saved, how do I use it in such a way that I can put some input as a sentence and it shows me how much positive/negative by probability the sentence is?</p>

<p>Given below is some relevant code.</p>

<pre><code>word_model = gensim.models.Word2Vec(train_x+test_x, size=100, min_count=1, window=5, iter=100)

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, 
                weights=[pretrained_weights]))

model.add(Dropout(0.2))
model.add(Conv1D(filters,
             kernel_size,
             padding='valid',
             activation='relu',
             strides=1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims))
model.add(Dropout(0.2))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.summary()
model.fit(train_x, train_y,
      batch_size=batch_size,
      epochs=epochs,
      validation_data=(test_x, test_y))
</code></pre>
",2018-01-29 18:30:36,2018-01-29 19:07:09,How to use saved Keras Model for sentiment classification?,<keras><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15545,48559764,2018-02-01 09:52:09,,"<p>As i say in the title i would like to load pre-tranined model </p>

<p>using gensim is possibile for example but with fasttext say:</p>

<p><a href=""https://radimrehurek.com/gensim/models/wrappers/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/wrappers/fasttext.html</a></p>

<pre><code> ""Note that due to limitations in the FastText API, you cannot continue training with a model loaded this way, though you can query for word similarity etc.""
</code></pre>

<p>With Word2vec say it is possibile continue the traning of your own model not a pretranind end i do not know with Glove.</p>

<p>Can you point me to any library or something for load a pre-tranined model and continue the traning on my own sentences ?</p>

<p>Or in case i can load the pretranined model into a neural network and after continue the traning with my own vectors ?  (maybe using get_keras_embedding ? )</p>
",,2018-02-01 09:52:09,"NLU FastText, Glove or Word2Vec Load Pre-trained model and Add new word to vocabulary",<neural-network><nlp><word2vec><gensim><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
15556,48674084,2018-02-07 22:08:08,,"<p>I am using gensim lda for topic modeling and getting the results like so:</p>

<p>Topic 1: word1 word2 word3 word4</p>

<p>Topic 2: word4 word1 word2 word5</p>

<p>Topic 3: word1 word4 word5 word6</p>

<p>However using mallet on same lda does not produce duplicate words across topics. I have ~20 documents with >1000 words each that I train the lda on. How to get rid of words appearing across multiple topics?</p>
",,2018-04-10 09:21:58,Words appearing across all topics in lda,<python><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
15560,48562396,2018-02-01 12:06:32,,"<p>I am new in NLP. I am trying to extract the summary of the paragraphs using Gensim in python. </p>

<p>I am facing a problem with a short paragraph, it is giving me a warning as given below and doesn't give me a summary of the short paragraph.</p>

<p>Here is my code in Python:</p>

<pre><code> import logging
 logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
 from gensim.summarization import summarize

text = ""short paragraph""
print ('Summary:')
print (summarize(text))
</code></pre>

<p>It is giving me warning as follows:</p>

<pre><code>2018-02-01 17:31:47,247 : WARNING : Input text is expected to have at least 10 sentences.
2018-02-01 17:31:47,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2018-02-01 17:31:47,258 : INFO : built Dictionary(52 unique tokens: ['clearli', 'adult', 'chang', 'member', 'visit']...) from 4 documents (total 70 corpus positions)
2018-02-01 17:31:47,262 : WARNING : Input corpus is expected to have at least 10 documents.
2018-02-01 17:31:47,285 : WARNING : Couldn't get relevant sentences.
</code></pre>

<p>The output is(Printing only summary label not the actual summary of the short paragraph):</p>

<pre><code>Summary:
</code></pre>

<p>Am I missing something? Is there any other library for the same.</p>
",,2018-02-01 13:18:46,Text Summarization with Gensim with short paragraph,<python><python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15566,48614304,2018-02-05 00:14:38,,"<p>I'm trying to build a Gensim word2vec model by using an external vocabulary. I know Gensim has an internal vocabulary generator however I do not have the same control over them. My problem code is simply. </p>

<pre><code>import gensim
from sklearn.feature_extraction.text import CountVectorizer

corpus = corpusCleaner(raw_corpus)
vocabularyGenerator = CountVectorizer(strip_accents=""ascii"", stop_words=""english"")
vocabularyGenerator.fit(corpus)
vocabulary = vocabularyGenerator.vocabulary_
model = gensim.models.Word2Vec()
model.build_vocab_from_freq(vocabulary)
</code></pre>

<p>I'm getting
C:\Anaconda3\envs\workflow\lib\site-packages\gensim\models\word2vec.py:1235: RuntimeWarning: overflow encountered in int_scalars
  retain_pct = retain_total * 100 / max(original_total, 1)</p>
",,2018-02-05 00:14:38,Gensim build_vocab_from_freq overflow Error,<python><nlp><overflow><gensim><vocabulary>,,,CC BY-SA 3.0,False,False,True,False,True
15572,48635364,2018-02-06 04:09:16,,"<p>I have read lots of examples regarding doc2vec, but I couldn't find any answer. Like a real example, I want to build a model with doc2vec and then train it with some ML models. after that, how can I get the vector of a raw string with the exact trained Doc2vec model? because I need to predict with my ML model with the same size and logical vector</p>
",,2018-02-06 22:20:19,load Doc2Vec model and get new sentence's vectors for test,<nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15589,48606331,2018-02-04 08:50:48,,"<p>I am new to natural language processing.
I have a list of blog titles, for example (Not real data, but you get the point):</p>

<pre><code>docs = [""Places to Eat"", ""Places to Visit"", ""Top 10 Things to Do in Singapore""]...
</code></pre>

<p>There are about 3000 over titles and I want to use LDA in Python to generate topics for each of this title. Assuming that I have already cleaned and tokenised these texts using nltk package and removed the stopwords, I will end up with:</p>

<pre><code>texts = [[""places"",""eat""],[""places"",""visit""]]...
</code></pre>

<p>I then proceed to convert these texts into Bag-of-words:</p>

<pre><code>from gensim import corpora, models
dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Corpus data looks like this:</p>

<pre><code>[(0, 1), (1, 1)]...
</code></pre>

<p>Model creation:</p>

<pre><code>import gensim
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=20)
</code></pre>

<p>How do I make use of this model to generate a list of topics - For example ""Eat"", ""Visit"", etc. for each of this titles? I understand that the output might contain probabilities but I would like to string them together with only the text.</p>
",,2018-02-05 01:33:41,How to generate a topic from a list of titles using LDA (Python)?,<python><nlp><nltk><gensim><lda>,,,CC BY-SA 3.0,True,False,True,False,False
15595,48690415,2018-02-08 16:35:04,,"<p>I'm attempting to load some pre-trained vectors into a gensim <code>Word2Vec</code> model, so they can be retrained with new data. My understanding is I can do the retraining with <code>gensim.Word2Vec.train()</code>. However, the only way I can find to load the vectors is with <code>gensim.models.KeyedVectors.load_word2vec_format('path/to/file.bin', binary=True)</code> which creates an object of what is usually the <code>wv</code> attribute of a <code>gensim.Word2Vec</code> model. But this object, on it's own, does not have a <code>train()</code> method, which is what I need to retrain the vectors. </p>

<p>So how do I get these vectors into an actual <code>gensim.Word2Vec</code> model?</p>
",2018-02-08 20:20:43,2018-02-08 20:20:43,Load vectors into gensim Word2Vec model - not KeyedVectors,<machine-learning><nlp><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15601,48661163,2018-02-07 10:16:11,,"<p>I want to get the similarity of one document to other documents. I use gensim. The program can run correctly, but after some steps it exits with Segmentation fault.</p>

<p>Below is my code:</p>

<pre><code>from gensim import corpora, models, similarities
docs = [['Looking', 'for', 'the', 'meanings', 'of', 'words'],
        ['phrases'],
        ['and', 'expressions'],
        ['We', 'provide', 'hundreds', 'of', 'thousands', 'of', 'definitions'],
        ['synonyms'],
        ['antonyms'],
        ['and', 'pronunciations', 'for', 'English', 'and', 'other', 'languages'],
        ['derived', 'from', 'our', 'language', 'research', 'and', 'expert', 'analysis'],
        ['We', 'also', 'offer', 'a', 'unique', 'set', 'of', 'examples', 'of', 'real', 'usage'],
        ['as', 'well', 'as', 'guides', 'to:']]
dictionary = corpora.Dictionary(docs)
corpus = [dictionary.doc2bow(text) for text in docs]
nf=len(dictionary.dfs)
index = similarities.SparseMatrixSimilarity(corpus, num_features=nf)
phrases = [['This',
            'section',
            'gives',
            'guidelines',
            'on',
            'writing',
            'in',
            'everyday',
            'situations'],
           ['from',
            'applying',
            'for',
            'a',
            'job',
            'to',
            'composing',
            'letters',
            'of',
            'complaint',
            'or',
            'making',
            'an',
            'insurance',
            'claim'],
           ['There',
            'are',
            'plenty',
            'of',
            'sample',
            'documents',
            'to',
            'help',
            'you',
            'get',
            'it',
            'right',
            'every',
            'time'],
           ['create',
            'a',
            'good',
            'impression'],
           ['and',
            'increase',
            'the',
            'likelihood',
            'of',
            'achieving',
            'your',
            'desired',
            'outcome']]
phrase2word=[dictionary.doc2bow(text,allow_update=True) for text in phrases]
sims=index[phrase2word]
</code></pre>

<p>It can run normally until get sims, but it cannot get sims, and using <code>gdb</code> gets the following info:</p>

<blockquote>
  <p>Program received signal SIGSEGV, Segmentation fault.
  0x00007fffd881d809 in csr_tocsc (n_row=5, n_col=39,
  Ap=0x4a4eb10, Aj=0x9fc6ec0, Ax=0x1be4a00, Bp=0xa15f6a0, Bi=0x9f3ee80,
  Bx=0x9f85f60) at scipy/sparse/sparsetools/csr.h:411 411<br>
  scipy/sparse/sparsetools/csr.h: .</p>
</blockquote>
",2018-02-08 06:30:03,2018-02-09 05:36:37,gensim.similarities.SparseMatrixSimilarity get segmentation-fault,<python><c++><segmentation-fault><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15617,48623214,2018-02-05 13:01:28,,"<p>I have a 60000 documents which i processed in <code>gensim</code> and got a 60000*300 matrix. I exported this as a <code>csv</code> file. When i import this in <code>ELKI</code> environment and run <code>Kmeans</code> clustering, i am getting below error.</p>

<pre><code>Task failed
de.lmu.ifi.dbs.elki.data.type.NoSupportedDataTypeException: No data type found satisfying: NumberVector,field AND NumberVector,variable
Available types: DBID DoubleVector,variable,mindim=266,maxdim=300 LabelList
    at de.lmu.ifi.dbs.elki.database.AbstractDatabase.getRelation(AbstractDatabase.java:126)
    at de.lmu.ifi.dbs.elki.algorithm.AbstractAlgorithm.run(AbstractAlgorithm.java:81)
    at de.lmu.ifi.dbs.elki.workflow.AlgorithmStep.runAlgorithms(AlgorithmStep.java:105)
    at de.lmu.ifi.dbs.elki.KDDTask.run(KDDTask.java:112)
    at de.lmu.ifi.dbs.elki.application.KDDCLIApplication.run(KDDCLIApplication.java:61)
    at [...]
</code></pre>

<p>Below is the ELKI settings i have used
<a href=""https://i.stack.imgur.com/QdG3V.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QdG3V.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ECMlW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ECMlW.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/XyMJu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XyMJu.jpg"" alt=""enter image description here""></a></p>
",2018-02-06 09:42:32,2018-02-09 09:25:14,ELKI Kmeans clustering Task failed error for high dimensional data,<cluster-analysis><k-means><gensim><doc2vec><elki>,,,CC BY-SA 3.0,False,False,True,False,False
15622,48683025,2018-02-08 10:18:25,,"<p>I have a question regarding concatenating two doc2vec models. I followed the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">official gensim IMDB example</a> on <code>doc2vec</code> and implemented example data.</p>

<p>When concatenating two models (PV-DM + PV-DBOW), as outlined in the original paper, I wondered that the concatenated model appears not to have 200-dim, like the two input models, but 400-dim:</p>

<pre><code>Shape Train(11948, **400**)
Shape Test(2987, **400**)
</code></pre>

<p>The input shapes were each:</p>

<pre><code>np.asarray(X_train).shape)
(11948, **200**)
(2987, **200**)
</code></pre>

<p><strong>Is this correct?</strong> I expected the number of dimensions to be 200 again. </p>
",,2018-02-08 13:21:14,Concatenating two doc2vec models: Vector dimensions doubled,<machine-learning><concatenation><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15627,48703067,2018-02-09 09:53:11,,"<ol>
<li>Build_vocab extend my old vocabulary? </li>
</ol>

<p>For example, my idea is when I use doc2vec(s) to train a model, it just builds the vocabulary from the datasets.  If I want to extend it, I need to use build_vocab()</p>

<ol start=""2"">
<li>Where should I use it?  Should I put it after ""gensim.doc2vec()""?  </li>
</ol>

<p>For example:</p>

<pre><code>sentences = gensim.models.doc2vec.TaggedLineDocument(f_path)
dm_model = gensim.models.doc2vec.Doc2Vec(sentences, dm=1, size=300, window=8, min_count=5, workers=4)
dm_model.build_vocab()
</code></pre>
",2020-01-28 20:23:07,2020-01-28 20:23:07,how to use build_vocab in gensim?,<nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
15628,48705138,2018-02-09 11:47:28,,"<p>I noticed that when adding documents to a gensim Dictionary, execution time jumps from 0.2s to more than 6s when reaching 2 million words.</p>

<p>The code below is a quick example. I loop through int and add the number to the dictionary at each iteraion.</p>

<pre><code>from gensim import corpora
import time



dict_transcript = corpora.Dictionary()


for i in range(1,10000000):

    start_time = time.time()

    doc = [str(i)]

    dict_transcript.add_documents([doc])

    print(""Iter ""+str(i)+"" done in "" + str(time.time() - start_time) + ' w/ '+str(len(doc)) + ' words and dico size ' +
          str(len(dict_transcript)))
</code></pre>

<p>I do get the following output when reaching 2 million words:</p>

<pre><code>Iter 1999999 done in 0.0 w/ 1 words and dico size 1999999
Iter 2000000 done in 0.0 w/ 1 words and dico size 2000000
Iter 2000001 done in 0.0 w/ 1 words and dico size 2000001
Iter 2000002 done in 7.940511226654053 w/ 1 words and dico size 2000001
</code></pre>

<p>Is there any reason why? And does anyone know how to bypass that problem?
I'm using this dictionary on a big corpus that I tokenize into bigrams so I'm expecting the dictionary to be a few million rows.</p>

<p>Many thanks</p>
",2018-02-09 12:10:13,2018-02-09 12:36:39,Why adding documents to gensim Dictionary gets slow when reaching 2 million words?,<python><dictionary><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15633,48644212,2018-02-06 13:27:58,,"<p>I am trying to save lda output in a dictionary, in which words and their probabilities will be keys and values and then save this dictionary in json but I don't know how to achieve this. When I simply try to save it in json it is some kind of binary format. Here is the code which I have tried so far:</p>

<pre><code>   filename = sys.argv[1]
    lda = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,alpha = 0.001, , passes=50,iterations=300)
    for i in range(0, lda.num_topics):
        with open(filename, 'w') as outfile:
            outfile.write(json.dumps('{}\n'.format('Topic #' + str(i + 1) + ': ')))
            for word, prob in lda.show_topic(i, topn=10):
                outfile.write(json.dumps('{}:{}\n'.format(word,prob)))
            outfile.write('\n')
</code></pre>
",2018-02-06 16:17:37,2018-02-06 16:17:37,How to make dictionary of lda outputs and then save it into json,<python><json><python-3.x><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
15637,48717970,2018-02-10 06:32:46,,"<p>I am trying to implement something similar in <a href=""https://arxiv.org/pdf/1603.04259.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.04259.pdf</a> using awesome gensim library however I am having trouble improving quality of results when I compare to Collaborative Filtering.</p>

<p>I have two models one built on Apache Spark and other one using gensim Word2Vec on grouplens 20 million ratings dataset. My apache spark model is hosted on AWS <a href=""http://sparkmovierecommender.us-east-1.elasticbeanstalk.com"" rel=""nofollow noreferrer"">http://sparkmovierecommender.us-east-1.elasticbeanstalk.com</a>
and I am running gensim model on my local. However when I compare the results I see superior results with CF model 9 out of 10 times(like below example more similar to searched movie - affinity towards Marvel movies) </p>

<p>e.g.:- If I search for Thor movie I get below results </p>

<p><strong><em>Gensim</em></strong></p>

<ul>
<li>Captain America: The First Avenger (2011) </li>
<li>X-Men: First Class (2011)</li>
<li>Rise of the Planet of the Apes (2011) </li>
<li>Iron Man 2 (2010) </li>
<li>X-Men Origins: Wolverine (2009) </li>
<li>Green Lantern (2011) </li>
<li>Super 8 (2011) </li>
<li>Tron:Legacy (2010) </li>
<li>Transformers: Dark of the Moon (2011)</li>
</ul>

<p><strong><em>CF</em></strong></p>

<ul>
<li>Captain America: The First Avenger</li>
<li>Iron Man 2</li>
<li>Thor: The Dark World</li>
<li>Iron Man</li>
<li>The Avengers</li>
<li>X-Men: First Class</li>
<li>Iron Man 3</li>
<li>Star Trek</li>
<li>Captain America: The Winter Soldier</li>
</ul>

<p>Below is my model configuration, so far I have tried playing with window, min_count and size parameter but not much improvement.</p>

<pre><code>word2vec_model = gensim.models.Word2Vec(
    seed=1,
    size=100, 
    min_count=50, 
    window=30)

word2vec_model.train(movie_list, total_examples=len(movie_list), epochs=10)
</code></pre>

<p>Any help in this regard is appreciated.</p>
",2018-02-10 06:49:55,2018-02-14 01:53:07,Gensim: Word2Vec Recommender accuracy Improvement,<word2vec><gensim><recommendation-engine>,,,CC BY-SA 3.0,False,False,True,False,False
15646,48733918,2018-02-11 16:49:51,,"<p>I'm having trouble with the most_similar method in Gensim's Doc2Vec model. When I run most_similar I only get the similarity of the first 10 tagged documents (based on their tags-always from 0-9). For this code I have topn=5, but I've used topn=len(documents) and I still only get the similarity for the first 10 documents</p>

<p>Tagged documents:</p>

<pre><code>tokenizer = RegexpTokenizer(r'\w+')
taggeddoc=[]

for index,wod in enumerate(model_data):
    wordslist=[]
    tagslist=[]
    tokens = tokenizer.tokenize(wod)

    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(tokens))).split(), str(index)) 
    taggeddoc.append(td)

documents=taggeddoc
</code></pre>

<p>Instantiate the model:</p>

<pre><code>model=gensim.models.Doc2Vec(documents, dm=0, dbow_words=1, iter=1, alpha=0.025, min_alpha=0.025, min_count=10)
</code></pre>

<p>Train the model:</p>

<pre><code>for epoch in range(100):
    if epoch % 10 == 0:
        print(""Training epoch {}"".format(epoch))
    model.train(documents, total_examples=model.corpus_count, epochs=model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
</code></pre>

<p>Problem is here (I think):</p>

<pre><code>new = model_data[100].split()
new_vector = model.infer_vector(new)
sims = model.docvecs.most_similar([new_vector], topn=5)
print(sims)
</code></pre>

<p>Output:</p>

<pre><code>[('3', 0.3732905089855194), ('1', 0.36121609807014465), ('7', 0.35790640115737915), ('9', 0.3569292724132538), ('2', 0.3521473705768585)]
</code></pre>

<p>Length of documents is the same before and after training the model. Not sure why it's only returning similarity for the first 10 documents.</p>

<p>Side question: In anyone's experience, is it better to use Word2Vec or Doc2Vec if the input documents are very short (~50 words) and there are >2,000 documents? Thanks for the help!</p>
",,2018-02-12 03:46:40,Gensim Doc2Vec Most_Similar,<python><nlp><deep-learning><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15670,48743444,2018-02-12 09:46:17,,"<p><strong>Does <code>gensim.model.TfidfModel</code> have the term frequency saved?</strong></p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">docs</a>, they use the formula:</p>

<pre><code>weights_i_j = frequency_i_j * log_2(D / doc_freq_i)
</code></pre>

<p>And when I prob the attributes of the <code>dir(model)</code> (TfidfModel object) with the following code:</p>

<pre><code>&gt;&gt;&gt; import gensim.downloader as api
&gt;&gt;&gt; from gensim.models import TfidfModel
&gt;&gt;&gt; from gensim.corpora import Dictionary
&gt;&gt;&gt;
&gt;&gt;&gt; dataset = api.load(""text8"")
&gt;&gt;&gt; dct = Dictionary(dataset)  # fit dictionary
&gt;&gt;&gt; corpus = [dct.doc2bow(line) for line in dataset]  # convert dataset to BoW format
&gt;&gt;&gt;
&gt;&gt;&gt; model = TfidfModel(corpus)  # fit model
&gt;&gt;&gt; dir(model)
</code></pre>

<p>I'm getting:</p>

<pre><code>['__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_adapt_by_suffix',
 '_apply',
 '_load_specials',
 '_save_specials',
 '_smart_save',
 'dfs',
 'id2word',
 'idfs',
 'initialize',
 'load',
 'normalize',
 'num_docs',
 'num_nnz',
 'save',
 'wglobal',
 'wlocal']
</code></pre>

<p>But I can't seem to find where are the term frequencies stored. </p>

<p><strong>If the term frequencies are not saved, is there a reason why?</strong> Since it's already stored to compute the weights anyways. </p>

<p><strong>Is there a way to retrieve the term frequencies somehow during the fitting process?</strong></p>
",,2018-06-20 10:33:04,Does gensim.model.TfidfModel have the term frequency saved?,<python><nlp><counter><gensim><tf-idf>,,,CC BY-SA 3.0,False,False,True,False,False
15695,48842866,2018-02-17 15:28:11,,"<p>I am using gensim in python 3 as shown within image below </p>

<p><a href=""https://i.stack.imgur.com/xMbap.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xMbap.png"" alt="" Image""></a></p>

<p>In line no 11 I am getting the following error:</p>

<p><a href=""https://i.stack.imgur.com/NoGO4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NoGO4.png"" alt=""Image""></a></p>
",2018-02-17 18:52:31,2018-05-08 15:25:45,'gensim.models.doc2vec' has no attribute 'LabeledSentence',<python-3.x><sublimetext3><sentiment-analysis><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15698,48803598,2018-02-15 09:16:53,,"<p>I want to automate the threshold process in hierarchical clustering process, What i want to do is , instead of inputting threshold value manually , How do i  check if i have clusters in range of 30 to 50 , if clusters are not in range of 30-50 , change the threshold value through code , by 0.1 or 0.2 in python</p>

<pre><code>    import pickle
    import re
    import string
    import sys
    # import gensim
    # from gensim import corpora
    from time import time

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.cluster.hierarchy as sch
    from nltk.corpus import stopwords
    from nltk.stem.wordnet import WordNetLemmatizer
    from scipy.cluster.hierarchy import dendrogram, linkage
    from scipy.spatial.distance import pdist
    from scipy.spatial.distance import squareform
    from sklearn.decomposition import NMF, LatentDirichletAllocation
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfVectorizer
    from stop_word_complaints import complaint_stop_words

 tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=1, token_pattern=r'\b\w+\b',
                                       max_features=n_features, stop_words=list(stop), analyzer='word')
    X = tfidf_vectorizer.fit_transform(corpus).toarray()

    non_zero_features = np.where(np.sum(X, axis=1) != 0)[0]
    print(""done in %0.3fs."" % (time() - t0))
    print(""pdist ..."")
    t0 = time()
    cos_dist = pdist(X[non_zero_features, :], 'cosine')
    print(""done in %0.3fs."" % (time() - t0))
    dists = np.asarray(squareform(cos_dist))
    dists[np.isnan(dists)] = 1
    # cos_dist[np.isnan(cos_dist)] = 0
    # dists[np.argwhere(np.isnan(dists))] = 1
    print(""linkage ..."")
    np.savetxt(str_path + ""_dist_1.csv"", dists, delimiter=',')
    # pickle.dump(dists, open(str_path + ""_dist.p"", ""wb""))
    t0 = time()
    linkage_matrix = linkage(dists, ""average"")
    print(""done in %0.3fs."" % (time() - t0))
    np.savetxt(str_path + ""linkage_matrix.csv"", linkage_matrix, delimiter=',')
    # linkage_matrix = np.loadtxt(str_path + ""linkage_matrix.csv"", delimiter=',')
    # pickle.dump(linkage_matrix, open(str_path + ""linkage_matrix.p"", ""wb""))
    dendrogram(linkage_matrix)
    # create figure &amp; 1 axis
    fig, ax = plt.subplots(nrows=1, ncols=1)  # create figure &amp; 1 axis

    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('sample index')
    plt.ylabel('distance')
    dendrogram(
        linkage_matrix
        # leaf_rotation=90.,  # rotates the x axis labels
        # leaf_font_size=3.,  # font size for the x axis labels
    )
    plt.show()
    fig.savefig(str_path + 'Agglo_Heirachy_dendo.png')  # save the figure to file

min_th = min(linkage_matrix[:,2])
max_th = max(linkage_matrix[:,2])
clusters =  get_clusters(linkage_matrix, min_th, max_th)
</code></pre>
",2018-02-19 09:12:52,2018-02-19 09:12:52,Automating Clusters in Hierarchical clustering using threshold,<python><scikit-learn><nltk><hierarchical-clustering>,,,CC BY-SA 3.0,True,False,True,False,True
15704,48749858,2018-02-12 15:27:30,,"<p>I want to train word2vec (using gensim) on a large corpus data. The information I have is only co-occurence of any two words. My data has the format of<br>
word__tab__context_word__tab__Number<br>
(e.g: danger of 10, meaning '<em>danger</em>' and '<em>of</em>' co-occured 10 times in a window size of 5 in the corpus) for each line. 
Does word2vec of gensim take such input? I have searched through gensim tutorials and havn't seen any examples like this. </p>

<p>Thanks a lot for help. 
Li </p>
",,2018-02-14 01:40:34,Can I use word-context-count pairs as input to gensim's Word2Vec,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15724,48821863,2018-02-16 07:21:15,,"<p>I Already have a Doc2Vec model. I have trained it with my train data.</p>

<p>Now after a while I want to use Doc2Vec for my test data. I want to add my test data vocabulary to my existing model's vocabulary. How can I do this?
I mean how can I update my vocabulary?</p>

<p>Here is my model:</p>

<pre><code>    model = model.load('my_model.Doc2vec')
</code></pre>
",,2018-02-16 23:25:57,add new vocabulary to existing Doc2vec model,<word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15730,48808989,2018-02-15 13:56:11,,"<pre><code>import os
import gensim.models as g
import logging
import gensim
os.chdir(""/home/ai/path"");
#doc2vec parameters
vector_size = 300
window_size = 5  
min_count = 1 
sampling_threshold = 1e-5 
negative_size = 5 
train_epoch = 100
dm= 0 

worker_count = 2 #number of parallel processes

#pretrained word embeddings
pretrained_emb = ""GoogleNews-vectors-negative300.bin""

#input corpus
train_corpus = ""mydata.txt""

#output model
saved_path = ""Googlemodel.bin""

#enable logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %
(message)s', 
level=logging.INFO)

#train doc2vec model
docs = g.doc2vec.TaggedLineDocument(train_corpus)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, 
min_count=min_count, sample=sampling_threshold, workers=worker_count, 
hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, 
pretrained_emb=pretrained_emb, iter=train_epoch)
</code></pre>

<p>Size of <code>GoogleNews-vectors-negative300.bin</code> is 3.6 GB, my data size is <strong>455 MB</strong>. </p>

<p>After running this code or training process completion, my output model comes out with <strong>850 MB</strong> only.</p>
",2018-02-17 15:17:34,2018-02-17 15:17:34,Is there any way to retrain pretrained GoogleNews-vectors-negative300.bin model with our data?,<python><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15733,48898325,2018-02-21 04:45:14,,"<p>I am trying to train with new labelled document(TaggedDocument) with the pre-trained model.</p>

<p>Pretrained model is the trained model with documents which the unique id with label1_index, for instance, Good_0, Good_1 to Good_999
And the total size of trained data is about 7000</p>

<p>Now, I want to train the pre-trained model with new documents which the unique id with label2_index, for instance, Bad_0, Bad_1... to Bad_1211
And the total size of trained data is about 1211</p>

<p>The train itself was successful without any error, but the problem is that whenever I try to use 'most_similar' it only suggests the similar document labelled with Good_... where I expect the labelled with Bad_.</p>

<p>If I train altogether from the beginning, it gives me the answers I expected - it infers a newly given document similar to either labelled with Good or Bad. </p>

<p>However, the practice above will not work as the one trained altogether from the beginning.</p>

<p>Is continuing train not working properly or did I make some mistake?</p>
",2018-02-21 04:51:02,2018-02-21 19:08:43,gensim doc2vec train more documents from pre-trained model,<gensim><doc2vec><pre-trained-model><resuming-training>,,,CC BY-SA 3.0,False,False,True,False,False
15735,48917449,2018-02-21 23:52:20,,"<p>I am converting a <code>gensim</code> w2v file to a <code>Tensorboard</code> tsv file with this code:</p>

<pre><code>with open(outfiletsv, 'w+b') as file_vector:
    with open(outfiletsvmeta, 'w+b') as file_metadata:
        for word in model.index2word:
            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
            vector_row = '\t'.join(str(x) for x in model[word])
            file_vector.write(vector_row + '\n')
</code></pre>

<p>It results in this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
~\_repos\special\word2vec2tensor.py in &lt;module&gt;()
     79 
     80     logger.info(""running %s"", ' '.join(sys.argv))
---&gt; 81     word2vec2tensor(args.input, args.output, args.binary)
     82     logger.info(""finished running %s"", os.path.basename(sys.argv[0]))

~\_repos\special\word2vec2tensor.py in word2vec2tensor(word2vec_model_path, tensor_filename, binary)
     61                 file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
     62                 vector_row = '\t'.join(str(x) for x in model[word])
---&gt; 63                 file_vector.write(vector_row + '\n')
     64 
     65     logger.info(""2D tensor file saved to %s"", outfiletsv)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I added <code>b</code> to the original <code>w+</code> in the open file pieces to counteract the opposite issue (<code>TypeError: write() argument must be str, not bytes</code>). </p>

<p>I tried adding `vector_row = vector_row.encode('UTF-8'), but this did not work. </p>

<p>How do I remedy this <code>TypeError</code>?</p>
",,2018-02-22 00:09:13,"TypeError: a bytes-like object is required, not 'str' when converting gensim to tensorboard",<python><python-3.x><typeerror><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15786,48941648,2018-02-23 05:26:07,,"<p>Given a model, e.g.</p>

<pre><code>from gensim.models.word2vec import Word2Vec


documents = [""Human machine interface for lab abc computer applications"",
""A survey of user opinion of computer system response time"",
""The EPS user interface management system"",
""System and human system engineering testing of EPS"",
""Relation of user perceived response time to error measurement"",
""The generation of random binary unordered trees"",
""The intersection graph of paths in trees"",
""Graph minors IV Widths of trees and well quasi ordering"",
""Graph minors A survey""]

texts = [d.lower().split() for d in documents]

w2v_model = Word2Vec(texts, size=5, window=5, min_count=1, workers=10)
</code></pre>

<p>It's possible to remove the word from the w2v vocabulary, e.g.</p>

<pre><code># Originally, it's there.
&gt;&gt;&gt; print(w2v_model['graph'])
[-0.00401433  0.08862179  0.08601206  0.05281207 -0.00673626]

&gt;&gt;&gt; print(w2v_model.wv.vocab['graph'])
Vocab(count:3, index:5, sample_int:750148289)

# Find most similar words.
&gt;&gt;&gt; print(w2v_model.most_similar('graph'))
[('binary', 0.6781558990478516), ('a', 0.6284914612770081), ('unordered', 0.5971308350563049), ('perceived', 0.5612867474555969), ('iv', 0.5470727682113647), ('error', 0.5346164703369141), ('machine', 0.480206698179245), ('quasi', 0.256790429353714), ('relation', 0.2496253103017807), ('trees', 0.2276223599910736)]

# We can delete it from the dictionary
&gt;&gt;&gt; del w2v_model.wv.vocab['graph']
&gt;&gt;&gt; print(w2v_model['graph'])
KeyError: ""word 'graph' not in vocabulary""
</code></pre>

<p>But when we do a similarity on other words after deleting <code>graph</code>, we see the word <code>graph</code> popping up, e.g.</p>

<pre><code>&gt;&gt;&gt; w2v_model.most_similar('binary')
[('unordered', 0.8710334300994873), ('ordering', 0.8463168144226074), ('perceived', 0.7764195203781128), ('error', 0.7316686511039734), ('graph', 0.6781558990478516), ('generation', 0.5770125389099121), ('computer', 0.40017056465148926), ('a', 0.2762695848941803), ('testing', 0.26335978507995605), ('trees', 0.1948457509279251)]
</code></pre>

<p><strong>How to remove a word completely from a Word2Vec model in gensim?</strong> </p>

<hr>

<h1>Updated</h1>

<p>To answer @vumaasha's comment:</p>

<blockquote>
  <p>could you give some details as to why you want to delete a word</p>
</blockquote>

<ul>
<li><p>Lets say my universe of words in all words in the corpus to learn the dense relations between all words. </p></li>
<li><p>But when I want to generate the similar words, it should only come from a subset of domain specific word.</p></li>
<li><p>It's possible to generate more than enough from <code>.most_similar()</code> then filter the words but lets say the space of the specific domain is small, I might be looking for a word that's ranked 1000th most similar which is inefficient. </p></li>
<li><p>It would be better if the word is totally removed from the word vectors then the <code>.most_similar()</code> words won't return words outside of the specific domain.</p></li>
</ul>
",2018-02-23 06:17:40,2019-08-14 03:55:22,How to remove a word completely from a Word2Vec model in gensim?,<python><dictionary><word2vec><gensim><del>,,,CC BY-SA 3.0,False,False,True,False,False
15792,49021389,2018-02-28 03:14:02,,"<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>

<p>The example of training data (length>10)</p>

<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]
</code></pre>

<p>with some pre-treatment</p>

<pre><code>doc_=[d.strip().split("" "") for d in doc]
doc_tagged = []
for i in range(len(doc_)):
  tagd = TaggedDocument(doc_[i],str(i))
  doc_tagged.append(tagd)
</code></pre>

<p>tagged docs</p>

<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],
  dtype='&lt;U32'), tags='117')
</code></pre>

<p>fit a doc2vec model</p>

<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(doc_tagged)
model.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)
</code></pre>

<p>then i get the final model</p>

<pre><code>len(model.docvecs)
</code></pre>

<p>the result is 10...</p>

<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.
So, my question is:
How to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)
Is <code>model.docvecs</code> designed to provide all training docvecs?</p>
",2018-02-28 13:39:00,2018-02-28 13:39:00,Doc2vec: Only 10 docvecs in gensim doc2vec model?,<machine-learning><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15797,48928739,2018-02-22 13:33:38,,"<p>I have a set of documents in a df. I am transforming those documents to vectors with <code>gensim</code> <code>Doc2Vec</code>:</p>

<pre><code>def read_corpus(documents):
    for i, plot in enumerate(documents):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])

train_corpus = list(read_corpus(df.note))

model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I then save the model and convert the .w2v file to tsv files. Finally, I overwrite the metadata tsv file so that it is meaningful:</p>

<pre><code>model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)

%run word2vec2tensor.py -i doc_tensor.w2v -o notes 

with open('notes_metadata.tsv','w') as w:
    w.write('created_by\tnote_type\n')
    for i,j in zip(df.created_by, df.note_type):
        w.write(""%s\t%s\n"" % (i,j))
</code></pre>

<p>At this point, I have two tsv files: one contains the vectors and the other contains metadata about the vectors. I got to this point by following this tutorial: <a href=""http://nbviewer.jupyter.org/github/RaRe-Technologies/gensim/blob/8f7c9ff4c546f84d42c220dcf28543500747c171/docs/notebooks/Tensorboard_visualizations.ipynb#Training-the-Doc2Vec-Model"" rel=""nofollow noreferrer"">http://nbviewer.jupyter.org/github/RaRe-Technologies/gensim/blob/8f7c9ff4c546f84d42c220dcf28543500747c171/docs/notebooks/Tensorboard_visualizations.ipynb#Training-the-Doc2Vec-Model</a>.</p>

<p>Now I would like to embed this model and the tsv files in a local Tensorboard. I tried this:</p>

<pre><code># load model
embedding = model.docvecs.vectors_docs

# setup a TensorFlow session
tf.reset_default_graph()
sess = tf.InteractiveSession()
X = tf.Variable([0.0], name='embedding')
place = tf.placeholder(tf.float32, shape=embedding.shape)
set_x = tf.assign(X, place, validate_shape=False)
sess.run(tf.global_variables_initializer())
sess.run(set_x, feed_dict={place: embedding})

# create a TensorFlow summary writer
summary_writer = tf.summary.FileWriter('log', sess.graph)
config = projector.ProjectorConfig()
embedding_conf = config.embeddings.add()
embedding_conf.tensor_name = 'embedding:0'
embedding_conf.metadata_path = os.path.join('log', 'metadata.tsv')
projector.visualize_embeddings(summary_writer, config)
</code></pre>

<p>This code ran without error, but when I type <code>tensorboard --logdir=log</code> and go to the localhost location, it looks like this:</p>

<p><a href=""https://i.stack.imgur.com/dQWmi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dQWmi.png"" alt=""enter image description here""></a></p>

<p>My folder structure looks like this:</p>

<pre><code>project
   - jupyter_notebook_from_which_I_run_my_code.ipynb
   - log
        - events.out.tfevents.1519305293.COMPUTERNAME
        - notes_metadata.tsv
        - notes_tensor.tsv
        - projector_config.pbtxt
</code></pre>

<p>If I click ""Choose File"" in the TensorBoard projector and choose my notes_tensor.tsv file, it says ""Graph visualization failed: The graph is empty. Make sure that the graph is passed to the tf.summary.FileWriter after the graph is defined.</p>

<p>How do I get the tsv files to show up in the projector for tSNE and PCA visualizations like in the tutorial I linked to earlier?</p>

<p><strong>Update:</strong> I tried adding these two lines:</p>

<pre><code>saver = tf.train.Saver([X])
saver.save(sess, os.path.join('log', 'model2.ckpt'), 1)
</code></pre>

<p>This added these files to <code>log</code>:</p>

<pre><code>checkpoint
model2.ckpt-1.data-00000-of-00001
model2.ckpt-1.index
model2.ckpt-1.meta
</code></pre>

<p>It also gave TensorBoard the <code>Projector</code> tab!</p>

<p><a href=""https://i.stack.imgur.com/vx1nA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vx1nA.png"" alt=""enter image description here""></a></p>

<p>However, there is an error fetching metadata.tsv. This is because it doesn't exist. It's also looking in /log/log instead of just /log. When I dismiss that error, click ""Load"", and choose notes_metadata.tsv, nothing happens.</p>
",2018-02-22 13:58:37,2018-02-22 13:58:37,Embedding Gensim Doc2Vec Tensorboard,<python><tensorflow><gensim><tensorboard><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15802,48962171,2018-02-24 11:10:57,,"<p>I tried to follow <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">this.</a><br>
But some how I wasted a lot of time ending up with nothing useful.<br>
I just want to train a <code>GloVe</code> model on my own corpus (~900Mb corpus.txt file).
I downloaded the files provided in the link above and compiled it using <code>cygwin</code> (after editing the demo.sh file and changed it to <code>VOCAB_FILE=corpus.txt</code> . should I leave <code>CORPUS=text8</code> unchanged?)
the output was:  </p>

<ol>
<li>cooccurrence.bin </li>
<li>cooccurrence.shuf.bin  </li>
<li>text8</li>
<li>corpus.txt</li>
<li>vectors.txt</li>
</ol>

<p>How can I used those files to load it as a <code>GloVe</code> model on python?</p>
",2020-01-27 06:21:12,2020-06-09 16:10:08,How to Train GloVe algorithm on my own corpus,<nlp><stanford-nlp><gensim><word2vec><glove>,,,CC BY-SA 4.0,False,False,True,True,False
15820,48999199,2018-02-27 00:19:17,,"<p>I am a bit new to gensim and right now I am trying to solve the problem which involves using the doc2vec embeddings in keras. I wasn't able to find existing implementation of doc2vec in keras - as far as I see in all examples I found so far everyone just uses the gensim to get the document embeddings.</p>

<p>Once I trained my doc2vec model in gensim I need to export embeddings weights from genim into keras somehow and it is not really clear on how to do that. I see that</p>

<pre><code>model.syn0
</code></pre>

<p>Supposedly gives the word2vec embedding weights (according to <a href=""https://codekansas.github.io/blog/2016/gensim.html"" rel=""nofollow noreferrer"">this</a>). But it is unclear how to do the same export for document embeddings. Any advise?</p>

<p>I know that in general I can just get the embeddings for each document directly from gensim model but I want to fine-tune the embedding layer in keras later on, since doc embeddings will be used as a part of a larger task hence they might be fine-tuned a bit.</p>
",,2020-01-06 09:54:11,Export gensim doc2vec embeddings into separate file to use with keras Embedding layer later,<keras><gensim><word-embedding><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15833,48949166,2018-02-23 13:40:21,,"<p>I am using <code>gensim</code> to train a <code>Doc2Vec</code> model on documents assigned to particular people. There are 10 million documents and 8,000 people. I don't care about all 8,000 people. I care about a specific group of people (say anywhere from 1 to 500). </p>

<p>The people I'm interested in could change day-to-day, but I will never need to look at the full population. The end goal is to have the resulting vectors of the people I am interested in. I am currently training the model each time on the documents assigned to the specific people.</p>

<p>Should I train the model on all 10 million documents? Or should I train the model on only the documents assigned to the people I'm interested in? If it's important to train it on all 10 million documents, how would I then get the vectors only for the people I'm interested in?</p>
",,2018-02-23 13:54:25,Gensim Doc2Vec Training,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15835,48968688,2018-02-24 23:25:23,,"<p>I have trained a <code>gensim</code> Doc2Vec model on five million documents, and those documents are tagged with a unique ID (IDNO). Now I am loading the model, and accessing a specific subset of the vectors based on IDNO. First, I load a <code>pandas</code> df from a database and it looks like this:</p>

<pre><code>IDNO    author   document
123XYZ  john     the cat sat
234FGH  jane     the dog ran
345RTY  jane     the hippo ate
</code></pre>

<p>Then I load the model:</p>

<pre><code>model = Doc2Vec.load('documents_doc2vec_vectorsize100_mincount2_epochs50.model')
</code></pre>

<p>Then I access the three vectors that are in my df:</p>

<pre><code>row_id_list = list(df.row_id)
vectors_tuple = itemgetter(*row_id_list)(model.docvecs)
embedding = np.asarray(vectors_tuple)
</code></pre>

<p>Then I create all the necessary <code>TensorBoard</code> files:</p>

<pre><code>tf.reset_default_graph()
sess = tf.InteractiveSession()

X = tf.Variable([0.0], name='embedding')
place = tf.placeholder(tf.float32, shape=embedding.shape)
set_x = tf.assign(X, place, validate_shape=False)

sess.run(tf.global_variables_initializer())
sess.run(set_x, feed_dict={place: embedding})

summary_writer = tf.summary.FileWriter('log', sess.graph)

config = projector.ProjectorConfig()
embedding_conf = config.embeddings.add()
embedding_conf.tensor_name = 'embedding:0'
embedding_conf.metadata_path = os.path.join('metadata','metadata.tsv')
projector.visualize_embeddings(summary_writer, config)

saver = tf.train.Saver([X])
saver.save(sess, os.path.join('log', 'model1.ckpt'), 1)
</code></pre>

<p>When I run <code>tensorboard --logdir=log</code>, <code>TensorBoard</code> loads, but it says points and dimensions are loading. When I enter Chrome Developer Tools, I get this error:</p>

<pre><code>Uncaught TypeError: Cannot read property 'length' of undefined
    at (index):147401
    at Array.filter (&lt;anonymous&gt;)
    at (index):147399
    at XMLHttpRequest.xhr.onload ((index):143698)
</code></pre>

<p>I had <code>TensorBoard</code> working before, but that was when I set <code>embedding</code> to <code>model.docvecs.vectors_docs</code> rather than accessing specific vectors and pushing them into a <code>numpy</code> array.</p>

<p>Why is this happening? </p>
",,2018-03-16 20:23:05,TensorBoard UncaughtTypeError: Cannot read property 'length' of undefined,<python><tensorflow><gensim><tensorboard>,,,CC BY-SA 3.0,False,False,True,False,False
15839,48934154,2018-02-22 17:53:38,,"<p>I have two different corpus and what i want is to train the model with both and to do it it I thought that it could be something like this:</p>

<pre><code>model.build_vocab(sentencesCorpus1)
model.build_vocab(sentencesCorpus2)
</code></pre>

<p>Would it be right?</p>
",,2018-02-22 18:17:06,Can i build vocaburay in twice with gensim word2vec or doc2vec?,<python><word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15847,48914436,2018-02-21 19:59:24,,"<p>I have a large sparse word embedding matrix that is trained from sklearn tfidf which has nothing to do with the Gensim word2vec. </p>

<p>It is very similar to: <a href=""https://stackoverflow.com/questions/47959639/gensim-word2vec-transfer-learning-from-a-non-gensim-model"">gensim Word2vec transfer learning (from a non-gensim model)</a> and <a href=""https://stackoverflow.com/questions/46297740/how-to-turn-embeddings-loaded-in-a-pandas-dataframe-into-a-gensim-model"">How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?</a></p>

<p>However, given that the matrix is very sparse, I would like to store them more memore efficiently and reload them by gensim KeyedVecotrs. Or create the KeyedVectors instance without saving the sparse matrix and then directly save the gensim word2vec object. </p>

<p>Thanks. </p>

<hr>

<p>follow-up:</p>

<p>I end up successfully doing in this way:</p>

<pre><code>from collections import Counter, OrderedDict
import gensim
wc = Counter(corpus) # corpus = list of tokens in my documents
vocab = OrderedDict()
# building vocab for gensim
for idx, word in enumerate(words):
    tmp = gensim.models.keyedvectors.Vocab()
    tmp.index = idx
    tmp.count = wc[word]
    vocab[word] = tmp
w2v = gensim.models.keyedvectors.KeyedVectors()
w2v.index2word = words
w2v.vector_size = Xsparse.shape[1]
w2v.syn0 = Xsparse.toarray()  # have to make the matrix dense here
w2v.vocab = vocab
w2v.save(...)
</code></pre>

<p>However, I have to make the Xsparse word embedding matrix dense. Not sure if there is any more efficient way. Thanks for any answer.</p>
",2018-02-21 21:01:13,2018-02-21 21:01:13,How to efficiently transform a sparse word embedding matrix to gensim KeyedVectors object,<python><sparse-matrix><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,True
15854,48990935,2018-02-26 14:36:55,,"<p>I have a list of words in my python programme. Now I need to iterate through this list and find out the semantically similar words and put them into another list. I have been trying to do this using gensim with word2vec but could find a proper solution.This is what I have implemeted up to now. I need a help on how to iterate through the list of words in the variable sentences and find the semantically similar words and save it in another list.</p>

<pre><code>import gensim, logging

import textPreprocessing, frequentWords , summarizer
from gensim.models import Word2Vec, word2vec

import numpy as np
from scipy import spatial

sentences = summarizer.sorteddict

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
</code></pre>
",2018-02-27 10:44:10,2018-02-27 10:44:10,How to find semantic similarity using gensim and word2vec in python,<python><machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15870,48953871,2018-02-23 18:08:41,,"<p>I have three documents in a df:</p>

<pre><code>id    author    document
12X   john      the cat sat
12Y   jane      the dog ran
12Z   jane      the hippo ate
</code></pre>

<p>These documents are converted into a corpus of <code>TaggedDocuments</code> with the tags being the typical practice of semantically meaningless ints:</p>

<pre><code>def read_corpus(documents):
    for i, plot in enumerate(documents):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])

train_corpus = list(read_corpus(df.document))
</code></pre>

<p>This corpus is then used to train my <code>Doc2Vec</code> model:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>

<p>The resulting vectors of the model are accessed like this:</p>

<pre><code>model.docvecs.vectors_docs
</code></pre>

<p>How would I tie the original df to the resulting vectors? Now that all the documents are trained and vectors are identified for each one, I want to query the set of vectors by author. For example, if I want to return a set of vectors only for Jane, how would I do so?</p>

<p>I think the basic idea is to identify the int tags that correspond to Jane and then do something like this to access them:</p>

<pre><code>from operator import itemgetter 
a = model.docvecs.vectors_docs
b = [1, 2]
itemgetter(*b)(a)
</code></pre>

<p>How would I identify the tags though? They are only meaningful to the model and the tagged documents, so they don't join back to my original df.</p>
",2018-02-23 19:01:44,2018-02-23 19:49:19,Gensim Doc2Vec Access Vectors by Document Author,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15882,49048758,2018-03-01 11:29:00,,"<p>I am doing text classification and plan to use word2vec word embeddings and pass it to Conv1D layers for text classification. I have a <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataframe</a> which contains the texts and corresponding labels(sentiments). I have used the gensim module and used word2vec algorithm to generate the word-embedding model. The code I used:</p>

<pre><code>import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']
df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)
model = Word2Vec(df_tokenized, min_count=1)
</code></pre>

<p>I plan to use CNN and use this word-embedding model. But how should I use this word-embedding model for my cnn? What should be my input?</p>

<p>I plan to use something like(obviously not with the same hyper-parameters):</p>

<pre><code>model = Sequential()
model.add(layers.Embedding(max_features, 128, input_length=max_len))
model.add(layers.Conv1D(32, 7, activation='relu'))
model.add(layers.MaxPooling1D(5))
model.add(layers.Conv1D(32, 7, activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(1))
</code></pre>

<p>Can somebody help me out and point me in the right direction? Thanks in advance.</p>
",2018-03-01 15:16:15,2018-03-05 08:48:56,Word2vec with Conv1D for text classification confusion,<python><keras><conv-neural-network><word2vec><multiclass-classification>,,,CC BY-SA 3.0,True,False,True,False,False
15883,48994062,2018-02-26 17:37:19,,"<p>What I have achieved so far are models that can not be read by a person. I need to save the model as plain text to use it with a certain software, which requires that the model be this way.</p>

<p>I tried the following:</p>

<pre><code>model = models.doc2vec.Doc2Vec(size=300, min_count=0, alpha=0.025, min_alpha=0.025)
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)
model.save('mymodel.txt')
</code></pre>

<p>But I get:</p>

<pre><code>Process finished with exit code -1073741571 (0xC00000FD)
</code></pre>

<p>I do not know if I should pass a specific parameter.</p>
",,2018-02-26 18:01:22,Is there a way to save a Gensim doc2vec model as plain text (.txt)?,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15887,48998064,2018-02-26 22:22:03,,"<p>my text looks as follows:</p>

<pre><code>text=['paris', 'shares', 'concerns', 'ecb', 'language', 'eroding', 'status', 'currency', 'union', 
'diluting', 'legal', 'obligation', 'most', 'countries', 'join', 'ultimately', 'however', 'welcomes', 
'britain', 'support', 'more', 'integrated', 'eurozone', 'recognises', 'uk', 'euro', 'means', 
'obliged', 'choose', 'between', 'euro', 'pound', 'comment', 'article', 'moved', 'debates', 
'february', 'language', 'english', 'web']

from gensim.corpora.dictionary import Dictionary

dictionary=Dictionary(text) 
</code></pre>

<p>The error I'm getting:</p>

<blockquote>
  <p>TypeError: doc2bow expects an array of unicode tokens on input, not a
  single string</p>
</blockquote>

<p>I've tried to transform my text into a list of words to no avail. Also, I've tried to transform it to unicode to no avail. I'm no python expert just trying to analyse some text. My next step would be to check how often each token appears in the document called text. I'm using the ipython notebook. </p>
",2018-02-28 14:21:24,2018-02-28 14:21:24,"GENSIM: 'TypeError: doc2bow expects an array of unicode tokens on input, not a single string' when trying to create mapping for dictionary",<ipython><nltk>,,,CC BY-SA 3.0,True,False,True,False,False
15889,49088689,2018-03-03 20:05:07,,"<p>Does anyone know how to load a tsv file with embeddings generated from StarSpace into Gensim? Gensim documentation seems to use Word2Vec a lot and I couldn't find a pertinent answer.</p>

<p>Thanks,</p>

<p>Amulya</p>
",2018-03-04 06:13:37,2018-06-30 13:48:21,How to load embeddings (in tsv file) generated from StarSpace,<gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
15903,49142365,2018-03-07 00:46:23,,"<p>I have built a gensim Doc2vec model. Let's call it doc2vec. Now I want to find the most relevant words to a given document according to my doc2vec model. </p>

<p>For example, I have a document about ""java"" with the tag ""doc_about_java"". When I ask for similar documents, I get documents about other programming languages and topics related to java. So my document model works well.</p>

<p>Now I want to find the most relevant words to ""doc_about_java"".</p>

<p>I follow the solution from the closed question <a href=""https://stackoverflow.com/questions/46047506/how-to-find-most-similar-terms-words-of-a-document-in-doc2vec"">How to find most similar terms/words of a document in doc2vec?</a> and it gives me seemingly random words, the word ""java"" is not even among the first 100 similar words:</p>

<pre><code>docvec = doc2vec.docvecs['doc_about_java']
print doc2vec.most_similar(positive=[docvec], topn=100)
</code></pre>

<p>I also tried like this:</p>

<pre><code>print doc2vec.wv.similar_by_vector(doc2vec[""doc_about_java""])
</code></pre>

<p>but it didn't change anything. How can I find the most similar words to a given document?</p>
",2018-03-07 00:56:19,2018-03-19 18:35:21,How to get most similar words to a document in gensim doc2vec?,<word2vec><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15932,49109014,2018-03-05 11:03:48,,"<p>I am doing text classification in Keras. First, I am creating an embedding matrix with Word2Vec and passing it to Keras <code>Embedding</code> layer. Then I am running <code>Conv1D</code> on top of it. This is the <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataset</a> I am using. Here is my code below:</p>

<pre><code>from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D,GlobalMaxPooling1D
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import pandas as pd
from nltk.tokenize import word_tokenize


# def dataframe_to_list_of_words(df_name, col):
#   df = pd.read_csv(df_name)
#   lst = df[col].drop_duplicates().values.tolist()
#   tokenized_sents = [word_tokenize(i) for i in lst]
#   tokenized_sents_mod = [word for sublist in tokenized_sents for word in sublist]
#   return tokenized_sents_mod

# def convert_data_to_index(string_data, wv):
#     index_data = []
#     for word in string_data:
#         if word in wv:
#             index_data.append(wv.vocab[word].index)
#     return index_data

df=pd.read_csv('emotion_merged_dataset.csv')
texts=df['text']
labels=df['sentiment']

df_tokenized=df.apply(lambda row: word_tokenize(row['text']), axis=1)


model = Word2Vec(df_tokenized, min_count=1,size=300)
##############
embedding_matrix = np.zeros((len(model.wv.vocab), 300))
for i in range(len(model.wv.vocab)):
#     print(model.wv.index2word[i])
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
################
labels=df['sentiment']
encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
labels_encoded= np_utils.to_categorical(encoded_Y)
#########################

maxlen=30
tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
data = pad_sequences(sequences, maxlen=37)
############################
embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                      weights=[embedding_matrix],trainable=False)
model=Sequential()
model.add(embeddings)
model.add(Conv1D(32,7,activation='relu'))
model.add(MaxPooling1D(5))
model.add(Conv1D(32,7,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(labels_encoded.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(data, labels, validation_split=0.2, epochs=10, batch_size=100)
</code></pre>

<p>I am getting the following error when I am running my code:</p>

<pre><code>Error when checking target: expected dense_1 to have shape (None, 8) but got array with shape (19283, 1)
</code></pre>

<p>Can someone please help me?</p>
",2018-03-05 13:08:15,2018-03-05 13:08:15,Error while doing text-classification in Keras,<python><nlp><keras><conv-neural-network><word2vec>,,,CC BY-SA 3.0,True,False,True,False,True
15943,49128847,2018-03-06 10:41:27,,"<p>Suppose I have data to be trained in sentence_stream</p>

<pre><code>phrases = Phrases(sentence_stream)
bigram_model = Phraser(phrases)
</code></pre>

<p>Now, If I try my bigram_model on some test data and check the output</p>

<pre><code>sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>Now, Suppose If want to add custom bigrams like the_mayor in my bigram_model so that the output should contain</p>

<pre><code>[u'the_mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>Any suggestions on how to configure the bigram_model?</p>
",,2018-03-06 10:41:27,how to configure bigram model in gensim to include custom bigrams?,<python-3.x><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
15965,49205736,2018-03-10 05:37:03,,"<p>For my application I'm comparing the similarity of one document against all other documents because I want to find the most similar other documents. In Gensim this can be done efficiently using the <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">MatrixSimilarity method</a>. </p>

<p>In Spacy's <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">documentation</a> they have the example for comparing multiple documents, however for many documents the loop is not an efficient implementation:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

doc1 = nlp(u""The labrador barked."")
doc2 = nlp(u""The labrador swam."")
doc3 = nlp(u""the labrador people live in canada."")

for doc in [doc1, doc2, doc3]:
    labrador = doc[1]
    dog = nlp(u""dog"")
    print(labrador.similarity(dog))
</code></pre>

<p>If someone could please suggest an efficient way compare one document to all others in Spacy it would be much appreciated. </p>

<p>I believe it may involve using a <a href=""https://spacy.io/usage/processing-pipelines"" rel=""nofollow noreferrer"">pipeline</a>, but I'm not sure how to use those. </p>

<p>I'll note that the example above from the documentation seems to have an <a href=""https://github.com/explosion/spaCy/issues/1791"" rel=""nofollow noreferrer"">issue</a>, so any ideas for how get around that issue are also welcome. </p>
",2018-03-10 14:43:52,2018-03-11 09:37:22,In Spacy how can I efficiently compare the similarity of one document to all others?,<performance><gensim><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
15978,49155392,2018-03-07 15:19:24,,"<p>When I train Doc2vec (using Gensim's Doc2vec in Python) on corpus of about 10k documents (each has few hundred words) and then infer document vectors using the same documents, they are not at all similar to the trained document vectors. I would expect they would be at least somewhat similar.</p>

<p>That is I do <code>model.docvecs['some_doc_id']</code> and <code>model.infer_vector(documents['some_doc_id'])</code>.</p>

<p>Cosine distances between trained and inferred vectors for few first documents:</p>

<pre><code>0.38277733326
0.284007549286
0.286488652229
0.173178792
0.370117008686
0.275438070297
0.377647638321
0.171194493771
0.350615143776
0.311795353889
0.342757165432
</code></pre>

<p>As you can see, they are not really similar. If the similarity is so terrible even for documents used for training, I can't even begin to try to infer unseen documents.</p>

<p>Training configuration:</p>

<pre><code>model = Doc2Vec(documents=documents, dm=1, size=100, window=6, alpha=0.1, workers=4, 
seed=44, sample=1e-5, iter=15, hs=0, negative=8, dm_mean=1, min_alpha=0.01, min_count=2)
</code></pre>

<p>Inferring:</p>

<pre><code>model.infer_vector(tokens, steps=20, alpha=0.025)
</code></pre>

<p>Note on the side: Documents are always preprocessed the same way (I checked that the same list of tokens goes into training and into inferring).</p>

<p>Also I played with parameters around a bit, too, and results were similar. So if your suggestion would be something like ""try increasing or decreasing this or that training parameter"", I've most likely tried it. Maybe I just didn't come across the 'correct' parameters though.</p>

<p>Thanks for any suggestions as to what can I do to make it work better.</p>

<p>EDIT: I am willing and able to use any other available Python implementation of paragraph vectors (doc2vec). It doesn't have to be this one. If you know of another that can achieve better results.</p>

<p>EDIT: <strong>Minimal working example</strong></p>

<pre><code>import fnmatch
import os
from scipy.spatial.distance import cosine
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from keras.preprocessing.text import text_to_word_sequence

files = {}
folder = 'some path'  # each file contains few regular sentences
for f in fnmatch.filter(os.listdir(folder), '*.sent'):
    files[f] = open(folder + '/' + f, 'r', encoding=""UTF-8"").read()

documents = []
for k, v in files.items():
    words = text_to_word_sequence(v, lower=True)  # converts string to list of words, removes commas etc.
    documents.append(TaggedDocument(tags=[k], words=words))

d2 = Doc2Vec(size=200, documents=documents)

for doc in documents:
    trained = d2.docvecs[doc.tags[0]]
    inferred = d2.infer_vector(doc.words, steps=50)
    print(cosine(trained, inferred))  # cosine similarity from scipy
</code></pre>
",2018-03-08 16:41:54,2018-03-08 16:41:54,Gensim's Doc2vec - inferred vector isn't similar,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
15992,49229336,2018-03-12 06:34:35,,"<p>I am trying to train word embeddings(word2vec) on my own dataset using gensim library.</p>

<p><code>model = Word2Vec(sentences=alp[:20],size=100, window=6, min_count=5)
</code>
where alp is a list of list containing tokens of individual sentences in my corpus.</p>

<p>I get the following error whenever I try to train the w2v model.Please help.</p>

<pre><code>`Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 867, in worker_loop
    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 785, in _do_train_job
    tally += train_batch_cbow(self, sentences, alpha, work, neu1, 
self.compute_loss)
  File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow (./gensim/models/word2vec_inner.c:5642)
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`

`Exception in thread Thread-1:
    Traceback (most recent call last):
      File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
        self.run()
      File ""/usr/lib/python3.5/threading.py"", line 862, in run
        self._target(*self._args, **self._kwargs)
      File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 867, in worker_loop
        tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
      File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 785, in _do_train_job
        tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)
      File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow (./gensim/models/word2vec_inner.c:5642)
    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`
</code></pre>

<p>`</p>
",,2018-03-12 09:21:16,Trouble running gensim Word2Vec,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16010,49210010,2018-03-10 14:10:15,,"<p>I want to add new words into a trained gensim word2vec model using a new text dataset. However, I want to preserve the old word embeddings and just add the new words from the dataset into the existing model. This means simple retraining of the old model with the new text dataset isn't an option as it will readjust the vectors of the previous word embeddings that are also in the new text dataset. Can you give any suggestions regarding this task? I would like something like Gensim's doc2vec infer feature where you feed the model some text input and it gives a vector as an output. Thanks.</p>
",,2019-09-03 20:53:51,How to infer new word vectors from a gensim word2vec model?,<neural-network><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16036,49230376,2018-03-12 07:50:42,,"<p>I trained <code>LDA</code> model on my preprocessed corpus (i forget to save preprocessed data which was in  form of list of list) Is this possible to recover this data from trained model or not ?</p>
",2018-03-18 22:14:54,2018-03-18 22:14:54,is possible to extract bow from gensim lda model,<dataset><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16043,49183643,2018-03-08 22:36:26,,"<p>I'm working with a large dataset of Yelp reviews for a machine learning research project. Gensim has worked well so far, however, when I build the vocabulary with <code>doc2vec.build_vocab()</code> on the over 5,000,000 documents I have...the indices appear to all be collected into a 64-key dictionary (which should certainly not be the case).</p>

<p>Below is the script I made for tagging the documents, building the vocabulary, and training the model.</p>

<pre><code>import os
import time
import pandas as pd
import numpy as np
from collections import namedtuple
from gensim.models.doc2vec import Doc2Vec
from keras.preprocessing.text import text_to_word_sequence

# keras helper function
def text2_word_seq(review):
  return text_to_word_sequence(review, 
       filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', 
       lower=True, split="" "")

# instantiate the model
d2v = Doc2Vec(vector_size=300, 
  window=6, min_count=5, workers=os.cpu_count()-1)

chunksize = 5000
train_data = pd.read_json(""dataset/review.json"",
    chunksize=chunksize,
    lines=True)

Review = namedtuple('Review', 'words tags')
documents = list()
for i, data in enumerate(train_data):
    print(""Looked at %d chunks, %d documents"" % 
       (i, i*chunksize), end='\r', flush=True)
    users = data.user_id.values
    for j, review in enumerate(data.text):
        documents.append(Review(text2_word_seq(review), users[j]))

# build the vocabulary 
d2v.build_vocab(documents.__iter__(), update=False,
   progress_per=100000, keep_raw_vocab=False, trim_rule=None)

# train the model
d2v.train(documents, total_examples=len(documents), epochs=10)
d2v.save('d2v-model-v001')
</code></pre>

<p>After saving the model and loading it with <code>genim.models.Doc2Vec.load()</code>, the model's <code>docvecs.doctags</code> is of length 64. Each tag I am using when building the vocabulary is a user id. It is not necessarily unique, but there are thousands of users (not 64). Also, the tags appear as single characters - which is not expected...</p>

<pre><code>&gt;&gt;&gt; len(x.docvecs.doctags)
</code></pre>

<p>64</p>

<pre><code>&gt;&gt;&gt; x.docvecs.doctags

{'Y': Doctag(offset=27, word_count=195151634, doc_count=1727798), 
'j': Doctag(offset=47, word_count=198241878, doc_count=1739169), 
'4': Doctag(offset=17, word_count=195902251, doc_count=1728095), 
'J': Doctag(offset=50, word_count=197884244, doc_count=1741666), 
'W': Doctag(offset=41, word_count=198804200, doc_count=1741269), 
'O': Doctag(offset=23, word_count=196212468, doc_count=1728735), 
'o': Doctag(offset=9, word_count=194177928, doc_count=1709768), 
'n': Doctag(offset=3, word_count=193799059, doc_count=1714620), 
'3': Doctag(offset=34, word_count=197320036, doc_count=1725467), 
'F': Doctag(offset=10, word_count=195614702, doc_count=1729058) ...
</code></pre>

<p>What am I doing wrong here?  </p>
",,2018-03-09 01:16:54,gensim docvecs.doctags incorrect indices,<python><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16050,49202413,2018-03-09 21:29:55,,"<p>I set up an environment in anaconda for running gensim. it's been working great. today I updated gensim and some other packages in the environment. Now I get the following error in the terminal window. in my jupyter notebook, the kernel dies and it can't be restarted and when I try to import gensim it says module not found. I can't figure out where to start or what went wrong. I've been writing code for weeks and everything has been hunky-dory until i updated. any clues what to do?</p>

<p>Why will some code never be executed and what is ""image"" and why can't it find it?</p>

<p>Here is the complete terminal output for the failed session:</p>

<pre><code>Last login: Fri Mar  9 13:11:37 on ttys000
GWSB-FUN304-M1:~ dlhoffman$ /Users/dlhoffman/.anaconda/navigator/a.tool ; exit;
[I 13:11:41.634 NotebookApp] JupyterLab alpha preview extension loaded from /Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyterlab
JupyterLab v0.27.0
Known labextensions:
[I 13:11:41.636 NotebookApp] Running the core application with no additional extensions or settings
[I 13:11:41.640 NotebookApp] Serving notebooks from local directory: /Users/dlhoffman
[I 13:11:41.640 NotebookApp] 0 active kernels 
[I 13:11:41.640 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/?token=cb91dc32623736db4e7cf2baedee4284f4d3adc00cdf9f5c
[I 13:11:41.640 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 13:11:41.644 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=cb91dc32623736db4e7cf2baedee4284f4d3adc00cdf9f5c
[I 13:11:41.758 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[I 13:12:06.185 NotebookApp] Kernel started: 38a44a07-dfc6-4e61-8a69-df62ddc60d46
/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cffi/__pycache__/_cffi_ext.c:239:3: warning: 
      code will never be executed [-Wunreachable-code]
  _cffi_check__zmq_msg_t(0);
  ^~~~~~~~~~~~~~~~~~~~~~
/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cffi/__pycache__/_cffi_ext.c:272:3: warning: 
      code will never be executed [-Wunreachable-code]
  _cffi_check__zmq_pollitem_t(0);
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~
2 warnings generated.
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:09.186 NotebookApp] KernelRestarter: restarting kernel (1/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:12.194 NotebookApp] KernelRestarter: restarting kernel (2/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:15.202 NotebookApp] KernelRestarter: restarting kernel (3/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[W 13:12:16.348 NotebookApp] Timeout waiting for kernel_info reply from 38a44a07-dfc6-4e61-8a69-df62ddc60d46
[I 13:12:18.210 NotebookApp] KernelRestarter: restarting kernel (4/5)
kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 restarted
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[W 13:12:21.219 NotebookApp] KernelRestarter: restart failed
[W 13:12:21.219 NotebookApp] Kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 died, removing from map.
kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 restarted failed!
[W 13:12:21.232 NotebookApp] Kernel deleted before session
[W 13:12:21.233 NotebookApp] 410 DELETE /api/sessions/b337ffd0-4c75-4d57-bb97-279e3a377e2d (::1) 1.80ms referer=http://localhost:8888/notebooks/Dropbox/_Tom%20and%20Donna/_Jupyter%20Donna/Donna%20pre-process%20the%20IFTTT%20data.ipynb
</code></pre>
",,2018-05-15 22:06:17,"updated environment in anaconda and now kernel dies and ""code will never be executed""",<python-3.x><image><kernel><anaconda><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16097,49274321,2018-03-14 09:43:55,,"<p>I am using Word2Vec for word embeddings. I want to project a word W on an axis which represent the similarity of the word W to two given word W1 and W2 in a way that we can see to which word W is more similiar to, like this:
<a href=""https://i.stack.imgur.com/3NpB5.jpg"" rel=""nofollow noreferrer"">image</a></p>

<p>What is the best way to do it?</p>
",2018-03-14 13:13:09,2018-03-14 13:13:09,Project a word on an axis of similarity between 2 words,<vector><data-visualization><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16109,49262453,2018-03-13 17:35:03,,"<p>Is it possible to leverage the pretrained model e.g. GLOVE and use it to further train a corpus. </p>

<p>Any example will be very helpful.</p>
",,2018-03-13 17:35:03,Use pretrained models to further train current corpus,<nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16118,49279990,2018-03-14 14:14:05,,"<p>I am trying to use gensim through IronPython, but installing this package seems to be impossible. I tried it with IronPython 2.7.7. with the following commands:</p>

<pre><code>ipy -X:Frames -m install ensurepip
ipy -X:Frames -m pip install setuptools
ipy -X:Frames -m pip install gensim
</code></pre>

<p>Does anyone know if it is possible to use the gensim-package in IronPython? Otherwise, I have to switch a part of my .NET application over to CPython, where I have gensim working.</p>

<p>Thanks in advance!</p>
",2018-03-14 14:18:10,2018-03-14 14:18:10,How to install gensim for IronPython?,<c#><python><pip><ironpython><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16127,49313542,2018-03-16 05:05:14,,"<p>I have trained LDA model on 2000 URL's(containing articles) on a particular topic in Python3. Can we predict new corpus based on the trained model?</p>
",2018-03-16 05:16:29,2019-12-08 13:31:52,Can we predict new corpus using trained LDA model?,<python-3.6><gensim><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
16133,49414962,2018-03-21 19:23:05,,"<p>I am attempting to train an encoding for a text using gensim. I run the text over 5000 iterations (Document is ~250,000 words long, and Gensim is training over ~7500 frequent words), and the summed training loss for each iteration reduces from ~800,000 to 4,000 in that time. That said, when plotted, it is clear that training ended early and that further training would reduce the loss. However, when I load back in the saved model and run training:</p>

<pre><code>model = Word2Vec.load(""encodings"")
model.train(lines, total_examples=model.corpus_count, epochs=model.iter, compute_loss=True, callbacks=[logLoss, saveModel])
</code></pre>

<p>Training appears to start over from scratch with the initial training error ~800,000 and reduces back down to 4,000. I am saving the model with <code>model.save(""encodings"")</code>, and the saved model appears to be at least partially trained as word vector similarities seem somewhat reasonable. I want to further train my encoding, but this isn't working. (Note, initially training my encoding for more time ie. <code>iter=10000</code> doesn't improve my loss (it starts at 800,000 and ends at 4,000). Reducing the initial alpha or min_alpha over this extended period does not help either. Initial training is run with:</p>

<pre><code>model = Word2Vec(lines, min_count=2, size=300, workers=8, sg=1, iter=5000, compute_loss=True, callbacks=[logLoss, saveModel])
</code></pre>
",,2018-03-21 19:23:05,Gensim resume training starting training from scratch,<gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16164,49380138,2018-03-20 09:11:53,,"<p>I made a word embedding with this code:</p>

<pre><code>with open(""text.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)
    model = Word2Vec(sentences,workers=4, window=5)
</code></pre>

<p>I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between <code>model[""word""]</code>,<code>model.wv.most_similar()</code>, <code>model.similar_by_vector()</code> and <code>model.similarity()</code>?
Which one should I use?</p>
",2018-05-30 10:51:19,2018-05-30 10:51:19,Word2Vec Python similarity,<python><similarity><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16165,49380258,2018-03-20 09:17:00,,"<p>I tried doing text clustering using LDA, but it isn't giving me distinct clusters. Below is my code</p>

<pre><code>#Import libraries
from gensim import corpora, models
import pandas as pd
from gensim.parsing.preprocessing import STOPWORDS
from itertools import chain

#stop words
stoplist = list(STOPWORDS)
new = ['education','certification','certificate','certified']
stoplist.extend(new)
stoplist.sort()

#read data
dat = pd.read_csv('D:\data_800k.csv',encoding='latin').Certi.tolist()
#remove stop words
texts = [[word for word in document.lower().split() if word not in stoplist] for document in dat]
#dictionary
dictionary = corpora.Dictionary(texts)
#corpus
corpus = [dictionary.doc2bow(text) for text in texts]
#train model
lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=25, workers=4,minimum_probability=0)
#print topics
lda.print_topics(num_topics=25, num_words=7)
#get corpus
lda_corpus = lda[corpus]
#calculate cutoff score
scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus]]))


#threshold
threshold = sum(scores)/len(scores)
threshold
**0.039999999971137644**

#cluster1
cluster1 = [j for i,j in zip(lda_corpus,dat) if i[0][1] &gt; threshold]

#cluster2
cluster2 = [j for i,j in zip(lda_corpus,dat) if i[1][1] &gt; threshold]
</code></pre>

<p>The problem is there are overlapping elements in cluster1, which tend to be present in cluster2 and so on.</p>

<p>I also tried to increase threshold manually to 0.5, however it is giving me the same issue</p>
",2018-03-21 11:17:54,2018-03-21 11:17:54,Inefficiency of topic modelling for text clustering,<python><cluster-analysis><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
16171,49402113,2018-03-21 09:08:39,,"<p>I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_1</code>; in the second, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_2</code>. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:</p>

<pre><code>import os
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

with open(""HarryPotter1.txt"", 'r') as original, \
        open(""HarryPotter1_1.txt"", 'w') as mod1, \
        open(""HarryPotter1_2.txt"", 'w') as mod2:

    data=original.read()
    data_1 = data.replace(""Hermione"", 'Hermione_1')
    data_2 = data.replace(""Hermione"", 'Hermione_2')
    mod1.write(data_1 + r""\n"")
    mod2.write(data_2 + r""\n"")

with open(""longText.txt"",'w') as longFile:
    with open(""HarryPotter1_1.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)
    with open(""HarryPotter1_2.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)


model = """"
word_vectors = """"
modelName = ""ModelTest""
vectorName = ""WordVectorsTestst""

answer2 = raw_input(""Overwrite  embeddig? (yes or n)"")
if(answer2 == 'yes'):
    with open(""longText.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)

    model = Word2Vec(sentences,workers=4, window=5,min_count=5)

    model.save(modelName)
    model.wv.save_word2vec_format(vectorName+"".bin"",binary=True)
    model.wv.save_word2vec_format(vectorName+"".txt"", binary=False)
    model.wv.save(vectorName)

    word_vectors = model.wv

else:
    model = Word2Vec.load(modelName)
    word_vectors = KeyedVectors.load_word2vec_format(vectorName + "".bin"", binary=True)

    print(model.wv.similarity(""Hermione_1"",""Hermione_2""))
    print(model.wv.distance(""Hermione_1"",""Hermione_2""))
    print(model.wv.most_similar(""Hermione_1""))
    print(model.wv.most_similar(""Hermione_2""))
</code></pre>

<p>How is possible that <code>model.wv.most_similar(""Hermione_1"")</code> and <code>model.wv.most_similar(""Hermione_2"")</code> give me different output? 
Their neighbour are completely different. This is the output of the four print:</p>

<pre><code>0.00799602753634
0.992003972464
[('moments,', 0.3204237222671509), ('rose;', 0.3189219534397125), ('Peering', 0.3185565173625946), ('Express,', 0.31800806522369385), ('no...', 0.31678506731987), ('pushing', 0.3131707012653351), ('triumph,', 0.3116190731525421), ('no', 0.29974159598350525), ('them?""', 0.2927379012107849), ('first.', 0.29270970821380615)]
[('go?', 0.45812922716140747), ('magical', 0.35565727949142456), ('Spells.""', 0.3554503619670868), ('Scabbets', 0.34701400995254517), ('cupboard.""', 0.33982667326927185), ('dreadlocks', 0.3325180113315582), ('sickening', 0.32789379358291626), ('First,', 0.3245708644390106), ('met', 0.3223033547401428), ('built', 0.3218075931072235)]
</code></pre>
",2018-03-21 09:21:47,2018-03-21 09:51:06,Gensim Word2Vec most similar different result python,<python><string><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16201,49478142,2018-03-25 16:30:27,,"<p>I have trained and saved a model with doc2vec in colab as</p>

<pre><code>model = gensim.models.Doc2Vec(vector_size=size_of_vector, window=10, min_count=5, workers=16,alpha=0.025, min_alpha=0.025, epochs=40)
model.build_vocab(allXs)
model.train(allXs, epochs=model.epochs, total_examples=model.corpus_count)
</code></pre>

<p>The model is saved in a folder not accessible from my drive but which I can see as:</p>

<pre><code>from os import listdir
from os.path import isfile, getsize
from operator import itemgetter

files = [(f, getsize(f)) for f in listdir('.') if isfile(f)]
files.sort(key=itemgetter(1), reverse=True)

for f, size in files:
    print ('{} {}'.format(size, f))
print ('({} files {} total size)'.format(len(files), sum(f[1] for f in files)))
</code></pre>

<p>The output is:</p>

<pre><code>79434928 Model_after_train.docvecs.vectors_docs.npy
9155086 Model_after_train
1024 .rnd
(3 files 88591038 total size)
</code></pre>

<p>To move the two files in the same shared directory as the notebook</p>

<pre><code>folder_id = FolderID

for f, size in files:
  if 'our_first_lda' in f:  
    file = drive.CreateFile({'parents':[{u'id': folder_id}]})
    file.SetContentFile(f)
    file.Upload()
</code></pre>

<p>The problem that I am facing now are two:
1) gensim creates two files when saving the model. Which one should I load?</p>

<p>2) when I try to load a file or the other with:</p>

<pre><code>from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from googleapiclient.discovery import build
drive_service = build('drive', 'v3')

file_id = FileID


import io
from googleapiclient.http import MediaIoBaseDownload

request = drive_service.files().get_media(fileId=file_id)
downloaded = io.BytesIO()
downloader = MediaIoBaseDownload(downloaded, request)
done = False
while done is False:
  _, done = downloader.next_chunk()
model = doc2vec.Doc2Vec.load(downloaded.read())
</code></pre>

<p>I am not able to load the model getting the error:</p>

<pre><code>TypeError: file() argument 1 must be encoded string without null bytes, not str
</code></pre>

<p>Any suggestion?</p>
",,2018-03-26 05:24:35,Load a saved Doc2Vec model in Colab,<python><gensim><google-colaboratory><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16204,49384123,2018-03-20 12:22:12,,"<p>I have changed the code in word2vec.py to this:</p>

<pre><code>\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\word2vec.py in reset_weights(self, hs, negative, wv)
   1417         for i in xrange(len(wv.vocab)):
   1418             # construct deterministic seed from word AND seed argument
-&gt; 1419             wv.vectors[i] = self.seeded_vector( """".join(str(wv.index2word[i]),str(self.seed)), wv.vector_size)
   1420         if hs:
   1421             self.syn1 = zeros((len(wv.vocab), self.layer1_size), dtype=REAL)
</code></pre>

<p>But I receive the error:</p>

<pre><code>TypeError: unsupported operand type(s) for +: 'int' and 'str'
</code></pre>

<p>On line 1419, where the arrow is pointed. I don't understand how the operand is concatenating an int and str when both are within a str() method?</p>

<p>I am using Anaconda environment if thats matters in python3</p>
",,2018-09-07 07:12:05,Gensim 3.4.0 word2vec unsupported operand type(s) for +: 'int' and 'str',<python><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16213,49424470,2018-03-22 09:05:12,,"<p>I am trying to implement a Cosine Similarity/Distance function similar to <code>gensim.similarity</code> function or  <code>cosine</code> function (from <code>scipy.spatial.distance</code>).
Here is the example:</p>

<pre><code>x = model.wv['best']
y = model.wv['amber']
print(cosine(x,y))
</code></pre>

<p>The result is '1.0', according to the <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html#scipy-spatial-distance-cosine"" rel=""nofollow noreferrer"">documentation</a> that means that a dot product is 0.</p>

<p>When I use <code>gensim.similarity</code> function I get a different result:</p>

<pre><code>print(model.wv.similarity('amber','best'))
0.0757624812318
</code></pre>

<p>From here I can find Distance by subtracting similarity value from 1.
I have tried to implement the Cosine Similarity by myself by writing the code above:</p>

<pre><code> cosine_similarity = np.dot(model.wv['best'], model.wv['amber'])/(np.linalg.norm(model.wv['best'])* np.linalg.norm(model.wv['amber']))
</code></pre>

<p>output:</p>

<pre><code>nan
/Users/art/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in float_scalars
</code></pre>

<p>When I do just this:</p>

<pre><code>amber = model.wv['amber']
best = model.wv['best']
res= np.dot(amber, best)
</code></pre>

<p>I get no warning message but <code>0</code> as a result.</p>

<p>Any idea whats wrong with my implementation and the <code>scipy</code> implementation of similarity?</p>
",,2018-03-22 09:05:12,Different Cosine Similarity functions give different results for Word2vec vectors,<python-3.x><scipy><word2vec><cosine-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
16214,49428431,2018-03-22 12:13:42,,"<p>I am using gensim word2vec to return the most similar text from a corpus matching the query text. For instance, here are some relevant lines of code that start things off:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('/users/myuser/method_approaches/google_news_requirements/GoogleNews-vectors-negative300.bin.gz', binary=True)
instance = WmdSimilarity(processed_set, model, num_best=10)
</code></pre>

<p>And then I have this very simple function which runs the instance when passed to the multiprocessor:</p>

<pre><code>def get_most_similar_for_a_given_text(instance,text,output):
    i=instance[text]
    output.put(i)
</code></pre>

<p>And then I have a batch multiprocessing script</p>

<pre><code>def get_most_similar_for_all_texts_in_set(processed_set, instance):
    output = mp.Queue()
    # Setup a list of processes that we want to run
    processes = [mp.Process(target=get_most_similar_for_a_given_text, args=(instance, text, output)) for text in processed_set]
    num_cores = mp.cpu_count()
    Scaling_factor_batch_jobs = 3
    number_of_jobs = len(processes)
    num_jobs_per_batch = num_cores * Scaling_factor_batch_jobs
    num_of_batches = int(number_of_jobs // num_jobs_per_batch)+1 
    print('\n'+'Running batches now...')
    for i in tqdm.tqdm(range(num_of_batches)):
        # although the floor/ceilings look like things are getting double counted, for instance with ranges being 0:24,24:48,48.. etc.. this is not the case, for whatever reason it doesn't work like that
        if i&lt;num_of_batches-1: # true for all but last one
            floor_job = int(i * num_jobs_per_batch) # int because otherwise it's a float and mp doesn't like that
            ceil_job  = int(floor_job + num_jobs_per_batch)
            # Run processes
            for p in processes[floor_job : ceil_job]:
                p.start()
            for p in processes[floor_job : ceil_job]:
                p.join()
            for p in mp.active_children():
                p.terminate()
            print(floor_job,ceil_job)
        else: # true on last job, which picks up the missing batches that were lost due to rounding in the num_of_batches/num_jobs_per_batch formulas
            floor_job = int(i * num_jobs_per_batch)
            # Run processes
            for p in processes[floor_job:]:
                p.start()
            for p in processes[floor_job:]:
                p.join()
            for p in mp.active_children():
                p.terminate()
            print(floor_job,len(processes))
    # Get process results from the output queue
    results = [output.get() for p in tqdm.tqdm(processes)]
    np.save('/users/josh.flori/method_approaches/numpy_files/wmd_results_list.npy', results)
    return results
</code></pre>

<p>What actually happens when I run this is that it runs batches 1:4 fine. Those batches account for texts 0:96 in processed_set, which is the text that I am looping through. But then it gets to the 5th batch, of texts 96:120, it appears to simply stop processing but does not fail or quit or crash or do anything. Visually, it looks like it is still running but it's not as my cpu activity goes back down and the progress bar stops moving.</p>

<p>I visually inspected texts 96:120 from processed_set and nothing looked weird. I then ran the get_most_similar_for_a_given_text function on those texts in isolation, outside of the multiprocessing function, and they completed just fine.</p>

<p>Anyway, to reiterate, it always happens at batch 5. Does anyone have any insight here? I am not very familiar with how multiprocessing works.</p>

<p>Thanks again</p>
",,2018-03-22 14:48:08,multiprocessing batches suddenly halts in python,<python><multiprocessing><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16217,49388929,2018-03-20 16:02:47,,"<p>I am new to stackoverflow and python so please bear with me.
I am trying to run an Latent Dirichlet Analysis on a text corpora with the gensim package in python using PyCharm editor. I prepared the corpora in R and exported it to a csv file using this R command:</p>
<pre><code>write.csv(testdf, &quot;C://...//test.csv&quot;, fileEncoding = &quot;utf-8&quot;) 
</code></pre>
<p>Which creates the following csv structure (though with much longer and already preprocessed texts):</p>
<pre><code>,&quot;datetimestamp&quot;,&quot;id&quot;,&quot;origin&quot;,&quot;text&quot;
1,&quot;1960-01-01&quot;,&quot;id_1&quot;,&quot;Newspaper1&quot;,&quot;Test text one&quot;
2,&quot;1960-01-02&quot;,&quot;id_2&quot;,&quot;Newspaper1&quot;,&quot;Another text&quot;
3,&quot;1960-01-03&quot;,&quot;id_3&quot;,&quot;Newspaper1&quot;,&quot;Yet another text&quot;
4,&quot;1960-01-04&quot;,&quot;id_4&quot;,&quot;Newspaper2&quot;,&quot;Four Five Six&quot;
5,&quot;1960-01-05&quot;,&quot;id_5&quot;,&quot;Newspaper2&quot;,&quot;Alpha Bravo Charly&quot;
6,&quot;1960-01-06&quot;,&quot;id_6&quot;,&quot;Newspaper2&quot;,&quot;Singing Dancing Laughing&quot;
</code></pre>
<p>I then try the following essential python code (based on the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">gensim tutorials</a>) to perform simple LDA analysis:</p>
<pre><code>import gensim
from gensim import corpora, models, similarities, parsing
import pandas as pd
from six import iteritems
import os
import pyLDAvis.gensim

class MyCorpus(object):
     def __iter__(self):
             for row in pd.read_csv('//mpifg.local/dfs/home/lu/Meine Daten/Imagined Futures and Greek State Bonds/Topic Modelling/Python/test.csv', index_col=False, header = 0 ,encoding='utf-8')['text']:
                 # assume there's one document per line, tokens separated by whitespace
                 yield dictionary.doc2bow(row.split())

if __name__ == '__main__':
    dictionary = corpora.Dictionary(row.split() for row in pd.read_csv(
        '//.../test.csv', index_col=False, encoding='utf-8')['text'])
    print(dictionary)
    dictionary.save(
        '//.../greekdict.dict')  # store the dictionary, for future reference

    ## create an mmCorpus
    corpora.MmCorpus.serialize('//.../greekcorpus.mm', MyCorpus())
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    dictionary = corpora.Dictionary.load('//.../greekdict.dict')
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    # train model
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, iterations=1000)
</code></pre>
<p>I get the following error codes and the code exits:</p>
<blockquote>
<p>...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:832: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2736: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2914: DeprecationWarning: invalid escape sequence \g</p>
<p>\...\Python\venv\lib\site-packages\pyLDAvis_prepare.py:387:
DeprecationWarning:
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing</p>
</blockquote>
<p>I cannot find any solution and to be honest neither have any clue where exactly the problem comes from. I spent hours making sure that the encoding of the csv is utf-8 and exported (from R) and imported (in python) correctly.</p>
<p>What am I doing wrong or where else could I look at? Cheers!</p>
",2020-06-20 09:12:55,2018-03-21 09:33:25,"Python LDA gensim ""DeprecationWarning: invalid escape sequence""",<r><python-3.x><export-to-csv><gensim><deprecation-warning>,,,CC BY-SA 3.0,False,False,True,False,False
16222,49429971,2018-03-22 13:29:27,,"<p>I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: <em>xxxx.model.wv</em> and a bigger one <em>xxxx.model.wv.syn0.npy</em>.</p>

<p>When I call the following line:</p>

<pre><code>gensim.models.Word2Vec.load('xxxx.model.wv')
</code></pre>

<p>I get the following error:</p>

<pre><code>AttributeError: 'EuclideanKeyedVectors' object has no attribute 'negative'
</code></pre>

<p>How can I solve this error?</p>
",2018-03-22 15:07:41,2020-09-15 01:42:30,How can I load Word2vec with Gensim without getting an AttributeError?,<python><word2vec><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16224,49431270,2018-03-22 14:29:51,,"<p>When building a python gensim word2vec <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">model</a>, is there a way to see a doc-to-word matrix?</p>

<p>With input of <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code> I'd see something like*:</p>

<pre><code>      first  second  sentence
doc0    1       0        1
doc1    0       1        1
</code></pre>

<p>*I've illustrated 'human readable', but I'm looking for a scipy (or other) matrix, indexed to <code>model.wv.index2word</code>.</p>

<p>And, can that be transformed into a word-to-word matrix (to see co-occurences)? Something like:</p>

<pre><code>          first  second  sentence
first       1       0        1
second      0       1        1  
sentence    1       1        2   
</code></pre>

<p>I've already implemented something like <a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">word-word co-occurrence matrix</a> using CountVectorizer. It works well. However, I'm already using gensim in my pipeline and speed/code simplicity matter for my use-case. </p>
",,2018-03-31 14:54:35,word co-occurrence matrix from gensim,<python><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16238,49410113,2018-03-21 15:16:47,,"<p>I have the following code and I made sure its extension and name are correct. However, I still get the error outputted as seen below.  </p>

<p>I did see another person asked a similar question here on Stack Overflow, and read the answer but it did not help me.</p>

<p><a href=""https://stackoverflow.com/questions/44045881/failed-to-load-a-bin-gz-pre-trained-words2vecx"">Failed to load a .bin.gz pre trained words2vecx</a></p>

<p>Any suggestions how to fix this?</p>

<p>Input:</p>

<pre><code>import gensim
word2vec_path = ""GoogleNews-vectors-negative300.bin.gz""
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
</code></pre>

<p>Output:</p>

<pre><code>OSError: Not a gzipped file (b've')
</code></pre>
",,2020-06-19 12:49:05,OSError: Not a gzipped file (b've') python,<python><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16245,49339461,2018-03-17 16:53:25,,"<p>I want to do word_embedding using LDA to represent each of documents in my corpus with a vector that each dimension shows one of the detected topics by LDA model, but I don't know how to do it. Any suggestion will be appreciated.
I use python 3.6 and gensim library for LDA.</p>
",,2018-03-17 16:53:25,Using LDA for word embedding,<machine-learning><gensim><lda><topic-modeling><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16258,49391597,2018-03-20 18:22:22,,"<p>The command model.most_similar(positive=['france'], topn=100) gives the top 100 most similar words to ""france"". However, I would like to know if there is a method which will output the most similar words above a similarity threshold to a given word. Is there a method like the following?:
model.most_similar(positive=['france'], threshold=0.9)</p>
",2018-03-21 00:59:14,2019-11-26 12:43:27,Applying word2vec to find all words above a similarity threshold,<word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16271,49470302,2018-03-24 21:49:25,,"<p>I want to build a model that can classification news into specific categorize. As i imagine that i will put all the selected train paper into specific label category then you word2vec for training and generate model?. I wonder does it possible?. 
I have try some small example to build vocab in gensim but it keep telling me that word doesn't exist in vocab.. I'm so confuse.</p>

<pre><code> randomTxt = 'loop is good. loop infinity is not good. they are good at some point.'
x = randomTxt.split() #This finds words in the document
a = Counter(x)
print x
w1 = 'so'
model1 = Word2Vec(randomTxt,min_count=0) 
print model1.wv['loop']
</code></pre>

<p>I wonder if anyone have idea or know how to build from the beginning dataset can help me with this ? Or maybe some documentation is good.
I have read this docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a>
but as i follow like above, it keep telling me loop doesn't exist in vocabulary word2vec build.</p>
",,2018-03-24 21:49:25,Documentation topic classification using word2vec,<python><classification><text-mining><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16272,49471037,2018-03-24 23:30:57,,"<p>I'm training a word2vec model on a corpus and then querying the model. </p>

<p>This works fine, but I am running an experiment and need to call the model for different conditions, save the model for each condition, query the model for each condition, and then save the output from the queries into a csv file, say, for further analyses of all the conditions.  </p>

<p>I've studied the gensim documentation and searched around, but can't figure out what to do. </p>

<p>I asked the gensim folks and they said that since the result of ""most_similar"" is a python object I can save it with pickle or save as txt, csv, whatever format I want. </p>

<p>Sounds great, but I don't have a clue how to start. Here's my code - could you help me ""fill in the blanks"" even with something simple that I can research further and expand on my own?</p>

<pre><code>#train the model
trained_model = gensim.models.Word2Vec(some hyperparamters)

#save the model in the format that is appropriate for querying by writing it to disk and call it stored_model
trained_model.save(some_filename)

#read in the stored model from disk and call it retrieved_model
retrieved_model = gensim.models.Word2Vec.load(some_filename)

#query the retrieved model
#each of these queries produces a tuple of 10 'word', cosine similarity pairs
retrieved_model.wv.most_similar(positive=['smartthings', 'amazon'], negative=['samsung'])
retrieved_model.wv.most_similar(positive=['light', 'nest'], negative=['hue'])
retrieved_model.wv.most_similar(positive=['shopping', 'new_york_times'], negative=['ebay'])
.
.
.
#store the results of all these queries in a csv so they can be analyzed.
?
</code></pre>
",2018-03-25 01:41:40,2020-07-30 09:51:14,save results of word2vec model query in a csv file?,<python><csv><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16293,49564330,2018-03-29 20:15:54,,"<p>I am currently working with 9600 documents and applying gensim LDA. For training part, the process seems to take forever to get the model. I've tried to use multicore function as well, but it seems not working. I ran whole almost 3-days and I still can not get the lda model. I've checked some features of my data and the codes. I read this question <a href=""https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing"">gensim LdaMulticore not multiprocessing?</a>, but still don't get the solutions.</p>

<pre><code>corpora.MmCorpus.serialize('corpus_whole.mm', corpus)
corpus = gensim.corpora.MmCorpus('corpus_whole.mm')
dictionary = gensim.corpora.Dictionary.load('dictionary_whole.dict')

dictionary.num_pos
12796870

print(corpus)
MmCorpus(5275227 documents, 44 features, 11446976 non-zero entries)

# lda model training codes
lda = models.LdaModel(corpus, num_topics=45, id2word=dictionary,\
 update_every=5, chunksize=10000,  passes=100)

ldanulti = models.LdaMulticore(corpus, num_topics=45, id2word=dictionary,\
                            chunksize=10000, passes=100, workers=3)
</code></pre>

<p>This is my config to check BLAS, which I am not sure I installed proper one.
One thing I struggled here is, I can not use the command apt-get to install packages on my mac. I've installed Xcode but it still gives me an error. </p>

<pre><code>python -c 'import scipy; scipy.show_config()'
lapack_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
lapack_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
</code></pre>

<p>I have poor understanding on how to use shardedcorpus in python with my dictionary and corpora, so any helps will be appreciated! I haven't slept for 3 days to figure this problem!! Thanks!!</p>
",,2018-03-30 14:28:54,Extremely slow LDA training model with large corpora python gensim,<python><machine-learning><multiprocessing><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
16307,49514111,2018-03-27 13:24:37,,"<p>I have trained an LDA model using gensim library and I am using it to extract topic vectors of a document and I am using the following code</p>

<pre><code>def clean_doc(data_string):    
    global en_stop
    tokenizer = RegexpTokenizer(r'\w+') #Create appropriate tokenizer
    p_stemmer = PorterStemmer() #Create object from Porter Stemmer
    #clean and tokenize document string
    raw = data_string.lower()
    tokens = tokenizer.tokenize(raw)
    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    return stemmed_tokens

def infer_lda_vector(s, dictionary, model, dimensions):
    #s = s.decode('utf-8')
    vector = [0.0]*dimensions
    s = clean_doc(s)
    bow_vector = dictionary.doc2bow(s)   
    lda_vector = model[bow_vector]            
    for i in lda_vector:
        vector[i[0]] = i[1]
    return vector
</code></pre>

<p>I call it as follows:</p>

<pre><code>text = ""this a test""
lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
</code></pre>

<p>This exact piece of code was working when I was using Python2.7 but when I updated my system to Python3.x, its throwing the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-36-723f03d03620&gt; in &lt;module&gt;()
      1 text = ""this a a test""
----&gt; 2 lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
      3 lda_vector

&lt;ipython-input-34-885205b68d9e&gt; in infer_lda_vector(s, dictionary, model, dimensions)
     34     s = clean_doc(s)
     35     bow_vector = dictionary.doc2bow(s)
---&gt; 36     lda_vector = model[bow_vector]
     37     for i in lda_vector:
     38         vector[i[0]] = i[1]

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1158             `(topic_id, topic_probability)` 2-tuples.
   1159         """"""
-&gt; 1160         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1161 
   1162     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    979         if minimum_probability is None:
    980             minimum_probability = self.minimum_probability
--&gt; 981         minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output
    982 
    983         if minimum_phi_value is None:

TypeError: '&gt;' not supported between instances of 'float' and 'NoneType'
</code></pre>

<p>What am I doing wrong?</p>
",2018-03-27 13:39:46,2018-03-28 06:58:53,TypeError: '>' not supported between instances of 'float' and 'NoneType',<python><gensim><lda>,,,CC BY-SA 3.0,False,False,True,False,False
16312,49526981,2018-03-28 05:48:00,,"<p>I am using the below gibberish review data to train a doc2vec model in gensim. I face 2 errors.</p>

<p>1st : TaggedDocument takes 2 argument, I am unable to pass the <code>Sr</code> field as the 2nd argument so I resort to simple character<code>('tag')</code> in order to proceed further.</p>

<p>2nd: When I reach near the end of the code into for loop I get the following error.</p>

<p><strong><em>ValueError</strong>: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count.</em></p>

<pre><code>| Sr   | review                                                     |
|------|------------------------------------------------------------|
| 123  | This is frustrating                                        |
| 456  | I am eating in a bowl and this is frustrating              |
| 678  | Summer has come and the weather is hot and I feel very hot |
| 1234 | When will winter come back I love the cool weather         |

import pandas as pd
import numpy as np
import gensim

file = pd.read_csv('/Users/test_text.csv')

file1 = [line.split() for line in file.review]

sent = [gensim.models.doc2vec.TaggedDocument(lines,'tag') for lines in file1]
model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025,min_count=1)  
model.build_vocab(sent)
for epoch in range(10):
        model.train(sent)
        model.alpha -= 0.002
        model.min_alpha = model.alpha 
</code></pre>
",,2018-03-30 11:22:15,Doc2vec in gensim using csv,<python><nlp><data-science><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16316,49350340,2018-03-18 16:37:29,,"<p>I'm currently working on a project where I'm trying to create a sentiment analysis of news articles from german news outlets (rougly 60.000 articles from 9 different sites) using word2vec.</p>

<p>My current approach is:</p>

<ul>
<li>dumping the articles' text into a file (one per news outlet) </li>
<li>feeding the resulting text corpus to the model (one per news outlet)  word vectors </li>
<li>using a data set of positive/negative german words with a weight between -1 and 1 indicating the sentiment to create a ""sentiment score"" for words in articles by using the similarity function that gensim.word2vec provides</li>
<li>creating an average score for a number of news articles that are not included in the training data</li>
</ul>

<p>Now the problem is that I am not sure which words I should select for the analysis. I could either use every word in the article or maybe just filter relevant words and then create the scores.</p>

<p>Do you think this a valid/good approach? Do you maybe have any idea what a better approach would be?</p>
",,2018-03-18 16:37:29,Sentiment analysis of news articles using word2vec,<python><sentiment-analysis><word2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16327,49527668,2018-03-28 06:36:26,,"<p>After converting a list of text documents to corpora dictionary and then converting it to a bag of words model using:</p>

<pre><code>dictionary = gensim.corpora.Dictionary(docs) # docs is a list of text documents
corpus = [dictionary.doc2bow(doc) for doc in docs]
</code></pre>

<p>We can find out the index value of particular words in the dictionary using:</p>

<pre><code>dictionary.doc2idx([""righteous"",""height""])
</code></pre>

<p>Is there any way to find the word stored in dictionary at particular index?</p>
",,2018-03-31 09:14:46,"Understanding how words are stored in dictionary of gensim corpus after using ""gensim.corpora.Dictionary(TEXT)""",<python><gensim><corpus>,,,CC BY-SA 3.0,False,False,True,False,False
16331,49585674,2018-03-31 08:07:33,,"<p>I will need a little help with diagnosing some problem I am experiencing with some text vector process. Actually, I am  trying apply doc2vec word embedding to obtain a vector for a classification task. After I run the code I get some errors which has been quite difficult to figure out, as I am pretty new. Below are the codes and the outputs</p>

<pre><code>    def constructLabeledSentences(data):
    sentences=[]
    for index, row in data.iteritems():
        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))
    return sentences

    x_raw_doc_sentences = constructLabeledSentences(x_raw_train['Text'])
x_raw_doc_model = Doc2Vec(min_count=5, window=5, vector_size=300, sample=0.001, negative=5, workers=4, epochs=10,seed=1)
x_raw_doc_model.build_vocab(x_raw_doc_sentences)
x_raw_doc_model.train(x_raw_doc_sentences, total_examples=x_raw_doc_model.corpus_count, epochs=x_raw_doc_model.epochs)
</code></pre>

<p>After running the model, I tried to extract the vectors with:</p>

<pre><code>x_raw_doc_train_arrays = np.zeros((x_raw_train.shape[0], 300))
for i in range (x_raw_train.shape[0]):
    x_raw_doc_train_arrays[i]=x_raw_doc_model.docvecs['Text_'+str(i)]
</code></pre>

<p>and this is the output i get:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-106-bc0222fef295&gt; in &lt;module&gt;()
      1 x_raw_doc_train_arrays = np.zeros((x_raw_train.shape[0], 300))
      2 for i in range (x_raw_train.shape[0]):
----&gt; 3     x_raw_doc_train_arrays[i]=x_raw_doc_model.docvecs['Text_'+str(i)]
      4 
      5 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in __getitem__(self, index)
   1197                 return self.vectors_docs[self._int_index(index, self.doctags, self.max_rawint)]
   1198             return vstack([self[i] for i in index])
-&gt; 1199         raise KeyError(""tag '%s' not seen in training corpus/invalid"" % index)
   1200 
   1201     def __contains__(self, index):

KeyError: ""tag 'Text_4' not seen in training corpus/invalid""
</code></pre>

<p>Is there anything I did wrong, or should be doing that I haven't?</p>
",2018-03-31 08:07:59,2018-03-31 16:39:21,tag 'Text_4' not seen in training corpus/invalid,<python><python-3.x><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16338,49532089,2018-03-28 10:27:08,,"<p>I'm using gensim to do a LDA topic modeling work.
My data was pretreated by some other people. He gave me two things.
the mmcorpus file(imported by <code>gensim.corpora.MmCorpus</code> function)
the dictionary file(imported by <code>gensim.corpora.Dictionary.load</code> function)
I created the LDA model successfully and adjusted the superparameter ALPHA from 0.5-1.5 and I drew a visualized chart like this:
<a href=""https://i.stack.imgur.com/zRgHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRgHA.png"" alt=""enter image description here""></a>
I was confused why there are several tall bars there. And I found some strange words like this:
<a href=""https://i.stack.imgur.com/4dTsQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4dTsQ.png"" alt=""enter image description here""></a>
Interestingly the letter ""b"" which I haven't seen before appears. The man who gave me the data said the  letter ""b"" may generated automatically when he converted the data into bytes type. He doesn't know how to erase the ""b""  neither do I. How can I delete the ""b"" when I just have the mmcorpus file and the dictionary file?
Please!</p>
",,2018-04-12 16:47:54,How to remove a word in LDA analysis by gensim,<python><text-mining><gensim><lda><stop-words>,,,CC BY-SA 3.0,False,False,True,False,False
16354,49643974,2018-04-04 06:10:35,,"<p>I want to perform text classification using word2vec.
I got vectors of words.</p>

<pre><code>ls = []
sentences = lines.split(""."")
for i in sentences:
    ls.append(i.split())
model = Word2Vec(ls, min_count=1, size = 4)
words = list(model.wv.vocab)
print(words)
vectors = []
for word in words:
    vectors.append(model[word].tolist())
data = np.array(vectors)
data
</code></pre>

<p>output:</p>

<pre><code>array([[ 0.00933912,  0.07960335, -0.04559333,  0.10600036],
       [ 0.10576613,  0.07267512, -0.10718666, -0.00804013],
       [ 0.09459028, -0.09901826, -0.07074171, -0.12022413],
       [-0.09893986,  0.01500741, -0.04796079, -0.04447284],
       [ 0.04403428, -0.07966098, -0.06460238, -0.07369237],
       [ 0.09352681, -0.03864434, -0.01743148,  0.11251986],.....])
</code></pre>

<p>How can i perform classification (product &amp; non product)?</p>
",,2020-10-03 09:56:02,How to do Text classification using word2vec,<python-3.x><word2vec><gensim><text-classification>,,,CC BY-SA 3.0,False,False,True,False,False
16358,49628691,2018-04-03 11:15:01,,"<p>I'm trying to load an already trained word2vec model downloaded from <a href=""http://hlt.isti.cnr.it/wordembeddings/"" rel=""nofollow noreferrer"">here</a> by using the following code, as suggested by the aforementioned website:</p>

<pre><code>from gensim.models import Word2Vec
model=Word2Vec.load('wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')
</code></pre>

<p>When I try to execute that code, I get the following error:</p>

<pre><code>UserWarning: detected Windows; aliasing chunkize to chunkize_serial
warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
Traceback (most recent call last):
File ""d:\DavideV\documents\visual studio 2017\Projects\tesi\tesi\tesi.py"", line 112, in &lt;module&gt;
model=Word2Vec.load('wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')
File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 979, in load
return load_old_word2vec(*args, **kwargs)
File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 155, in load_old_word2vec
'size': old_model.vector_size,
AttributeError: 'Word2Vec' object has no attribute 'vector_size'
</code></pre>

<p>I suppose that this is due to the fact that the model has probably been trained with a previous version of gensim, but I would prefer to avoid to retrain it.</p>

<p>How can I solve this problem? Thanks.</p>
",,2018-04-03 11:15:01,Gensim Word2Vec object has no attribute vector_size when loading file,<python-3.x><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16366,49681525,2018-04-05 21:06:37,,"<p>I have around a 1000 documents, and I have trained them using gensim's doc2vec class. I need all the 1000 docvecs from the model to perform a kmeans clstering. But the maximum docvecs I'm able to get is 10. Any idea, how to get all of them?. Below is my code snippet.</p>

<p>`</p>

<pre><code>tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()),tags=str(i)) for i, _d in enumerate(data)]

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

X= model.docvecs.doctag_syn0 

print(X)

true_k = 3
km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
km.fit(X)

clusters = km.labels_.tolist()
</code></pre>

<p>`</p>
",,2018-04-06 08:57:47,How to get all docvecs of a model,<python><k-means><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16376,49631758,2018-04-03 13:47:23,,"<p>I am struggling with Doc2Vec and I cannot see what I am doing wrong.
I have a text file with sentences. I want to know, for a given sentence, what is the closest sentence we can find in that file.</p>

<p>Here is the code for model creation:</p>

<pre><code>sentences = LabeledLineSentence(filename)

model = models.Doc2Vec(size=300, min_count=1, workers=4, window=5, alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
model.train(sentences, epochs=50, total_examples=model.corpus_count)
model.save(modelName)
</code></pre>

<p>For test purposes, here is my file:</p>

<pre><code>uduidhud duidihdd
dsfsdf sdf sddfv
dcv dfv dfvdf g fgbfgbfdgnb
i like dogs
sgfggggggggggggggggg ggfggg
</code></pre>

<p>And here is my test:</p>

<pre><code>test = ""i love dogs"".split()
print(model.docvecs.most_similar([model.infer_vector(test)]))
</code></pre>

<p>No matter what parameter for training, this should obviously tell me that the most similar sentence is the 4th one (SENT_3 or SENT_4, I don't know how their indexes work, but the sentence labels are this form). But here is the result:</p>

<pre><code>[('SENT_0', 0.15669342875480652),
 ('SENT_2', 0.0008485736325383186),
 ('SENT_4', -0.009077289141714573)]
</code></pre>

<p>What am I missing ? And if I try with the same sentence (I LIKE dogs), I have SENT_2, then 1 then 4... I really don't get it. And why such low numbers ? And when I run few times in a row with a load, I don't get the same results either.</p>

<p>Thanks for your help</p>
",2018-04-03 13:57:17,2018-04-03 17:22:25,Gensim Doc2Vec most_similar() method not working as expected,<python><nlp><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 3.0,False,False,True,False,False
16405,49635325,2018-04-03 16:50:21,,"<p>I would like to tag a list of documents by <code>Gensim TaggedDocument()</code>, and then pass these documents as in input of <code>Doc2Vec()</code>. </p>

<p>I have read the documentation about <code>TaggedDocument</code> <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">here</a>, but I don' t have understood what exactly are the parameters <code>words</code> and <code>tags</code>.</p>

<p>I have tried:</p>

<pre><code>texts = [[word for word in document.lower().split()]
          for document in X.values]

texts = [[token for token in text]
          for text in texts]

model = gensim.models.Doc2Vec(texts, vector_size=200)
model.train(texts, total_examples=len(texts), epochs=10)
</code></pre>

<p>But I get the error <code>'list' object has no attribute 'words'</code>.</p>
",,2018-04-03 17:10:18,How to properly tag a list of documenta by Gensim TaggedDocument(),<nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16410,49676060,2018-04-05 15:21:59,,"<p>I am creating a chatbot. So, i need word2vec file in binary format.
When i am loading bin file then i am getting this type of error.</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 975, in load
return super(Word2Vec, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 629, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 278, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 395, in load
obj = unpickle(fname)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 1302, in unpickle
return _pickle.load(f, encoding='latin1')_pickle.

UnpicklingError: invalid load key, '3'.
</code></pre>
",2018-04-05 15:39:30,2018-04-06 12:53:58,"UnpicklingError: invalid load key, '3'",<python-3.x><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16427,49710537,2018-04-07 18:21:06,,"<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>

<p>So my question is, how do I get the embedding weights loaded by gensim into the PyTorch embedding layer.</p>

<p>Thanks in Advance!</p>
",2018-08-10 12:26:52,2020-04-03 08:18:38,PyTorch / Gensim - How to load pre-trained word embeddings,<python><neural-network><pytorch><gensim><embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16432,49784513,2018-04-11 21:09:56,,"<p>I looked for all suggestion, where everyone says to break the string into tokens by split function. All that has been done already, but still it seems to have same error again and again.</p>

<pre><code>for r in words:
        if not r in stop_words:
            processed_txt+=str(str(ps.stem(r) + "" ""))
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(processed_txt)
    #print(tokens)
    dictionary = corpora.Dictionary(tokens)
    #corpus = [dictionary.doc2bow(text) for text in tokens]
    print(dictionary)
</code></pre>

<p>So now it gives below error.</p>

<pre><code>raise TypeError(""doc2bow expects an array of unicode tokens on input, not a 
single string"")
TypeError: doc2bow expects an array of unicode tokens on input, not a single 
string
</code></pre>

<p>and the output under ""tokens"" variable seems like this as below.</p>

<pre><code>['becom', 'effect', 'willingli', 'without', 'need', 'obtain', 'knowledg', 'other', 'obtain', 'acquir', 'must', 'testamentari','claim', 'ownership', 'task', 'establish', 'endow', 'recept', 'willing', 'willsend', 'anoth', 'given', 'efficaci', 'presuppos']
</code></pre>

<p>Please help.</p>
",,2018-04-11 21:09:56,"TypeError: doc2bow expects an array of unicode tokens on input, not a single string",<python><tokenize><gensim><corpus>,2018-04-13 09:56:11,,CC BY-SA 3.0,False,False,True,False,False
16470,49699065,2018-04-06 18:33:14,,"<p>I am recieving the following error: when calling gensim.models.Word2Vec() on a corpus object that is iteratable.</p>

<pre><code>File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 542, in __init__
        self.build_vocab(sentences, trim_rule=trim_rule)
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 621, in build_vocab
        self.finalize_vocab(update=update)  # build tables &amp; arrays
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 845, in finalize_vocab
        self.reset_weights()
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1270, in reset_weights
        self.wv.syn0[i] = self.seeded_vector(self.wv.index2word[i] + str(self.seed))
    TypeError: can only concatenate tuple (not ""str"") to tuple
</code></pre>

<p>The corpus is defined like so:</p>

<pre><code>class DataSet:
    """"""
    Holds the dataset and the methods associated with it
    """"""

    def __init__(self, dir, verbose, categories):
        self.dir = dir
        self.verbose = verbose
        self.dictionary = None
        self.categories = categories
        self.type = None


    @staticmethod
    def iter_documents():
        """"""
        Generator: iterate over all relevant documents
        :return: yields one document (=list of utf8 tokens) at a time
        """"""
        for root, dirs, files in os.walk(DIR_PROCESSED):
            for fname in filter(lambda fname: fname.endswith('.txt'), files):
                document = open(os.path.join(root, fname)).read()
                yield gensim.utils.tokenize(document, errors='ignore')

    def __iter__(self):
        """"""
        __iter__ is a generator =&gt; Dataset is a streamed iterable
        :return: sparse dictionary
        """"""
        for tokens in DataSet.iter_documents():
            yield self.dictionary.doc2bow(tokens)
</code></pre>

<p>With a subclass XMLDataset: that contains a static method</p>

<p>It is an instance of XMLDataset that is passed as a corpus to <code>gensim.models.Word2Vec()</code></p>

<p>What could the issue be?</p>

<p>EDIT:</p>

<p>Dictionary is updated like so:</p>

<pre><code>self.dictionary = gensim.corpora.Dictionary(DataSet.iter_documents())
</code></pre>

<p>Then gensim is called on the dataset:   </p>

<pre><code>corpus = DataSet() #roughly
model = gensim.models.Word2Vec(corpus, size=dim, window=5, workers=workers)  # mincount
</code></pre>

<p>If I were to iterate through corpus and print each element, I get the output needed for train the model, like so:</p>

<pre><code>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 3), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 2), (30, 1), (31, 2), (32, 2), (33, 2), (34, 5), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 2), (74, 1), (75, 2), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 3), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 3), (97, 2), (98, 1)]
</code></pre>
",2018-04-11 23:15:49,2018-04-11 23:15:49,"Gensim TypeError: can only concatenate tuple (not ""str"") to tuple",<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16471,49767270,2018-04-11 05:37:30,,"<p>I have a niche corpus of ~12k docs, and I want to test near-duplicate documents with similar meanings across it - think article about the same event covered by different news organisations. </p>

<p>I have tried gensim's Word2Vec, which gives me terrible similarity score(&lt;0.3) even when the test document is <em>within</em> the corpus, and I have tried SpaCy, which gives me >5k documents with similarity > 0.9. I tested SpaCy's most similar documents, and it was mostly useless.</p>

<p>This is the relevant code.                                                              </p>

<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=40)
doc = preprocess(query)
vec_bow = dictionary.doc2bow(doc)
vec_lsi_tfidf = lsi[tfidf[vec_bow]] # convert the query to LSI space
index = similarities.Similarity(corpus = corpus, num_features = len(dictionary), output_prefix = ""pqr"")
sims = index[vec_lsi_tfidf] # perform a similarity query against the corpus
most_similar = sorted(list(enumerate(sims)), key = lambda x:x[1])

for mid in most_similar[-100:]:
    print(mid, file_list[mid[0]])
</code></pre>

<p>Using gensim I have found a decent approach, with some preprocessing, but the similarity score is still quite low. Has anyone faced such a problem, and are there are some resources or suggestions that could be useful?</p>
",2018-04-11 05:44:36,2018-04-12 17:09:17,Document similarity in Spacy vs Word2Vec,<python-3.x><nlp><gensim><spacy>,,,CC BY-SA 3.0,False,True,True,False,False
16473,49768453,2018-04-11 06:58:06,,"<p>I have names of employees saved in a text file. I processed the file and compared a name that already exist.
When I checked using most_similar method, I found that it returns totally unrelated name even if the exact same name exist in the corpus.</p>

<pre><code>import gensim

training_file='todel.txt'
mylist=list()
with open(training_file, encoding=""iso-8859-1"") as f:
    for i, line in enumerate(f):
        mylist.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(mylist)

inferred_vector=model.infer_vector(['aakash', 'prakash', 'patel'])

sims = model.docvecs.most_similar([inferred_vector])

' '.join(mylist[sims[0][0]].words)
</code></pre>

<p>How do I correctly train the data to return (closely) matching names?</p>
",,2018-04-11 09:46:18,Using a model to compare name and surname,<machine-learning><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16478,49829787,2018-04-14 09:10:00,,"<p>I created a model from mongodb db news and I tagged the documents by mongo collection id</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
i=0
docs=[]
for artical in lstcontent:
    doct = TaggedDocument(clean_str(artical), [lstids[i]])
    docs.append(doct)
    i+=1
</code></pre>

<p>after that I created the model by</p>

<pre><code>pretrained_emb='tweet_cbow_300/tweets_cbow_300'
saved_path = ""documentmodel/doc2vec_model.bin""
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)
model.save(saved_path)
</code></pre>

<p>when I using the model by the code :</p>

<pre><code>import gensim.models as g
import codecs
model=""documentmodel/doc2vec_model.bin""
start_alpha=0.01
infer_epoch=1000
m = g.Doc2Vec.load(model)
sims = m.docvecs.most_similar(['5aa94578094b4051695eeb10'])
sims
</code></pre>

<p>the output is</p>

<pre><code>[('5aa944c1094b4051695eeaef', 0.9255372881889343),
('5aa945c1094b4051695eeb1d', 0.9222575426101685),
('5aa94584094b4051695eeb12', 0.9210859537124634),
('5aa945d2094b4051695eeb20', 0.9083569049835205),
('5aa945c7094b4051695eeb1e', 0.905883252620697),
('5aa9458f094b4051695eeb14', 0.9054019451141357),
('5aa944c7094b4051695eeaf0', 0.9019848108291626),
('5aa94589094b4051695eeb13', 0.9012798070907593),
('5aa945b1094b4051695eeb1a', 0.9000773429870605),
('5aa945bc094b4051695eeb1c', 0.8999895453453064)]
</code></pre>

<p>the ids not related with 5aa94578094b4051695eeb10
where is my proplem !?</p>
",,2018-04-14 23:14:25,gensim model return ids not related with input doc2vec,<word2vec><gensim><cosine-similarity><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16490,49811018,2018-04-13 06:56:51,,"<p>i have two separate data sets, one is resumes and the other is demands, using gensim doc2vec, i created models for each and i am able to query similar words in each data sets, but now, i need to merge these two models into one and query for resumes in demands and attain the similarity or matching between them. My data sets are in plain txt files in which the the two resumes or demands are separated by * . Please find my implementation below, any suggestions would be highly appreciated.
Thanks.</p>

<pre><code>import gensim
import os
import collections
import smart_open
import random


def read_corpus(fname, tokens_only=False):

    with open(fname) as f:
      i=0
      for  line in (f.read().split('&amp;&amp;')):
        if len(line)&gt;1:
            if tokens_only:
                yield gensim.utils.simple_preprocess(line)
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])
            i+=1    


vocabulary = read_corpus('D:\Demand.txt')
train_corpus = list(vocabulary)
print(train_corpus[:2])

model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
print(model.infer_vector(['trainings', 'certifications', 'analyst', 'unix', 'jdbc','testing']))
model.docvecs.most_similar(positive=[model.infer_vector(['spark', 'sqoop'])])
model.most_similar('unix')
</code></pre>
",2018-04-13 09:06:24,2018-04-13 09:06:24,matching between two separate documents using gensim doc2vec,<gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16502,49847602,2018-04-15 22:59:31,,"<p>I was wondering how can I limited the Google's Word2Vec to my vocabulary.
Google's Word2 vec link:<a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a></p>

<p>This is what I have:</p>

<pre><code>import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)

embedding_matrix = np.zeros((len(my_vocabulary), 300))
</code></pre>

<p>where my vocabulary is a list of unique words in my corpus. 
How can I feel the embedding matrix only for words in my_vocabulary?
In addition, I would like to have the flexibility that if my word does not exist in the Google's word2vec to be filled with zeros.</p>

<p>Thanks</p>
",,2018-04-16 09:24:37,Adjust Google's Word2Vec loaded with Gensim to your vocabulary and then create the embedding vector,<python><word2vec><gensim><embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16509,49867648,2018-04-16 23:29:15,,"<p>I am trying to use Gensim.phrases library to identify phrases in a text. </p>

<p>I used the following: </p>

<pre><code>bigram = models.Phrases(txt_to_words,min_count=min_count, threshold=threshold,common_terms=common_terms)
</code></pre>

<p>And I get the error: </p>

<pre><code>&lt;ipython-input-13-1c8b06a0b078&gt; in words_to_phrases(txt_to_words, min_count, threshold)
     33     common_terms=[""of"", ""with"", ""without"", ""and"", ""or"", ""the"", ""a"",""in"",""to"",""is"",""but""]
     34 
---&gt; 35     bigram = models.Phrases(txt_to_words,min_count=min_count, threshold=threshold,common_terms=common_terms)
     36 
     37     # trigram

TypeError: __init__() got an unexpected keyword argument 'common_terms'
</code></pre>

<p>I have the latest gensim package 2.0+ </p>

<p>Any idea why it is not recognizing the common_terms parameter?</p>
",,2018-04-17 00:04:43,Genism Phrase library not accepting common_terms,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16517,49750112,2018-04-10 09:30:19,,"<p>I have a text file with my precomputed word vectors in the following format (example):</p>

<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\n</code></p>

<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""noreferrer"">here</a>:</p>

<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>

<p>However this gives the following error:</p>

<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>

<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>
",,2018-04-10 09:40:47,Gensim: how to load precomputed word vectors from text file,<python><python-3.x><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16527,49761033,2018-04-10 18:53:50,,"<p>I seem to be getting all the correct results until the very last step. My array of results keeps coming back empty. </p>

<p>I'm trying to follow this tutorial to compare 6 sets of notes:</p>

<p><a href=""https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"" rel=""nofollow noreferrer"">https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python</a></p>

<p>I have this so far:</p>

<pre><code>#tokenize an array of all text
raw_docs = [Notes_0, Notes_1, Notes_2, Notes_3, Notes_4, Notes_5]
gen_docs = [[w.lower() for w in word_tokenize(text)]
           for text in raw_docs]

#create dictionary
dictionary_interactions = gensim.corpora.Dictionary(gen_docs)
print(""Number of words in dictionary: "", len(dictionary_interactions))
#create a corpus
corpus_interactions = [dictionary_interactions.doc2bow(gen_docs) for gen_docs in gen_docs]
len(corpus_interactions)
#convert to tf-idf model
tf_idf_interactions = gensim.models.TfidfModel(corpus_interactions)
#check for similarities between docs
sims_interactions = gensim.similarities.Similarity('C:/Users/JNproject', tf_idf_interactions[corpus_interactions],
                               num_features = len(dictionary_interactions))

print(sims_interactions)
print(type(sims_interactions))
</code></pre>

<p>with the output:</p>

<pre><code>Number of words in dictionary:  46364
Similarity index with 6 documents in 0 shards (stored under C:/Users/Jeremy Bice/JNprojects/Company/Interactions/sim_interactions)
&lt;class 'gensim.similarities.docsim.Similarity'&gt;
</code></pre>

<p>That seems right so I continue with this:</p>

<pre><code>query_doc = [w.lower() for w in word_tokenize(""client is"")]
print(query_doc)
query_doc_bow = dictionary_interactions.doc2bow(query_doc)
print(query_doc_bow)
query_doc_tf_idf = tf_idf_interactions[query_doc_bow]
print(query_doc_tf_idf)

#check for similarities between docs
sims_interactions[query_doc_tf_idf]
</code></pre>

<p>and my output is this:</p>

<pre><code>['client', 'is']
[(335, 1), (757, 1)]
[]
array([ 0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)
</code></pre>

<p>How do I get an output here?</p>
",,2018-04-17 12:19:20,gensim.similarities.docsim.Similarity returns empty when queried,<python-3.x><nltk><jupyter-notebook><gensim>,,,CC BY-SA 3.0,True,False,True,False,False
16541,49891527,2018-04-18 05:02:29,,"<p>I was having trouble with the ""most_similar"" call in a FastText model, from my understanding, Fasttext should be able to obtain results for words that aren't in the vocabulary, but I'm getting a ""Not in Vocabulary"" error, even when prior to saving and loading, the call was perfectly fine.</p>

<p>Here's the code from juypter.</p>

<pre><code>import gensim as gensim

model = gensim.models.FastText(my_sentences, size=100, window=5, min_count=3, workers=4, sg=1)
model.wv.most_similar(positive=['iPhone 6'])
</code></pre>

<p>Returns</p>

<pre><code>[('iPhone7', 0.942690372467041),
('iPhone7.', 0.9395840764045715),
('iPhone5s', 0.9379133582115173),
('iPhone6s', 0.9338586330413818),
('iPhone5S', 0.9335439801216125),
('iPhone5.', 0.9318809509277344),
('iPhone', 0.9314558506011963),
('iPhone6', 0.9268479347229004),
('iPhone4s', 0.9223971366882324),
('iPhone5', 0.9212019443511963)]
</code></pre>

<p>So far so good, now I save the model.</p>

<pre><code>model.wv.save_word2vec_format(""example_fasttext.txt"", binary=False)
</code></pre>

<p>Then load it up again:</p>

<pre><code>from gensim.models import KeyedVectors
new_model = KeyedVectors.load_word2vec_format('example_fasttext.txt', binary=False, limit=50000)
</code></pre>

<p>Then I do the exact most_similar call from the model I just loaded:</p>

<pre><code>new_model.most_similar(positive=['iPhone 6'])
</code></pre>

<p>But results now are:</p>

<pre><code>KeyError: ""word 'iPhone 6' not in vocabulary""
</code></pre>

<p>Any idea what I did wrong?</p>
",,2018-07-23 13:47:21,"Gensim FastText - KeyError: ""word not in vocabulary""",<gensim><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
16549,49800622,2018-04-12 15:36:01,,"<p>I am running the latest version of Python:</p>

<pre><code>'3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:14:23) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'
</code></pre>

<p>Upon trying to import gensim like so:</p>

<pre><code>from gensim.corpora import Dictionary
import numpy as np
</code></pre>

<p>I get the following error:</p>

<pre><code>/anaconda/lib/python3.6/site-packages/boto/provider.py in &lt;module&gt;()
     32 
     33 import boto
---&gt; 34 from boto import config
     35 from boto.compat import expanduser
     36 from boto.pyami.config import Config

ImportError: cannot import name 'config'
</code></pre>

<p>I have tried updating Python, all of the packages and their dependencies, and so on.  Nothing seems to be working.</p>

<p>Any thoughts?</p>
",,2019-02-28 18:37:08,Python 3.6: ImportError: cannot import name 'config' when trying to import gensim,<python><jupyter><boto><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16556,49926774,2018-04-19 17:19:54,,"<p>I am using python3.5 and trying to repeat the code implemented in this video moment:</p>

<p><a href=""https://youtu.be/BkeQzJt0f5A?t=73"" rel=""nofollow noreferrer"">https://youtu.be/BkeQzJt0f5A?t=73</a>
<a href=""https://youtu.be/BkeQzJt0f5A?t=73"" rel=""nofollow noreferrer""></a></p>

<p>In the piece of code below ""TypeError: slice indices must be integers or None or have an index method"" is happening:</p>

<pre><code>with open(""metadata.tsv"", ""w+"") as file_metadata:
    for i,word in enumerate(model.wv.index2word[:max]):
        w2v[i] = model.wv[word]
        file_metadata.write(word + ""\n"")
</code></pre>

<p>How can I fix this to get the output from w2v like the video?</p>
",2018-04-19 17:56:53,2018-04-19 18:06:58,"gensim word2vec model.wv.index2word ""TypeError: slice indices must be integers or None or have an __index__ method"" in enumerate",<python-3.x><word2vec><gensim><tensorboard><enumerate>,,,CC BY-SA 3.0,False,False,True,False,False
16558,49929066,2018-04-19 19:44:29,,"<p>I am looking for a good approach using python libraries to tackle the following problem:</p>

<p>I have a dataset with a column that has product description. The values in this column can be very messy and would have a lot of other words that are not related to the product. I want to know which rows are about the same product, so I would need to tag each description sentence with its main topics. For example, if I have the following: 
""500 units shoe green sport tennis import oversea plastic"", I would like the tags to be something like: ""shoe"", ""sport"". So I am looking to build an approach for semantic tagging of sentences, not part of speech tagging. Assume I don't have labeled (tagged) data for training. </p>

<p>Any help would be appreciated.</p>
",,2018-04-20 07:03:35,Clear approach for assigning semantic tags to each sentence (or short documents) in python,<python-2.7><nlp><nltk><gensim><semantic-analysis>,,,CC BY-SA 3.0,True,False,True,False,False
16561,49967931,2018-04-22 16:00:55,,"<p>I am trying to use <code>Doc2Vec</code> to convert sentences to vectors, then use those vectors to train a tensorflow classifier.</p>

<p>I am a little confused at what tags are used for, and how to extract all of the document vectors from <code>Doc2Vec</code> after it has finished training.</p>

<p>My code so far is as follows:</p>

<pre><code>fake_data = pd.read_csv('./sentences/fake.txt', sep='\n')
real_data = pd.read_csv('./sentences/real.txt', sep='\n')
sentences = []

for i, row in fake_data.iterrows():
    sentences.append(TaggedDocument(row['title'].lower().split(), ['fake', len(sentences)]))

for i, row in real_data.iterrows():
    sentences.append(TaggedDocument(row['title'].lower().split(), ['real', len(sentences)]))

model = gensim.models.Doc2Vec(sentences)
</code></pre>

<p>I get vectors when I do <code>print(model.docvecs[1])</code> etc, but they are different every time I remake the model.</p>

<p>First of all: have I used <code>Doc2Vec</code> correctly?
Second: Is there a way I can grab all documents tagged 'real' or 'fake', then turn them into a numpy array and pass it into tensorflow?</p>
",2018-04-23 17:07:30,2018-04-23 17:07:30,How to use vectors from Doc2Vec in Tensorflow,<python><tensorflow><nlp><word2vec><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16571,49858283,2018-04-16 13:16:19,,"<p>I am trying to add a FastText embedding layer to the famous text classification architecture with CNN: <a href=""https://github.com/dennybritz/cnn-text-classification-tf"" rel=""nofollow noreferrer"">https://github.com/dennybritz/cnn-text-classification-tf</a></p>

<p>I load my FastText embedding like this: </p>

<pre><code>embedding = np.load('embedding/my_embedding_file')
# type(embedding): gensim.models.fasttext.FastText
vocab = embedding.wv.vocab
vocab_processor.fit(vocab)
x_train = np.array(list(vocab_processor.transform(x_train)))
x_dev = np.array(list(vocab_processor.transform(x_dev)))
vocabulary_size = len(vocab) #300k
</code></pre>

<p>And below is how I add my embedding layer in Tensorflow: </p>

<pre><code># Embedding layer
with tf.device('/cpu:0'), tf.name_scope(""embedding""):
    embedding_weights_ = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_size]), trainable=False, name=""embedding_weights"")
    embedding_weights = tf.assign(embedding_weights_, self.embedding_placeholder)

    self.embedded_chars = tf.nn.embedding_lookup(embedding_weights, self.input_x)
    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1) 
# Create a convolution + maxpool layer for each filter size
pooled_outputs = []
for i, filter_size in enumerate(filter_sizes):
    with tf.name_scope(""conv-maxpool-%s"" % filter_size):
        # Convolution Layer
        filter_shape = [filter_size, embedding_size, 1, num_filters]
        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
        conv = tf.nn.conv2d(
            self.embedded_chars_expanded, #my embedding to the next layer
            W,
            strides=[1, 1, 1, 1],
            padding=""VALID"",
            name=""conv"")
</code></pre>

<p>Initialization of the CNN model:</p>

<pre><code>with tf.Graph().as_default():
    session_conf = tf.ConfigProto(
      allow_soft_placement=FLAGS.allow_soft_placement,
      log_device_placement=FLAGS.log_device_placement)
    sess = tf.Session(config=session_conf)
    with sess.as_default():
        cnn = TextCNN(
            sentence_length = max_document_length,
            num_classes=17,
            vocab_size=len(vocab_processor.vocabulary_),
            embedding_size=FLAGS.embedding_dim,
            filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),
            num_filters=FLAGS.num_filters,
            l2_reg_lambda=FLAGS.l2_reg_lambda)
</code></pre>

<p>Single training step: </p>

<pre><code>def train_step(x_batch, y_batch):
            feed_dict = {
              cnn.input_x: x_batch,
              cnn.input_y: y_batch,
              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,
              cnn.embedding_placeholder:embedding
            }
            _, step, summaries, loss, accuracy = sess.run(
                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
                feed_dict)
            time_str = datetime.datetime.now().isoformat()
            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))
            train_summary_writer.add_summary(summaries, step)
</code></pre>

<p>For the reference, below is how my x_train and y_train looks like: </p>

<p><a href=""https://i.stack.imgur.com/Gxb0v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gxb0v.png"" alt=""x_train and y_train""></a></p>

<p>When I start training, I get this error:</p>

<pre><code>  File ""&lt;ipython-input-24-2ec193e9123f&gt;"", line 122, in &lt;module&gt;
    train_step(x_batch, y_batch)

  File ""&lt;ipython-input-24-2ec193e9123f&gt;"", line 92, in train_step
    feed_dict)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1089, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\core\numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)

ValueError: setting an array element with a sequence.
</code></pre>

<p>This error seems like a numpy array creation error, where it is tried to create an array element as a sequence. 
BUT:
I don't receive any error when I remove the embedding while my train data is completely the same thus batches are as well. </p>

<p>What could cause this error?</p>
",2018-04-16 14:09:11,2018-04-16 14:09:11,Value error when adding a word embedding layer to the CNN model,<python><tensorflow><conv-neural-network><word-embedding><fasttext>,,,CC BY-SA 3.0,False,False,True,False,False
16583,49929170,2018-04-19 19:52:08,,"<p>The following line works fine:</p>

<pre><code>import gensim
</code></pre>

<p>while the following line generates error:</p>

<pre><code>import gensim.test.utils
</code></pre>

<p>Error:</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-98-49e2788ad1a0&gt; in &lt;module&gt;()
----&gt; 1 import gensim.test.utils

ImportError: No module named 'gensim.test.utils'
</code></pre>

<p>I was working on converting GloVe to word2vec format:
<a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">glove2word2vec</a></p>
",,2018-04-19 19:52:08,import gensim vs import gensim.test.utils,<python><gensim><word-embedding>,,,CC BY-SA 3.0,False,False,True,False,False
16604,50009030,2018-04-24 18:58:29,,"<p>What is the correct way to use gensim's Phrases and preprocess_string together ?, i am doing this way but it a little contrived.</p>

<pre><code>from gensim.models.phrases import Phrases
from gensim.parsing.preprocessing import preprocess_string
from gensim.parsing.preprocessing import strip_tags
from gensim.parsing.preprocessing import strip_short
from gensim.parsing.preprocessing import strip_multiple_whitespaces
from gensim.parsing.preprocessing import stem_text
from gensim.parsing.preprocessing import remove_stopwords
from gensim.parsing.preprocessing import strip_numeric
import re
from gensim import utils

# removed ""_"" from regular expression
punctuation = r""""""!""#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^`{|}~""""""

RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)


def strip_punctuation(s):
    """"""Replace punctuation characters with spaces in `s` using :const:`~gensim.parsing.preprocessing.RE_PUNCT`.

    Parameters
    ----------
    s : str

    Returns
    -------
    str
        Unicode string without punctuation characters.

    Examples
    --------
    &gt;&gt;&gt; from gensim.parsing.preprocessing import strip_punctuation
    &gt;&gt;&gt; strip_punctuation(""A semicolon is a stronger break than a comma, but not as much as a full stop!"")
    u'A semicolon is a stronger break than a comma  but not as much as a full stop '

    """"""
    s = utils.to_unicode(s)
    return RE_PUNCT.sub("" "", s)



my_filter = [
    lambda x: x.lower(), strip_tags, strip_punctuation,
    strip_multiple_whitespaces, strip_numeric,
    remove_stopwords, strip_short, stem_text
]


documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
test  = "" "".join(bigram[sent])


print(preprocess_string(test))
print(preprocess_string(test, filters=my_filter))
</code></pre>

<p>The result is:</p>

<pre><code>['mayor', 'new', 'york']
['mayor', 'new_york'] #correct
</code></pre>

<p>part of the code was taken from: <a href=""https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim/35748858"">How to extract phrases from corpus using gensim</a></p>
",2020-09-24 08:09:27,2020-09-24 08:09:27,Correct way of using Phrases and preprocess_string gensim,<python><python-3.x><nlp><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16607,49919642,2018-04-19 11:10:45,,"<p>Error as on screen : 
On import of nltk :</p>

<blockquote>
  <p>ImportError: cannot import name raise_unorderable_types</p>
</blockquote>

<p>I've already checked these :</p>

<ol>
<li><a href=""https://stackoverflow.com/questions/31270361/why-shows-error-import-nltk"">Why shows error &quot;import nltk&quot;?</a>
already.</li>
<li><p>reinstalled nltk using : pip2 install nltk --upgrade</p></li>
<li><p>when I try command line level nltk commands like nltk.download , it still throws the same error</p></li>
<li>Unable to resolve why, I checked dist_packages for python2.7 , it has nltk in it.</li>
<li>Checked by trying to import Other packages in dist_packages like gensim, It works.</li>
</ol>

<p>Python version : 2.7.6
IPython Shell Version : 1.2.1</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-1-b06499430ee0&gt; in &lt;module&gt;()
----&gt; 1 import nltk

/usr/local/lib/python2.7/dist-packages/nltk/__init__.py in &lt;module&gt;()
    103 # Import top-level functionality into top-level namespace
    104 
--&gt; 105 from collocations import *
    106 from decorators import decorator, memoize
    107 from featstruct import *

/usr/local/lib/python2.7/dist-packages/nltk/collocations.py in &lt;module&gt;()
     34 from operator import itemgetter as _itemgetter
     35 
---&gt; 36 from nltk.probability import FreqDist
     37 from nltk.util import ingrams
     38 from nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures

/usr/local/lib/python2.7/dist-packages/nltk/probability.py in &lt;module&gt;()
     46 from operator import itemgetter
     47 from itertools import imap, islice
---&gt; 48 from collections import defaultdict
     49 
     50 ##//////////////////////////////////////////////////////

/usr/local/lib/python2.7/dist-packages/nltk/collections.py in &lt;module&gt;()
     20 from six import text_type
     21 
---&gt; 22 from nltk.internals import slice_bounds, raise_unorderable_types
     23 from nltk.compat import python_2_unicode_compatible
     24 

ImportError: cannot import name raise_unorderable_types
</code></pre>
",2018-04-19 12:54:41,2018-04-19 12:54:41,Error in importing NLTK :,<python><import><nltk>,,,CC BY-SA 3.0,True,False,True,False,False
16631,50033595,2018-04-26 01:39:21,,"<p>I am trying to understand how LDA can be used for text-retrieval, and I am currently using the gensim's LdaModel model for implementing LDA, here: <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a>. </p>

<p>I have managed to identify the k topics and their most-used words, and I understand that LDA is about probabilistic distributions of topics and how words are distributed within those topics in the documents, so that much makes sense. </p>

<p>That said, I do not understand how to use the LdaModel to retrieve the documents that are relevant to a string input of search query eg ""negative effects of birth control"". I have tried inferring topic distributions on the search query and finding similarities between the topic distribution on the search query and the topic distributions from the corpus using gensim's similarities.MatrixSimilarity to compute cosine similarity like so:</p>

<p><code>lda = LdaModel(corpus, num_topics=10)
 index = similarities.MatrixSimilarity(lda[corpus])
 query = lda[query_bow]
 sims = index[query]</code></p>

<p>But the performance isn't really good. What I figure is that finding the topic distribution of the search query is not too meaningful because there is usually only 1 topic in the search query. But I don't know how else I could implement this on the LdaModel on gensim. Any advice would be really appreciated, I am new to topic modeling and maybe I am missing something that's glaringly obvious to me? Thanks!</p>
",,2019-09-02 08:41:32,How to use gensim's LDA to conduct text-retrievals from queries?,<gensim><information-retrieval><lda><topic-modeling>,,,CC BY-SA 3.0,False,False,True,False,False
16634,50046643,2018-04-26 15:18:50,,"<p>I'd like to use the reticulate package to run gensim from R. I'm not sure I fully understand the syntax of reticulate because I can get this to work with the default function settings but it fails when I try to pass more arguments. </p>

<pre><code>library(reticulate)
gensim &lt;- import(""gensim"")

model&lt;-gensim$models$Word2Vec$load(""word2vec_gensim"")

matrix(unlist(model$wv$most_similar(""queen"")),ncol=2,byrow=T)
      [,1]                           [,2]               
 [1,] ""princess""                     ""0.76466166973114"" 
 [2,] ""king""                         ""0.728749990463257""
 [3,] ""prince""                       ""0.653270363807678""
 [4,] ""lady""                         ""0.611525416374207""
 [5,] ""consort""                      ""0.609499335289001""
 [6,] ""duchess""                      ""0.608054518699646""
 [7,] ""monarch""                      ""0.606827557086945""
 [8,] ""lady-in-waiting""              ""0.605596661567688""
 [9,] ""empress""                      ""0.602727890014648""
 [10,] ""wiki/margrethe_ii_of_denmark"" ""0.59738427400589""
</code></pre>

<p>but...</p>

<pre><code>matrix(unlist(model$wv$most_similar(""queen"",topn = 25)),ncol=2,byrow=T)

Error in py_call_impl(callable, dots$args, dots$keywords) : 
TypeError: Partition index must be integer
</code></pre>

<p>Here ""word2vec_gensim"" is a pre-trained model, I can't include it because it is a large file but pick your fav pre-trained model. I think my issue is in how I'm providing additional args to the python function. </p>

<p>EDIT:I figured it out</p>

<p>looks like the R to python communication doesn't handle numbers as expected.</p>

<pre><code>matrix(unlist(model$wv$most_similar(""queen"",topn = as.integer(25))),ncol=2,byrow=T)
</code></pre>

<p>works</p>
",2018-04-26 16:40:55,2018-04-27 15:15:56,How to run a Python gensim functions from R with reticulate,<python><r><gensim><reticulate>,,,CC BY-SA 3.0,False,False,True,False,False
16635,50048484,2018-04-26 17:03:20,,"<p>I am trying to join a lists of appended sentences into a large a string text object so that I can use it as an input for the Gensim summarize module. However, when I try to do this, it says the input sentences are less than 2. But when I run a split on the text, I see multiple sentences but it counts each sentence once instead of the total of sentences together. And the variable r is a string type object. I would like to concatenate the sentences together into one large string so it can be read through the Gensim summarize module.</p>

<p><strong>Sample Code:</strong></p>

<pre><code>import re
ruling_corpora  = re.findall(""\.?([^\.].\*?I find[^\.]*\. |[^\.]*$In sum[^\.]*\. |[^\.]*$agree[^\.]*\.)"", tokenized, re.I |re.DOTALL |re.M)[1:-1]

for r in ruling_corpora:                                   
    print(type(r))
    rc= ''.join(r)
    print(summarize(rc))
</code></pre>

<p>SAMPLE OUTPUT:</p>

<pre><code>raise ValueError(""input must have more than one sentence"")
ValueError: input must have more than one sentence
</code></pre>

<p>Here is an example of my input I want to summarize with the Gensim summarizer. The numbers underneath each string represent the count of sentences ending in periods:</p>

<pre><code>####Beginning of File### LUMB65.BL23607963.xml
Background Content: ANDERSON INITIAL DECISIONOn January 13, 2015, the appellant filed this appeal arguing that the agency's decision not to renew his term limited appointment which expired on January 28, 2015, is in error.  

 For the reasons discussed below, this appeal is DISMISSED for lack of jurisdiction without a hearing.
1
There is nothing in the agreement that curtails the agency's ability not to extend the term appointment. 
 IdIn reviewing the appellant's arguments, the appellant fails to establish that the Board has jurisdiction to review the agency's decision not to renew his time-limited appointment at issue in this appeal.
 Following a review of the record evidence, I find that the appellant has failed to non-frivolously allege Board jurisdiction over this appeal on any basis.
 Accordingly, this appeal must be dismissed for lack of jurisdiction.
1
####End of File### LUMB65.BL23607963.xml
</code></pre>
",2018-04-26 21:17:03,2018-04-27 13:31:50,joining sentences from a list in python3,<python><regex><list><join><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16638,49962749,2018-04-22 05:18:53,,"<p>I'm trying to train a doc2vec model using the gensim library on 50 million sentences of variable length.</p>

<p>Some tutorials (eg. <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a>) have a <code>model.build_vocab</code> step before the actual training process. This part has been running for 3 hours now without any updates.</p>

<p>Is this step necessary for the training process? Why could this step be taking so long since it's just a linear pass over the data?</p>

<p>Using gensim version 3.4.0 with python 3.6.0</p>
",,2018-04-26 06:33:49,Gensim build_vocab taking too long,<python-3.x><nlp><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16661,50038347,2018-04-26 08:31:46,,"<p>While learning Doc2Vec library, I got stuck on the following question.</p>

<p><strong>Do gensim Doc2Vec distinguish between the same Sentence with positive and negative context?</strong></p>

<p>For Example:</p>

<p>Sentence A: ""I love Machine Learning""</p>

<p>Sentence B: ""I do not love Machine Learning""</p>

<p>If I train sentence A and B with doc2vec and find cosine similarity between their vectors:</p>

<ol>
<li>Will the model be able to distinguish the sentence and give a cosine similarity very less than 1 or negative?</li>
<li>Or Will the model represent both the sentences very close in vector space and give cosine similarity close to 1, as mostly all the words are same except the negative word (do not).</li>
</ol>

<p>Also, If I train only on sentence A and try to infer Sentence B, will both vectors be close to each other in vector space.?</p>

<p>I would request the NLP community and Doc2Vec experts for helping me out in understanding this.</p>

<p>Thanks in Advance !!</p>
",2018-04-26 08:39:38,2018-04-26 16:49:05,Do gensim Doc2Vec distinguish between same Sentence with positive and negative context.?,<python><nlp><gensim><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16669,49984763,2018-04-23 15:30:02,,"<p>I apologise in advance as I cannot reproduce the dataset I'm working with. So I am just going to describe steps and hope someone is familiar with the whole process. </p>

<p>I'm trying to use LDA Gensim to extract topics from a list of text documents. </p>

<pre><code>from gensim.models import LdaModel
from gensim.corpora import Dictionary
</code></pre>

<p>I build <code>dictionary</code> and <code>corpus</code>:</p>

<pre><code>dictionary = Dictionary(final_docs)
corpus = [dictionary.doc2bow(doc) for doc in final_docs]
</code></pre>

<p>where <code>final_docs</code> is a list of lists with cleaned tokens for each text like this: </p>

<pre><code>final_docs = [['cat','dog','animal'],['school','university','education'],...['music','dj','pop']]
</code></pre>

<p>then I initiate the model like this:</p>

<pre><code># Set training parameters:
num_topics = 60
chunksize = 100
passes = 20
iterations = 400
eval_every = None 

# Make an index to word dictionary
temp = dictionary[0]  # This is only to ""load"" the dictionary.
id2word = dictionary.id2token

model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                           alpha='auto', eta='auto', \
                           iterations=iterations, num_topics=num_topics, \
                           passes=passes, eval_every=eval_every)
</code></pre>

<p>I can print topics and terms (10 most important). And they make sense. So it seems working fine.</p>

<pre><code>for idx in range(n_topics):
    print(""Topic #%s:"" % idx, model.print_topic(idx, 10))
</code></pre>

<p>BUT I struggle to plot all the documents as clusters using Bokeh. (And I really need Bokeh because I compare the same plot from different models). I know I have to reduce dimensionality to 2. And I try to do it using CountVectorizer and then T-sne:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
docs_vect = [' '.join(txt) for txt in final_docs]
cvectorizer = CountVectorizer(min_df=6, max_df=0.50, max_features=10000, stop_words=stop)
cvz = cvectorizer.fit_transform(docs_vect)
X_lda = model.fit_transform(cvz)
</code></pre>

<p>But I get this error: <code>AttributeError: 'LdaModel' object has no attribute 'fit_transform'</code>
I'm definitely doing something wrong with CountVectorizer. Could anyone help me out?</p>
",2018-04-23 16:42:25,2018-04-23 16:42:25,Plot clusters from LDA Gensim with Bokeh,<python-3.x><vectorization><gensim><dimensionality-reduction>,,,CC BY-SA 3.0,False,False,True,False,True
16671,50038358,2018-04-26 08:32:17,,"<p>What is the best way to visualize a Word2Vec model using TensorFlow's Embedding Projector?
is there a way to export the Word2Vec model's vectors to the format that Embedding Projector expects? or is there a built in function in tensorflow for that?</p>

<p>Thanks!</p>
",,2019-05-07 18:07:39,Visualize a Word2Vec model using Embedding Projector,<tensorflow><nlp><data-visualization><word2vec><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16674,50084316,2018-04-29 07:25:27,,"<p>I have been trying to install the gensim package in python using pip, using the <code>pip.main(['install','gensim'])</code>. It works and starts downloading, but then I get the following error:   </p>

<p><a href=""https://i.stack.imgur.com/hkgyw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hkgyw.png"" alt=""error in shell""></a></p>

<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
gensim from https://files.pythonhosted.org/packages/82/f2/c2f2c87ed72483fce010fbfea1a3adbd168c0f0dafc878cbfb5a76381b03/gensim-3.4.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl#sha256=7bafe3f2fd49738942ef04396cb1e50a38283fe02203e5d4c66588daa01fb87c:
    Expected sha256 7bafe3f2fd49738942ef04396cb1e50a38283fe02203e5d4c66588daa01fb87c
         Got        728e9e79db209cfb0699c815c30a6169cefa61f46ef3471937100e4173fdbb3d
</code></pre>

<p>Any help would be appreciated </p>
",2018-04-29 08:02:00,2018-04-30 06:29:21,error when downloading the gensim package in python,<python><gensim>,,,CC BY-SA 3.0,False,False,True,False,False
16685,50089525,2018-04-29 17:42:20,,"<p>I am training document embeddings on a ~20 million sentences and using parallel processing in gensim. I'm creating my model and training with the following code</p>

<pre><code>class read_corpus(object):

    def __init__(self, fname, n):
        self.fname = fname
        self.n = n

    def __iter__(self):
        num_notes = 0
        with open(self.fname, 'r') as f:
            while num_notes &lt; n:
                note = next(f)
                sentence_id, sentence = note.split('\t')

                # remove the newline character after each line and split into words
                sentence = sentence[:-1].split(' ')

                # some processing


                yield TaggedDocument(sentence, [sentence_id])
                num_notes += 1


def model(fname, vector_size, min_count,
          n_epochs, model_name,
          n, prev_model_name=None):


    data = read_corpus(fname, n)

    if prev_model_name is not None:
        model = Doc2Vec.load(prev_model_name)
    else:
        model = Doc2Vec(vector_size=vector_size,
                        min_count=min_count,
                        workers=4,
                        window=8,
                        alpha=0.1,
                        min_alpha=0.0001)

        model.build_vocab(data)

    model.train(data, total_examples=model.corpus_count, epochs=n_epochs)
    model.save(model_name)
</code></pre>

<p>After 6 - 8 epochs, the logging information shows that the training gets stuck waiting for a worker thread.
Note: the logging information says ""EPOCH 1"" because I'm training in a for loop. </p>

<p><code>... 
INFO : EPOCH 1 - PROGRESS: at 99.71% examples, 162493 words/s, in_qsize 8, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.81% examples, 162528 words/s, in_qsize 7, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.91% examples, 162560 words/s, in_qsize 7, out_qsize 0
INFO : worker thread finished; awaiting finish of 3 more threads
INFO : worker thread finished; awaiting finish of 2 more threads</code></p>

<p>It's been stuck here for several hours.</p>

<p>I had a similar output on a previous run. But the logging stopped at <code>INFO : worker thread finished; awaiting finish of 3 more threads</code></p>
",2018-05-02 18:30:17,2018-11-07 01:46:56,Gensim worker thread stuck,<python><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16689,50146901,2018-05-03 04:19:39,,"<p>code like this:</p>

<pre><code>train_corpus = ""sentence_all.txt""
sentences = LineSentence(train_corpus)
model = Word2Vec(sentences, size=vector_size,  window=window_size, min_count=min_count, workers=worker_count, iter=train_epoch)
print(model[''])
</code></pre>

<p>the corpus file has been processd as list of token by LineSentence in gensim like this:</p>

<pre><code>['', '', '', '', '', '', ',', '', '', '', '', '', '', '1997', ',', '', '', '', '', '', '', '', '', ',', '', '', '', '', '', '22.1', '', '']
['', '', '', '', ',', '', '', '', '', '', ',', '', '', '', '', '', ',', '', '', '19', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ',', '', '', '', '', '', '']
</code></pre>

<p>then get the error:</p>

<pre><code>KeyError: ""word '' not in vocabulary""
</code></pre>

<p>but only a few tokens are not in vocabulary, the others can get their word vector, then I don't know the reason.</p>
",,2018-06-24 17:26:21,KeyError: word not in vocabulary when I use gensim.Word2Vec to process chinese tokens,<python><nlp><word2vec><gensim><keyerror>,,,CC BY-SA 4.0,False,False,True,False,False
16706,50076131,2018-04-28 11:39:03,,"<p>I am trying to check the semantic and syntactic performance of a doc2vec model- <code>doc2vec_model.accuracy(questions-words)</code>, but it doesnt seem to function since <a href=""https://radimrehurek.com/gensim/models/deprecated/doc2vec.html"" rel=""nofollow noreferrer"">models.deprecated.doc2vec  Deep learning with paragraph2vec</a>, says it has been deprecated since version 3.3.0 in the gensim package.It gives this error message </p>

<pre><code>AttributeError: 'Doc2Vec' object has no attribute 'accuracy'
</code></pre>

<p>Though it works with word2vec model well, is there any way I can get it done apart from <code>doc2vec_model.accuracy(questions-words)</code>? or it's impossible?</p>
",,2018-04-28 20:04:37,semantic and syntactic performance of Doc2vec model,<python-3.x><word-embedding><doc2vec>,,,CC BY-SA 3.0,False,False,True,False,False
16707,50104095,2018-04-30 15:56:17,,"<p>I would like to use trigrams and bigrams because I do not want to use just unigrams. </p>

<pre><code>bigramer = gensim.models.Phrases(sentences)
model = Word2Vec(bigramer[sentences], workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)
from nltk import bigrams
from nltk import trigrams
from gensim.models import Phrases
from gensim.models.phrases import Phraser
trigrams = Phrases(bigrams[sentence_stream])
</code></pre>

<p>However, I am having this error.</p>

<pre><code>NameErrorTraceback (most recent call last)
&lt;ipython-input-161-15b0101c13b1&gt; in &lt;module&gt;()
----&gt; 1 trigrams = Phrases(bigrams[sentence_stream])

NameError: name 'sentence_stream' is not defined
</code></pre>
",,2018-05-03 14:44:06,Getting Name Error name 'sentence_stream' is not defined,<python><nltk><nameerror>,,,CC BY-SA 3.0,True,False,True,False,False
16730,50117021,2018-05-01 13:06:14,,"<p>I am new to <a href=""https://fasttext.cc/docs/en/support.html"" rel=""nofollow noreferrer"">fastText</a>, a library for efficient learning of word representations and sentence classification. I am trying to <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">generate word-vector for huge data set</a>. But in single process it's taking significantly long time.</p>

<p>So let me put my questions clearly:</p>

<ul>
<li>Are there any options which I can use to speedup the single fastText process?</li>
<li>Is there any way to generate word-vector in parallel fastText processes?</li>
<li>Are there any other implementation or workaround available which can solve the problem, as I read <a href=""https://fasttext.cc/docs/en/faqs.html"" rel=""nofollow noreferrer"">caffe2 implementation is available</a>, but I am unable to find it.</li>
</ul>

<p>Thanks</p>
",2018-05-01 17:04:42,2018-05-01 17:04:42,Is there a way to use fastText's word representation process in parallel?,<nlp><word2vec><gensim><fasttext><caffe2>,,,CC BY-SA 3.0,False,False,True,False,False
16750,50207170,2018-05-07 04:37:30,,"<p>Hello I am fairly new to word2vec, I wrote a small program to teach myself</p>

<pre><code>import gensim
from gensim.models import Word2Vec

sentence=[['Yellow','Banana'],['Red','Apple'],['Green','Tea']]
model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)
print(model.similarity('Yellow', 'Banana'))
</code></pre>

<p>The similarity came out to be:
-0.048776340629810115</p>

<p>My question is why not is the similarity between banana and yellow closer to 1 like .70 or something. What am I missing? Kindly guide me.</p>
",,2018-05-07 06:58:26,Understanding model.similarity in word2vec,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16758,50191231,2018-05-05 15:44:52,,"<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
",2018-11-15 22:04:04,2018-11-15 22:04:04,How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit,<python><nlp><word2vec><gensim><word-sense-disambiguation>,,,CC BY-SA 4.0,False,False,True,False,False
16768,50118868,2018-05-01 15:04:33,,"<pre><code>ldamodel, fequency_list,vect = create_ldamodel(documents = chatTurn.case_content, num_topics = 6)
</code></pre>

<p>I am running the above LDA code and it appears the statement as below.</p>

<p>/Users/user/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:497: RuntimeWarning: overflow encountered in exp
  expElogthetad = np.exp(Elogthetad)</p>

<p>The LDA model function that I used is as follow:</p>

<pre><code>def create_ldamodel(documents, num_topics):
    vect = CountVectorizer(stop_words = 'english')
    X = vect.fit_transform(documents.apply(lambda x:x.lower()))
    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)
    id_map = dict((v,k) for k, v in vect.vocabulary_.items())
    ldamodel = gensim.models.ldamodel.LdaModel(corpus,      num_topics=num_topics,
                                          id2word = id_map)

fequency_list = dict()
for i in list(ldamodel[corpus]):
    for j_k, j_v in i:
        if j_k in fequency_list:
            fequency_list[j_k] += j_v
        else:
            fequency_list[j_k] = 0

return (ldamodel, fequency_list,vect)
</code></pre>

<p>May I know is there any solution to it?</p>
",,2018-12-10 23:20:54,How to RuntimeWarning: overflow encountered in exp in LDA,<python><lda>,,,CC BY-SA 3.0,False,False,True,False,False
16774,50212449,2018-05-07 10:50:07,,"<p>I try to map sentences to a vector in order to make sentences comparable to each other. To test gensim's Doc2Vec model, I downloaded sklearn's newsgroup dataset and trained the model on it.</p>
<p>In order to compare two sentences, I use model.infer_vector() and I am wondering why two calls using the same sentence delivers me different vectors:</p>
<pre><code>model = Doc2Vec(vector_size=100, window=8, min_count=5, workers=6)
model.build_vocab(documents)

epochs=10
for epoch in range(epochs):
    print(&quot;Training epoch %d&quot; % (epoch+1))
    model.train(documents,  total_examples=len(documents), epochs=epochs)

    v1 = model.infer_vector(&quot;I feel good&quot;)
    v2 = model.infer_vector(&quot;I feel good&quot;)
    print(np.linalg.norm(v1-v2)) 
</code></pre>
<p>Output:</p>
<blockquote>
<p>Training epoch 1</p>
<p>0.41606528</p>
<p>Training epoch 2</p>
<p>0.43440753</p>
<p>Training epoch 3</p>
<p>0.3203116</p>
<p>Training epoch 4</p>
<p>0.3039317</p>
<p>Training epoch 5</p>
<p>0.68224543</p>
<p>Training epoch 6</p>
<p>0.5862567</p>
<p>Training epoch 7</p>
<p>0.5424634</p>
<p>Training epoch 8</p>
<p>0.7618142</p>
<p>Training epoch 9</p>
<p>0.8170159</p>
<p>Training epoch 10</p>
<p>0.6028216</p>
</blockquote>
<p>If I set alpha and min_alpha = 0 I get consistent vectors for the &quot;I feel fine&quot; and &quot;I feel good&quot;, but the model gives me the same vector in every epoch, so it does not seem to learn anything:</p>
<blockquote>
<p>Training epoch 1</p>
<p>0.043668125</p>
<p>Training epoch 2</p>
<p>0.043668125</p>
<p>Training epoch 3</p>
<p>0.043668125</p>
<p>Training epoch 4</p>
<p>0.043668125</p>
<p>Training epoch 5</p>
<p>0.043668125</p>
<p>Training epoch 6</p>
<p>0.043668125</p>
<p>Training epoch 7</p>
<p>0.043668125</p>
<p>Training epoch 8</p>
<p>0.043668125</p>
<p>Training epoch 9</p>
<p>0.043668125</p>
<p>Training epoch 10</p>
<p>0.043668125</p>
</blockquote>
<p>So my questions are:</p>
<ol>
<li><p>Why do I even have the possibility to specify a learning rate for inference? I would expect that the model is only changed during training and not during inference.</p>
</li>
<li><p>If I specify alpha=0 for inference, why does the distance between those two vectors not change during different epochs?</p>
</li>
</ol>
",2020-06-20 09:12:55,2018-05-07 18:20:02,Gensim Doc2Vec - Why does infer_vector() use alpha?,<gensim><embedding><sentence><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
16776,50213754,2018-05-07 12:01:11,,"<p>I'm working on something using gensim.</p>

<p>In gensim, var <code>index</code> usually means an object of <code>gensim.similarities.&lt;cls&gt;</code>.</p>

<p>At first, I use <code>gensim.similarities.Similarity(filepath, ...)</code> to save index as a file, and then loads it by <code>gensim.similarities.Similarity.load(filepath + '.0')</code>. Because <code>gensim.similarities.Similarity</code> default save index to shards file like <code>index.0</code>.</p>

<p>When index file becoming larger, it automatically seperate into more shards, like <code>index.0</code>,<code>index.1</code>,<code>index.2</code>......</p>

<p>How can I load these shards file? <code>gensim.similarities.Similarity.load()</code> can only load one file.</p>

<p>BTW: I have try to find the answer in gensim's doc, but failed.</p>
",,2019-11-04 15:50:00,How to load index shards by gensim.similarities.Similarity,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
16779,50195948,2018-05-06 03:17:06,,"<h2>Word2Vec</h2>

<p>Currently I am trying to perform text classification on a text corpus. In order to do so, I have decided to perform <code>word2vec</code> with the help of <code>gensim</code>. In order to do so, I have the code below: </p>

<pre><code>sentences = MySentences(""./corpus_samples"") # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>My sentences is basically a class that handles the File <em>I/O</em></p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>Now we can get the vocabulary of the model that has been created through these lines: </p>

<pre><code>print(model.wv.vocab)
</code></pre>

<p>The output of which is below(sample): </p>

<pre><code>t at 0x106f19438&gt;, 'raining.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19470&gt;, 'fly': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194a8&gt;, 'rain.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194e0&gt;, 'So': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19518&gt;, 'Ohhh,': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19550&gt;, 'weird.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;}
</code></pre>

<p>As of now, the dictionary that is the vocabulary, contains the word string and a <code>&lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;</code> object or such. I want to be able to query an index of a particular word. In order to make my training data like: </p>

<pre><code>w91874 w2300 w6 w25363 w6332 w11 w767 w297441 w12480 w256 w23270 w13482 w22236 w259 w11 w26959 w25 w1613 w25363 w111 __label__4531492575592394249
w17314 w5521 w7729 w767 w10147 w111 __label__1315009618498473661
w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492
w30877 w72 w11 w2828 w141417 w77033 w10147 w111 __label__4970306416006110305
w3332 w1107 w4809 w1009 w327 w84792 w6 w922 w11 w2182 w79887 w1099 w111 __label__-3645735357732416904
w471 w14752 w1637 w12348 w72 w31330 w930 w11569 w863 w25 w1439 w72 w111 __label__-5932391056759866388
w8081 w5324 w91048 w875 w13449 w1733 w111 __label__3812457715228923422
</code></pre>

<p>Where the <code>wxxxx</code> represents the index of the word within the vocabulary and the label represents the class. </p>

<hr>

<h2>Corpora</h2>

<p>Some of the solutions that I have been experimenting with, is the <code>corpora</code> utility of <code>gensim</code>:  </p>

<pre><code>corpora = gensim.corpora.dictionary.Dictionary(sentences, prune_at=2000000)
print(corpora)
print(getKey(corpora,'am'))
</code></pre>

<p>This gives me a nice dictionary of the words, but this corpora vocabulary is not the same as the one created by the <code>word2vec</code> function mentioned above. </p>
",,2018-05-07 06:36:10,I am trying to get the key of a particular word from a Word2Vec Vocabulary,<python><dictionary><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
16782,50227208,2018-05-08 06:26:16,,"<p>Using <a href=""https://github.com/dav/word2vec"" rel=""nofollow noreferrer"">https://github.com/dav/word2vec</a>, I have built Word2Vec embedding. I have to use that output in Python's Gensim's Word2Vec. </p>

<p>I have VectorsFile.txt and VocabuloryFile.txt for my Corpus.txt.  </p>

<p>How to do that .. </p>

<p>Can you please .. ?  </p>
",,2018-05-08 06:26:16,Converting Word2Vec embedding in C to Python's Gensim Word2vec,<python><c><tensorflow><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
16791,50161445,2018-05-03 18:11:44,,"<p>I trained a FastText model in Gensim. I want to use it to encode my sentences. Specifically, I want to use this feature from native FastText:</p>

<pre><code>./fasttext print-word-vectors model.bin &lt; queries.txt
</code></pre>

<p>How to I save the model in Gensim so that it is the correct binary format that can be understood by native FastText?</p>

<p>I am using FastText 0.1.0 and Gensim 3.4.0 under Python 3.4.3.</p>

<p>In essence, I need the inverse of the load_binary_data() as given in the <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">Gensim FastText doc</a>.</p>
",2018-05-04 04:41:01,2018-05-05 15:43:05,How to load Gensim FastText model in native FastText,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
16800,50198409,2018-05-06 09:55:21,,"<p>I am Working On Sentiment Analysis on Amazon Food Reviews and I am trying to apply Word2Vec on the Reviews and Visualise it Using t-SNE.</p>

<p>I was easily able to Visualise using Bag of words representation of the same using following code:</p>

<pre><code>    from sklearn.manifold import TSNE
    data_2000 = final_counts[0:2000,:]
    top_2000 = data_2000.toarray()
    labels = final['Score']
    labels_2000 = labels[0:2000]

    model = TSNE(n_components=2, random_state=0)
    tsne_data = model.fit_transform(top_2000)

    # creating a new data frame which help us in ploting the result 

      tsne_data = np.vstack((tsne_data.T, labels_2000)).T
      tsne_df = pd.DataFrame(data=tsne_data, columns=(""Dim_1"", ""Dim_2"", 
      ""label""))

    # Ploting the result of tsne

       sns.FacetGrid(tsne_df, hue=""label"", size=6).map(plt.scatter, 
      'Dim_1', 'Dim_2').add_legend()
       plt.show()
</code></pre>

<p>Also, The same code doesn't work when I feed w2v_model model which is of type gensim.models.word2vec.Word2Vec</p>

<p>I obtained the model by using following Code:</p>

<pre><code>     w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, 
     workers=4)
</code></pre>
",,2019-12-07 16:44:20,How to apply t-SNE on Word2Vec Model,<python-3.x><machine-learning><deep-learning><nltk><amazon-machine-learning>,,,CC BY-SA 4.0,True,False,True,False,True
16801,50214899,2018-05-07 13:02:05,,"<p>I am facing the following error when trying to update my gensim's <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">LdaModel</a>: </p>

<blockquote>
  <p>IndexError: index 6614 is out of bounds for axis 1 with size 6614</p>
</blockquote>

<p>I checked why were other people having this issue on <a href=""https://groups.google.com/forum/#!topic/gensim/T0GMxE7YZqM"" rel=""nofollow noreferrer"">this thread</a>, but I am using the same dictionary from the beginning to the end, which was their error.</p>

<p>As I have a big dataset, I am loading it chunk by chunk (using pickle.load). I am building the dictionary in this way, iteratively, thanks to this piece of code :</p>

<p></p>

<pre><code> fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
dictionary = Dictionary()
chunk_no = 0
while 1:
  try:
    t0 = time()
    documents_lda = pickle.load(fr_documents_lda)
    chunk_no += 1
    dictionary.add_documents(documents_lda)
    t1 = time()
    print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
  except EOFError:
    print(""Finished going through pickle"")
    break
</code></pre>

<p>Once built for the whole dataset, I am training the model in the same fashion, iteratively, this way :</p>

<pre><code>fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
first_iter = True
chunk_no = 0
lda_gensim = None
while 1:
  try:
    t0 = time()
    documents_lda = pickle.load(fr_documents_lda) 
    chunk_no += 1
    corpus = [dictionary.doc2bow(text) for text in documents_lda]
    if first_iter:
      first_iter = False
      lda_gensim = LdaModel(corpus, num_topics=no_topics, iterations=100, offset=50., random_state=0, alpha='auto')
    else:
      lda_gensim.update(corpus)
    t1 = time()
    print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
  except EOFError:
    print(""Finished going through pickle"")
    break
</code></pre>

<p>I also tried updating the dictionary at every chunk, i.e. having
</p>

<pre><code>dictionary.add_documents(documents_lda)
</code></pre>

<p>right before
</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in documents_lda]
</code></pre>

<p>in the last piece of code. Finally, I tried setting the allow_update argument of doc2bow to True. Nothing works.</p>

<p>FYI, the size of my final dictionary is 85k. The size of my dictionary built only from the first chunk is 10k. The error occurs on the second iteration, when it passes in the else condition, when calling the update method.</p>

<p>The error is raised by the line  <code>expElogbetad = self.expElogbeta[:, ids]</code>
, called by <code>gamma, sstats = self.inference(chunk, collect_sstats=True)</code>, itself called by <code>gammat = self.do_estep(chunk, other)</code>, itself called by <code>lda_gensim.update(corpus)</code>.</p>

<p>Is anyone having an idea on how to fix this, or what is happening ?</p>

<p>Thank you in advance.</p>
",,2020-06-24 07:55:06,IndexError when trying to update gensim's LdaModel,<python-3.x><gensim><lda><topic-modeling><index-error>,,,CC BY-SA 4.0,False,False,True,False,False
16829,50311253,2018-05-12 22:40:50,,"<p>I am trying to use the weights from my word2vec model as weights for the Embedding layer of my neural network in keras. The <a href=""https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation"">example code</a> that I'm following uses: </p>

<pre><code>word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, 
                                window=5, iter=100)
pretrained_weights = word_model.wv.syn0
keras_model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, 
                weights=[pretrained_weights]))
</code></pre>

<p>I understand that word2vec creates vectors for each word, in this case of size 100. </p>

<p><code>pretrained_weights.shape</code> returns (1350,100), but I am not sure what the 1350 number means. </p>

<p><code>keras_model.predict(np.array([word_model.wv.vocab['test'].index]))</code> returns a vector of size 1350, which I am not sure how to interpret (the response the model was trained on is a vector of size 7200). </p>

<p>I can run the example code and get results fine, but I would like to know why it works.</p>
",2018-05-13 04:42:07,2018-05-13 04:42:07,How to interpret shape of word2vec weights?,<python><neural-network><keras><deep-learning><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16834,50253681,2018-05-09 12:40:01,,"<p>I am working on a word embedding project. I am using Amazon SageMaker for this purpose. The BlazingText algorithm in the Amazon SageMaker produced fast result than the other options. But I don't see any facility to get the prediction model or the weights. The output consists only the vectors file from which I cannot generate the model.
Is there any way by which I can get the model with the vector file? I need this to predict new words. Thanks in advance.</p>
",2018-05-09 12:44:23,2019-09-10 08:52:20,Amazon SageMaker BlazingText,<amazon-web-services><nlp><word2vec><gensim><amazon-sagemaker>,,,CC BY-SA 4.0,False,False,True,False,False
16841,50275623,2018-05-10 14:44:57,,"<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>

<p>Running them with one word gives identical results, for example:
model.most_similar(['obama']) and similar_by_vector(model['obama'])</p>

<p>but if I give it an equation:</p>

<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>gives:</p>

<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]
</code></pre>

<p>while with:</p>

<pre><code>q = model['king'] - model['man'] + model['woman']
model.similar_by_vector(q)
</code></pre>

<p>gives:</p>

<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]
</code></pre>

<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>

<p>Thanks!</p>
",,2018-05-10 16:32:01,Difference between most_similar and similar_by_vector in gensim word2vec?,<nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
16848,50290762,2018-05-11 10:55:47,,"<p>I want to interpret the topics in my lda topic model, so i am using pyldavis..
But somehow i can't get pyldavis to run. Here is the code: </p>

<pre><code>import gensim
from gensim import corpora
from gensim.corpora import Dictionary


dictionary = corpora.Dictionary(lemmatized_list)
print(dictionary)
print(dictionary.token2id)
corpus = [dictionary.doc2bow(text) for text in lemmatized_list]
print(corpus)

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = 
dictionary, passes=10)
print(ldamodel.print_topics(num_topics=5, num_words=3))

import pyLDAvis.gensim
pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
</code></pre>

<p>then, after i reach to the last part of the code where i have to visualize using pyldavis, its showing the following error:</p>

<pre><code>    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-16-a16bc334f38f&gt; in &lt;module&gt;()
      1 import pyLDAvis.gensim
      2 pyLDAvis.enable_notebook()
----&gt; 3 pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
      4 term_ix = np.sort(topic_info.index.unique().values)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pyLDAvis\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)
    108     """"""
    109     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)
--&gt; 110     return vis_prepare(**opts)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pyLDAvis\_prepare.py in prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)
    396 
    397    topic_info         = _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)
--&gt; 398    token_table        = _token_table(topic_info, term_topic_freq, vocab, term_frequency)
    399    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)
    400    client_topic_order = [x + 1 for x in topic_order]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pyLDAvis\_prepare.py in _token_table(topic_info, term_topic_freq, vocab, term_frequency)
    265    # term-topic frequency table of unique terms across all topics and all values of lambda
    266    term_ix = topic_info.index.unique()
--&gt; 267    term_ix.sort()
    268    top_topic_terms_freq = term_topic_freq[term_ix]
    269    # use the new ordering for the topics

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexes\base.py in sort(self, *args, **kwargs)
   2098 
   2099     def sort(self, *args, **kwargs):
-&gt; 2100         raise TypeError(""cannot sort an Index object in-place, use ""
   2101                         ""sort_values instead"")
   2102 

TypeError: cannot sort an Index object in-place, use sort_values instead
</code></pre>

<p>Any suggestions on how to get this error sorted out...would be really helpful. Thanks!</p>
",,2018-05-11 11:16:25,Error in visualizing LDA Topic Model,<python><nltk><lda><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
16854,50237247,2018-05-08 15:28:24,,"<p>I get this error when I load the google pre-trained word2vec to train doc2vec model with my own data. Here is part of my code:</p>

<pre><code>model_dm=doc2vec.Doc2Vec(dm=1,dbow_words=1,vector_size=400,window=8,workers=4)
model_dm.build_vocab(document)
model_dm.intersect_word2vec_format('home/xxw/Downloads/GoogleNews-vectors-negative300.bin',binary=True)
model_dm.train(document)
</code></pre>

<p>But I got this error:</p>

<blockquote>
  <p>'Doc2Vec' object has no attribute 'intersect_word2vec_format'</p>
</blockquote>

<p>Can you help me with the error? I get the google model from <a href=""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</a>, and my gensim is the latest version I think.</p>
",2018-05-08 17:53:42,2018-05-08 17:53:42,gensim: 'Doc2Vec' object has no attribute 'intersect_word2vec_format' when I load the Google pre-trained word2vec model,<word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16859,50278744,2018-05-10 17:52:21,,"<p>I currently have following script that helps to find the best model for a doc2vec model. It works like this: First train a few models based on given parameters and then test against a classifier. Finally, it outputs the best model and classifier (I hope).</p>

<p><strong>Data</strong></p>

<p>Example data (data.csv) can be downloaded here: <a href=""https://pastebin.com/takYp6T8"" rel=""noreferrer"">https://pastebin.com/takYp6T8</a>
Note that the data has a structure that should make an ideal classifier with 1.0 accuracy.</p>

<p><strong>Script</strong></p>

<pre><code>import sys
import os
from time import time
from operator import itemgetter
import pickle
import pandas as pd
import numpy as np
from argparse import ArgumentParser

from gensim.models.doc2vec import Doc2Vec
from gensim.models import Doc2Vec
import gensim.models.doc2vec
from gensim.models import KeyedVectors
from gensim.models.doc2vec import TaggedDocument, Doc2Vec

from sklearn.base import BaseEstimator
from gensim import corpora

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


dataset = pd.read_csv(""data.csv"")

class Doc2VecModel(BaseEstimator):

    def __init__(self, dm=1, size=1, window=1):
        self.d2v_model = None
        self.size = size
        self.window = window
        self.dm = dm

    def fit(self, raw_documents, y=None):
        # Initialize model
        self.d2v_model = Doc2Vec(size=self.size, window=self.window, dm=self.dm, iter=5, alpha=0.025, min_alpha=0.001)
        # Tag docs
        tagged_documents = []
        for index, row in raw_documents.iteritems():
            tag = '{}_{}'.format(""type"", index)
            tokens = row.split()
            tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))
        # Build vocabulary
        self.d2v_model.build_vocab(tagged_documents)
        # Train model
        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.iter)
        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)


param_grid = {'doc2vec__window': [2, 3],
              'doc2vec__dm': [0,1],
              'doc2vec__size': [100,200],
              'logreg__C': [0.1, 1],
}

pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('log', LogisticRegression())])

log_grid = GridSearchCV(pipe_log, 
                        param_grid=param_grid,
                        scoring=""accuracy"",
                        verbose=3,
                        n_jobs=1)

fitted = log_grid.fit(dataset[""posts""], dataset[""type""])

# Best parameters
print(""Best Parameters: {}\n"".format(log_grid.best_params_))
print(""Best accuracy: {}\n"".format(log_grid.best_score_))
print(""Finished."")
</code></pre>

<p>I do have following questions regarding my script (I combine them here to avoid three posts with the same code snippet):</p>

<ol>
<li>What's the purpose of <code>def __init__(self, dm=1, size=1, window=1):</code>? Can I possibly remove this part, somehow (tried unsuccessfully)?</li>
<li>How can I add a <code>RandomForest</code> classifier (or others) to the GridSearch workflow/pipeline?</li>
<li>How could a train/test data split added to the code above, as the current script only trains on the full dataset?</li>
</ol>
",2019-01-31 19:00:03,2019-01-31 19:00:03,Pipeline and GridSearch for Doc2Vec,<scikit-learn><pipeline><gensim><grid-search>,,,CC BY-SA 4.0,False,False,True,False,True
16861,50328915,2018-05-14 11:13:01,,"<p>I wanna use the LDA in gensim for topic modeling over a few thousand documents.
Therefore Im using a csv-File as Input in the format of a term-document-matrix.</p>

<p>Currently it occurs an error when running the following code:</p>

<pre><code>from gensim import corpora

import_path =""TDM.csv""

dictionary = corpora.csvcorpus(import_path, labels='true')
</code></pre>

<p>The error is the following:</p>

<pre><code>dictionary = corpora.csvcorpus(import_path, labels='true')

AttributeError: module 'gensim.corpora' has no attribute 'csvcorpus'
</code></pre>

<p>Am I using the module correctly and if so, where is my mistake?</p>

<p>Thanks in advance.</p>
",,2018-05-21 06:07:51,CSV Input in gensim LDA via corpora.csvcorpus,<python-3.x><csv><gensim><lda><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
16878,50370240,2018-05-16 11:53:32,,"<p>I just got a hold of Google's word2vec model and am quite new to the concept. i am trying to extract the main feature of a paragraph using the following method.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('../../usr/myProject/word2vec/GoogleNews-vectors-negative300.bin', binary=True)

...

for para in paragraph_array:
    para_name = ""para_""+ file_name + '{0}'
    sentence_array = d[para_name.format(number_of_paragraphs)] = []

    # Split Paragraph on basis of '.' or ? or !.
    for l in re.split(r""\.|\?|\!"", para):
        # Split line into list using space.
        sentence_array.append(l)
        #sentence_array.append(l.split("" ""))

     print (model.wv.most_similar(positive=para, topn = 1))
</code></pre>

<p>But am getting the following error where it says that the paragraph checked is not a word in the vocabulary.</p>

<blockquote>
  <p>KeyError: 'word \'The Republic of Ghana is a country in West Africa. It borders Cte d\'Ivoire (also known as Ivory Coast) to the west, Burkina Faso to the north, Togo to the east, and the Gulf of Guinea to the south. The word ""Ghana"" means ""Warrior King"", Jackson, John G. Introduction to African Civilizations, 2001. Page 201.  and was the source of the name ""Guinea"" (via French Guinoye) used to refer to the West African coast (as in Gulf of Guinea).\' not in vocabulary'</p>
</blockquote>

<p>Now I am aware that the <code>most_similar()</code> function expects a single array. But I would like to know how this can be translated to extract one main feature or word that displays the  main concept of the paragraph using the word2vec model.</p>

<p><strong>Modified</strong></p>

<p>I modified the above code to pass the word_array into the <code>most_similar()</code> method and I' getting the following error.</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/new_approach.py"", line 108, in 
      print(model.wv.most_similar(positive=word_array, topn=1))
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 361, in most_similar
      for word, weight in positive + negative:
  ValueError: too many values to unpack (expected 2)</p>
</blockquote>

<p><strong>Modified Implementation</strong></p>

<pre><code>for sentence in sentence_array:
    if sentence:
        for w in re.split(r""\.|\?|\!|\@|\#|\$|\%|\^|\&amp;|\*|\(|\)|\-"",   sentence):
            split_word = w.split("" "")
            if split_word:
                word_array.append(split_word)
print(model.wv.most_similar(positive=word_array, topn=1))
</code></pre>

<p>Any suggestions in this regard are much appreciated.</p>
",2018-05-18 05:54:39,2018-05-21 04:44:34,Extract main feature of paragraphs using word2vec,<python><word2vec><feature-extraction>,,,CC BY-SA 4.0,False,False,True,False,False
16885,50352777,2018-05-15 14:27:28,,"<p>I want to train a LSTM model with Tensorflow. I have a text data as input and I get doc2vec of each paragraph of the text and pass it to the lstm layers but I get ValueError because of inconsistency of shape rank.
I've searched through Stackoverflow for similar questions and some tutorials, but I couldn't solve this error. Do you have any idea what should I do?
Here is the error:</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""writeRNN.py"", line 97, in 
      outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 627, in dynamic_rnn
      dtype=dtype)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in _dynamic_rnn_loop
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in 
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 761, in with_rank_at_least
      raise ValueError(""Shape %s must have rank at least %d"" % (self, rank))
  ValueError: Shape (?, ?) must have rank at least 3</p>
</blockquote>

<p>And below is the code:</p>

<pre><code>lstm_size = 128
lstm_layers = 1
batch_size = 50
learning_rate = 0.001

# Create the graph object
graph = tf.Graph()
# Add nodes to the graph
with graph.as_default():
    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')
    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
with graph.as_default():
    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)
    initial_state = cell.zero_state(batch_size, tf.float32)

with graph.as_default():
    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)


with graph.as_default():
    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)
    cost = tf.losses.mean_squared_error(labels_, predictions)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


with graph.as_default():
    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        state = sess.run(initial_state)
        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):
            feed = {inputs_: x, labels_: y[:, None], keep_prob: 0.5, initial_state: state}
            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)       
</code></pre>

<p>I got error on outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state) Line as the error I described.
doc2vec model is trained on gensim and converts each sentence into a vector with 100 value.
I tried to change inputs_ shape and labels_ shape but also I get same error!
I really don't know what should I do?!</p>

<p>I really thank if you could answer my question.</p>
",2018-05-15 14:32:58,2018-05-15 20:01:46,Shape ValueError in LSTM network using Tensorflow,<python><tensorflow><nlp><lstm><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16891,50373248,2018-05-16 14:11:43,,"<p>I am trying to generate Word2vec vectors.</p>

<p>I have pandas data frame.</p>

<p>I transformed it into tokens.</p>

<p><code>df[""token""]</code></p>

<p>Used Word2vec from gensim.models</p>

<pre><code>model = w2v.Word2Vec(
sentences=df[""token""],
seed=seed,
workers=num_workers,
size=num_features,
min_count=min_word_count,
window=context_size,
sample=downsampling
)
</code></pre>

<p>How do I transform my dataframe df now?</p>

<p>That is what is the equivalent of doing </p>

<pre><code>model.transform(df)
</code></pre>
",2018-05-16 21:40:34,2018-06-07 07:52:42,How to generate Word2vec Vectors in Python?,<python><neural-network><nlp><text-mining><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16892,50373381,2018-05-16 14:17:03,,"<p>While using ""<a href=""https://github.com/RaRe-Technologies/gensim/blob/8766edcd8e4baf3cfa08cdc22bb25cb9f2e0b55f/gensim/summarization/keywords.py#L200"" rel=""nofollow noreferrer"">keywords()</a>"" in summarization/keywords.py file, I am getting the same set of tags, no matter what value I choose for pos_tagger=['NN'], ['JJ'] or ['NN','JJ']</p>

<pre><code>from gensim.summarization import keywords
import requests

url='https://www.nytimes.com/2018/05/16/opinion/ramadan-spirit-america.html'
text = requests.get(url).text

print keywords(text,words=15,pos_filter=('NN'),lemmatize=True,scores=True)
print keywords(text,words=15,pos_filter('NN','JJ'),lemmatize=True,scores=True)
print keywords(text,words=15,pos_filter=('JJ'),lemmatize=True,scores=True)
</code></pre>

Expected Results

<p>If  I am giving pos_filter as 'NN', only nouns should come as tags, however, tags like ""started"", ""looking"" are also coming as output.
Similarly, there is no difference in the output irresepective of pos_filter='NN', pos_filter='NN','JJ', pos_filter='JJ' </p>

<p><strong>What is the correct way of using pos_filter to reflect appropriate output?</strong></p>

Actual Results

<p>student:0.20870111939889552, muslims:0.18960896637225794, americans:0.18895097005190414, ramadan:0.17605599898176202, month:0.12130699512494893, started:0.11817668681654464, community:0.11691583075245701, places:0.1117677772315554, spirituality:0.103727092629442, car:0.09988305780275739, white:0.09747271853405554, trump:0.09747271853405551, looking:0.09538360210000996, president:0.09538360210000986, black:0.0920316444206821</p>

<p>student:0.2087011193988958, muslims:0.18960896637225758, americans:0.1889509700519042, ramadan:0.17605599898176225, month:0.12130699512494901, started:0.11817668681654461, community:0.11691583075245732, places:0.11176777723155559, spirituality:0.10372709262944187, car:0.099883057802757, trump:0.09747271853405544, white:0.09747271853405512, president:0.0953836021000099, looking:0.09538360210000954, black:0.09203164442068222</p>

<p>student:0.20870111939889593, muslims:0.1896089663722575, americans:0.1889509700519037, ramadan:0.17605599898176255, month:0.1213069951249494, started:0.11817668681654483, community:0.11691583075245665, places:0.11176777723155547, spirituality:0.10372709262944207, car:0.09988305780275722, white:0.09747271853405541, trump:0.09747271853405526, looking:0.09538360210000975, president:0.0953836021000096, black:0.09203164442068222</p>
",,2018-05-16 14:17:03,Correct way to use pos_tagger option in gensim + keywords extraction,<keyword><gensim><pos-tagger><summarization>,,,CC BY-SA 4.0,False,False,True,False,False
16893,50264369,2018-05-10 01:53:40,,"<p>We are having n number of documents. Upon submission of new document by user, our goal is to inform him about possible duplication of existing document (just like stackoverflow suggests questions may already have answer).</p>

<p>In our system, new document is uploaded every minute and mostly about the same topic (where there are more chance of duplication).</p>

<p>Our current implementation includes gensim doc2vec model trained on documents (tagged with unique document ids). We infer vector for new document and find most_similar docs (ids) with it. Reason behind choosing doc2vec model is that we wanted to take advantage of semantics to improve results. As far as we know, it does not support online training, so we might have to schedule a cron or something that periodically updates the model. But scheduling cron will be disadvantageous as documents come in a burst. User may upload duplicates while model is not yet trained for new data. Also given huge amount of data, training time will be higher.</p>

<p>So i would like to know how such cases are handled in big companies. Are there any better alternative? or better algorithm for such problem?</p>
",,2018-05-10 04:45:59,Document similarity in production environment,<python><machine-learning><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16898,50340657,2018-05-15 00:12:49,,"<p>is it possible to plot a pyLDAvis with a Mallet implementation of LDA ? I have no troubles with LDA_Model but when I use Mallet I get :</p>

<pre><code>'LdaMallet' object has no attribute 'inference'
</code></pre>

<p>My code :</p>

<pre><code>pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(mallet_model, corpus, id2word)
vis
</code></pre>
",,2019-01-31 23:28:37,pyLDAvis with Mallet LDA implementation : LdaMallet object has no attribute 'inference',<gensim><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
16907,50390455,2018-05-17 11:35:16,,"<p>I extracted 145,185,965 sentences (14GB) out of the english wikipedia dump and I want to train a Doc2Vec model based on these sentences. Unfortunately I have 'only' 32GB of RAM and get a <em>MemoryError</em> when trying to train. Even if I set the min_count to 50, gensim tells me that it would need over 150GB of RAM. I don't think that further increasing the min_count would be a good idea, because the resulting model would be not very good (just a guess). But anyways, I will try it with 500 to see if memory is sufficient then.</p>

<p>Are there any possibilities to train such a large model with limited RAM?</p>

<p>Here is my current code:</p>

<pre><code>corpus = TaggedLineDocument(preprocessed_text_file)
model = Doc2Vec(vector_size=300, 
                window=15, 
                min_count=50,  #1
                workers=16, 
                dm=0, 
                alpha=0.75, 
                min_alpha=0.001, 
                sample=0.00001,
                negative=5)
model.build_vocab(corpus)
model.train(corpus, 
            epochs=400, 
            total_examples=model.corpus_count, 
            start_alpha=0.025, 
            end_alpha=0.0001)
</code></pre>

<p>Are there maybe some obvious mistakes I am doing? Using it completely wrong?</p>

<p>I could also try reducing the vector size, but I think this will result in much worse results as most papers use 300D vectors.</p>
",,2018-05-17 17:04:50,gensim - Doc2Vec: MemoryError when training on english Wikipedia,<python><out-of-memory><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16908,50390582,2018-05-17 11:41:46,,"<p>When reading the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec documentation of gensim</a>, I get a bit confused about some options. For example, the constructor of Doc2Vec has a parameter <em>iter</em>:</p>

<blockquote>
  <p>iter (int)  Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>Why does the train method then also have a similar parameter called <em>epochs</em>?</p>

<blockquote>
  <p>epochs (int)  Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>What is the difference between both? There's one more paragraph on it in the docs:</p>

<blockquote>
  <p>To avoid common mistakes around the models ability to do multiple
  training passes itself, an explicit epochs argument MUST be provided.
  In the common and recommended case, where train() is only called once,
  the models cached iter value should be supplied as epochs value.</p>
</blockquote>

<p>But I do not really understand why the constructor needs a <em>iter</em> parameter and what exactly should be provided for it.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw that there is also the possibility to specify the corpus directly in the constructor rather than calling train() separately. So I think in this case, <em>iter</em> would be used and otherwise <em>epochs</em>. Is that correct?</p>

<p>If so, what is the difference between specifying the corpus in the constructor and calling train() manually? Why would one choose the one or other?</p>

<p><strong>EDIT 2</strong>:</p>

<p>Although not mentioned in the docs, <em>iter</em> is now depreciated as parameter of Doc2Vec. It was renamed to <em>epochs</em> to be consistent with the parameter of <em>train()</em>. Training seems to work with that, although I struggle with <a href=""https://stackoverflow.com/questions/50390455/gensim-doc2vec-memoryerror-when-training-on-english-wikipedia"">MemoryErrors</a>.</p>
",2018-05-17 14:04:15,2018-05-17 17:13:01,gensim - Doc2Vec: Difference iter vs. epochs,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16922,50413059,2018-05-18 14:02:29,,"<p>I want to use Dynamic Topic Modeling by Blei et al. (<a href=""http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf"" rel=""nofollow noreferrer"">http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf</a>) for a large corpus of nearly 3800 patent documents.
Does anybody has experience in using the DTM in the gensim package?
I identified two models: </p>

<ol>
<li>models.ldaseqmodel  Dynamic Topic Modeling in Python <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">Link</a></li>
<li>models.wrappers.dtmmodel  Dynamic Topic Models (DTM) <a href=""https://radimrehurek.com/gensim/models/wrappers/dtmmodel.html"" rel=""nofollow noreferrer"">Link</a></li>
</ol>

<p>Which one did you use, of if you used both, which one is ""better""? In better words, which one did/do you prefer?</p>
",,2019-04-14 12:42:47,Dynamic Topic Modeling with Gensim / which code?,<python-3.x><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
16926,50306710,2018-05-12 13:22:37,,"<p>I am running gensim on Linux Suse. I can start my python program but on startup I get: </p>

<blockquote>
  <p>C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>GCC is installed. Does anyone know what I have to do?</p>
",2018-05-12 17:56:14,2018-05-12 17:56:14,"Gensim: ""C extension not loaded, training will be slow.""",<pip><word2vec><gensim><opensuse><suse>,,,CC BY-SA 4.0,False,False,True,False,False
16932,50361783,2018-05-16 02:49:54,,"<p>I have a corpus of sentences. Each of them may contain marked compound words. 
For example: </p>

<blockquote>
  <p>This is an example_sentence followed by another awesome_paragraph</p>
</blockquote>

<p>.
I want to get embedding vector for all tokens and compound words</p>

<blockquote>
  <p>(this, is, an, example, sentence, followed, by, another, awesome,
  paragraph, example_sentence, awesome_paragraph)</p>
</blockquote>

<p>Can I do this with gensim or which library should I use?</p>
",2018-05-16 07:40:09,2018-05-16 07:40:09,Vector representation for token and compound word,<python><machine-learning><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16933,50362506,2018-05-16 04:32:06,,"<p>I am using <code>Doc2vec</code> to get vectors from words.
Please see my below code:</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
f = open('test.txt','r')

trainings = [TaggedDocument(words = data.strip().split("",""),tags = [i]) for i,data in enumerate(f)


model = Doc2Vec(vector_size=5,  epochs=55, seed = 1, dm_concat=1)

model.build_vocab(trainings)
model.train(trainings, total_examples=model.corpus_count, epochs=model.epochs)

model.save(""doc2vec.model"")

model = Doc2Vec.load('doc2vec.model')
for i in range(len(model.docvecs)):
    print(i,model.docvecs[i])
</code></pre>

<p>I have a <code>test.txt</code> file that its content has 2 lines and contents of these 2 lines is the same (they are ""a"")
I trained with doc2vec and got the model, but the problem is although the contents of 2 lines is the same, doc2vec gave me 2 different vectors.</p>

<pre><code>0 [ 0.02730868  0.00393569 -0.08150548 -0.04009786 -0.01400406]
1 [ 0.03916578 -0.06423566 -0.05350181 -0.00726833 -0.08292392]
</code></pre>

<p>I dont know why this happened. I thought that these vectors would be the same.
Can you explain that? And if I want to make the same vectors for the sames words, what should I do in this case?</p>
",2018-05-16 20:08:46,2018-05-16 21:06:02,Why Doc2vec gives 2 different vectors for the same texts,<python><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16940,50413300,2018-05-18 14:15:17,,"<p>I am currently trying to implement a neural network that uses a doc2vec vector, and then uses that to work further. 
I have a machine which only allows me to use tensorflow (this is a requirement!), so I need a model to transform a sentence / paragraph to a vector. </p>

<p>I know about gensim's <code>doc2vec</code> and <a href=""https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py"" rel=""nofollow noreferrer"">this</a> implementation. I have experience with gensim's implementation, but it apparently does not use tensorflow in the backend. The latter link, however, does not work without a few hours / days of debugging it seems.</p>

<p>I would be helpful for any links and recommendations! </p>
",2018-06-06 08:23:35,2018-06-06 08:23:35,"What are document to vector methods (doc2vec), that rely on a tensorflow backend",<python><tensorflow><nlp><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16948,50383746,2018-05-17 05:12:51,,"<pre><code>#!/usr/bin/env python
# -*- coding: utf-8  -*-    
import warnings

warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')  

import logging
import os.path
import sys
import multiprocessing

# from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

if __name__ == '__main__':    
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    min_count=100
    data_dir='/opt/mengyuguang/word2vec/'
    inp = data_dir + 'wiki.zh.simp.seg.txt'
    outp1 = data_dir + 'wiki.zh.min_count{}.model'.format(str(min_count))
    outp2 = data_dir + 'wiki.zh.min_count{}.vector'.format(str(min_count))

    # train cbow
    model = Word2Vec(LineSentence(inp), size=300,
                     workers=multiprocessing.cpu_count(),min_count=min_count)

    # save
    model.save(outp1)
    model.wv.save_word2vec_format(outp2, binary=False)
</code></pre>

<p>Firstly,I trained word embedding with the code above, I don't think there is anything wrong with it. And I created a list <code>vocab</code> to store the words in the vector file. Then </p>

<pre><code>vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length) 
pretrain = vocab_processor.fit(vocab)
</code></pre>

<p>Vocab is a list of  415657 words. And I got a vocabulary of 412722. I know that <code>vocab_processor.fit</code> won't take upper and lower case as two words. This is really strange. How is this happening?<br>
I checked the vector file again. There are no overlapping words at all.</p>
",2018-05-17 08:57:05,2018-06-08 02:49:06,Got a smaller vocabulary after using tf.contrib.learn.preprocessing.VocabularyProcessor,<python><tensorflow><nlp><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
16950,50396111,2018-05-17 16:13:44,,"<p>I load a word2vec-format file and I want to calculate the similarities between vectors, but I don't know what this issue means.</p>

<pre><code>from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import KeyedVectors
import numpy as np

model = KeyedVectors.load_word2vec_format('it-vectors.100.5.50.w2v')

similarities = cosine_similarity(model.vectors)


---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
&lt;ipython-input-54-1d4e62f55ebf&gt; in &lt;module&gt;()
----&gt; 1 similarities = cosine_similarity(model.vectors)

/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py in cosine_similarity(X, Y, dense_output)
    923         Y_normalized = normalize(Y, copy=True)
    924 
--&gt; 925     K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)
    926 
    927     return K

/usr/local/lib/python3.5/dist-packages/sklearn/utils/extmath.py in safe_sparse_dot(a, b, dense_output)
    138         return ret
    139     else:
--&gt; 140         return np.dot(a, b)
    141 
    142 

MemoryError: 
</code></pre>

<p>What it means?
Thank you!</p>
",,2018-05-17 17:16:41,Cosine similarity with word2vec,<scikit-learn><nlp><word2vec><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,True
16956,50438428,2018-05-20 19:03:03,,"<p>I am struggling to understand the usage of doc2vec. I trained a toy model on a set of documents using some sample code I saw on googling. Next I want to find the document that the model considers to be the closest match to documents in my training data. Say my document is ""This is a sample document"".</p>

<pre><code>test_data = word_tokenize(""This is a sample document"".lower())
v = model.infer_vector(test_data)
print(v)
# prints a numpy array.

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)
# prints [('0', 0.8838234543800354), ('1', 0.875300943851471), ('3', 
#          0.8752948641777039), ('2', 0.865660548210144)]
</code></pre>

<p>I searched a fair bit but I am confused how to interpret similar_doc. I want to answer the question: ""which documents in my training data most closely match the document 'This is a sample document'"", so how do I map the similar_doc output back to the training data? I did not understand the array of tuples, the second half of each tuple must be a probability but what are '0', '1' etc?</p>
",,2018-05-21 14:39:32,doc2vec get most similar document,<python><machine-learning><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
16958,50492676,2018-05-23 15:50:18,,"<p>I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. <em>It didn't work</em>. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.<br>
Equivalent to my code:  </p>

<pre><code>import gensim
corpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]
model = gensim.models.Word2Vec(iter = 5,size = 64)
model.build_vocab(corpus)
# save memory
vectors = model.wv
del model
vectors.save_word2vec_format(""vect.txt"",binary = False)
</code></pre>

<p>That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.<br>
One idea that has been presented to me is implementing the appropriate tensorflow code, but I dont know how to code that, just import files in the live demo.  </p>

<p>Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do.<br>
  Ok. Done with that too! Thanks for your help!</p>
",2018-05-24 19:06:32,2020-03-11 14:31:33,Visualize Gensim Word2vec Embeddings in Tensorboard Projector,<python><tensorflow><gensim><tensorboard><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
16964,50366293,2018-05-16 08:49:11,,"<p>I am working on a project to classify customer feedback into buckets based on the topic of the feedback comment. So , I need to classify the sentence into one of the topics among a list of pre-defined topics. </p>

<p><strong>For example :</strong></p>

<p>""I keep getting an error message every time I log in"" has to be tagged with ""login"" as the topic.</p>

<p>""make the screen more colorful"" has to be tagged with ""improvements"" as the topic.</p>

<p>So the <strong>topics are very specific to the product and the context</strong>.</p>

<p>LDA doesn't seem to work for me(correct me if i'm wrong). It detects the topics in a general sense like ""Sports"" , ""Politics"" , ""Technology"" etc. But I need to detect specific topics as mentioned above.</p>

<p>Also , I don't have labelled data for training. All I have is the comments.
So supervised learning approach doesn't look like an option.</p>

<p><strong>What I have tried so far:</strong></p>

<p>I trained a gensim model with google news corpus (its about 3.5 gb).
I am cleaning the sentence by removing stop words , punctuation marks etc.
I am finding , to what topic among the set of topics each word is closest to and tag the word to that topic. With an idea that the sentence might contain more words closer to the topic it is referring to than not , I am picking up the topic(s) to which maximum number of words in the sentence is mapped to.</p>

<p><strong>For example:</strong></p>

<p>If 3 words in a sentence is mapped to ""login"" topic and 2 words in the sentence is mapped to ""improvement"" topic , I am tagging the sentence to ""login"" topic. </p>

<p>If there is a clash between the count of multiple topics , I return all the topics with the maximum count as the topic list.</p>

<p>This approach is giving me fair results. But its not good enough.</p>

<p><strong>What will be the best approach to tackle this problem?</strong></p>
",2018-05-16 08:54:57,2020-10-06 14:42:33,How to classify a sentence into one of the pre-defined topic bucket using an unsupervised approach,<python><machine-learning><nlp><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
16966,50326147,2018-05-14 08:38:13,,"<p>As I was just experimenting with NLP then I was working on sarcasm detection but in meanwhile I had put this code. </p>

<p><strong>sarcasmextractor.py</strong></p>

<pre><code># coding: utf-8

# Importing the library

# In[2]:

import io
import sys
import os
import numpy as np
import pandas as pd
import nltk
import gensim
import csv, collections
from textblob import TextBlob
from sklearn.utils import shuffle
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from sklearn.feature_extraction import DictVectorizer
import pickle
import replace_emoji


# Define a class to load the SentimentWordnet and write methods to calculate the scores

# In[4]:

class load_senti_word_net(object):
    """"""
    constructor to load the file and read the file as CSV
    6 columns - pos, ID, PosScore, NegScore, synsetTerms, gloss
    synsetTerms can have multiple similar words like abducting#1 abducent#1 and will read each one and calculaye the scores
    """"""

    def __init__(self):
        sent_scores = collections.defaultdict(list)
        with io.open(""SentiWordNet_3.0.0_20130122.txt"") as fname:
            file_content = csv.reader(fname, delimiter='\t',quotechar='""')

            for line in file_content:                
                if line[0].startswith('#') :
                    continue                    
                pos, ID, PosScore, NegScore, synsetTerms, gloss = line
                for terms in synsetTerms.split("" ""):
                    term = terms.split(""#"")[0]
                    term = term.replace(""-"","""").replace(""_"","""")
                    key = ""%s/%s""%(pos,term.split(""#"")[0])
                    try:
                        sent_scores[key].append((float(PosScore),float(NegScore)))
                    except:
                        sent_scores[key].append((0,0))

        for key, value in sent_scores.items():
            sent_scores[key] = np.mean(value,axis=0)

        self.sent_scores = sent_scores    

    """"""
    For a word,
    nltk.pos_tag([""Suraj""])
    [('Suraj', 'NN')]
    """"""

    def score_word(self, word):
        pos = nltk.pos_tag([word])[0][1]
        return self.score(word, pos)

    def score(self,word, pos):
        """"""
        Identify the type of POS, get the score from the senti_scores and return the score
        """"""

        if pos[0:2] == 'NN':
            pos_type = 'n'
        elif pos[0:2] == 'JJ':
            pos_type = 'a'
        elif pos[0:2] =='VB':
            pos_type='v'
        elif pos[0:2] =='RB':
            pos_type = 'r'
        else:
            pos_type =  0

        if pos_type != 0 :    
            loc = pos_type+'/'+word
            score = self.sent_scores[loc]
            if len(score)&gt;1:
                return score
            else:
                return np.array([0.0,0.0])
        else:
            return np.array([0.0,0.0])

    """"""
    Repeat the same for a sentence
    nltk.pos_tag(word_tokenize(""My name is Suraj""))
    [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Suraj', 'NNP')]    
    """"""    

    def score_sentencce(self, sentence):
        pos = nltk.pos_tag(sentence)
        print (pos)
        mean_score = np.array([0.0, 0.0])
        for i in range(len(pos)):
            mean_score += self.score(pos[i][0], pos[i][1])

        return mean_score

    def pos_vector(self, sentence):
        pos_tag = nltk.pos_tag(sentence)
        vector = np.zeros(4)

        for i in range(0, len(pos_tag)):
            pos = pos_tag[i][1]
            if pos[0:2]=='NN':
                vector[0] += 1
            elif pos[0:2] =='JJ':
                vector[1] += 1
            elif pos[0:2] =='VB':
                vector[2] += 1
            elif pos[0:2] == 'RB':
                vector[3] += 1

        return vector



# Now let's extract the features
# 
# ###Stemming and Lemmatization

# In[5]:

porter = nltk.PorterStemmer()
sentiments = load_senti_word_net()


# In[7]:

def gram_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(str(sentence))
    token = nltk.word_tokenize(sentence_rep)
    token = [porter.stem(i.lower()) for i in token]        

    bigrams = nltk.bigrams(token)
    bigrams = [tup[0] + ' ' + tup[1] for tup in bigrams]
    grams = token + bigrams
    #print (grams)
    for t in grams:
        features['contains(%s)'%t]=1.0



# In[8]:

import string
def sentiment_extract(features, sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)    
    token = [porter.stem(i.lower()) for i in token]   
    mean_sentiment = sentiments.score_sentencce(token)
    features[""Positive Sentiment""] = mean_sentiment[0]
    features[""Negative Sentiment""] = mean_sentiment[1]
    features[""sentiment""] = mean_sentiment[0] - mean_sentiment[1]
    #print(mean_sentiment[0], mean_sentiment[1])

    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in token]).strip())
        features[""Blob Polarity""] = text.sentiment.polarity
        features[""Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""Blob Polarity""] = 0
        features[""Blob Subjectivity""] = 0
        print(""do nothing"")


    first_half = token[0:int(len(token)/2)]    
    mean_sentiment_half = sentiments.score_sentencce(first_half)
    features[""positive Sentiment first half""] = mean_sentiment_half[0]
    features[""negative Sentiment first half""] = mean_sentiment_half[1]
    features[""first half sentiment""] = mean_sentiment_half[0]-mean_sentiment_half[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in first_half]).strip())
        features[""first half Blob Polarity""] = text.sentiment.polarity
        features[""first half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""first Blob Polarity""] = 0
        features[""first Blob Subjectivity""] = 0
        print(""do nothing"")

    second_half = token[int(len(token)/2):]
    mean_sentiment_sechalf = sentiments.score_sentencce(second_half)
    features[""positive Sentiment second half""] = mean_sentiment_sechalf[0]
    features[""negative Sentiment second half""] = mean_sentiment_sechalf[1]
    features[""second half sentiment""] = mean_sentiment_sechalf[0]-mean_sentiment_sechalf[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in second_half]).strip())
        features[""second half Blob Polarity""] = text.sentiment.polarity
        features[""second half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""second Blob Polarity""] = 0
        features[""second Blob Subjectivity""] = 0
        print(""do nothing"")  





# In[9]:

features = {}
sentiment_extract(features,""a long narrow opening"")


# In[11]:

def pos_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)
    token = [ porter.stem(each.lower()) for each in token]
    pos_vector = sentiments.pos_vector(token)
    for j in range(len(pos_vector)):
        features['POS_'+str(j+1)] = pos_vector[j]
    print (""done"")



# In[12]:

features = {}
pos_features(features,""a long narrow opening"")


# In[13]:

def capitalization(features,sentence):
    count = 0
    for i in range(len(sentence)):
        count += int(sentence[i].isupper())
    features['Capitalization'] = int(count &gt; 3)
    print (count)


# In[14]:

features = {}
capitalization(features,""A LoNg NArrow opening"")


# In[15]:

import topic
topic_mod = topic.topic(nbtopic=200,alpha='symmetric')


# In[16]:

topic_mod = topic.topic(model=os.path.join('topics.tp'),dicttp=os.path.join('topics_dict.tp'))


# In[17]:

def topic_feature(features,sentence,topic_modeler):    
    topics = topic_modeler.transform(sentence)    
    for j in range(len(topics)):
        features['Topic :'] = topics[j][1]



# In[18]:

topic_feature(features,""A LoNg NArrow opening"",topic_mod)


# In[19]:

def get_features(sentence, topic_modeler):
    features = {}
    gram_features(features,sentence)
    pos_features(features,sentence)
    sentiment_extract(features, sentence)
    capitalization(features,sentence)
    topic_feature(features, sentence,topic_modeler)
    return features


# In[20]:

df = pd.DataFrame()
df = pd.read_csv(""dataset_csv.csv"", header=0, sep='\t')
df.head()


# In[17]:

import re

for i in range(0,df.size):
    temp = str(df[""tweets""][i])
    temp = re.sub(r'[^\x00-\x7F]+','',temp)
    featureset.append((get_features(temp,topic_mod), df[""label""][i]))


# In[20]:

c = []
for i in range(0,len(featureset)):
    c.append(pd.DataFrame(featureset[i][0],index=[i]))

result = pd.concat(c)


# In[22]:

result.insert(loc=0,column=""label"",value='0')


# In[23]:

for i in range(0, len(featureset)):
    result[""label""].loc[i] = featureset[i][1]   



# In[25]:

result.to_csv('feature_dataset.csv')


# In[3]:

df = pd.DataFrame()
df = pd.read_csv(""feature_dataset.csv"", header=0)
df.head()


# In[4]:

get_ipython().magic('matplotlib inline')

import matplotlib as matplot 
import seaborn

result = df


# In[5]:

X = result.drop(['label','Unnamed: 0','Topic :'],axis=1).values


# In[6]:

Y = result['label']


# In[7]:

import pickle
import pefile
import sklearn.ensemble as ek
from sklearn import cross_validation, tree, linear_model
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
from sklearn import svm
from sklearn.linear_model import LinearRegression
import sklearn.linear_model as lm


# In[29]:

model = { ""DecisionTree"":tree.DecisionTreeClassifier(max_depth=10),
         ""RandomForest"":ek.RandomForestClassifier(n_estimators=50),
         ""Adaboost"":ek.AdaBoostClassifier(n_estimators=50),
         ""GradientBoosting"":ek.GradientBoostingClassifier(n_estimators=50),
         ""GNB"":GaussianNB(),
         ""Logistic Regression"":LinearRegression()   
}


# In[8]:

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y ,test_size=0.2)


# In[9]:

X_train = pd.DataFrame(X_train)
X_train = X_train.fillna(X_train.mean())

X_test = pd.DataFrame(X_test)
X_test = X_test.fillna(X_test.mean())


# In[38]:

results_algo = {}
for algo in model:
    clf = model[algo]
    clf.fit(X_train,y_train.astype(int))
    score = clf.score(X_test,y_test.astype(int))
    print (""%s : %s "" %(algo, score))
    results_algo[algo] = score



# In[39]:

winner = max(results_algo, key=results_algo.get)


# In[40]:

clf = model[winner]
res = clf.predict(X_test)
mt = confusion_matrix(y_test, res)
print(""False positive rate : %f %%"" % ((mt[0][1] / float(sum(mt[0])))*100))
print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))


# In[41]:

from sklearn import metrics
print (metrics.classification_report(y_test, res))


# In[34]:

test_data = ""public meetings are awkard for me as I can insult people but I choose not to and that is something that I find difficult to live with""


# In[101]:

test_data=""I purchased this product 4.47 billion years ago and when I opened it today, it was half empty.""


# In[82]:

test_data=""when people see me eating and ask me are you eating? No no I'm trying to choke myself to death #sarcastic""


# In[102]:

test_feature = []
test_feature.append((get_features(test_data,topic_mod)))


# In[104]:

test_feature


# In[105]:

c = []

c.append(pd.DataFrame(test_feature[0],index=[i]))

test_result = pd.concat(c)
test_result = test_result.drop(['Topic :'],axis=1).values


# In[106]:

res= clf.predict(test_result)
</code></pre>

<p>But it is giving me the following error:</p>

<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
[('a', 'DT'), ('long', 'JJ'), ('narrow', 'JJ'), ('open', 'JJ')]
[('a', 'DT'), ('long', 'JJ')]
[('narrow', 'JJ'), ('open', 'JJ')]
done
5
Traceback (most recent call last):
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 276, in &lt;module&gt;
    topic_feature(features,""A LoNg NArrow opening"",topic_mod)
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 268, in topic_feature
    topics = topic_modeler.transform(sentence)    
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\topic.py"", line 42, in transform
    return self.lda[corpus_sentence]     
  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 1160, in __getitem__
    return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
AttributeError: 'LdaModel' object has no attribute 'minimum_phi_value'
</code></pre>

<p>Code for <strong>topic.py</strong>:</p>

<pre><code>from gensim import corpora, models, similarities
import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
import replace_emoji

class topic(object):
    def __init__(self, nbtopic = 100, alpha=1,model=None,dicttp=None):
        self.nbtopic = nbtopic
        self.alpha = alpha
        self.porter = nltk.PorterStemmer()
        self.stop = stopwords.words('english')+['.','!','?','""','...','\\',""''"",'[',']','~',""'m"",""'s"",';',':','..','$']
        if model!=None and dicttp!=None:
            self.lda = models.ldamodel.LdaModel.load(model)
            self.dictionary =  corpora.Dictionary.load(dicttp)

    def fit(self,documents):

        documents_mod = documents
        tokens = [nltk.word_tokenize(sentence) for sentence in documents_mod]
        tokens = [[self.porter.stem(t.lower()) for t in sentence if t.lower() not in self.stop] for sentence in tokens]        

        self.dictionary = corpora.Dictionary(tokens)
        corpus = [self.dictionary.doc2bow(text) for text in tokens]
        self.lda = models.ldamodel.LdaModel(corpus,id2word=self.dictionary, num_topics=self.nbtopic,alpha=self.alpha)

        self.lda.save('topics.tp')
        self.dictionary.save('topics_dict.tp')

    def get_topic(self,topic_number):

        return self.lda.print_topic(topic_number)

    def transform(self,sentence):

        sentence_mod = sentence
        tokens = nltk.word_tokenize(sentence_mod)
        tokens = [self.porter.stem(t.lower()) for t in tokens if t.lower() not in self.stop] 
        corpus_sentence = self.dictionary.doc2bow(tokens)

        return self.lda[corpus_sentence]     
</code></pre>

<p>The overall code is found here <a href=""https://github.com/surajr/SarcasmDetection"" rel=""nofollow noreferrer"">overall code</a>.</p>
",2018-05-14 13:38:07,2018-05-14 13:38:07,AttributeError: 'LdaModel' object has no attribute 'minimum_phi_value',<python><tensorflow><nlp><gensim><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,True
16969,50477192,2018-05-22 22:16:44,,"<p>I have persisted a Word2Vec model to a binary file. I am trying to load it into a serverless API adapted from this <a href=""https://medium.com/@patrickmichelberger/how-to-deploy-a-serverless-machine-learning-microservice-with-aws-lambda-aws-api-gateway-and-d5b8cbead846"" rel=""nofollow noreferrer"">blog</a> and using <a href=""https://ryan-cranfill.github.io/gensim-aws-lambda/"" rel=""nofollow noreferrer"">this</a> as a basis.</p>

<p>This works fine locally:</p>

<pre><code>self.model = KeyedVectors.load_word2vec_format('./models/models.bin', binary=True)
</code></pre>

<p>but when calling the file from S3 it errors with:</p>

<blockquote>
  <p>'IOError: [Errno 2] No such file or directory: '46659 100\n,|PUNCT
  \xec>\xd8>\xaf\xa8\x95'</p>
</blockquote>

<pre><code>def load_model(key):
response = S3.get_object(Bucket=BUCKET_NAME, Key=key)
model_str = response['Body'].read()

model = KeyedVectors.load_word2vec_format(model_str, binary=True)
return model
</code></pre>
",,2018-09-25 12:29:21,Loading Word2Vec binary model from S3 into Gensim fails,<python><amazon-s3><gensim><serverless>,,,CC BY-SA 4.0,False,False,True,False,False
16974,50478046,2018-05-23 00:02:54,,"<p>I am using gensim library for loading pre-trained word vectors from GoogleNews dataset. this dataset contains 3000000 word vectors each of 300 dimensions. when I want to load GoogleNews dataset, I receive a memory error. I have tried this code before without memory error and I don't know why I receive this error now.
I have checked a lot of sites for solving this issue but I cant understand.
this is my code for loading GoogleNews:</p>

<pre><code>import gensim.models.keyedvectors as word2vec
model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)
</code></pre>

<p>and this is the error I received:</p>

<pre><code>File ""/home/mahsa/PycharmProjects/tensor_env_project/word_embedding_DUC2007/inspect_word2vec-master/word_embeddings_GoogleNews.py"", line 8, in &lt;module&gt;
    model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)
  File ""/home/mahsa/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 212, in load_word2vec_format
    result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)
MemoryError
</code></pre>

<p>can anybody help me? thanks.</p>
",,2020-04-17 06:08:03,memory error when using gensim for loading word2vec,<python><word2vec><gensim><word-embedding><google-news>,,,CC BY-SA 4.0,False,False,True,False,False
16999,50463415,2018-05-22 08:45:36,,"<p>I did LDA over a corpus of documents with topic_number=5. As a result, I have five vectors of words, each word associates with a weight or degree of importance, like this:</p>
<pre><code>Topic_A = {(word_A1,weight_A1), (word_A2, weight_A2), ... ,(word_Ak, weight_Ak)}
Topic_B = {(word_B1,weight_B1), (word_B2, weight_B2), ... ,(word_Bk, weight_Bk)}
.
.
Topic_E = {(word_E1,weight_E1), (word_E2, weight_E2), ... ,(word_Ek, weight_Ek)}
</code></pre>
<p>Some of the words are common between documents. Now, I want to know, how I can calculate the similarity between these vectors. I can calculate cosine similarity (and other similarity measures) by programming from scratch, but I was thinking, there might be an easier way to do it. Any help would be appreciated. Thank you in advance for spending time on this.</p>
<blockquote>
<ul>
<li><p>I am programming with Python 3.6 and gensim library (but I am open to any other library)</p>
</li>
<li><p>I know someone else has asked similar question (<a href=""https://stackoverflow.com/questions/48115965/cosine-similarity-and-lda-topics"">Cosine Similarity and LDA topics</a>) but becasue he didn't get the answer, I ask it again</p>
</li>
</ul>
</blockquote>
",2020-06-20 09:12:55,2018-05-22 12:59:01,Calculating the similarity between two vectors,<python-3.x><nlp><gensim><lda><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
17005,50501364,2018-05-24 05:05:37,,"<p>I am trying to visualize word2vec i created from amazon reviews corpus.....i sampled about 5k positive and 5k negative rows....the score column contain whether the reviews are positive or negative....
Here's my code:
**</p>

<pre><code>For avg w2v i did this(list of sent contains the avg w2v for each review)
w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)
Y = w2v_model[w2v_model.wv.vocab]
tsne = TSNE(n_components=2, perplexity = 30)
tsne_data = tsne.fit_transform(Y)
</code></pre>

<p>**</p>

<p>Now i want to plot these according to the score i.e blue dots for positive and red for negative.......i don't know how to do this!!.....
Any help would be appreciated..</p>
",2018-05-24 08:03:15,2018-05-24 08:28:58,Word2vec tsne plot,<python><plot><visualization><word2vec><dimensionality-reduction>,,,CC BY-SA 4.0,False,False,True,False,False
17008,50535278,2018-05-25 18:51:58,,"<p>I'm trying to replicate Mikolov's work of PV-DM + PV-DBOW. He says that both algorithms should be used in order to get better results. For this reason I'm trying to train the model and then give the document tags to t-SNE.
Using Gensim's Doc2Vec I can get the document tags with <code>docvecs.vectors_docs</code>, but the concatenated structure doesn't appear to have the document tags of the joint model. It is still treating the models as separate entities.
(This I can see from the variable explorer)</p>

<p>I'm also using the <code>ConcatenatedDoc2Vec</code> from gensim.</p>

<p>Can anyone help me? Is there a way I can get the document tags from the concatenated new entity and not the individual ones?</p>
",2018-05-25 19:18:07,2018-05-28 00:57:50,Gensim Doc2Vec getting the doc tags from the Concatenated model,<python><model><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17013,50516951,2018-05-24 19:36:38,,"<p>My team has been given a task of looking at ~3,800 documents to see which are useful to be rebranded with our updated company branding guidelines.  It will take us forever, so I figured I would at least try to summarize the documents with Gensim, extract some keywords, and write the file name, summary, and keywords to a CSV. I'm doing this in the latest Jupyter Notebook using the Python 3 kernel.</p>

<p>I'm a python newbie, but I've done this in the past by looping through the directory and haven't had an issue.</p>

<p>This time after about 2 or so hours, the memory utilization starts creeping up to around 12GB and stays there for about an hour then returns down to normally fluctuationg between 100-300MB. Then an hour or two later the memory jumps back up and stays there and doesn't seem to process any more files...I left it running for 24 hours.  Being new to Python I'm not familiar with how to debug this and I'm not sure if it is because of my loop, some type of file encoding issues, or just a corrupt file or two in general.</p>

<p>Any help would be extremely greatful.  Code below.</p>

<pre><code>#Start java tika server before running

import os, sys, csv
import tika
from tika import parser
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.summarization import summarize, keywords

# Set path to directory where files are
path = 'C:/Users/john/Desktop/WWS-Local' #directory has 3,800 files of multiple types
os.chdir(path)

exFileTypes = ('.zip', '.jpg', '.mp4', '.msg', '.oft', '.txt', '.png') #Don't care about these file types
with open('C:/Users/john/Desktop/winProcessedFiles.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(['File Name', 'Summary', 'Keywords'])
    for file in os.listdir('.'):
        if not file.endswith(exFileTypes):
            try:
                parsed = parser.from_file(file)
                text = parsed[""content""]
                text = text.strip('\n')
                text = text.encode('ascii','ignore').decode('ascii')
                summary = summarize(text, word_count=200)
                kw = keywords(text, words=15)
                writer.writerow([file, summary, kw])
            except Exception as e:
                print(file, e)
                writer.writerow([file, e, 'ERROR'])
            finally:
                pass
</code></pre>

<p>I started doing some basic logging and have been looking at my log file every so often.  My memory usage has spiked and it seems like the document being processed is taking an extremely long time.</p>

<p>I noticed this in the log:</p>

<p>INFO:root:102381-manufacturing-automotive-competitive-assessment-english-letter.pptx parsed in 10.553594589233398s
INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])
INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(2545 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(3386 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary(3963 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary(4335 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary(4565 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary(4736 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:built Dictionary(4923 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...) from 66286 documents (total 1009200 corpus positions)</p>

<p>Could this be the cause of the increased memory usage?  And why is it creating multiple dictionaries for this file when it looks like it only creates one dictionary for the other files?</p>
",2018-05-25 02:22:05,2018-05-25 02:22:05,Python - Extracting text and summarize multiple file types in directory with Tika and Gensim,<python-3.x><jupyter-notebook><gensim><apache-tika>,,,CC BY-SA 4.0,False,False,True,False,False
17015,50521304,2018-05-25 03:57:03,,"<p>I'm trying to cluster some descriptions using LSI. As the dataset that I have is too long, I'm clustering based on the vectors obtained from the models instead of using the similarity matrix, which requires too much memory, and if I pick a sample, the matrix generated doesn't correspond to a square (this precludes the use of MDS).</p>

<p>However, after running the model and looking for the vectors I'm getting different vector's lengths in the descriptions. Most of them have a length of 300 (the num_topics argument in the model), but some few, with the same description, present a length of 299.</p>

<p>Why is this happening? Is there a way to correct it?</p>

<pre><code>dictionary = gensim.corpora.Dictionary(totalvocab_lemmatized)
dictionary.compactify()

corpus = [dictionary.doc2bow(text) for text in totalvocab_lemmatized]

###tfidf model
tfidf = gensim.models.TfidfModel(corpus, normalize = True)
corpus_tfidf = tfidf[corpus]

###LSI model
lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)
vectors =[]
for n in lemmatized[:100]:
    vec_bow = dictionary.doc2bow(n)
    vec_lsi = lsi[vec_bow]
    print(len(vec_lsi))
</code></pre>
",2018-05-25 05:48:20,2019-05-27 15:30:46,Why I get different length of vectors using gensim LSI model?,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17017,50447523,2018-05-21 11:06:45,,"<p>I am a C# programmer and new to Python. Now I am working on Python form Visual Studio 2017.</p>
<p>I have already written a program in C# to do some text processing tasks, but now I need to accomplish my work in python as it provides advanced functions for dealing with Natural Language Processing.</p>
<p>Specifically, I need to pass a <code>List&lt;List&gt;</code> parameter from my C# application to the Python application.</p>
<p>The passed parameter will be used instead of <code>doc_term_matrix</code> inline 48 at the following python snippet:</p>
<p><a href=""https://i.stack.imgur.com/TsWk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TsWk4.png"" alt=""enter image description here"" /></a></p>
<p>Another issue to be handled is the casting of the different data types, as I want to import a <code>List&lt;List&lt;double&gt;&gt;</code>, as shown below, from C# to be used as the parameter returned from the <code>dictionary.doc2bow(doc)</code>  function in Python.</p>
<p><a href=""https://i.stack.imgur.com/Fmq9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fmq9E.png"" alt=""enter image description here"" /></a></p>
<p>I have no idea how to do so. So, please help me solve this issue.</p>
",2020-10-02 05:07:21,2020-10-02 05:07:21,Passing parameters from C# application to Python application [for using in gensim] in Visual Studio 2017,<c#><python><visual-studio-2017><parameter-passing><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17026,50408740,2018-05-18 10:00:20,,"<p>I have the following code and I think I am getting the vectors in a wrong way, because for example the vectors of two documents that are 100% identical are not the same.</p>

<pre><code>def getDocs(corpusPath):
    """"""Function for processings documents as TaggedDocument""""""
    # Loop over all the files in corpus
    for file in glob.glob(os.path.join(corpusPath, '*.csv')):
        # getWords is a function that gets the words from the provided directory
        # os.path.basename(file) takes the filename from the complete path
        yield TaggedDocument(words=getWords(file), tags=[os.path.basename(file)])

def getModel(corpusPath, outputName):
    # Get documents words from path
    documents = getDocs(corpusPath)

    cores = multiprocessing.cpu_count()

    # Initialize the model
    model = models.doc2vec.Doc2Vec(vector_size=100, epochs=10, min_count=1, max_vocab_size=None, alpha=0.025, min_alpha=0.01, workers=cores)

    # Build Vocabulary
    model.build_vocab(documents)

    # Train the model
    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

    # Save the model as shown below
    model.save_word2vec_format(outputName, doctag_vec=True, word_vec=False, prefix="""")
</code></pre>

<p>And the output has to be like this:</p>

<pre><code>12571 100
134602.csv 0.00691074 0.157398 0.0921498 0.126362 0.158668 -0.0753151 -0.164655 0.0883756 0.0407546 0.15239 -0.0145177 0.061617 -0.0891562 -0.0417054 -0.0858589 0.00102948 0.0161595 2.13553e-05 -0.0668119 0.0450828 0.117537 -0.0729031 -0.0580456 -0.00258632 -0.104359 0.136366 -0.144994 -0.12065 -0.121757 0.0830929 -0.16462 -0.0151503 0.0399056 0.160027 -0.0787732 -0.00789994 -0.094897 0.00608254 -0.0661624 0.129721 0.163127 -0.0793746 -0.0964145 0.0606208 0.0875067 0.0161015 -0.132051 -0.0491245 -0.154828 0.133222 -0.0687664 0.120808 -0.111705 -0.053042 -0.0912231 -0.111089 0.0443708 -0.139493 0.0607425 -0.161168 0.0786498 0.150048 0.146688 -0.0837242 -0.0553738 -0.117545 0.0986267 -0.0923841 0.098877 -0.12193 -0.062616 -0.0845228 -0.0636123 0.0823107 -0.0826875 0.139011 -0.0923962 0.0288433 0.137355 0.121588 -0.145517 0.160373 0.0628389 -0.0764258 -0.107213 0.0421445 0.137447 -0.0658571 0.0424128 0.0672861 0.109817 -0.126953 -0.0453275 0.0834503 0.0974179 0.00825522 -0.165445 -0.0213084 -0.0292943 -0.162938
125202.csv 0.106642 0.167441 -0.0275412 0.130408 -0.107533 0.091452 0.0103496 -0.0214623 0.0873943 -0.0465384 -0.165227 -0.0540914 -0.00923723 0.175378 -0.051865 0.0107003 -0.179349 0.0683971 -0.159605 0.0644916 0.136338 0.111336 -0.0805002 0.00214934 -0.0490576 0.151279 -0.0397022 0.075442 -0.0278023 -0.0636982 0.174473 0.087985 -0.0714066 -0.0800442 -0.103995 -0.0228613 0.157171 -0.0678672 -0.161953 0.0839289 -0.155191 -0.00721683 0.0586751 -0.0474399 -0.122106 0.170611 0.157929 0.075531 -0.13505 0.093849 -0.119415 0.0386302 0.0139714 0.0756701 -0.0810199 -0.111754 0.112905 0.130293 -0.126257 -0.00654255 -0.0369909 -0.072449 0.0257127 0.0716955 0.103714 -0.0842208 -0.0534867 -0.095218 0.127797 -0.029322 0.161806 -0.177695 -0.0684089 0.0623551 0.06396 0.0828089 -0.0590939 0.0180832 -0.0591218 0.136139 -0.153984 0.108085 -0.127018 -0.0847872 -0.167081 0.0199622 0.0209045 0.0320618 0.0591803 0.0809688 0.0799196 0.15632 -0.0519707 0.0270171 -0.163197 -0.0846849 -0.176135 -0.0120047 -0.0697305 0.014441
116200.csv -0.0182099 -0.130409 -0.138414 -0.0310527 -0.0274882 -0.0711805 -0.0628653 -0.144249 -0.166021 -0.0242265 -0.130593 -0.141916 0.0119525 0.0500143 -0.147568 -0.036778 0.110357 0.0439302 -0.132496 -0.105203 0.0356234 0.0982645 0.134903 -0.0648039 -0.0566216 0.138991 -0.0467151 -0.140643 0.139711 0.0943256 0.0576583 0.0644239 0.00136725 -0.0296913 0.0612566 0.148131 0.067239 0.100442 0.0665155 0.104861 -0.0498524 0.0995954 -0.115922 -0.00524584 0.0491675 0.159028 0.132554 0.0479373 0.141164 0.173129 0.022317 -0.000446397 0.0867293 -0.155649 -0.0675728 -0.0981307 -0.0806008 -0.0107237 -0.103454 -0.0753868 -0.0551634 0.170743 0.0495554 0.11536 -0.0294355 0.061617 0.126016 -0.04804 -0.0315217 -0.169522 -0.0892494 -0.025444 0.0672556 0.166157 0.0647261 0.0944827 -0.0792354 0.0182105 0.118192 0.000124603 -0.10565 -0.155033 0.107355 0.150469 -0.104327 -0.162604 -0.0218357 0.145972 -0.145784 -0.00176559 0.153054 -0.16377 -0.11736 0.0892985 -0.0212026 0.0511168 -0.146278 -0.0134697 -0.0540684 0.0791529
148597.csv -0.15473 0.0955252 0.0432369 -0.0945614 0.136283 -0.102851 0.0847211 -0.0396431 -0.0467567 0.17154 0.153097 0.0693114 0.163837 0.135897 0.146128 -0.167215 -0.152268 -0.11602 0.0282252 -0.0779752 -0.0829204 0.018318 0.00621094 0.0707405 0.0968831 0.00652018 -0.0568833 0.0916579 -0.0400151 -0.0391421 -0.0548217 -0.173926 -0.110223 -0.0317329 -0.02952 -0.129147 0.0698902 -0.154276 -0.157658 -0.14261 0.032107 -0.0385964 -0.0587693 0.0212146 0.143626 0.142041 -0.0530896 -0.133748 0.131452 0.13672 0.148338 0.160325 -0.113424 0.0678939 -0.0229337 -0.170486 -0.156904 0.0710402 0.00277802 0.120395 0.0360002 -0.0593753 0.155915 -0.0620641 -0.112055 0.0153659 0.147731 -0.0249911 0.0360584 -0.0402479 0.022273 0.00174414 -0.0178126 -0.116679 0.0191754 -0.0089874 0.083151 -0.168562 -0.160357 -0.0659622 0.0248376 0.045583 0.127733 -0.0675122 -0.0734585 0.113653 0.166756 0.0723445 0.0554671 -0.0751338 0.0481711 -0.00127609 0.0560728 0.124651 -0.0495638 0.0985305 -0.110315 0.0672438 0.096637 0.104245
166916.csv 0.168698 0.0629846 0.0248923 -0.105248 0.172408 -0.0322083 0.174124 -0.113572 -0.0104922 0.0429484 -0.0306917 0.022368 -0.0584265 0.0337984 -0.0225754 0.143456 -0.121288 -0.133673 0.0677091 0.0583681 0.0390327 -0.141176 0.0694527 -0.0290526 -0.129707 -0.0765447 0.071578 0.146411 -0.112526 0.103688 -0.110703 0.0781341 0.0318269 0.105218 0.0177797 0.123248 0.158062 0.0370042 -0.137394 0.0246147 0.00653834 0.166063 -0.100149 -0.0479191 -0.0702838 0.0690037 0.114349 -0.0274343 0.014801 -0.0421596 0.0694873 0.0662955 -0.12477 -0.0088994 0.104959 0.149459 0.16611 0.0265376 -0.134808 0.101123 0.0431258 0.0584757 -0.0315779 0.121671 -0.0380923 -0.0897689 -0.0237933 0.110452 -0.0039647 0.106183 -0.165717 -0.16557 0.136988 0.121843 0.0722612 -0.00844494 0.175932 -0.0751714 0.152611 -0.0646956 0.105122 -0.108245 0.0583691 0.113012 0.171521 -0.0258976 0.0851889 -0.0941529 0.153386 0.0455267 -0.0259182 -0.0437207 -0.150415 0.132313 -0.143572 -0.0281547 -0.00231613 -0.00760185 -0.147233 -0.167408
148291.csv 0.00976907 0.168438 -0.0919878 -0.164332 -0.138181 -0.149775 -0.0394723 0.027946 0.0662307 -0.00850593 0.12174 0.106023 -0.11512 0.0694538 0.128228 0.066019 0.0805346 0.00220964 -0.0465066 0.0923588 0.121286 0.168551 0.0462572 0.0221805 -0.119831 0.00797117 -0.00709804 -0.0222688 0.0938169 0.100695 0.133902 0.15964 0.0544278 -0.0504766 -0.0539783 -0.0158389 0.0280565 -0.10531 0.112356 -0.0349924 0.155673 0.0491142 0.171533 -0.044268 0.0560867 -0.135758 0.114202 -0.120608 0.0373457 -0.0847815 0.0285375 -0.0101114 0.0169282 -0.00141743 -0.028344 -0.00979434 -0.0599551 0.0554465 -0.0583942 -0.169627 0.167471 -0.00661054 0.114252 -0.00489984 0.167312 0.144928 0.0376684 -0.118885 0.0426739 0.169052 0.00265325 0.146609 0.163534 -0.100965 -0.101386 0.127619 0.148285 -0.0881821 -0.100448 -0.044064 0.106071 0.0239426 0.0733384 -0.0962991 0.0939341 0.0659483 0.122844 -0.140426 -0.0485195 0.0645185 0.037179 0.0963829 -0.109955 -0.151168 -0.0413991 -0.0556731 -0.173456 -0.167728 -0.128145 0.150923
...
</code></pre>

<p>Where the first word of each line is the name of each file, and what follows is the corresponding vector for that file. I need to save the vectors in this way to use an external software.</p>
",,2018-05-18 20:49:12,Gensim Doc2Vec: I'm gettting different vectors from documents that are identical,<python><gensim><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17030,50555303,2018-05-27 18:35:54,,"<p>When I put the following command in anaconda prompt</p>

<pre><code>conda install -c anaconda gensim
</code></pre>

<p>Python stops working and shows the following error message:</p>

<p><a href=""https://i.stack.imgur.com/HsTdz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HsTdz.png"" alt=""enter image description here""></a></p>

<p>How do I deal with this problem?</p>
",2020-05-05 14:57:44,2020-05-05 14:57:44,How to install gensim from anaconda prompt?,<python><machine-learning><nlp><anaconda><conda>,,,CC BY-SA 4.0,False,False,True,False,False
17031,50557993,2018-05-28 01:17:01,,"<p>In the Python code:</p>
<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>
<p>I want to find a way to fill the values of the <code>corpus_tfidf</code> manually as I already have a list of lists of tfidfs for each document in the corpus, calculated using specific equations.</p>
<p>So, how to use them to fill the <code>corpus_tfidf</code> instead of recalculating them using <code>gensim</code> calculations.</p>
<p>I want to use my values to be passed for the <code>gensim</code> LSI and LDA models.</p>
",2020-10-07 02:56:40,2020-10-07 02:56:40,How to set the values of Tfidf Model in gensim manually,<python><gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
17034,50466643,2018-05-22 11:32:20,,"<p>I have trained my own word2vec model in gensim and I am trying to load that model in spacy. First, I need to save it in my disk and then try to load an init-model in spacy but unable to figure out exactly how.</p>

<pre><code>gensimmodel
Out[252]:
&lt;gensim.models.word2vec.Word2Vec at 0x110b24b70&gt;

import spacy
spacy.load(gensimmodel)

OSError: [E050] Can't find model 'Word2Vec(vocab=250, size=1000, alpha=0.025)'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
",,2020-05-07 09:09:20,"In spacy, how to use your own word2vec model created in gensim?",<model><word2vec><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
17046,50598129,2018-05-30 06:54:18,,"<p>Using <code>gensim</code>:</p>

<pre><code>from gensim.models import TfidfModel
from gensim.corpora import Dictionary

sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

dataset = [sent0, sent1]
vocab = Dictionary(dataset)
corpus = [vocab.doc2bow(sent) for sent in dataset] 
model = TfidfModel(corpus)

# To retrieve the same pd.DataFrame format.
documents_tfidf_lol = [{vocab[word_idx]:tfidf_value for word_idx, tfidf_value in sent} for sent in model[corpus]]
documents_tfidf = pd.DataFrame(documents_tfidf_lol)
documents_tfidf.fillna(0, inplace=True)

documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    dog mr  quick
0   0.707107    0.0 0.707107
1   0.000000    1.0 0.000000
</code></pre>

<p>If we do the TF-IDF computation manually, </p>

<pre><code>sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

documents = pd.DataFrame.from_dict(list(map(Counter, [sent0, sent1])))
documents.fillna(0, inplace=True, downcast='infer')
documents = documents.apply(lambda x: x/sum(x))  # Normalize the TF.
documents.head()

# To compute the IDF for all words.
num_sentences, num_words = documents.shape

idf_vector = [] # Lets save an ordered list of IDFS w.r.t. order of the column names.

for word in documents:
  word_idf = math.log(num_sentences/len(documents[word].nonzero()[0]))
  idf_vector.append(word_idf)

# Compute the TF-IDF table.
documents_tfidf = pd.DataFrame(documents.as_matrix() * np.array(idf_vector), 
                               columns=list(documents))
documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 0.693147    0.0 0.0 0.0 0.000000    0.0 0.693147    0.0
1   0.0 0.0 0.000000    0.0 0.0 0.0 0.693147    0.0 0.000000    0.0
</code></pre>

<p>If we use <code>math.log2</code> instead of <code>math.log</code>:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
1   0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
</code></pre>

<p>It looks like <code>gensim</code>:</p>

<ul>
<li>remove the non-salient words from the TF-IDF model, it's evident when we <code>print(model[corpus])</code></li>
<li>maybe the log base seem to be different from the log_2</li>
<li>maybe there's some normalization going on. </li>
</ul>

<p>Looking at <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel</a> , the <code>smart</code> scheme difference would have output different values but it's not clear in the docs what is the default value.</p>

<p><strong>What is the default smartirs for gensim TfidfModel?</strong></p>

<p><strong>What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?</strong></p>
",2018-05-30 07:04:34,2019-01-04 05:36:44,What is the default smartirs for gensim TfidfModel?,<python><nlp><gensim><information-retrieval><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
17048,50573054,2018-05-28 20:25:36,,"<p><strong>Full Description</strong></p>

<p>I am starting to work with word embedding and found a great amount of information about it. I understand, this far, that I can train my own word vectors or use previously trained ones, such as Google's or Wikipedia's, which are available for the English language and aren't useful to me, since I am working with texts in <em>Brazilian Portuguese</em>. Therefore, I went on a hunt for pre-trained word vectors in Portuguese and I ended up finding <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan's List of Pretrained Word Embeddings</a> which led me to Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> from which I learned about Rami Al-Rfou's <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>. After downloading both, I unsuccessfully have been trying to simply load the word vectors.</p>

<p><strong>Short Description</strong></p>

<p>I can't load pre-trained word vectors; I am trying <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> and <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>.</p>

<p><strong>Downloads</strong></p>

<ul>
<li><a href=""https://drive.google.com/open?id=0B0ZXk88koS2KRDcwcV9IVWFTeUE"" rel=""nofollow noreferrer"">Kyubyong's pre-trained word2vector format word vectors for Portuguese</a>;</li>
<li><a href=""https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c1ch6rdnp89glqmi8g81ev2somslu7cs/1527537600000/10341224892851088318/*/0B5lWReQPSvmGNEh0VTdmSHlHZ1k?e=download"" rel=""nofollow noreferrer"">Polyglot's pre-trained word vectors for Portuguese</a>;</li>
</ul>

<p><strong>Loading attempts</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
First attempt: using Gensim as suggested by <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan</a>;</p>

<pre><code>from gensim.models import KeyedVectors
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
word_vectors = KeyedVectors.load_word2vec_format(kyu_path, binary=True)
</code></pre>

<p>And the error returned:</p>

<pre><code>[...]
File ""/Users/luisflavio/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The zip downloaded also contains other files but all of them return similar errors.</p>

<p><em><a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a></em>
First attempt: following <a href=""http://nbviewer.jupyter.org/gist/aboSamoor/6046170"" rel=""nofollow noreferrer"">Al-Rfous's instructions</a>;</p>

<pre><code>import pickle
import numpy
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
words, embeddings = pickle.load(open(pol_path, 'rb'))
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/Desktop/Python/w2v_loading_tries.py"", line 14, in &lt;module&gt;
    words, embeddings = pickle.load(open(polyglot_path, ""rb""))

UnicodeDecodeError: 'ascii' codec can't decode byte 0xd4 in position 1: ordinal not in range(128)
</code></pre>

<p>Second attempt: using <a href=""https://polyglot.readthedocs.io/en/latest/Embeddings.html"" rel=""nofollow noreferrer"">Polyglot's word embedding load function</a>;</p>

<p>First, we have to install polyglot via pip:</p>

<pre><code>pip install polyglot
</code></pre>

<p>Now we can import it:</p>

<pre><code>from polyglot.mapping import Embedding
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
embeddings = Embedding.load(polyglot_path)
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/anaconda3/lib/python3.6/codecs.py"", line 321, in decode
(result, consumed) = self._buffer_decode(data, self.errors, final)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p><strong>Extra Information</strong></p>

<p>I am using python 3 on MacOS High Sierra.</p>

<p><strong>Solutions</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
As pointed out by <a href=""https://stackoverflow.com/a/50579950?noredirect=1"">Aneesh Joshi</a>, the correct way to load Kyubyong's model is by calling the native load function of Word2Vec.</p>

<pre><code>from gensim.models import Word2Vec
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
model = Word2Vec.load(kyu_path)
</code></pre>

<p>Even though I am more than grateful for Aneesh Joshi solution, polyglot seems to be a better model for working with Portuguese. Any ideas about that one?</p>
",2018-05-29 16:28:57,2020-04-19 12:57:13,UnicodeDecodeError error when loading word2vec,<python><word2vec><gensim><python-unicode><polyglot>,,,CC BY-SA 4.0,False,False,True,False,False
17052,50579266,2018-05-29 08:02:10,,"<p>According to the original paper <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>, the inference on unseen paragraph can be done by</p>

<p><em>training the inference stage to get paragraph vectors <strong>D</strong> for new
paragraphs (never seen before) by adding more columns
in <strong>D</strong> and gradient descending on <strong>D</strong> while holding <strong>W, U, b</strong>
fixed</em></p>

<p>This <em>inference stage</em> can be done in gensim by <code>infer_vector()</code>.
If I have <code>window = 5</code> for doc2vec model, and attempts to infer paragraph with whose some sentences are <code>len(sentence) &lt; 5</code>.</p>

<p>such as :</p>

<p><code>model = Doc2Vec(window=5)
paragraph = [['I', 'am', 'groot'], ['I', 'am', 'groot', 'I', 'am', 'groot']]
model.infer_vector(paragraph)
</code></p>

<p>In this case, should I pre-pad my inferring vector with special NULL word symbol so that all length of sentences in the paragraph should be bigger than window size ? </p>

<p>such as :</p>

<p><code>paragraph = [['I', 'am', 'groot', NULL, NULL], ['I', 'am', 'groot', 'I', 'am', 'groot']]</code></p>
",,2018-05-30 18:24:55,does doc2vec(gensim) infer_vector needs window-size padded sentence?,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17059,50635465,2018-06-01 02:53:05,,"<p>When Word2Vec model is trained, there are three outputs created.</p>

<ul>
<li>model</li>
<li>model.wv.syn0</li>
<li>model.syn1neg</li>
</ul>

<p>I have a couple of questions regarding these models.</p>

<ol>
<li><p>How are these outputs essentially different from each other?</p></li>
<li><p>Which model to look at if I want to access trained results? </p></li>
</ol>

<p>Thanks in advance !</p>
",2018-06-01 22:23:26,2018-06-01 22:23:26,Word2Vec model output types,<word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17071,50655405,2018-06-02 09:26:26,,"<p>I have a question related to gensim. I like to know whether it is recommended or necessary to use pickle while saving or loading a model (or multiple models), as I find scripts on GitHub that do either.    </p>

<pre><code>mymodel = Doc2Vec(documents, size=100, window=8, min_count=5, workers=4)
      mymodel.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
</code></pre>

<p>See <a href=""https://radimrehurek.com/gensim/models/deprecated/doc2vec.html"" rel=""noreferrer"">here</a></p>

<p><strong>Variant 1:</strong></p>

<pre><code>import pickle
# Save
mymodel.save(""mymodel.pkl"")  # Stores *.pkl file
# Load
mymodel = pickle.load(""mymodel.pkl"")
</code></pre>

<p><strong>Variant 2:</strong></p>

<pre><code># Save
model.save(mymodel) # Stores *.model file
# Load
model = Doc2Vec.load(mymodel)
</code></pre>

<p>In <code>gensim.utils</code>, it appears to me that there is a pickle function embedded: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py</a></p>

<p>def save 
  ...
  try:
              _pickle.dump(self, fname_or_handle,   protocol=pickle_protocol)
  ...</p>

<p><strong>Goal of my question:</strong>
I would be glad to learn 1) whether I need pickle (for better memory management) and 2) in case, why it's better than loading *.model files.</p>

<p>Thank you!</p>
",2018-06-04 16:38:42,2018-06-12 15:27:28,gensim: pickle or not?,<memory><model><pickle><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17079,50564928,2018-05-28 11:04:09,,"<p>Creating doc2vec model</p>

<p>x:List of Sentences(Movie reviews)</p>

<p>length of x =2000</p>

<pre><code>doc2vec_data = []
for line in x:
temp = ''.join(str(token) for token in line.lower())
doc2vec_data.append(temp)

File = open('doc2vec_data.txt', 'w',encoding=""utf-8"") 
for item in doc2vec_data:
File.write(""%s\n"" % item)

sentences=gensim.models.doc2vec.TaggedLineDocument(""doc2vec_data.txt"")

d2v =gensim.models.Doc2Vec(sentences, dm=0,window = 5, 
                   size=5,
                   iter = 100, workers=32,dbow_words=1,
                   alpha=2,min_aplha=0.5)
</code></pre>

<p>Creating Numpy array of vectors:
as doc2vec model cant be directly used in keras sequential model.</p>

<pre><code>vec=np.array([d2v.infer_vector(item) for item in x])
</code></pre>

<p>Keras Sequential Model:</p>

<pre><code>model=Sequential()
model.add(Embedding(2000,128,input_length=vec.shape[1]))
model.add(LSTM(200,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',
      optimizer='rmsprop',
      metrics=['accuracy'])
</code></pre>

<p>y:sentence labels (0 and 1)</p>

<pre><code>model.fit(vec,y,
  batch_size=32,epochs=8,
  verbose=1)
</code></pre>

<p>Above code is giving me this error-</p>

<pre><code>InvalidArgumentError                      Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in 
_do_call(self, fn, *args)
1349     try:
-&gt; 1350       return fn(*args)
1351     except errors.OpError as e:

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in 
_run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
 1328                                    feed_dict, fetch_list, target_list,
-&gt; 1329                                    status, run_metadata)
 1330 

~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in 
__exit__(self, type_arg, value_arg, traceback_arg)
472             compat.as_text(c_api.TF_Message(self.status.status)),  
--&gt; 473             c_api.TF_GetCode(self.status.status))
474     # Delete the underlying status object from memory otherwise it 
stays alive

InvalidArgumentError: indices[0,0] = -19 is not in [0, 2000)
 [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, 
validate_indices=true, 
_device=""/job:localhost/replica:0/task:0/device:CPU:0""] 
(embedding_1/embeddings/read, embedding_1/Cast)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-34-3d0fc0b22a78&gt; in &lt;module&gt;()
 1 model.fit(vec,y,
  2          batch_size=32,epochs=8,
----&gt; 3          verbose=1) 

InvalidArgumentError: indices[0,0] = -19 is not in [0, 2000)
 [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, 
validate_indices=true, 
_device=""/job:localhost/replica:0/task:0/device:CPU:0""] 
(embedding_1/embeddings/read, embedding_1/Cast)]]
</code></pre>

<p>Could someone please tell me what's the error and how do i solve it ?</p>
",,2018-05-28 11:15:42,How to use sentence vectors from doc2vec in keras Sequntial model for sentence sentiment analysis?,<python><keras><deep-learning><sentiment-analysis>,,,CC BY-SA 4.0,False,False,True,False,False
17080,50658576,2018-06-02 15:47:53,,"<p>I am trying to execute in parallel <a href=""https://github.com/amirouche/wikimark/"" rel=""nofollow noreferrer"">some machine learning algorithm</a>.</p>

<p>When I use multiprocessing, it's slower than without. My wild guess is that the <code>pickle</code> serialization of the models I use slowing down the whole process. So the question is: <em>how can I initialize the pool's worker with an initial state so that I don't need to serialize/deserialize for every single call the models?</em></p>

<p>Here is my current code:</p>

<pre><code>import pickle
from pathlib import Path
from collections import Counter
from multiprocessing import Pool

from gensim.models.doc2vec import Doc2Vec

from wikimark import html2paragraph
from wikimark import tokenize


def process(args):
    doc2vec, regressions, filepath = args
    with filepath.open('r') as f:
        string = f.read()
    subcategories = Counter()
    for index, paragraph in enumerate(html2paragraph(string)):
        tokens = tokenize(paragraph)
        vector = doc2vec.infer_vector(tokens)
        for subcategory, model in regressions.items():
            prediction = model.predict([vector])[0]
            subcategories[subcategory] += prediction
    # compute the mean score for each subcategory
    for subcategory, prediction in subcategories.items():
        subcategories[subcategory] = prediction / (index + 1)
    # keep only the main category
    subcategory = subcategories.most_common(1)[0]
    return (filepath, subcategory)


def main():
    input = Path('./build')
    doc2vec = Doc2Vec.load(str(input / 'model.doc2vec.gz'))
    regressions = dict()
    for filepath in input.glob('./*/*/*.model'):
        with filepath.open('rb') as f:
            model = pickle.load(f)
        regressions[filepath.parent] = model

    examples = list(input.glob('../data/wikipedia/english/*'))

    with Pool() as pool:
        iterable = zip(
            [doc2vec] * len(examples),  # XXX!
            [regressions] * len(examples),  # XXX!
            examples
        )
        for filepath, subcategory in pool.imap_unordered(process, iterable):
            print('* {} -&gt; {}'.format(filepath, subcategory))


if __name__ == '__main__':
    main()
</code></pre>

<p>The lines marked with <code>XXX!</code> point to the data that serialized when I call <code>pool.imap_unodered</code>. There at least 200MB of data that is serialized.</p>

<p>How can I avoid serialization?</p>
",,2018-06-02 16:52:37,How to initialize a pool of python multiprocessing workers with a shared state?,<python><scikit-learn><nlp><data-science><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
17083,50583937,2018-05-29 12:03:00,,"<p>A few papers on the topics of word and document embeddings (word2vec, doc2vec) mention that they used the Stanford CoreNLP framework to tokenize/lemmatize/POS-tag the input words/sentences:</p>

<blockquote>
  <p>The  corpora  were  lemmatized and POS-tagged with the Stanford CoreNLP (Manning  et  al.,  2014)  and  each  token  was  replaced with its lemma and POS tag</p>
</blockquote>

<p>(<a href=""http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf"" rel=""noreferrer"">http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf</a>)</p>

<blockquote>
  <p>For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP</p>
</blockquote>

<p>(<a href=""https://arxiv.org/pdf/1607.05368.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.05368.pdf</a>)</p>

<p>So my questions are:</p>

<ul>
<li><p>Why does the first paper apply POS-tagging? Would each token then be replaced with something like <code>{lemma}_{POS}</code> and the whole thing used to train the model? Or are the tags used to filter tokens? 
For example, gensims WikiCorpus applies lemmatization per default and then only keeps a few types of part of speech (verbs, nouns, etc.) and gets rid of the rest. So what is the recommended way?</p></li>
<li><p>The quote from the second paper seems to me like they only split up words and then lowercase them. This is also what I first tried before I used WikiCorpus. In my opinion, this should give better results for document embeddings as most of POS types contribute to the meaning of a sentence. Am I right?</p></li>
</ul>

<p>In the original doc2vec paper I did not find details about their pre-processing.</p>
",,2018-05-29 12:03:00,NLP: Pre-processing in doc2vec / word2vec,<nlp><stanford-nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,True,False
17087,50618993,2018-05-31 07:31:41,,"<p>I want to get word embeddings for the words in a corpus. I decide to use pretrained word vectors in <em>GoogleNews</em> by <em>gensim</em> library. But my corpus contains some words that are not in GoogleNews words. for these missing words, I want to use arithmatic mean of n most similar words to it in GoggoleNews words. First I load GoogleNews and check that the word ""to"" is in it?</p>

<pre><code>#Load GoogleNews pretrained word2vec model
model=word2vec.KeyedVectors.Load_word2vec_format(""GoogleNews-vectors-negative33.bin"",binary=True)
print(model[""to""])
</code></pre>

<p>I receive an error: <code>keyError: ""word 'to' not in vocabulary""</code>
is it possible that such a large dataset doesn't have this word? this is true also for some other common word like ""a""!</p>

<p>For adding missing words to word2vec model,first I want to get indices of words that are in GoogleNews. for missing words I have used index 0.</p>

<pre><code>#obtain index of words
word_to_idx=OrderedDict({w:0 for w in corpus_words})
word_to_idx=OrderedDict({w:model.wv.vocab[w].index for w in corpus_words if w in model.wv.vocab})
</code></pre>

<p>then I calculate the mean of embedding vectors of most similar words to each missing word.</p>

<pre><code>missing_embd={}
for key,value in word_to_idx.items():
    if value==0:
        similar_words=model.wv.most_similar(key)
        similar_embeddings=[model.wv[a[0]] for a in similar_words]
        missing_embd[key]=mean(similar_embeddings)
</code></pre>

<p>And then I add these news embeddings to word2vec model by:</p>

<pre><code>for word,embd in missing_embd.items():
    # model.wv.build_vocab(word,update=True)
    model.wv.syn0[model.wv.vocab[word].index]=embd
</code></pre>

<p>There is an un-consistency. When I print missing_embed, it's empty. As if there were not any missing words.
But when I check it by this:</p>

<pre><code>for w in tokens_lower:
    if(w in model.wv.vocab)==False:
        print(w)
        print(""***********"")
</code></pre>

<p>I found a lot of missing words.
Now, I have 3 questions:
1- why <strong><em>missing_embed</em></strong> is empty while there are some missing words?
2- Is it possible that GoogleNews doesn't have words like ""to""?
3- how can I append new embeddings to word2vec model? I used <strong><em>build_vocab</em></strong> and <strong><em>syn0</em></strong>. Thanks.</p>
",,2018-12-30 05:36:09,add new words to GoogleNews by gensim,<python><word2vec><gensim><google-news>,,,CC BY-SA 4.0,False,False,True,False,False
17095,50530747,2018-05-25 13:50:44,,"<p>In Gensim's doc2vec implementation, <code>gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar</code> returns the tags and cosine similarity of the documents most similar to the query document. What if I want the actual documents themselves and not the tags? Is there a way to do that directly without searching for the document associated with the tag returned by <code>most_similar</code>?</p>

<p>Also, is there documentation on this? I can't seem to find the documentation for half of Gensim's classes.</p>
",,2018-05-28 00:31:24,Gensim doc2vec most_similar equivalent to get full documents,<python-3.x><nlp><text-mining><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17096,50531181,2018-05-25 14:13:42,,"<p>i'm trying to cluster some documents with word2vec and numpy.</p>

<p><code>w2v = W2VTransformer()
X_train = w2v.fit_transform(X_train)</code></p>

<p>When I run the fit or fit_transform I get this error:</p>

<blockquote>
  <p>Exception in thread Thread-8:
  Traceback (most recent call last):
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\threading.py"", line 916, in _bootstrap_inner
      self.run()
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\threading.py"", line 864, in run
      self._target(*self._args, **self._kwargs)
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py"", line 99, in _worker_loop
      tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 539, in _do_train_job
      tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)
    File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow
  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>

<p>(X_train is a 2D numpy array of strings)</p>

<p>Does anyone know a solution?
Thank you</p>
",,2018-08-29 13:30:06,Fit method of gensim.sklearn_api.w2vmodel.W2VTransformer throws error when inputed 2-dimensional array of strings,<python><arrays><python-3.6><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
17102,50567108,2018-05-28 13:00:20,,"<p>I am trying to generate the summary of a large text file using Gensim Summarizer. 
I am getting memory error. Have been facing this issue since sometime, any help
would be really appreciated. feel free to ask for more details.</p>

<pre><code>from gensim.summarization.summarizer import summarize

file_read =open(""xxxxx.txt"",'r')
Content= file_read.read()


def Summary_gen(content):
    print(len(Content))
    summary_r=summarize(Content,ratio=0.02)
    print(summary_r)


Summary_gen(Content)
</code></pre>

<p>The length of the document is:</p>

<pre><code>365042
</code></pre>

<p>Error messsage:</p>

<pre><code>    ---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
&lt;ipython-input-6-a91bd71076d1&gt; in &lt;module&gt;()
     10 
     11 
---&gt; 12 Summary_gen(Content)

&lt;ipython-input-6-a91bd71076d1&gt; in Summary_gen(content)
      6 def Summary_gen(content):
      7     print(len(Content))
----&gt; 8     summary_r=summarize(Content,ratio=0.02)
      9     print(summary_r)
     10 

c:\python3.6\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    428     corpus = _build_corpus(sentences)
    429 
--&gt; 430     most_important_docs = summarize_corpus(corpus, ratio=ratio if word_count is None else 1)
    431 
    432     # If couldn't get important docs, the algorithm ends.

c:\python3.6\lib\site-packages\gensim\summarization\summarizer.py in summarize_corpus(corpus, ratio)
    367         return []
    368 
--&gt; 369     pagerank_scores = _pagerank(graph)
    370 
    371     hashable_corpus.sort(key=lambda doc: pagerank_scores.get(doc, 0), reverse=True)

c:\python3.6\lib\site-packages\gensim\summarization\pagerank_weighted.py in pagerank_weighted(graph, damping)
     57 
     58     """"""
---&gt; 59     adjacency_matrix = build_adjacency_matrix(graph)
     60     probability_matrix = build_probability_matrix(graph)
     61 

c:\python3.6\lib\site-packages\gensim\summarization\pagerank_weighted.py in build_adjacency_matrix(graph)
     92         neighbors_sum = sum(graph.edge_weight((current_node, neighbor)) for neighbor in graph.neighbors(current_node))
     93         for j in xrange(length):
---&gt; 94             edge_weight = float(graph.edge_weight((current_node, nodes[j])))
     95             if i != j and edge_weight != 0.0:
     96                 row.append(i)

c:\python3.6\lib\site-packages\gensim\summarization\graph.py in edge_weight(self, edge)
    255 
    256         """"""
--&gt; 257         return self.get_edge_properties(edge).setdefault(self.WEIGHT_ATTRIBUTE_NAME, self.DEFAULT_WEIGHT)
    258 
    259     def neighbors(self, node):

c:\python3.6\lib\site-packages\gensim\summarization\graph.py in get_edge_properties(self, edge)
    404 
    405         """"""
--&gt; 406         return self.edge_properties.setdefault(edge, {})
    407 
    408     def add_edge_attributes(self, edge, attrs):

MemoryError: 
</code></pre>

<p>I have tried looking up for this error on the internet, but, couldn't find a workable solution to this. </p>
",2018-05-29 15:37:08,2018-05-29 15:37:08,"Gensim Summarizer throws MemoryError, Any Solution?",<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17103,50569110,2018-05-28 14:59:16,,"<p>I tried creating a simple Doc2Vec model:</p>

<pre><code> sentences = []
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'rosse', u'con', u'tacco'], tags=[1]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'blu'], tags=[2]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarponcini', u'Emporio', u'Armani'], tags=[3]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'marca', u'italiana'], tags=[4]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'bianche', u'senza', u'tacco'], tags=[5]))

 model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
 model.build_vocab(sentences)  
</code></pre>

<p>But I end up with an empty vocabulary. With some debugging I saw that inside the build_vocab() function a dictionary is actually created by the vocabulary.scan_vocab() function, but it's being deleted by the following vocabulary.prepare_vocab() function. More deeply, this is the function that causes the problem:</p>

<pre><code>def keep_vocab_item(word, count, min_count, trim_rule=None):
    """"""Check that should we keep `word` in vocab or remove.

    Parameters
    ----------
    word : str
        Input word.
    count : int
        Number of times that word contains in corpus.
    min_count : int
        Frequency threshold for `word`.
    trim_rule : function, optional
        Function for trimming entities from vocab, default behaviour is `vocab[w] &lt;= min_reduce`.

    Returns
    -------
    bool
        True if `word` should stay, False otherwise.

    """"""
    default_res = count &gt;= min_count

    if trim_rule is None:
        return default_res # &lt;-- ALWAYS RETURNS FALSE
    else:
        rule_res = trim_rule(word, count, min_count)
        if rule_res == RULE_KEEP:
            return True
        elif rule_res == RULE_DISCARD:
            return False
        else:
            return default_res  
</code></pre>

<p>Does somebody understand the problem?</p>
",,2018-05-28 15:33:57,Gensim DOC2VEC trims and delete the vocabulary,<python><gensim><doc2vec><vocabulary>,,,CC BY-SA 4.0,False,False,True,False,False
17108,50643196,2018-06-01 12:13:08,,"<p>I am training a <code>fastText</code> model using <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""noreferrer""><code>gensim.models.fasttext</code></a>. However, I can't seem to find a method to compute the loss of the iteration for logging purposes. If I look at <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer""><code>gensim.models.word2vec</code></a>, it has the <code>get_latest_training_loss</code> method which allows you to print the training loss. Are there any alternatives or it is simply impossible?</p>
",,2018-06-01 12:13:08,Gensim FastText compute Training Loss,<python><nlp><word2vec><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
17112,50645331,2018-06-01 14:10:11,,"<p>I am trying to use <a href=""https://github.com/bmabey/pyLDAvis"" rel=""nofollow noreferrer"">pyLDAvis</a> to display the results of a Non-negative Matrix Factorization, which can be approached as a topic model. The model is built using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"" rel=""nofollow noreferrer"">sklearn's NMF</a>. I am doing all the scaling and transformations required, and then I feed <a href=""http://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare"" rel=""nofollow noreferrer"">pyLDAvis.prepare()</a> this way :</p>

<pre><code>code_vis_data_mmds = pyLDAvis.prepare(topic_term_dists = nmf_frobenius_cd_X_manual.components_, doc_topic_dists = pd.DataFrame(doc_topic_dists), doc_lengths = doc_lengths, vocab = dictionary.values(), term_frequency = term_frequency, mds='mmds')
</code></pre>

<ul>
<li>topic_term_dists parameter takes as values the components_ from my NMF</li>
<li>doc_topic_dists the result of the transform method of the NMF model applied to my tf-idf matrix</li>
<li>the vocab is a <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">gensim's corpora dictionary</a> I computed myself on the dataset, the same used to build the tf-idf matrix</li>
<li>the term-frequency comes from the sums along axis on my count matrix, also built from the dictionary</li>
</ul>

<p>This part of the code raises one deprecation warning as follows</p>

<blockquote>
  <p>C:\Users\VDE10\Anaconda3\lib\site-packages\pyLDAvis_prepare.py:387:
  DeprecationWarning: .ix is deprecated. Please use .loc for label based
  indexing or .iloc for positional indexing</p>
</blockquote>

<p>Apart from that, I also get three runtime warnings:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-48-59ece996466e&gt; in &lt;module&gt;()
     44 # pyLDAvis.save_html(code_vis_data_pcoa,""LDA_vis_pcoa.html"")
     45 
---&gt; 46 pyLDAvis.display(code_vis_data_mmds)

~\Anaconda3\lib\site-packages\pyLDAvis\_display.py in display(data, local, **kwargs)
    220         kwargs['d3_url'], kwargs['ldavis_url'], kwargs['ldavis_css_url'] = write_ipynb_local_js()
    221 
--&gt; 222     return HTML(prepared_data_to_html(data, **kwargs))
    223 
    224 def show(data, ip='127.0.0.1', port=8888, n_retries=50,

~\Anaconda3\lib\site-packages\pyLDAvis\_display.py in prepared_data_to_html(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)
    176                            d3_url=d3_url,
    177                            ldavis_url=ldavis_url,
--&gt; 178                            vis_json=data.to_json(),
    179                            ldavis_css_url=ldavis_css_url)
    180 

~\Anaconda3\lib\site-packages\pyLDAvis\_prepare.py in to_json(self)
    415 
    416     def to_json(self):
--&gt; 417        return json.dumps(self.to_dict(), cls=NumPyEncoder)

~\Anaconda3\lib\json\__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, default=default, sort_keys=sort_keys,
--&gt; 238         **kw).encode(obj)
    239 
    240 

~\Anaconda3\lib\json\encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--&gt; 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

~\Anaconda3\lib\json\encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--&gt; 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

~\Anaconda3\lib\site-packages\pyLDAvis\utils.py in default(self, obj)
    144         if isinstance(obj, np.float64) or isinstance(obj, np.float32):
    145             return float(obj)
--&gt; 146         return json.JSONEncoder.default(self, obj)

~\Anaconda3\lib\json\encoder.py in default(self, o)
    178         """"""
    179         raise TypeError(""Object of type '%s' is not JSON serializable"" %
--&gt; 180                         o.__class__.__name__)
    181 
    182     def encode(self, o):

TypeError: Object of type 'ValuesView' is not JSON serializable
</code></pre>

<p>I have seen that people were getting similar errors in <a href=""https://github.com/bmabey/pyLDAvis/issues/69"" rel=""nofollow noreferrer"">this issue</a>, but their solution does not correspond to my problem.
Any help would be much appreciated, don't hesitate to ask for further details.</p>
",,2018-06-01 14:10:11,pyLDAvis : Object of type 'ValuesView' is not JSON serializable,<python><json><serialization><visualization><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
17124,50607378,2018-05-30 14:34:49,,"<p>Im currently trying to evaluate my topic models with gensim topiccoherencemodel:</p>

<pre><code>from gensim.models.coherencemodel import CoherenceModel
cm_u_mass = CoherenceModel(model = model1, corpus = corpus1, coherence = 'u_mass')
coherence_u_mass = cm_u_mass.get_coherence()

print('\nCoherence Score: ', coherence_u_mass)
</code></pre>

<p>The output is just negative values. Is this correct? Can anybody provide a formula or something how u_mass works?</p>
",,2019-01-24 20:29:36,Negative Values: Evaluate Gensim LDA with Topic Coherence,<python-3.x><gensim><evaluation><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
17130,50627026,2018-05-31 14:45:48,,"<p>I want to generate a Topic to Topic Matrix in order to find similar topic to generate internal clusters with the function <code>gensim.models.ldamodel.diff</code> from gensim LDA.
How can I save my generated data into a csv with topics over topics and the distances (in this case hellinger distance) in the cells?
This code is not working for me:</p>

<pre><code>from gensim import models
import pandas

dateiname_model1 = ""lda.model""
model1 =  models.LdaModel.load(dateiname_model1)

topic_over_topic = model1.diff(model1, annotation=True)

topic_over_topic_speicherpfad = ""topic_over_topic_similarity.csv""
pandas.DataFrame(topic_over_topic).to_csv(topic_over_topic_speicherpfad, sep=';')
</code></pre>
",,2018-06-06 13:47:24,Topic Similarity in one model to csv Matrix,<python-3.x><export-to-csv><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17135,50697092,2018-06-05 09:48:50,,"<p>I'm trying to get the text with its punctuation as it is important to consider the latter in my doc2vec model.  However, the wikicorpus retrieve only the text. After searching the web I found these pages:</p>

<ol>
<li>Page from gensim github issues section. It was a question by someone where the answer was to subclass WikiCorpus (answered by Piskvorky). Luckily, in the same page, there was a code representing the suggested 'subclass' solution. The code was provided by Rhazegh. (<a href=""https://github.com/RaRe-Technologies/gensim/issues/552"" rel=""noreferrer"">link</a>)</li>
<li>Page from stackoverflow with a title: ""Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus"". However, no clear answer was provided and was treated in the context of spaCy. (<a href=""https://stackoverflow.com/questions/43500996/disabling-gensims-removal-of-punctuation-etc-when-parsing-a-wiki-corpus"">link</a>)</li>
</ol>

<p>I decided to use the code provided in page 1. My current code (mywikicorpus.py):</p>

<pre><code>import sys
import os
sys.path.append('C:\\Users\\Ghaliamus\\Anaconda2\\envs\\wiki\\Lib\\site-packages\\gensim\\corpora\\')

from wikicorpus import *

def tokenize(content):
    # override original method in wikicorpus.py
    return [token.encode('utf8') for token in utils.tokenize(content, lower=True, errors='ignore')
        if len(token) &lt;= 15 and not token.startswith('_')]

def process_article(args):
   # override original method in wikicorpus.py
    text, lemmatize, title, pageid = args
    text = filter_wiki(text)
    if lemmatize:
        result = utils.lemmatize(text)
    else:
        result = tokenize(text)
    return result, title, pageid


class MyWikiCorpus(WikiCorpus):
def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):
    WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)

    def get_texts(self):
        articles, articles_all = 0, 0
        positions, positions_all = 0, 0
        texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
        pool = multiprocessing.Pool(self.processes)
        for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):
            for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):
                articles_all += 1
                positions_all += len(tokens)
            if len(tokens) &lt; ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):
                continue
            articles += 1
            positions += len(tokens)
            if self.metadata:
                yield (tokens, (pageid, title))
            else:
                yield tokens
    pool.terminate()

    logger.info(
        ""finished iterating over Wikipedia corpus of %i documents with %i positions""
        "" (total %i articles, %i positions before pruning articles shorter than %i words)"",
        articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)
    self.length = articles  # cache corpus length
</code></pre>

<p>And then, I used another code by Pan Yang (<a href=""https://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"" rel=""noreferrer"">link</a>). This code initiates WikiCorpus object and retrieve the text. The only change in my current code is initiating MyWikiCorpus instead of WikiCorpus. The code (process_wiki.py):</p>

<pre><code>from __future__ import print_function
import logging
import os.path
import six
import sys
import mywikicorpus as myModule



if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments
    if len(sys.argv) != 3:
        print(""Using: python process_wiki.py enwiki-20180601-pages-    articles.xml.bz2 wiki.en.text"")
        sys.exit(1)
    inp, outp = sys.argv[1:3]
    space = "" ""
    i = 0

    output = open(outp, 'w')
    wiki = myModule.MyWikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        if six.PY3:
            output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        else:
            output.write(space.join(text) + ""\n"")
        i = i + 1
        if (i % 10000 == 0):
            logger.info(""Saved "" + str(i) + "" articles"")

    output.close()
    logger.info(""Finished Saved "" + str(i) + "" articles"")
</code></pre>

<p>Through command line I ran the process_wiki.py code. I got text of the corpus with the last line in the command prompt: </p>

<p>(2018-06-05 09:18:16,480: INFO: Finished Saved 4526191 articles)</p>

<p>When I read the file in python, I checked the first article and it was without punctuation. Example:</p>

<p>(anarchism is a political philosophy that advocates self governed societies based on voluntary institutions these are often described as stateless societies although several authors have defined them more specifically as institutions based on non hierarchical or free associations anarchism holds the state to be undesirable unnecessary and harmful while opposition to the state is central anarchism specifically entails opposing authority or hierarchical)</p>

<p>My two relevant questions, and I wish you can help me with them, please:</p>

<ol>
<li>is there any thing wrong in my reported pipeline above?</li>
<li>regardless such pipeline, if I opened the gensim  wikicorpus python code (<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""noreferrer"">wikicorpus.py</a>) and wanted to edit it, what is the line that I should add it or remove it or update it (with what if possible) to get the same results but with punctuation?</li>
</ol>

<p>Many thanks for your time reading this long post. </p>

<p>Best wishes,</p>

<p>Ghaliamus </p>
",,2019-03-14 17:13:40,How to get the wikipedia corpus text with punctuation by using gensim wikicorpus?,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,True,True,False,False
17145,50679537,2018-06-04 11:27:52,,"<p>I try to evaluate my dynamic topic models.
The model were generated with the gensim wrappers.
Are there any possible functions like perplexity or topic coherence equal to the ""normal"" topic modeling?</p>
",,2019-04-23 23:39:07,Evaluation of Dynamic Topic Models,<gensim><evaluation><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17147,50684894,2018-06-04 16:21:57,,"<p>I have trained doc2vec model by following this tutorial for 500.000 documents.
<a href=""https://github.com/abtpst/Doc2Vec/blob/master/trainDoc2Vec.py"" rel=""nofollow noreferrer"">https://github.com/abtpst/Doc2Vec/blob/master/trainDoc2Vec.py</a></p>
<p>However, when I try to find most_similar documents for a given document, the results have similarity higher than 1. As you can see in the code and the outputs, docvecs.similarity returns 0.246 and docvecs.most_similar return 9.996 for similarity between same two documents. You can see the code and the output below:</p>
<pre><code>from gensim.models import doc2vec
def myhash(obj):
    return hash(obj) % (2 ** 32)    
model = doc2vec.Doc2Vec(hashfxn=myhash)
model = doc2vec.Doc2Vec.load(&quot;d2v.model&quot;)

tag1 = &quot;012020171590&quot;
tag2 = &quot;0109201716181&quot;

print(tag1 in model.docvecs.doctags)
print(tag2 in model.docvecs.doctags)

print(model.docvecs.similarity(tag1, tag2))
print(model.docvecs.most_similar(tag1))
</code></pre>
<p>Output:</p>
<blockquote>
<p>True</p>
<p>True</p>
<p>0.24682570854972158</p>
<p>[('0109201716181', 9.996172904968262), ('0120201611372', 9.853036880493164), ('010120166996', 9.613503456115723), ('012020173027', 8.97104263305664), ('01202017423', 8.886014938354492), ('01002009541', 8.783470153808594), ('00002004106', 8.682585716247559), ('0109201616963', 8.671405792236328), ('011020171931', 8.659266471862793), ('011020175199', 8.573907852172852)]</p>
</blockquote>
<p>Library and System Versions:</p>
<p>Linux-4.9.0-6-amd64-x86_64-with-debian-9.4</p>
<p>Python 3.5.3 (default, Jan 19 2017, 14:11:04)</p>
<p>NumPy 1.14.3</p>
<p>SciPy 1.1.0</p>
<p>gensim 3.4.0</p>
<p>FAST_VERSION 1</p>
",2020-06-20 09:12:55,2018-06-04 16:21:57,Doc2vec most_similar method returns similarity score higher than 1,<python><python-3.x><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17148,50685212,2018-06-04 16:43:47,,"<p>I am using gensim to create word vectors based on my corpus like the following:</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>I was wondering if it is possible to start (or somehow avoid having) words at index 0 and 1? I would like my vocabulary to start at index 2, because I need to do other operations and if I keep 0 and 1 as indexes it gets a little confusing.</p>

<p>Thanks for the help!</p>
",,2018-06-04 19:59:31,Gensim word2vec - start vocabulary from index different than 0,<python><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17164,50737844,2018-06-07 09:33:45,,"<p>I try to get LDAMallet in gensim working, but get the following error</p>

<p>'C:\...\AppData\Local\Temp\eb09f5_state.mallet.gz' not found</p>

<p>The code</p>

<pre><code>ldamallet = gensim.models.\
wrappers.LdaMallet(mallet_path,
                   corpus=corpus,
                   num_topics=5,
                   id2word=dictionary)
</code></pre>

<p>(the num_topics is extremely small, but the test goes over 5 sentences; this has no problem in the regular gensim LdaModel)</p>

<p>thanks,</p>
",,2018-07-03 15:21:25,Mallet with Gensim: file-not-found,<gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
17166,50742031,2018-06-07 13:05:47,,"<p>Hi I was using gensim for topic modelling and was using Mallet and was executing this code I unzipped mallet in c drive as shown and also set the environment <code>MALLET_HOME</code> command. The code I was executing is</p>

<pre><code>mallet_path = r'c:/mallet-2.0.8/bin/mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, 
num_topics=20, id2word=id2word)
</code></pre>

<p>this gives me a error like this</p>

<pre><code>CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-58-6e0dbb876ee6&gt; in &lt;module&gt;()
----&gt; 1 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, 
corpus=corpus, 
   num_topics=20, id2word=id2word)

~\AppData\Local\Continuum\anaconda3\lib\site- 
packages\gensim\models\wrappers\ldamallet.py in __init__(self, mallet_path, 
corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, 
iterations, topic_threshold)
124         self.iterations = iterations
125         if corpus is not None:
--&gt; 126             self.train(corpus)
127 
128     def finferencer(self):

~\AppData\Local\Continuum\anaconda3\lib\site- 
packages\gensim\models\wrappers\ldamallet.py in train(self, corpus)
265 
266         """"""
--&gt; 267         self.convert_input(corpus, infer=False)
268         cmd = self.mallet_path + ' train-topics --input %s --num-topics 
%s  --alpha %s --optimize-interval %s '\
269             '--num-threads %s --output-state %s --output-doc-topics %s - 
-output-topic-keys %s '\

~\AppData\Local\Continuum\anaconda3\lib\site- 
packages\gensim\models\wrappers\ldamallet.py in convert_input(self, corpus, 
infer, serialize_corpus)
254             cmd = cmd % (self.fcorpustxt(), self.fcorpusmallet())
255         logger.info(""converting temporary corpus to MALLET format with 
%s"", cmd)
--&gt; 256         check_output(args=cmd, shell=True)
257 
258     def train(self, corpus):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\utils.py in 
check_output(stdout, *popenargs, **kwargs)
1804             error = subprocess.CalledProcessError(retcode, cmd)
1805             error.output = output
-&gt; 1806             raise error
1807         return output
1808     except KeyboardInterrupt:`

CalledProcessError: Command 'c:\mallet-2.0.8\bin\mallet import-file -- 
preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input 
C:\Users\apath009\AppData\Local\Temp\d186ea_corpus.txt --output 
C:\Users\apath009\AppData\Local\Temp\d186ea_corpus.mallet' returned non-zero 
exit status 1.
</code></pre>

<p>Please Help!!!</p>
",2018-06-07 13:43:06,2020-05-21 18:31:34,Python topic modelling error in mallet,<python-3.x><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
17179,50651861,2018-06-01 22:50:23,,"<p>I am trying to use <code>word2vec</code> in a scikit-learn pipeline.</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np

class ItemSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
pipeline_word2vec = Pipeline([
                ('selector', ItemSelector(key='X')),
                ('w2v', W2VTransformer()),
            ])

pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))
</code></pre>

<p>this gives me </p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-9e2dd309d07c&gt; in &lt;module&gt;()
     23                 ('w2v', W2VTransformer()),
     24             ])
---&gt; 25 pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))

/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py in fit(self, X, y)
     62             sg=self.sg, hs=self.hs, negative=self.negative, cbow_mean=self.cbow_mean,
     63             hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word, trim_rule=self.trim_rule,
---&gt; 64             sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
     65         )
     66         return self

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)
    525             batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window, seed=seed,
    526             hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 527             fast_version=FAST_VERSION)
    528 
    529     def _do_train_job(self, sentences, alpha, inits):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in __init__(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    336             self.train(
    337                 sentences, total_examples=self.corpus_count, epochs=self.epochs, start_alpha=self.alpha,
--&gt; 338                 end_alpha=self.min_alpha, compute_loss=compute_loss)
    339         else:
    340             if trim_rule is not None:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    609             sentences, total_examples=total_examples, total_words=total_words,
    610             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 611             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    612 
    613     def score(self, sentences, total_sentences=int(1e6), chunksize=100, queue_factor=2, report_delay=1):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    567             sentences, total_examples=total_examples, total_words=total_words,
    568             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 569             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    570 
    571     def _get_job_params(self, cur_epoch):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)
    239             epochs=epochs,
    240             total_examples=total_examples,
--&gt; 241             total_words=total_words, **kwargs)
    242 
    243         for callback in self.callbacks:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in _check_training_sanity(self, epochs, total_examples, total_words, **kwargs)
    599 
    600         if not self.wv.vocab:  # should be set by `build_vocab`
--&gt; 601             raise RuntimeError(""you must first build vocabulary before training the model"")
    602         if not len(self.wv.vectors):
    603             raise RuntimeError(""you must initialize vectors before training the model"")

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>in a jupyter notebook. Instead I seek a trained model. How can I fix this?</p>
",2018-06-02 00:25:21,2018-06-02 05:37:18,Using gensim word2vec in scikit-learn pipeline,<python><scikit-learn><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
17184,50723303,2018-06-06 14:46:50,,"<p>Say that I'm training a (Gensim) Word2Vec model with min_count=5. The documentation learns us what min_count does:</p>

<blockquote>
  <p>Ignores all words with total frequency lower than this.</p>
</blockquote>

<p>What is the effect of min_count on the context? Lets say that I have a sentence of frequent words (min_count > 5) and infrequent words (min_count &lt; 5), annotated with f and i:</p>

<blockquote>
  <p>This (f) is (f) a (f) test (i) sentence (i) which (f) is (f) shown (i) here (i)</p>
</blockquote>

<p>I just made up which word is frequently used and which word is not for demonstration purposes.</p>

<p>If I remove all infrequent words, we get a completely different context from which word2vec is trained. In this example, your sentence would be ""This is a which is"", which would then be a training sentence for Word2Vec. Moreover, if you have a lot of infrequent words, words that were originally very far away from each other are now placed within the same context.</p>

<p>Is this the correct interpretation of Word2Vec? Are we just assuming that you shouldn't have too many infrequent words in your dataset (or set a lower min_count threshold)?</p>
",,2018-06-06 21:48:04,How is Word2Vec min_count applied,<python><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17186,50723841,2018-06-06 15:11:38,,"<p>I would need to find something like the opposite of <code>model.most_similar()</code><br>
While <code>most_similar()</code> returns an array of words most similar to the one given as input, I need to find a sort of ""center"" of a list of words.</p>

<p>Is there a function in gensim or any other tool that could help me?</p>

<p>Example:<br>
Given <code>{'chimichanga', 'taco', 'burrito'}</code> the center would be maybe <code>mexico</code> or <code>food</code>, depending on the corpus that the model was trained on</p>
",2018-06-11 06:05:15,2018-06-11 06:05:15,Find the closest word to set of words,<python><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17189,50709355,2018-06-05 21:41:04,,"<p>I am using <code>GoogleNews-vectors-negative300.bin</code> model and <a href=""https://github.com/ian-beaver/pycontractions"" rel=""nofollow noreferrer"">pycontractions</a> library to determinate with machine learning the best option to expand contractions when there are ambiguous meanings like <code>I'd</code> with can be <code>I would</code> and <code>I had</code>. The size of this model is very large, around to 3.5Gb.</p>

<p>I think that 3.5Gb is a very large model to use for my purpose. Probably I'll never use all words representations in this model. Is there a way to reduce the size extracting only a subset of words representations that are useful to my purposes?</p>
",2018-06-06 00:31:30,2018-06-06 01:48:32,Small model from Google news Word2Vec model,<machine-learning><models><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17205,50692739,2018-06-05 05:38:40,,"<p>I am training with some documents with gensim's Doc2vec.  </p>

<p>I have two types of inputs:  </p>

<ol>
<li>Whole English Wikipedia: Each article of Wikipedia text is considered as one 
document for doc2vec training. (Total around 5.5 million articles or documents)</li>
<li>Some documents related to my project that are manually prepared and collected from some websites. (around 15000 documents).<br>
Where each document the size is around 100 sentences.</li>
</ol>

<p>Further, I want to use this model to infer sentences of size (10~20 words).</p>

<p>I request some clarification on my approach.<br>
Is the method of training over documents(size of each document approx. 100 sentences each) and then inferring over new sentence correct. ?</p>

<p>Or, should I train over only sentences and not documents and then infer over the new sentence.?</p>
",,2018-06-05 21:18:42,Can doc2vec be useful if training on Documents and inferring on sentences only,<python><gensim><training-data><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17216,50783082,2018-06-10 11:13:35,,"<p>I've already built my own Skip-Gram model by using gensim word2vec. I know that I can get the similarity score between two words, e.g. <code>model.wv.similarity('car', 'truck') -&gt; 0.75</code>. Now I want to know why they are such ""similars"".</p>

<p>Since Skip-Gram has been trained with the context words, I suppose that there is a way to get the most frequent context words shared between <code>car</code> and <code>truck</code>. Another example: if I have the following sentences, I'd like to get the word <code>slow</code> as ""most frequent context"":</p>

<ul>
<li><code>the car is slow</code></li>
<li><code>the truck is slow</code></li>
<li><code>the car is red</code></li>
</ul>

<p>Notice that <code>red</code> isn't appear with <code>truck</code>, so it shouldn't be returned as ""most frequent context"".</p>

<p>Is there any way to do this?</p>
",,2018-06-13 14:16:21,Get most frequent contexts between two words in word2vec,<machine-learning><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17226,50841484,2018-06-13 15:54:26,,"<p>I want to create a LDA topic model and am using SpaCy to do so, following a tutorial. The error I receive when I try to use spacy is one I cannot find on google, so I'm hoping someone here knows what it's about.</p>

<p>I'm running this code on Anaconda:</p>

<pre><code>import numpy as np
import pandas as pd
import re, nltk, spacy, gensim
# Sklearn
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint
# Plotting tools
import pyLDAvis
import pyLDAvis.sklearn
import matplotlib.pyplot as plt

df = pd.DataFrame(data)

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  
 # deacc=True removes punctuations

data_words = list(sent_to_words(data))
print(data_words[:1])

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    for sent in texts:
        doc = nlp("" "".join(sent)) 
        texts_out.append("" "".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))
    return texts_out

nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only Noun, Adj, Verb, Adverb
data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
print(data_lemmatized[:1])
</code></pre>

<p>And I receive the following error:</p>

<pre><code>File ""C:\Users\maart\AppData\Local\Continuum\anaconda3\lib\site-packages\_regex_core.py"", line 1880, in get_firstset
raise _FirstSetError()

_FirstSetError
</code></pre>

<p>The error must occur somewhere after the lemmatization, because the other parts work fine.</p>

<p>Thanks a bunch!</p>
",,2018-11-08 16:10:51,raise_FirstSetError in SpaCy topic modeling,<lda><spacy>,,,CC BY-SA 4.0,True,True,True,False,True
17229,50825058,2018-06-12 20:05:08,,"<p>I'm trying to load a pretrained word2vec model using gensim. Although the model is tagged, so every word has a tag, which tells what part of speech is this word represents.</p>

<p>For example:</p>

<p><code>big::adj 0.041660 0.045049 -0.204449 0.102298 0.045326 -0.172079 0.197417 -0.012363 0.127003 0.040437 -0.003397 0.048288 0.072291 0.044205 -0.055407 -0.075357 -0.154024 0.021732 0.224021 -0.243452 -0.048776 -0.002823 0.110283 -0.052014 0.104335 -0.108122 -0.033678 -0.098096 -0.012307 0.086673 -0.028013 0.005308 -0.196080 0.002180 -0.004461 0.021646 -0.051721 -0.123485 -0.230521 0.106092 -0.206776 0.137945 0.020572 0.071123 0.042434 0.123633 -0.001925 -0.172347 -0.040973 0.135886 0.057297 -0.027319 0.066697 0.138673 -0.028331 -0.094053 -0.160371 0.158397 0.053368 -0.002126 -0.111501 0.030450 -0.054284 -0.004832 -0.065144 0.030546 -0.011896 -0.103835 -0.007947 0.120997 0.178889 -0.155029 -0.054059 -0.313675 0.061776 -0.060536 0.038848 -0.097532 -0.038358 -0.032634 0.108534 0.067584 0.044829 0.003414 0.028115 -0.010523 0.131776 0.071750 0.045095 0.046262 0.001212 -0.005994 -0.022401 -0.036971 -0.024755 0.096701 -0.026736 -0.029698 -0.107293 -0.038610</code></p>

<p>Can anyone point me out, how to load such a model, so I can ask for model['big']? Right now, it just doesn't work, when I try KeyedVectors.load().</p>
",,2018-06-18 01:35:55,Gensim - how to deal with model word::tag,<load><gensim><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
17236,50860649,2018-06-14 15:07:58,,"<p>I'm using Google's Word2vec and I'm wondering how to get the top words that are predicted by a skipgram model that is trained using hierarchical softmax, given an input word?</p>

<p>For instance, when using negative sampling, one can simply multiply an input word's embedding (from the input matrix) with each of the vectors in the output matrix and take the one with the top value. However, in hierarchical softmax, there are multiple output vectors that correspond to each input word, due to the use of the Huffman tree. </p>

<p>How do we compute the likelihood value/probability of an output word given an input word in this case?</p>
",,2018-06-15 02:37:32,How to predict word using trained skipgram model?,<python><c++><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17239,50805556,2018-06-11 20:30:01,,"<p>I am using <code>gensim.models.ldamodel.LdaModel</code> to perform LDA, but I do not understand some of the parameters and cannot find explanations in the documentation. If someone has experience working with this, I would love further details of what these parameters signify.
Specifically, I do not understand:</p>

<ul>
<li><code>random_state</code></li>
<li><code>update_every</code></li>
<li><code>chunksize</code></li>
<li><code>passes</code></li>
<li><code>alpha</code></li>
<li><code>per_word_topics</code></li>
</ul>

<p>I am working with a corpus of 500 documents which are roughly around 3-5 pages each (I unfortunately cannot share a snapshot of the data because of confidentiality reasons). Currently I have set </p>

<ul>
<li><code>num_topics = 10</code></li>
<li><code>random_state = 100</code></li>
<li><code>update_every = 1</code></li>
<li><code>chunksize = 50</code></li>
<li><code>passes = 10</code></li>
<li><code>alpha = 'auto'</code></li>
<li><code>per_word_topics = True</code></li>
</ul>

<p>but this is solely based off of an example I saw and I am not sure how generalizable that is to my data.</p>
",2018-06-12 07:15:02,2018-06-12 07:48:19,Understanding parameters in Gensim LDA Model,<python><parameters><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17250,50828314,2018-06-13 02:33:19,,"<p>I am using gensim to load pre-trained fasttext model. I downloaded the English wikipedia trained model from fasttext <a href=""https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md"" rel=""noreferrer"">website</a>. </p>

<p>here is the code I wrote to load the pre-trained model: </p>

<pre><code>from gensim.models import FastText as ft
model=ft.load_fasttext_format(""wiki.en.bin"")
</code></pre>

<p>I try to check if the following phrase exists in the vocal(which rare chance it would as these are pre-trained model). </p>

<pre><code>print(""internal executive"" in model.wv.vocab)
print(""internal executive"" in model.wv)

False
True
</code></pre>

<p>So the phrase ""internal executive"" is not present in the vocabulary but we still have the word vector corresponding to that. </p>

<pre><code>model.wv[""internal executive""]
Out[46]:
array([ 0.0210917 , -0.15233646, -0.1173932 , -0.06210957, -0.07288644,
       -0.06304111,  0.07833624, -0.17026938, -0.21922196,  0.01146349,
       -0.13639058,  0.17283678, -0.09251394, -0.17875175,  0.01339212,
       -0.26683623,  0.05487974, -0.11843193, -0.01982722,  0.37037706,
       -0.24370994,  0.14269598, -0.16363597,  0.00328478, -0.16560239,
       -0.1450972 , -0.24787527, -0.01318423,  0.03277111,  0.16175713,
       -0.19367714,  0.16955379,  0.1972683 ,  0.09044111,  0.01731548,
       -0.0034324 , -0.04834719,  0.14321515,  0.01422525, -0.08803893,
       -0.29411593, -0.1033244 ,  0.06278021,  0.16452256,  0.0650492 ,
        0.1506474 , -0.14194389,  0.10778475,  0.16008648, -0.07853138,
        0.2183501 , -0.25451994, -0.0345991 , -0.28843886,  0.19964759,
       -0.10923116,  0.26665714, -0.02544454,  0.30637854,  0.04568949,
       -0.04798719, -0.05769338,  0.25762403, -0.05158515, -0.04426906,
       -0.19901046,  0.00894193, -0.17269588, -0.24747233, -0.19061406,
        0.14322804, -0.10804397,  0.4002605 ,  0.01409482, -0.04675362,
        0.10039093,  0.07260711, -0.0938239 , -0.20434211,  0.05741301,
        0.07592541, -0.02921724,  0.21137556, -0.23188967, -0.23164661,
       -0.4569614 ,  0.07434579,  0.10841205, -0.06514647,  0.01220404,
        0.02679767,  0.11840229,  0.2247431 , -0.1946325 , -0.0990666 ,
       -0.02524677,  0.0801085 ,  0.02437297,  0.00674876,  0.02088535,
        0.21464555, -0.16240154,  0.20670174, -0.21640894,  0.03900698,
        0.21772243,  0.01954809,  0.04541844,  0.18990673,  0.11806394,
       -0.21336791, -0.10871669, -0.02197789, -0.13249406, -0.20440844,
        0.1967368 ,  0.09804545,  0.1440366 , -0.08401451, -0.03715726,
        0.27826542, -0.25195453, -0.16737154,  0.3561183 , -0.15756823,
        0.06724873, -0.295487  ,  0.28395334, -0.04908851,  0.09448399,
        0.10877471, -0.05020981, -0.24595442, -0.02822314,  0.17862654,
        0.06452435, -0.15105674, -0.31911567,  0.08166212,  0.2634299 ,
        0.17043628,  0.10063848,  0.0687021 , -0.12210461,  0.10803893,
        0.13644943,  0.10755012, -0.09816817,  0.11873955, -0.03881042,
        0.18548298, -0.04769253, -0.01511982, -0.08552645, -0.05218676,
        0.05387992,  0.0497043 ,  0.06922272, -0.0089245 ,  0.24790663,
        0.27209425, -0.04925154, -0.08621719,  0.15918174,  0.25831223,
        0.01654229, -0.03617229, -0.13490392,  0.08033483,  0.34922174,
       -0.01744722, -0.16894792, -0.10506647,  0.21708378, -0.22582002,
        0.15625793, -0.10860757, -0.06058934, -0.25798836, -0.20142137,
       -0.06613475, -0.08779443, -0.10732629,  0.05967236, -0.02455976,
        0.2229451 , -0.19476262, -0.2720119 ,  0.03687386, -0.01220259,
        0.07704347, -0.1674307 ,  0.2400516 ,  0.07338555, -0.2000631 ,
        0.13897157, -0.04637206, -0.00874449, -0.32827383, -0.03435039,
        0.41587186,  0.04643605,  0.03352945, -0.13700874,  0.16430037,
       -0.13630766, -0.18546128, -0.04692861,  0.37308362, -0.30846512,
        0.5535561 , -0.11573419,  0.2332801 , -0.07236694, -0.01018955,
        0.05936847,  0.25877884, -0.2959846 , -0.13610311,  0.10905041,
       -0.18220575,  0.06902339, -0.10624941,  0.33002165, -0.12087796,
        0.06742091,  0.20762768, -0.34141317,  0.0884434 ,  0.11247049,
        0.14748637,  0.13261876, -0.07357208, -0.11968047, -0.22124515,
        0.12290633,  0.16602683,  0.01055585,  0.04445777, -0.11142147,
        0.00004863,  0.22543314, -0.14342701, -0.23209116, -0.00003538,
        0.19272381, -0.13767233,  0.04850799, -0.281997  ,  0.10343244,
        0.16510887,  0.08671653, -0.24125539,  0.01201926,  0.0995285 ,
        0.09807415, -0.06764816, -0.0206733 ,  0.04697794,  0.02000999,
        0.05817033,  0.10478792,  0.0974884 , -0.01756372, -0.2466861 ,
        0.02877498,  0.02499748, -0.00370895, -0.04728201,  0.00107118,
       -0.21848503,  0.2033032 , -0.00076264,  0.03828803, -0.2929495 ,
       -0.18218371,  0.00628893,  0.20586628,  0.2410889 ,  0.02364616,
       -0.05220835, -0.07040054, -0.03744286, -0.06718048,  0.19264086,
       -0.06490505,  0.27364203,  0.05527219, -0.27494466,  0.22256687,
        0.10330909, -0.3076979 ,  0.04852265,  0.07411488,  0.23980476,
        0.1590279 , -0.26712465,  0.07580928,  0.05644221, -0.18824042],
</code></pre>

<p>Now my confusion is that Fastext creates vectors for character ngrams of a word too. So for a word ""internal"" it will create vectors for all its character ngrams including the full word and then the final word vector for the word is the sum of its character ngrams. </p>

<p>However, how it is still able to give me vector of a word or even the whole sentence? Isn't fastext vector is for a word and its ngram? So what are these vector I am seeing for the phrase when its clearly two words?</p>
",2018-06-14 06:40:43,2019-03-29 22:36:40,How does the Gensim Fasttext pre-trained model get vectors for out-of-vocabulary words?,<python><nlp><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
17261,50752533,2018-06-08 02:31:19,,"<p>I am trying to fit a Word2Vec model. According to the documentation for Gensim's Word2Vec we do not need to call <code>model.build_vocabulary</code> before using it. 
But yet it is asking for me to do it. I have tried calling this function and it has not worked. I also fitted a Word2Vec model before without needing to call <code>model.build_vocabulary</code> . </p>

<p>Am I doing something wrong? Here is my code:</p>

<pre><code>from gensim.models import Word2Vec
dataset = pd.read_table('genemap_copy.txt',delimiter='\t', lineterminator='\n')

def row_to_sentences(dataframe):
    columns = dataframe.columns.values
    corpus = []
    for index,row in dataframe.iterrows():
        if index == 1000:
            break
        sentence = ''
        for column in columns:
            sentence += ' '+str(row[column])
        corpus.append([sentence])
    return corpus

corpus = row_to_sentences(dataset)
clean_corpus = [[sentence[0].lower()] for sentence in corpus ]


# model = Word2Vec()
# model.build_vocab(clean_corpus)
model = Word2Vec(clean_corpus, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>Help is greatly appreciated!
Also I am using macOS Sierra.
There is not much support online for using Gensim with Mac D: . </p>
",,2018-06-08 13:32:16,Gensim Word2Vec 'you must first build vocabulary before training the model',<python><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17271,50848942,2018-06-14 02:53:16,,"<p>Below is a gensim's example, but whenever I execute it,
it show different result, so I couldn't believe gensim works well.</p>

<pre><code>from gensim import corpora, models, similarities
from collections import defaultdict

documents = [""Human machine interface for lab abc computer applications"",          # 0
             ""A survey of user opinion of computer system response time"",          # 1
             ""The EPS user interface management system"",                           # 2
             ""System and human system engineering testing of EPS"",                 # 3
             ""Relation of user perceived response time to error measurement"",      # 4
             ""The generation of random binary unordered trees"",                    # 5
             ""The intersection graph of paths in trees"",                           # 6
             ""Graph minors IV Widths of trees and well quasi ordering"",            # 7 
             ""Graph minors A survey""]                                              # 8


stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
texts = [[token for token in text if frequency[token] &gt; 1]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
lda = models.LdaModel(corpus, id2word=dictionary, num_topics=2)
index = similarities.MatrixSimilarity(lda[corpus])


doc = ""Human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lda = lda[vec_bow]
sims = index[vec_lda]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims)

print(lda.get_document_topics(vec_bow))
</code></pre>

<p>result</p>

<p>[(<strong>0</strong>, 0.9986434), (4, 0.99792993), (2, 0.99722278), (3, 0.99651831), (1, 0.99158639), (5, 0.53059661), (6, 0.4146674), (8, 0.38019019), (7, 0.36143348)]
[(0, 0.18366596), (1, 0.81633401)]</p>

<p>[(<strong>1</strong>, 0.999605), (4, 0.9991864), (0, 0.998689), (5, 0.62957084), (6, 0.48837978), (8, 0.48152202), (3, 0.4541581), (7, 0.41751832), (2, 0.40637407)]
[(0, 0.80285221), (1, 0.19714773)]</p>

<p>[(<strong>7</strong>, 0.99957085), (8, 0.99660784), (0, 0.99202132), (5, 0.78449017), (6, 0.77530348), (2, 0.56972337), (3, 0.47117239), (4, 0.47092015), (1, 0.4172135)]
[(0, 0.25292286), (1, 0.74707717)]</p>

<p>Result 7 doesn't look simiar with ""Human computer interaction"" at all.
Thanks.</p>
",,2019-01-17 03:49:27,different result executing gensim examples,<python><document><similarity><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
17273,50849640,2018-06-14 04:30:20,,"<p>I'm following <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">this tutorial</a> </p>

<p>The result I'm getting has <code>nan</code> for the coefficients. My data set has has two columns: tweets and ingestion dates. I have copied the code exactly and just made a few substitutions like tweet-prepreocessor. Any thoughts? Does the original file need the target and target names column like in the tutorial?</p>

<pre><code># Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

[(0,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (1,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (2,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (3,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (4,...
</code></pre>
",2018-06-14 13:56:42,2018-08-22 18:06:02,Gensim coefficients are nan and all the same,<python><machine-learning><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17278,50866996,2018-06-14 22:53:55,,"<p>I am using python package Gensim for clustering, I first created a dictionary from tokenizing and lemmatizing sentences of the given text and then using this dictionary created corpus using following code:</p>

<pre><code> mydict = corpora.Dictionary(LemWords)
 corpus = [mydict.doc2bow(text) for text in LemWords]
</code></pre>

<p>I understand corpus would contain id of the words along with their frequency in each document. I wish to know the frequency of a given word in the whole corpus to find top terms in the corpus. I am wondering if there is any method available that return frequency of the term in the entire corpus </p>
",2018-06-15 07:15:23,2018-06-15 07:15:23,top terms in corpus gensim,<python><gensim><counting><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
17310,50839808,2018-06-13 14:30:54,,"<p>I tried to load the GoogleNews-vectors-negative300.bin and try the predict_output_word method, </p>

<p>I tested three ways, but every failed, the code and error of each way are shown below.</p>

<pre><code>import gensim
from gensim.models import Word2Vec
</code></pre>

<ol>
<li>The first:</li>
</ol>

<p>I first used this line:</p>

<pre><code>model=Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)


print(model.wv.predict_output_word(['king','man'],topn=10))
</code></pre>

<p>error:</p>

<pre><code>DeprecationWarning: Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.
</code></pre>

<ol start=""2"">
<li>The second:</li>
</ol>

<p>Then I tried:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)


print(model.wv.predict_output_word(['king','man'],topn=10))
</code></pre>

<p>error:</p>

<pre><code>AttributeError: 'Word2VecKeyedVectors' object has no attribute 'predict_output_word'
</code></pre>

<ol start=""3"">
<li>The third:
model = gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')
print(model.wv.predict_output_word(['king','man'],topn=10))</li>
</ol>

<p>error:
    _pickle.UnpicklingError: invalid load key, '3'.</p>

<p>I read the document at </p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>but still have no idea the namespace where the predict_output_word would be in.</p>

<p>Anybody can help?</p>

<p>Thanks.</p>
",,2018-06-13 17:11:22,load the GoogleNews-vectors-negative300.bin and predict_output_word,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17313,50906372,2018-06-18 09:28:25,,"<p>I am working on text classification task where my dataset contains a lot of abbreviations and proper nouns. For instance: <strong>Milka choc. bar</strong>.<br>
My idea is to use bidirectional LSTM model with word2vec embedding.<br>
And here is my problem how to code words, that not appears in the dictionary? 
I partially solved this problem by merging pre-trained vectors with randomly initialized. Here is my implementation:</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

from gensim.models.keyedvectors import KeyedVectors

word_vectors = KeyedVectors.load_word2vec_format('ru.vec', binary=False, unicode_errors='ignore')

EMBEDDING_DIM=300
vocabulary_size=min(len(word_index)+1,num_words)
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
for word, i in word_index.items():
    if i&gt;=num_words:
        continue
    try:
        embedding_vector = word_vectors[word]
        embedding_matrix[i] = embedding_vector
    except KeyError:
        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)

def LSTMModel(X,words_nb, embed_dim, num_classes):
    _input = Input(shape=(X.shape[1],))
    X = embedding_layer = Embedding(words_nb,
                            embed_dim,
                            weights=[embedding_matrix],
                            trainable=True)(_input)
   X = The_rest_of__the_LSTM_model()(X)
</code></pre>

<p>Do you think, that allowing the model to adjust the embedding weights is a good idea? 
Could you please tell me, how can I encode words like <strong>choc</strong>? Obviously, this abbreviation stands for <strong>chocolate</strong>.   </p>
",,2018-06-18 09:43:30,word2Vec and abbreviations,<python><keras><nlp><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17325,50857544,2018-06-14 12:26:38,,"<p>is there a possibility to evaluate the dynamic model (ldaseqmodel) like the ""normal"" lda model in values of perplexity and topic coherence?
I know that these values are printed into the logging.INFO, so another method would be to save the logging.INFO into a text file to search for these evaluation values after the simulation.
If method 1 (code to evaluate ldaseqmodel) doesnt exist, is it possible to save the logging.INFO into a text file?
Here is my code to generate the ldaseqmodel:</p>

<pre><code>from gensim import models, corpora
import csv
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

Anzahl_Topics1      = 10                

Zeitabschnitte      = [16, 19, 44, 51, 84, 122, 216, 290, 385, 441, 477, 375, 390, 408, 428, 192, 38]

TDM_dateipfad = './1gramm/TDM_1gramm_1998_2014.csv'

dateiname_corpus = ""./1gramm/corpus_DTM_1gramm.mm""

dateiname1_dtm  = ""./1gramm/DTM_1gramm_10.model""

ids = {} 
corpus = [] 

with open(TDM_dateipfad, newline='') as csvfile:
    reader = csv.reader(csvfile, delimiter=';', quotechar='|') 
    for rownumber, row in enumerate(reader): 
        for index, field in enumerate(row):
            if index == 0:
                if rownumber &gt; 0:
                    ids[rownumber-1] = field 
            else:
                if rownumber == 0:
                    corpus.append([])
                else:
                    corpus[index-1].append((rownumber-1, int(field))) 

corpora.MmCorpus.serialize(dateiname_corpus, corpus)

dtm1 = models.ldaseqmodel.LdaSeqModel(corpus=corpus, time_slice = Zeitabschnitte, id2word=ids, num_topics = Anzahl_Topics1, passes=1, chunksize=10000) 
dtm1.save(dateiname1_dtm)
</code></pre>
",,2019-04-14 11:46:07,Evaluation of ldaseqmodel in gensim,<python-3.x><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17327,50887821,2018-06-16 12:15:56,,"<p>I am using <a href=""https://github.com/scikit-learn-contrib/hdbscan"" rel=""nofollow noreferrer"">HDBSCAN</a> algorithm to create clusters from the documents I have. But to create a vector matrix from the words, I am using <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf</a> algorithm and want to use <a href=""https://en.wikipedia.org/wiki/GloVe_(machine_learning)"" rel=""nofollow noreferrer"">GloVe</a>. I have searched posts but could not understand how to use this algorithm.  I also read about <a href=""https://en.wikipedia.org/wiki/Gensim"" rel=""nofollow noreferrer"">Gensim</a> but did not understand how could I use this to implement <code>GloVe</code>. Here is what I am doing:</p>

<pre><code>import numpy as np
import pandas as pd
import nltk
import re
import os
import codecs
from sklearn import feature_extraction
import mpld3
import csv
import string
import time
import sys
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.externals import joblib
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

csvRows = []
nltk.download('stopwords')

title = []
synopses = []
filename = ""twitter-test-dataset.csv""
num_clusters = 10
pkl_file = ""doc_cluster.pkl""
generate_pkl = False

# pre-process data
with open(filename, 'r') as csvfile:
    # creating a csv reader object
    csvreader = csv.reader(csvfile)

    # extracting field names through first row
    fields = csvreader.next()

    # extracting each data row one by one
    duplicates = 0
    for row in csvreader:
        # removes the characters specified
        line = re.sub(r'[.,""!]+', '', row[2], flags=re.MULTILINE)
        line = re.sub(r'^RT[\s]+', '', line, flags=re.MULTILINE)  # removes RT
        line = re.sub(r'https?:\/\/.*[\r\n]*', '',
                    line, flags=re.MULTILINE)  # remove link
        line = re.sub(r'[:]+', '', line, flags=re.MULTILINE)
        line = (re.sub(
            ""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", line, flags=re.MULTILINE))
        line = filter(lambda x: x in string.printable,
                    line)  # filter non-ascii characers
        if line not in synopses:
            synopses.append(line)
            title.append(row[2])
        else:
            duplicates += 1

print(""Removed "" + str(duplicates) + "" rows"")


stopwords = nltk.corpus.stopwords.words('english')
stemmer = SnowballStemmer(""english"")


def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word for sent in nltk.sent_tokenize(
        text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems


def tokenize_only(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word.lower() for sent in nltk.sent_tokenize(text)
            for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    return filtered_tokens


totalvocab_stemmed = []
totalvocab_tokenized = []

for i in synopses:
    # for each item in 'synopses', tokenize/stem
    allwords_stemmed = tokenize_and_stem(i)
    # extend the 'totalvocab_stemmed' list
    totalvocab_stemmed.extend(allwords_stemmed)

    allwords_tokenized = tokenize_only(i)
    totalvocab_tokenized.extend(allwords_tokenized)

vocab_frame = pd.DataFrame(
    {'words': totalvocab_tokenized}, index=totalvocab_stemmed)

# print ""there are "" + str(vocab_frame.shape[0]) + "" items in vocab_frame""


# define vectorizer parameters
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                min_df=0.0, stop_words='english',
                                use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1, 3))

#CREATE TFIDF MATRIX
tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)
terms = tfidf_vectorizer.get_feature_names()


c = hdbscan.HDBSCAN(min_cluster_size=5)
#PASS TFIDF_MATRIX TO HDBSCAN
c.fit(tfidf_matrix)
print(c.labels_)
sys.exit()
</code></pre>

<p>As you could see in the above implementation, I have used <code>HDBSCAN</code> along with <code>tf-idf</code> for text clustering. How could I use <code>GloVe</code> in place of <code>tf-idf</code>?</p>
",,2018-06-16 18:24:29,How to use GloVe to generate vector matrix?,<python><vectorization><tf-idf><hdbscan>,,,CC BY-SA 4.0,True,False,True,False,True
17339,50910287,2018-06-18 13:08:57,,"<p>I am using gensim to load the fasttext's pre-trained word embedding</p>

<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')</code></p>

<p>But this gives me a memory error.</p>

<p>Is there any way I can load it?</p>
",2018-06-18 13:16:13,2018-06-18 22:20:29,loading of fasttext pre trained german word embedding's .vec file throwing out of memory error,<nlp><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
17341,50945561,2018-06-20 10:04:36,,"<p>I have problems with using w2v embeddings from google news. 
I downloaded <code>GoogleNews-vectors-negative300.bin.gz</code> and after running </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format('/home/slava/GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>I got error</p>

<blockquote>
  <p>IOerror: not a gzipped file</p>
</blockquote>

<p>Okay, i runned <code>gzip GoogleNews-vectors-negative300.bin</code> in console and 
<code>file GoogleNews-vectors-negative300.bin.gz</code> now says, that it is really gzip compressed data. 
But running </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format('/home/slava/GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>now returns </p>

<blockquote>
  <p>ValueError: need more than 0 values to unpack</p>
</blockquote>

<p>Full traceback:</p>

<pre><code>&gt; ValueError                                Traceback (most recent call
&gt; last) &lt;ipython-input-9-c4eebc3bcdb0&gt; in &lt;module&gt;()
&gt;       1 
&gt;       2 from gensim.models import Word2Vec
&gt; ----&gt; 3 model = gensim.models.KeyedVectors.load_word2vec_format('/home/slava/GoogleNews-vectors-negative300.bin.gz',
&gt; binary=True)
&gt; 
&gt; /home/slava/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.pyc
&gt; in load_word2vec_format(cls, fname, fvocab, binary, encoding,
&gt; unicode_errors, limit, datatype)
&gt;     205         with utils.smart_open(fname) as fin:
&gt;     206             header = utils.to_unicode(fin.readline(), encoding=encoding)
&gt; --&gt; 207             vocab_size, vector_size = map(int, header.split())  # throws for invalid file format
&gt;     208             if limit:
&gt;     209                 vocab_size = min(vocab_size, limit)
&gt; 
&gt; ValueError: need more than 0 values to unpack
</code></pre>

<p>How to fix this?</p>
",,2018-06-21 09:10:54,Word2Vec python ValueError: need more than 0 values to unpack,<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17342,50945820,2018-06-20 10:17:47,,"<p>I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.</p>

<p>The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. </p>

<p>I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  </p>

<p>Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. </p>

<p>Thank you!</p>
",2018-06-21 00:14:16,2018-06-21 00:14:16,Paragraph Vector or Doc2vec model size,<nlp><gensim><word-embedding><doc2vec><deeplearning4j>,,,CC BY-SA 4.0,False,False,True,False,False
17352,50879239,2018-06-15 16:05:51,,"<p>i want to parse my sentence . so i want use StanfordDependencyParser ,
everything is ok , there is no error but the output is not what i want.
i see no word of my sentence in result of output.
it seems stanfordparse doesnt work on my sentences .</p>

<p>i use this tools specifications : </p>

<ol>
<li>python 3.6</li>
<li>nltk 3.1</li>
<li>jre1.8.0_162</li>
</ol>

<p>i use below code for using stanford parse : </p>

<pre><code>import os
java_path = ""C:/Program Files/Java/jre1.8.0_162/bin/java.exe""
os.environ['JAVAHOME'] = java_path
from nltk.parse.stanford import StanfordDependencyParser
path_to_jar = 'D:/uni/Soft ware/2014-8-27/stanford-parser-full-2014-08-27/stanford-parser.jar'
path_to_models_jar = 'D:/uni/Soft ware/2014-8-27/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'
dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)
result = dependency_parser.raw_parse('I shot an elephant in my sleep')
dep = next(result)
print(dep)
</code></pre>

<p>at the output,i cant see any part of my sentence <strong>'I shot an elephant in my sleep'</strong> at the result , it seems that something is wrong.
the output is : </p>

<pre><code>E:\python_pycharm_project\gensim_cbow\venv\Scripts\python.exe E:/python_pycharm_project/gensim/venv/Cbow_gensim_2
defaultdict(&lt;function DependencyGraph.__init__.&lt;locals&gt;.&lt;lambda&gt; at 0x00000000003E2EA0&gt;,
{0: {'address': 0,
             'ctag': 'TOP',
             'deps': defaultdict(&lt;class 'list'&gt;, {'root': [2]}),
             'feats': None,
             'head': None,
             'lemma': None,
             'rel': None,
             'tag': 'TOP',
             'word': None},
         1: {'address': 1,
             'ctag': 'PRP',
 'deps': defaultdict(&lt;class 'list'&gt;, {}),
             'feats': '_',
             'head': 2,
             'lemma': '_',
             'rel': 'nsubj',
             'tag': 'PRP',
             'word': 'I'},
         2: {'address': 2,
             'ctag': 'VBD',
             'deps': defaultdict(&lt;class 'list'&gt;,
                                 {'dobj': [4],
                                  'nsubj': [1],
                                  'prep': [5]}),
             'feats': '_',
             'head': 0,
             'lemma': '_',
             'rel': 'root',
             'tag': 'VBD',
             'word': 'shot'},
         3: {'address': 3,
             'ctag': 'DT',
             'deps': defaultdict(&lt;class 'list'&gt;, {}),
             'feats': '_',
             'head': 4,
             'lemma': '_',
             'rel': 'det',
             'tag': 'DT',
             'word': 'an'},
         4: {'address': 4,
             'ctag': 'NN',
             'deps': defaultdict(&lt;class 'list'&gt;, {'det': [3]}),
             'feats': '_',
             'head': 2,
             'lemma': '_',
             'rel': 'dobj',
             'tag': 'NN',
             'word': 'elephant'},
         5: {'address': 5,
             'ctag': 'IN',
             'deps': defaultdict(&lt;class 'list'&gt;, {'pobj': [7]}),
             'feats': '_',
             'head': 2,
             'lemma': '_',
             'rel': 'prep',
             'tag': 'IN',
             'word': 'in'},
         6: {'address': 6,
             'ctag': 'PRP$',
             'deps': defaultdict(&lt;class 'list'&gt;, {}),
             'feats': '_',
             'head': 7,
             'lemma': '_',
             'rel': 'poss',
             'tag': 'PRP$',
             'word': 'my'},
         7: {'address': 7,
             'ctag': 'NN',
             'deps': defaultdict(&lt;class 'list'&gt;, {'poss': [6]}),
             'feats': '_',
             'head': 5,
             'lemma': '_',
             'rel': 'pobj',
             'tag': 'NN',
             'word': 'sleep'}})
Process finished with exit code 0
</code></pre>
",2018-06-15 20:19:20,2018-06-15 20:19:20,python StanfordDependencyParser output exit code 0,<python><stanford-nlp>,,,CC BY-SA 4.0,True,False,True,True,False
17359,50895571,2018-06-17 10:01:56,,"<p>Hi I am using Gensim Word2Vec for word embedding in python.</p>

<pre><code>from gensim.models import Word2Vec, KeyedVectors
</code></pre>

<p>But i am getting error like:</p>

<pre><code>from gensim import utils
# cannot import whole gensim.corpora, because that imports wikicorpus...
from gensim.corpora.dictionary import Dictionary
</code></pre>

<p>ImportError: cannot import name utils. Thank you</p>
",,2019-05-15 13:44:59,"Python module Gensim error ""cannot import name utils""",<python><pip><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17366,50933591,2018-06-19 17:06:00,,"<p>I am using Gensim for vector space model. after creating a dictionary and corpus from Gensim I calculated the (Term frequency*Inverse document  Frequency)TFIDF  using the following line</p>

<pre><code>Term_IDF  = TfidfModel(corpus)
corpus_tfidf = Term_IDF[corpus]
</code></pre>

<p>The corpus_tfidf contain list of the list having Terms ids and corresponding TFIDF. then I separated the TFIDF from ids using following lines:</p>

<pre><code> for doc in corpus_tfidf:
     for ids,tfidf in doc:    
         IDS.append(ids)
         tfidfmtx.append(tfidf)    
         IDS=[]
</code></pre>

<p>now I want to use k-means clustering so I want to perform cosine similarities of tfidf matrix the problem is Gensim does not produce square matrix so when I run following line it generates an error. I wonder how can I get the square matrix from Gensim to calculate the similarities of all the documents in vector space model. Also how to convert tfidf matrix (which in this case is a list of lists) into 2D NumPy array. any comments are much appreciated.</p>

<p>dumydist = 1 - cosine_similarity(tfidfmtx)</p>
",2018-06-19 17:27:54,2019-04-15 15:25:36,How to perform kmean clustering from Gensim TFIDF values,<numpy><k-means><gensim><tf-idf><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
17372,50859540,2018-06-14 14:10:47,,"<p>I am using gensim's tdidf model like so:</p>

<pre><code>from gensim import corpora, models

dictionary = corpora.Dictionary(some_corpus)
mapped_corpus = [dictionary.doc2bow(text)
                 for text in some_corpus]

tfidf = models.TfidfModel(mapped_corpus)
</code></pre>

<p>Now I'd like to apply thresholds to remove terms that appear too frequently (max_df) and too infrequently (min_df).  I know that scikit's CountVectorizer allows you to do this, but I can't seem to find how to set these thresholds in gensim's tfidf.  Could someone please help? </p>
",,2018-06-22 18:14:32,Is there a way to set min_df and max_df in gensim's tfidf model?,<gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
17381,50937881,2018-06-19 22:39:53,,"<p>I am not only interested in the final W0 and W1 (also, to some known as W and W'), but all the variations of these two matrices during the learning.</p>

<p>For now, I am using the gensim implementation, but compared to sklearn, gensim's API is not very well organized in my mind. Hence, I am open to moving to tf if need be, given that getting access to these values would be possible/easier.</p>

<p>I know I can hack the main code; my question is whether there already is a function/variable for it.</p>
",2019-05-26 11:04:37,2019-05-26 11:04:37,How to get all the weight updates from Word2Vec,<tensorflow><gensim><word2vec>,2019-05-26 11:04:40,,CC BY-SA 4.0,False,False,True,False,True
17382,50992153,2018-06-22 16:29:52,,"<p>running gensim Doc2Vec over ubuntu</p>

<p>Doc2Vec rejects my input with the error</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'words'</p>
</blockquote>

<pre><code>    import gensim from gensim.models  
    import doc2vec as dtv
    from nltk.corpus import brown
    documents = brown.tagged_sents()
    d2vmodel = &gt; dtv.Doc2Vec(documents, size=100, window=1, min_count=1, workers=1)
</code></pre>

<p>I have tried already from 
<a href=""https://stackoverflow.com/questions/36509957/why-gensim-doc2vec-give-attributeerror-list-object-has-no-attribute-words"">this SO question</a> and many variations with the same result</p>

<p>documents = [brown.tagged_sents()}
adding a hash function</p>

<p>If corpus is a .txt file I <em>can</em> utilize </p>

<pre><code>    documents=TaggedLineDocument(documents)
</code></pre>

<p>but that is often not possible</p>
",,2018-06-22 18:49:34,Doc2Vec input format,<gensim><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
17387,50914729,2018-06-18 17:32:32,,"<p>I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. </p>

<p>The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.</p>

<p>Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?</p>
",2018-06-18 20:40:39,2019-04-17 11:15:04,Gensim Word2Vec select minor set of word vectors from pretrained model,<python><keras><word2vec><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17389,50919655,2018-06-19 01:59:24,,"<p>I'm wondering why isn't the number of features the same as the number of unique tokens, but rather, in my case, they differ by one (1236 v.s. 1235)</p>
<blockquote>
<p>2018-06-19 04:54:45,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])</p>
<p>2018-06-19 04:54:45,182 : INFO : built Dictionary(1236 unique tokens: ['.', ':', .....]...) from 98 documents (total 10007 corpus positions)</p>
<p>2018-06-19 04:54:45,214 : INFO : collecting document frequencies</p>
<p>2018-06-19 04:54:45,215 : INFO : PROGRESS: processing document #0</p>
<p>2018-06-19 04:54:45,219 : INFO : calculating IDF weights for 98 documents and 1235 features (6993 matrix non-zeros)</p>
</blockquote>
",2020-06-20 09:12:55,2018-06-19 02:09:44,gensim tfidf number of unique tokens v.s. number of features,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17390,50953272,2018-06-20 16:45:07,,"<p>How do I use gensim to find out Jaccard index in vectors in the corpus?</p>
",,2018-06-21 04:30:48,Jaccard index in python for a corpus using gensim,<python-2.7><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17397,50976476,2018-06-21 19:48:13,,"<p>I have parsed 30 excel files and created a pandas dataframe. I have tokenized the words, taken out stop words and made bigrams. However when I try to lemmatize it gives me this error: TypeError: unhashable type: 'list'
Here's my code:</p>

<pre><code># Use simple pre-proces to clean up data and tokenize
def sent_to_words(sentences):
    for sentence in sentences:
    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

data_words = list(sent_to_words(data))

# Define Function for Removing stopwords
def remove_stopwords(texts):
    return[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

# Define function for bigrams
def make_bigrams(texts):
    return[bigram_mod[doc] for doc in texts]

#Remove Stop Words
data_words_nostops = remove_stopwords(data_words)
# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

#Define function for lemmatizing
from nltk.stem.wordnet import WordNetLemmatizer
def get_lemma(word):
    return WordNetLemmatizer().lemmatize(word)

#Lemmatize words
data_lemmatized = get_lemma(data_words_bigrams)
</code></pre>

<p>This is exactly where I get the error. How should I adjust my code to resolve this issue? Thank you in advance</p>

<p>as suggested, the first few lines of the dataframe</p>

<pre><code>df.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/pbIb8.png"" rel=""nofollow noreferrer"">dataframe snap</a></p>
",2018-06-22 14:32:22,2018-06-22 14:32:22,Type Error when Lemmatizing words using NLTK,<python-3.x><nltk><tokenize><wordnet><lemmatization>,,,CC BY-SA 4.0,True,False,True,False,False
17409,51014463,2018-06-24 22:25:09,,"<p>What is the effect of assigning the same label to a bunch of sentences in doc2vec? I have a collection of documents that I want to learn vectors using gensim for a ""file"" classification task where file refers to a collection of documents for a given ID. I have several ways of labeling in mind and I want to know what would be the difference between them and which is the best - </p>

<ul>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags and train. Repeat for others</p></li>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags. Then tokenize document into sentences and assign label <code>doc1</code> to its tags and then train with both full document and individual sentences. Repeat for others</p></li>
</ul>

<p>For example (ignore that the sentence isn't tokenized) -</p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1'])
TaggedDocument(words=[""It is small.""], tags=['doc1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1'])
</code></pre>

<ul>
<li>Similar to above, but also assign a unique label for each sentence along with <code>doc1</code>. The full document has the all the sentence tags along with <code>doc1</code>.</li>
</ul>

<p>Example - </p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1', 'doc1_sentence1', 'doc1_sentence2'])
TaggedDocument(words=[""It is small.""], tags=['doc1', 'doc1_sentence1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1', 'doc1_sentence2'])
</code></pre>

<p>I also have some additional categorical tags that I'd be assigning. So what would be the best approach?</p>
",,2018-06-25 00:27:46,Hierarchical training for doc2vec: how would assigning same labels to sentences of the same document work?,<python><nlp><word2vec><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17414,51049568,2018-06-26 18:44:00,,"<p>I am working on tokenizing, lemmatizing and removing stopwords from a document. However, Spacy is throwing an error saying that the token.pos_ module does not accept 'str'.  I believe strings are the proper format, correct me if I am wrong. How do I fix this error?</p>

<pre><code>words = []
classes = []
documents = []
ignore_words = ['?']
# loop through each sentence in our training data
for pattern in training_data:
    # tokenize each word in the sentence
    w = gensim.utils.simple_preprocess(str(pattern['sentence']), deacc=True)
    # add to our words list
    words.extend(w)
    # add to documents in our corpus
    documents.append((w, pattern['class']))
    # add to our classes list
    if pattern['class'] not in classes:
        classes.append(pattern['class'])

nltk.download('stopwords')
stop_words = stopwords.words('english')
stop_words.extend([""be"", ""use"", ""fig""])
words = [word for word in words if word not in stop_words] 

# stem and lower each word and remove duplicates
import en_core_web_lg
nlp = en_core_web_lg.load()
print(words[0:10])

words = [token.lemma_ for token in words if token.pos_ in postags]
words = list(set(words))

AttributeError                            Traceback (most recent call last)
&lt;ipython-input-72-5c31e2b5a13c&gt; in &lt;module&gt;()
     26 
     27 from spacy import tokens
---&gt; 28 words = [token.lemma_ for token in words if token.pos in postags]
     29 words = list(set(words))
     30 

&lt;ipython-input-72-5c31e2b5a13c&gt; in &lt;listcomp&gt;(.0)
     26 
     27 from spacy import tokens
---&gt; 28 words = [token.lemma_ for token in words if token.pos in postags]
     29 words = list(set(words))
     30 

AttributeError: 'str' object has no attribute 'pos'
</code></pre>
",,2018-06-26 19:01:45,AttributeError on spacy token.pos_,<python><nlp><spacy>,,,CC BY-SA 4.0,True,True,True,False,False
17415,50960914,2018-06-21 05:24:02,,"<p>While forming the Gensim LDA model, I got dictionary for my Data using following command</p>

<pre><code>    from gensim.corpora import Dictionary
    dictionary1 = Dictionary(docs)
    dictionary1.filter_extremes(no_below=10, no_above=0.75, keep_n = 1000)
</code></pre>

<p>Out of these 1000 most frequent tokens I manually removed 500 tokens so that the remaining tokens would be directly related to the topics I want to generate.
How can i further form corpus document out of this new dictionary formed which is of type dict. In which form should I use it as to train my LDA model?</p>
",,2018-06-21 05:43:25,Can I form corpus document for LDA model out of dictionary of type dict?,<python><dictionary><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17416,51105753,2018-06-29 16:10:47,,"<p>I use the gensim library to create a word2vec model. It contains the function <code>predict_output_words()</code> which I understand as follows:</p>

<p>For example, I have a model that is trained with the sentence: ""Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy.""</p>

<p>and then I use </p>

<p><code>model.predict_output_words(context_words_list=['Anarchism', 'does', 'not', 'offer', 'a', 'fixed', 'body', 'of', 'from', 'a', 'single', 'particular', 'world', 'view', 'instead', 'fluxing'], topn=10)</code>.</p>

<p>In this situation, could I get/predict the correct word or the omitted word 'doctrine'?</p>

<p>Is this the right way? Please explain this function in detail.</p>
",2018-06-29 18:49:25,2018-07-02 10:02:17,gensim function predict output words,<python><tensorflow><nlp><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17433,51030698,2018-06-25 19:33:43,,"<p>I am using gensim to create a bag of words model and I want to perform normalization. I found the documentation (<a href=""https://radimrehurek.com/gensim/models/normmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/normmodel.html</a>), but I am confused as to how to implement that given the code I have. Conversations is a list of tokenized documents, so essentially a list of lists when each element is a document.</p>

<pre><code>id2word = corpora.Dictionary(conversations)
id2word.filter_extremes(keep_n=5000, keep_tokens=None) 
corpus = [id2word.doc2bow(text) for text in conversations]
norm_corpus = NormModel(corpus)
</code></pre>

<p>Corpus is a sparse matrix, I believe. For each document, it has the non-zero frequency terms and the corresponding counts: [[(0, 2), (1, 5), (2, 4)...(92, 2), (93, 3)],...].</p>

<p>The last line with <code>norm_corpus</code> does not work when I try to input it into the following: <code>models.LsiModel(norm_corpus, id2word=id2word, num_topics=12)</code>. I get the type error message, 'int' object is not iterable. However, the documentation says to pass in a corpus so I'm confused. I would appreciate any help -- thanks!</p>
",2018-06-25 19:48:52,2018-06-25 19:48:52,Normalizing bag of words data in Gensim,<python><normalization><gensim><corpus><term-document-matrix>,,,CC BY-SA 4.0,False,False,True,False,False
17440,51150702,2018-07-03 09:10:04,,"<p>I am trying to find the dissimilarity between the two documents. I am using gensim and so far have obtained similarity score. </p>

<p>Is there any way to know the dissimilarity score and dissimilar features between two documents?
And how to evaluate it?</p>
",2019-09-02 12:18:27,2019-09-02 12:18:27,Dissimilar Features between two documents,<nlp><nltk><gensim><cosine-similarity>,,,CC BY-SA 4.0,True,False,True,False,False
17453,51168444,2018-07-04 07:49:20,,"<p>I have check the previous post <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">link</a> but it doesn't seems to work for my case:-</p>

<p>I have pre trained word2vec model:</p>

<pre><code>import gensim    
model = Word2Vec.load('w2v_model')
</code></pre>

<p>Now I have a pandas dataframe with keywords:</p>

<pre><code>keyword
corruption
people
budget
cambodia
.......
......
</code></pre>

<p>All I want to add the vectors for each keyword in its corresponding columns but
when I use <code>model['cambodia']</code> it throw me error as <code>KeyError: ""word 'cambodia' not in vocabulary""</code></p>

<p>so I have update the keyword as:</p>

<pre><code>model.train(['cambodia'])
</code></pre>

<p>But this won't work out for me, when I use 
<code>model['cambodia']</code> </p>

<p>it still giving an error as <code>KeyError: ""word 'cambodia' not in vocabulary""</code>. How to update new words into word2vec vocabulary so i can get its vectors? Expected output will be:-</p>

<pre><code>keyword    V1         V2          V3         V4            V5         V6   
corruption 0.07397  0.290874    -0.170812   0.085428    -0.148551   0.38846 
people      ..............................................................
budget      ...........................................................
</code></pre>
",2018-07-06 06:47:36,2018-07-26 08:52:18,How I can get the vectors for words that were not present in word2vec vocabulary?,<python-3.x><pandas><word2vec><gensim><text-classification>,,,CC BY-SA 4.0,False,False,True,False,False
17460,51132848,2018-07-02 09:25:08,,"<p>Is there a pre-trained doc2vec model with a large data set, like Wikipedia or similar?</p>
",2019-05-07 01:32:54,2019-05-07 01:32:54,Is there pre-trained doc2vec model?,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17461,51133162,2018-07-02 09:40:22,,"<p>I trained a w2v model on a big corpus, and I want to update it with a smaller one with new sentences (and new words).</p>

<p>In the first big training, I took the default parameters for alpha (0.025 with lin. decay  to 0.0001)
Now, I want to use <code>model.train</code> to update it. But from the doc I don't understand which (initial and final) learning rate will be used during this update of training.</p>

<p>From one side, if you also use 0.025 with lin. decay until 0.0001, it will be too strong for already existing words which appeared a lot in the first big corpus and that will be heavily changed, but from the other side for new words (added with model.build_vocab(sentences, update = True)) a low learning rate of 0.0001 is too small.</p>

<p>So my questions are : </p>

<ol>
<li>What is the default behaviour of <code>model.train</code> in the API on new sentences regarding the learning rate?</li>
<li><p>How I should choose the learning rate in order to take into account this issue of old/new words ?</p></li>
<li><p>[aside question] Why when I use 2 times model.train on the same sentences, the second time, it doesn't update the vectors ?</p></li>
</ol>
",2018-07-02 09:46:27,2018-07-02 15:17:31,word2vec gensim update learning rate,<python><machine-learning><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17465,51092771,2018-06-29 00:30:12,,"<p>It has been suggested that initializing a topic model using clusters of words can lead to higher quality models or more robust (consistent) inference. I am talking about initializing the optimizer, not setting a prior. Here is some code to illustrate what I want to do:</p>

<p>Create an LdaModel object, but don't pass in a corpus. </p>

<pre><code>lda_model =
LdaModel(
         id2word=id2word,
         num_topics=30,
         eval_every=10,
         pass=40,
         iterations=5000)
</code></pre>

<p>Next assign some property of the object, corresponding to the probabilities of drawing each word from a topic to a matrix of my own construction. </p>

<pre><code>lda_model.topics = my_topic_mat
</code></pre>

<p>Then fit the corpus: </p>

<pre><code>lda_model.update(corpus)
</code></pre>

<p>Thanks for the help!</p>
",2018-06-30 20:28:56,2018-06-30 23:08:24,How can I initialize a gensim LDA topic model?,<python><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
17471,51061171,2018-06-27 11:04:52,,"<p>I want to create a model that can predict a word missing in a sentence (the sentence has a whole which needs to be filled)</p>

<p>My dataset consists of ~1000 sentences, shortest sentence is length=6, longest sentence is length~120000. The vocabulary size of the dataset is only 90.</p>

<p>Would it make a huge difference for the model to have either onehot-encoding or a pre-trained word2vec (gensim) model? I use the pre-trained weights in a keras embedding layer: what would be the sense of ""freezing"" (not trainable) that layer as it did not make any difference regarding the accuracy in my case?</p>

<p>I tried both representation and there seems to be no difference regarding the accuracy of my test data. The highest accuracy I achieved was 55% correctly predicted.</p>

<p>Am I on the right track or is it the wrong approach to do that task? My machine is very slow which makes experiments with parameters so hard and time consuming.</p>

<p>I would be grateful for any advice or tip!</p>
",,2018-06-27 12:06:27,LSTM - sentence completion with word2vec,<keras><deep-learning><lstm><prediction><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17479,51120323,2018-07-01 04:08:17,,"<p>I have two type document, one is labeled, and other is not. 
I want use the labeled document to calculate tf, and use other unlabeled document to calculate idf.</p>

<pre><code>from gensim import corpora, models, similarities

dictionary = corpora.Dictionary(line.lower().split(',') for line in open('data/unannotated.csv',encoding='utf-8'))
dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None) 
</code></pre>

<p>This unannotated is unlabeled data, one line is one document.
Based on this data, I got a dict:</p>

<pre><code>{'get': 353867,
 'http': 351618,
 'u': 324711,
 'one': 291526,
 'go': 279237,
 'know': 265001,
 'good': 249368,
 'say': 236003,
 'like': 225619,
 'qatar': 223010,
 'time': 191195,
 'would': 188766,
 'think': 187128,
 'people': 182873,
 'make': 180120,
 'need': 166287,
 'take': 157106,
...
}
</code></pre>

<p>I use next piece of code to calculate tfidf, and it gives all same answer:0.9999999999999946</p>

<pre><code>def TFCalculator(question, word):
    if word not in question:
        return 0
    count = dict(Counter(question))
    q_len = len(question)
    return float(count[word]) / float(q_len)

def n_containing(unannotated, word):
    return float(unannotated.get(word,0))

def IDFCalculator(unannotated, word):
    return math.log(float(len(unannotated.keys())) / (1.0 + n_containing(unannotated, word)))

def tfidf(stem, unannotated):
    tfidfVector_ques = range(len(unannotated))
    for word in stem:
        tf= TFCalculator(stem,word)
        idf = IDFCalculator(unannotated, word)
        tfidf = tf * idf
        try:
            tfidfVector_ques[list(unannotated).index(word)] = tfidf
        except:
            pass
    return sparse.csr_matrix(tfidfVector_ques)

def cosine_similarities(question,comment,unannotated):
    X_tfidf = tfidf(question,unannotated)
    Y_tfidf = tfidf(comment,unannotated)
    similarities = cosine_similarity(X_tfidf,Y_tfidf)
    return similarities
</code></pre>
",,2018-07-01 04:08:17,how to cal tfidf in different way,<python><nlp><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
17481,51135118,2018-07-02 11:26:43,,"<p>Regarding word2vec with gensim,
Suppose you already trained a model on a big corpus, and you want to update it with new words from new sentences, but not update the words which already have a vector.
Is it possible to freeze the vectors of some words and update only some chosen words (like the new words) when calling <code>model.train</code> ?
Or maybe is there a trick to do it ?
Thanks.</p>
",,2018-07-02 15:25:39,Gensim Word2vec Freeze some wordvectors and Update others,<python><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17487,51042031,2018-06-26 11:39:53,,"<p>Our company has a lot of data that are issue which are stored in a database.We want to create a search engine so that people can check how the issues were previously dealt with.We cannot use any 3rd party api as there is sensitive data an we want to keep it as in house. Right now the approach is as following :- </p>

<ol>
<li>Clean up the data and then use a DOC2VEC to represent each issue as a vector .</li>
<li>Find the closest 5 issue using some distance metric.</li>
</ol>

<p>The problem is that the results are not at all useful.The problem is most of the data is one liner and some issue description.There are spelling mistakes and stack traces and other things. </p>

<p>Is this the right approch or should we switch to something else?
Right now we are testing on 200K data.
Thanks for the help.</p>
",,2018-06-26 11:39:53,Trying to make a search engine for issues,<machine-learning><word2vec><gensim><information-retrieval><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17494,51082572,2018-06-28 12:09:46,,"<p>I was working on a Amazon Sentiment Classification dataset, where I have to predict the sentiment based on the reviews given. However I was experimenting with 2 methods, one with the normal <code>Embedding</code> layer from Keras, and this is my architecture:</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>And the next one I was trying with <code>Word2Vec</code> from <code>gensim.models</code>. Here is my code:</p>

<pre><code>from gensim.models import Word2Vec
word_model = Word2Vec(df['reviewText'], size=200, min_count = 1, window = 5,sg=0, negative=5)
WV_DIM = 100
nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))
# we initialize the matrix with random numbers
wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0
for word, i in word_index.items():
    if i &gt;= MAX_NB_WORDS:
        continue
    try:
        embedding_vector = word_vectors[word]
        # words not found in embedding index will be all-zeros.
        wv_matrix[i] = embedding_vector
    except:
        pass    
model = Sequential()
model.add(Embedding(nb_words,
                     WV_DIM,
                     mask_zero=False,
                     weights=[wv_matrix],
                     input_length=MAX_SEQUENCE_LENGTH,
                     trainable=False))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>But I'm unable to understand what is the difference b/w just using the <code>model.add(Embedding(..))</code> and using the <code>word2vec</code> along with <code>Embedding</code>. I want to know the Maths behind the working of Keras's <code>Embedding</code> layer and how <code>word2vec</code>+<code>Embedding</code> is working out. </p>

<p>I've gone through <a href=""https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"">this post</a> to understand how Keras embedding layer work out but I want to understand the gradient descent and backpropagation for it and also for the combination of <code>word2vec</code> and <code>Embedding</code>.</p>

<p>P.S. For the both the codes I've given, I have skipped the <code>tokenize</code> and <code>pad_sequence</code> part.</p>
",,2018-06-28 12:09:46,Gradient Descent & Backpropagation difference b/w Embedding Layer in Keras and Word2Vec of Gensim,<python><keras><nlp><deep-learning><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17495,51062519,2018-06-27 12:12:53,,"<p>I've used gensim for text summarizing in Python. I want my summarized output to be stored in a different column in the same dataframe.</p>

<p>I've used this code:</p>

<pre><code>for n, row in df_data_1.iterrows():
        text=df_data_1['Event Description (SAP)']
        print(text)
        *df_data_1['Summary']=summarize(text)*
print(df_data_1['Summary'])
</code></pre>

<p>The error is coming on line 4 of this code, which states: TypeError: expected string or bytes-like object.</p>

<p>How to store the processed text in the pandas dataframe</p>
",2018-06-27 12:14:34,2018-06-27 14:39:59,Storing processed text in pandas dataframe,<python><loops><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17506,51157395,2018-07-03 14:44:59,,"<p>I am having a 1-word document that I want to transform to its bag-of-words representation:</p>

<p>so <code>doc</code> is <code>['party']</code> and <code>id2word.doc2bow(doc)</code> is <code>[(229, 1)]</code> which means the word is known.</p>

<p>However, if I call <code>get_document_topics()</code> with <code>doc_bow</code>, the result is an <em>empty</em> list:</p>

<pre class=""lang-py prettyprint-override""><code>id2word = lda.id2word 

# ..

doc_bow = id2word.doc2bow(doc)

t = lda.get_document_topics(doc_bow)

try:
    label, prob = sorted(t, key=lambda x: -x[1])[0]
except Exception as e:
    print('Error!')
    raise e
</code></pre>

<p>The only possible explanation I'd have here is that this document (the single word) cannot be assigned to <em>any</em> topic. Is this the reason why I am seeing this?</p>
",,2018-12-24 05:44:44,get_document_topics() returns empty list of topics,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17510,51160354,2018-07-03 17:47:44,,"<p>I used gensim to create a bag of words model. Although it is much longer in reality, here is the format outputted when creating a bag of words document-term matrix on the tokenized texts using Gensim:</p>

<pre><code>id2word = corpora.Dictionary(texts)
corpus = [id2word.doc2bow(text) for text in texts]

[[(0, 2),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 11),
  (385, 1),
  (386, 2),
  (387, 3),
  (388, 1),
  (389, 1),
  (390, 1)],
 [(4, 31),
  (8, 2),
  (13, 2),
  (16, 2),
  (17, 2),
  (26, 1),
  (28, 4),
  (29, 1),
  (30, 1)]]
</code></pre>

<p>This is a sparse matrix representation, and from what I understand other libraries represent the document-term matrix in a similar fashion as well. If the document-term matrix is non-sparse (meaning the zero entries are there as well), I know that I just have to (A.T*A), since A is of dimension (num. of documents by num. of terms), so multiplying the two will give the term co-occurrences. Ultimately, I want to get the top n co-occurrences (so get the top n term pairs that occur together in the same texts). How would I achieve this? I am not attached to Gensim for creating the BOW model. If another library like sklearn can do it more easily, I am very open. I would appreciate any advice/help/code with this problem -- thanks!</p>
",,2018-07-04 18:40:37,Computing top n word pair co-occurrences from document term matrix,<python><matrix><scikit-learn><gensim><text-analysis>,,,CC BY-SA 4.0,False,False,True,False,True
17513,51142294,2018-07-02 19:01:57,,"<p>Been following the documentation <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">here</a> and as well as in this link: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">Machine Learning Gensim Tutorial</a> and I'm at a complete loss for why this is happening. After tokenizing and lemmatizing my sentences, I put the sentences through a phraser, created a Dictionary, and inserted all the right variables into the model. Here is a sampling of my code:</p>

<pre><code>tokens =  [[euid, sent, gensim.parsing.preprocessing.preprocess_string(sent.lower(), filters=[strip_punctuation,
        strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, wordnet_stem])] for sent in sentences]
#these filters are all default gensim filters except for wordnet_stem, which uses a WordNetLemmatizer

 bigram = gensim.models.Phrases(bag_of_words)
bigram_mod = gensim.models.phrases.Phraser(bigram)
</code></pre>

<p>Sample token list looks like this: <code>['beautiful', 'Manager', 'tree', 'caring', 'great_place']</code> (completely made-up list)</p>

<pre><code>texts = [bigram_mod[t] for t in bag_of_words]
id2word = corpora.Dictionary(texts)
sent_wordfreq = [id2word.doc2bow(sent) for sent in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=sent_wordfreq,
                                            id2word=id2word,
                                            num_topics=5,
                                            update_every=1,
                                            alpha='auto',
                                            per_word_topics=True)
</code></pre>

<p>Here are the topics I'm getting:</p>

<blockquote>
  <p>[(0, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  (1, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  and this continues on 3 more times</p>
</blockquote>

<p>So not only are all the topics the same, each's weight is nan. What could be the issue?</p>
",2018-07-02 19:18:52,2019-07-26 03:52:30,Gensim LDAmodel error: NaN and all topics the same,<python><pandas><nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17520,51269058,2018-07-10 15:26:20,,"<p>Gensim's <code>Word2Vec</code> model takes as an  input a list of lists with the inner list containing individual tokens/words of a sentence. As I understand <code>Word2Vec</code> is used to ""quantify"" the context of words within a text using vectors. 
I am currently dealing with a corpus of text that has already been split into individual tokens and no longer contains an obvious sentence format (punctuation has been removed). I was wondering how should I input this into the <code>Word2Vec</code> model? </p>

<p>Say if I simply split the corpus into ""sentences"" of uniform length (10 tokens per sentence for example), would this be a good way of inputting the data into the model? </p>

<p>Essentially, <strong>I am wondering how the format of the input sentences (list of lists) affects the output of Word2Vec?</strong> </p>
",2018-07-10 17:59:54,2018-07-10 17:59:54,Use proxy sentences from cleaned data,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17524,51180848,2018-07-04 21:18:46,,"<p>So I have started to learn gensim for both word2vec and doc2vec and it works. The similarity scores actually work really well. For an experiment, however, I wanted to optimize a key word based search algorithm by comparing a single word and getting how similar it is to a piece of text. </p>

<p>What is the best way to do this? I considered averaging the the word vectors of all words in the text (maybe remove fill and stop word first) and and comparing this to the search word? But this really is just intuition, what would be the best way to do this?</p>
",,2018-07-05 21:00:30,How to I get the similiarity between a word to a document in gensim,<python><search><gensim><word2vec><doc2vec>,2018-07-07 11:50:00,,CC BY-SA 4.0,False,False,True,False,False
17536,51252324,2018-07-09 18:57:07,,"<p>I am trying to use gensim's doc2vec to create a model which will be trained on a set of documents and a set of labels. The labels were created manually and need to be put into the program to be trained on. So far I have 2 lists: a list of sentences, and a list of labels corresponding to that sentence. I need to use doc2vec specifically. Here is what I have tried so far.</p>

<pre><code>from gensim import utils
from gensim.models import Doc2Vec

tweets = [""A tweet"", ""Another tweet"", ""A third tweet"", ... , ""A thousandth-something tweet""]
labels_list = [1, 1, 3, ... , 16]

tagged_data = [tweets, labels_list]
model = Doc2Vec(size=20, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)
model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    model.train(tagged_data, total_examples=model.corpus_count, 
epochs=model.iter)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
</code></pre>

<p>I am getting the error on the line with <code>model.build_vocab(tagged_data)</code> that there is an <code>AttributeError: 'list' object has no attribute 'words'</code>. I googled this and it says to put it into a labeled sentence object, but I am not sure if that will work if I have predefined labels. So does anyone know how to put pre-defined labels into doc2vec? Thanks in advance.</p>
",,2018-07-10 03:30:43,Doc2Vec gensim with supervised data predefined labels,<python><gensim><supervised-learning><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17542,51233632,2018-07-08 15:46:00,,"<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>

<pre><code>model.wv.most_similar(positive = ['man'])
Out[14]: 
[('woman', 0.7380284070968628),
 ('lady', 0.6933152675628662),
 ('monk', 0.6662989258766174),
 ('guy', 0.6513140201568604),
 ('soldier', 0.6491742134094238),
 ('priest', 0.6440571546554565),
 ('farmer', 0.6366012692451477),
 ('sailor', 0.6297377943992615),
 ('knight', 0.6290514469146729),
 ('person', 0.6288090944290161)]
--------------------------------------------
</code></pre>

<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>

<pre><code>model.similarity('man', '')
__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will 
be removed in 4.0.0, use self.wv.similarity() instead).
Out[13]: 0.078265618974427215
</code></pre>

<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>

<p>This is what I am doing here:</p>

<pre><code>#Combining all the words together.
all_reviews=HindiWordsList + EnglishWordsList

#Training FastText model
cpu_count=multiprocessing.cpu_count()
model=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)
model.build_vocab(all_reviews)
model.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)
model.save(""word2vec_combined_50.bin"")
</code></pre>
",2018-07-10 10:20:52,2020-05-16 10:43:50,word2vec gensim multiple languages,<python><nlp><artificial-intelligence><word2vec><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17552,51311240,2018-07-12 17:16:40,,"<p>Using Word2vec (skip-gram) model in tensorflow , I wrote the code to obtain word embeddings from document-set.
The final embeddings are in numpy.ndarray format</p>

<p>Now to obtain similar documents , I need to use the WMD(Word Movers Distance) algorithm.</p>

<p>(I don't have much knowledge of gensim)
The gensim.similarities.WmdSimilarity() requires the embeddings to be in KeyedVectors data type (seems like) -- 
What can I do to implement WMD in my code.I have a tight deadline and can't give much time to writing the code of WMD from scratch .</p>
",,2018-07-15 18:03:37,How to use WmdSimilarity function provided in gensim along with word embeddings which are in numpy.ndarray data type,<python-3.6><gensim><word2vec><numpy-ndarray><wmd>,,,CC BY-SA 4.0,False,False,True,False,False
17570,51292498,2018-07-11 18:59:38,,"<p>I have a dataframe with column <code>keywords</code>:</p>

<pre><code>keywords
election
countries
majestic
dollar
....
....
...
</code></pre>

<p>I also have my own pretrained word2vec model using gensim from where I can get 20 dimension vectors for each word using <code>model['anyword']</code>. My question is </p>

<p>i) I want to assign these 20 dimension vectors as columns names (V1 to V20) corresponding to each keyword.</p>

<p>ii) if word is not present in word vocabulary then i want to assign the vectors as array of [0,0,0,,,,,0] corresponding to that word otherwise it will give an <code>error:word not present in vocabulary</code>. for example if word <code>majestic</code> is not present in vocab then <code>df</code> would should like </p>

<pre><code>keyword     V1     V2      V3 ............. V20
election   0.02    0.44    0.32.............0.12
countries  0.33    0.33    0.11............ 0.13
majestic   0       0       0   ............ 0
dollar     0.31    0.77    0.86............ 0.91
.......
.......
</code></pre>

<p>as far what I have done so far:-</p>

<pre><code>for i in df['keywords']:
    vectors=model['i']
</code></pre>

<p>I got array of vector but Im not getting how put it with columns names as <code>V1 V2 V3 V4....V20</code> in <code>df</code> and how to treat missing word as <code>'0'</code></p>
",,2018-07-16 13:17:20,Add word vectors as columns in pandas dataframe?,<python-3.x><pandas><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17575,51275484,2018-07-11 00:09:17,,"<p>We intend to deploy a trained model in production. since we can not keep the same in the code base, we need to upload into the cloud and refer it on runtime.</p>

<p>We are using kubernetes, and I'm relatively new to it. Below is my stepwise understanding on how to solve this.</p>

<ol>
<li>build a persistent volume with my trained model (size around 30MB)</li>
<li>mount the persistent volume into pod with a single container.</li>
<li>keep this pod running. refer to the model from a python script via pod.</li>
</ol>

<p>I tried referring documentation <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/"" rel=""nofollow noreferrer"">pv</a> with no luck. I also tried to move the model to PV via ""kubectl cp"", with no success.</p>

<p>Any idea on how to resolve this? any helps would be appreciated. </p>
",,2018-07-11 00:09:17,trained model on Kubernetes,<machine-learning><kubernetes><nlp><gensim><persistent-volumes>,,,CC BY-SA 4.0,False,False,True,False,False
17579,51330246,2018-07-13 17:35:01,,"<p>I am training multiple word2vec models with Gensim. Each of the word2vec will have the same parameter and dimension, but trained with slightly different data. Then I want to compare how the change in data affected the vector representation of some words.</p>

<p>But every time I train a model, the vector representation of the same word is wildly different. Their similarity among other words remain similar, but the whole vector space seems to be rotated.</p>

<p>Is there any way I can rotate both of the word2vec representation in such way that same words occupy same position in vector space, or at least they are as close as possible.</p>

<p>Thanks in advance.</p>
",,2018-07-13 18:09:20,How to rotate a word2vec onto another word2vec?,<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17603,51222901,2018-07-07 11:47:14,,"<p>I trained my <code>word2vec</code> model using the <code>gensim</code> package on <strong>6.4 GB text data</strong> which is preprocessed using the following code snippet :</p>

<pre><code>def read_input(input_file):
  with open(input_file, ""r"") as inp:
    inp_str = inp.read()

  inp_str = inp_str.strip('\n')
  inp_str = re.sub(r'\n', ' ', inp_str)
  lowercase = inp_str.lower()
  punc = lowercase.translate(str.maketrans('', '', string.punctuation))

  return (punc.translate(str.maketrans('','','1234567890')))

def read_(input_file):
  return( gensim.utils.simple_preprocess(input_file, deacc=True, min_len=3))          

doc = read_input('../train1.txt')
documents = read_(doc)
logging.info (""Done reading data file"")
</code></pre>

<p>But every time I  train the model, its size is <strong>147 Kb</strong> which doesn't seems right and when I tried generating vectors from the trained model it says :</p>

<pre><code>KeyError: ""word 'type' not in vocabulary""
</code></pre>

<p>The following is the code I used for training my word2vec model :</p>

<pre><code>old_model = Word2Vec.load('../word2vec_model')
old_model.train(documents, total_examples=old_model.corpus_count, epochs=7)

old_model.save('../word2vec_model1')

logging.info (""Saved the new word2vec model"")
</code></pre>

<p>Please help me resolving this issue.</p>
",2018-07-07 12:54:32,2018-07-07 14:09:46,Word2vec Model size is very small and it is not recognizing words,<python><python-3.x><word2vec><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17606,51281241,2018-07-11 08:56:14,,"<p>I am using gensim and executed the following code (simplified):</p>

<pre><code>model = gensim.models.Word2Vec(...)
mode.build_vocab(sentences)
model.train(...)
model.save('file_name')
</code></pre>

<p>After days my code finished <code>model.train(...)</code>. However, during saving, I experienced:</p>

<pre><code>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
</code></pre>

<p>I noticed that there were some npy files generated:</p>

<pre><code>&lt;...&gt;.trainables.syn1neg.npy
&lt;...&gt;.trainables.vectors_lockf.npy
&lt;...&gt;.wv.vectors.npy
</code></pre>

<p>Are those intermediate results I can re-use or do I have to rerun the entire process?</p>
",,2018-07-11 21:39:18,Gensim Word2Vec Model trained but not saved,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17617,51265111,2018-07-10 12:23:27,,"<p>I'm attempting to compare a tagged document consisting of a list of words to individual tags from a list of tags.</p>

<p>My code is as follows:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from gensim import similarities,corpora,models
import Load

documents = Load.get_doc('docs')

data = Doc2Vec.load('vectorised.model')

print('Data Loading finished')

tags = [['word1'],['word2'],['word3'],['word4'],['word5']]

tag_vectors = []

data.n_similarity(tags[0],documents[1])
</code></pre>

<p>The issue i'm having is running:</p>

<pre><code>data.n_similarity(tags[0],documents[1])
</code></pre>

<p>feeds back KeyError: ""word 'otherword' not in vocabulary</p>

<p>I want to get the similarity between the taggeddocument and the tag itself,
so my question is what do I need to change in my code so it checks correctly and gives back a similarity value?</p>

<p>n.b. I've replaced the actual words here with placeholders</p>
",,2019-11-09 02:30:16,Gensim n_similarity word not in vocabulary,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17626,51338824,2018-07-14 13:08:45,,"<p>I am a little confused to perform LDA in Python. 
I have a document file, which I want to run LDA and get topics. </p>

<pre><code>import docx
import nltk
import gensim
from gensim.models import hdpmodel, ldamodel
from gensim import corpora
def getText(filename):
    doc = docx.Document(filename)
    fullText = []
    for para in doc.paragraphs:
        fullText.append(para.text)
    return '\n'.join(fullText)
fullText=getText('ElizabethII.docx')

#create lda object
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in fullText]

all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) 
== 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=5, passes=15)
topics = lda.show_topics(num_words=4)
for topic in topics:
    print(topic)
corpus_lda = lda[corpus]

print(lda.show_topics())
</code></pre>

<p>and as a result I get this:</p>

<pre><code>(0, '0.723*""r"" + 0.211*""f"" + 0.025*""5"" + 0.013*""-""')
(1, '0.410*""e"" + 0.258*""t"" + 0.206*""h"" + 0.068*""m""')
(2, '0.319*""n"" + 0.162*""l"" + 0.113*""c"" + 0.101*""u""')
(3, '0.503*""i"" + 0.324*""d"" + 0.113*""b"" + 0.041*""9""')
(4, '0.355*""o"" + 0.307*""s"" + 0.106*""w"" + 0.052*""v""')
</code></pre>

<p>which makes me confused. Why I get characters instead of topics? Is it because of my docx file( which contains 1900 words?) or mistake in code? or maybe I should provide topics for sentences(paragraphs)? (how?)</p>
",2018-10-03 05:51:53,2018-10-03 05:51:53,"LDA in Python, I get characters not topics",<python><nltk><lda><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
17638,51287590,2018-07-11 14:05:07,,"<p>I wanted to write the code to find the similarity between two sentences and then I ended up writing this code using nltk and gensim. I used tokenization and  gensim.similarities.Similarity to do the work. But it ain't serving my purpose. 
It works fine until I introduce the last line of code. </p>

<pre><code>import gensim
import nltk

raw_documents = [""I'm taking the show on the road."",
             ""My socks are a force multiplier."",
         ""I am the barber who cuts everyone's hair who doesn't cut their 
own."",
         ""Legend has it that the mind is a mad monkey."",
        ""I make my own fun.""]
from nltk.tokenize import word_tokenize
gen_docs = [[w.lower() for w in word_tokenize(text)]
        for text in raw_documents]



dictionary = gensim.corpora.Dictionary(gen_docs)
print(dictionary[5])
print(dictionary.token2id['socks'])
print(""Number of words in dictionary:"",len(dictionary))
for i in range(len(dictionary)):
    print(i, dictionary[i])

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]
print(corpus)

tf_idf = gensim.models.TfidfModel(corpus)
print(tf_idf)
    s = 0
for i in corpus:
s += len(i)
print(s)

sims = gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                  num_features=len(dictionary))
print(sims)
print(type(sims))


query_doc = [w.lower() for w in word_tokenize(""Socks are a force for good."")]
print(query_doc)
query_doc_bow = dictionary.doc2bow(query_doc)
print(query_doc_bow)
query_doc_tf_idf = tf_idf[query_doc_bow]
print(query_doc_tf_idf)

sims[query_doc_tf_idf]
</code></pre>

<p>It throws this error. I couldn't find the answer for this anywhere on the internet.</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 679, in save
_pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
TypeError: file must have a 'write' attribute

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""semantic.py"", line 45, in &lt;module&gt;
    sims[query_doc_tf_idf]
  File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
503, in __getitem__
    self.close_shard()  # no-op if no documents added to index since last 
query
 File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
427, in close_shard
    shard = Shard(self.shardid2filename(shardid), index)
 File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
110, in __init__
    index.save(self.fullname())
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 682, in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, 
pickle_protocol=pickle_protocol)
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 538, in 
_smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 1337, in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on 
Windows
  File ""C:\Python36\lib\site-packages\smart_open\smart_open_lib.py"", line 
181, in smart_open
fobj = _shortcut_open(uri, mode, **kw)
  File ""C:\Python36\lib\site-packages\smart_open\smart_open_lib.py"", line 
287, in _shortcut_open
return io.open(parsed_uri.uri_path, mode, **open_kwargs)
</code></pre>

<p>Please help figure out where the problem is.</p>
",,2018-07-11 16:04:10,How to use gensim.similarities.Similarity to find similarity between two sentences,<python><python-3.x><nltk><gensim><corpus>,,,CC BY-SA 4.0,True,False,True,False,False
17644,51376241,2018-07-17 08:05:51,,"<p>While I trained d2v on a large text corpus I received these 3 files: </p>

<pre><code>doc2vec.model.trainables.syn1neg.npy

doc2vec.model.vocabulary.cum_table.npy

doc2vec.model.wv.vectors.npy
</code></pre>

<p>Bun final model has not saved, because there was not enough free space available on the disk. </p>

<pre><code>OSError: 5516903000 requested and 4427726816 written
</code></pre>

<p>Is there a way to resave my model using these files in a shorter time, than all training time? </p>

<p>Thank you in advance! </p>
",2018-07-17 08:35:03,2018-07-17 17:55:06,"Gensim Doc2vec trained, but not saved",<model><save><gensim><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17648,51323344,2018-07-13 10:45:17,,"<p>I'm trying to load glove vectors, with the following code</p>

<pre><code>en_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)
</code></pre>

<p>and I unexpectedly get the following error.</p>

<pre><code> File ""/home/k/Desktop/Work/Vector explorer/word2vec-explorer/vec_test_loader.py"", line 55, in make_model
en_model = KeyedVectors.load_word2vec_format(model_path, binary=is_bin)
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 1119, in load_word2vec_format
limit=limit, datatype=datatype)
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/utils_any2vec.py"", line 175, in _load_word2vec_format
vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
 File ""/home/k/.local/lib/python3.5/site-packages/gensim/models/utils_any2vec.py"", line 175, in &lt;genexpr&gt;
vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>

<p>Can someone help?</p>
",,2018-09-20 08:33:04,Can't load glove.6B.300d.txt,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17656,51245689,2018-07-09 12:32:10,,"<p>I am trying to understand the <code>epochs</code> parameter in the <code>Doc2Vec</code> function and <code>epochs</code> parameter in the <code>train</code> function. </p>

<p>In the following code snippet, I manually set up a loop of 4000 iterations. Is it required or passing 4000 as epochs parameter in the Doc2Vec enough? Also how <code>epochs</code> in <code>Doc2Vec</code> is different from epochs in <code>train</code>?</p>

<pre><code>documents = Documents(train_set)

model = Doc2Vec(vector_size=100, dbow_words=1, dm=0, epochs=4000,  window=5,
                seed=1337, min_count=5, workers=4, alpha=0.001, min_alpha=0.025)

model.build_vocab(documents)

for epoch in range(model.epochs):
    print(""epoch ""+str(epoch))
    model.train(documents, total_examples=total_length, epochs=1)
    ckpnt = model_name+""_epoch_""+str(epoch)
    model.save(ckpnt)
    print(""Saving {}"".format(ckpnt))
</code></pre>

<p>Also, how and when are the weights updated?</p>
",,2018-07-10 03:37:44,What does epochs mean in Doc2Vec and train when I have to manually run the iteration?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17670,51414910,2018-07-19 05:24:32,,"<p>Hey guys I have a pretrained binary file and I want to train it on my corpus. </p>

<p><strong>Approach I tried :</strong></p>

<p>I tried to extract the txt file from the bin file I had and use this as a word2vec file at time of loading and further trained it on my own corpus and saved the model but the model is performing badly for the words which are there in the pre-trained bin file (I used intersect_word2vec_format command for this.) </p>

<p><a href=""https://drive.google.com/open?id=1AdBYWP1lWrg9QsX4BZ63rpUihyR4BpvS"" rel=""nofollow noreferrer"">Here</a> is the script I used.</p>

<p>What should be my approach for my model to perform well on words from both the pre-trained file and my corpus?</p>
",,2018-07-24 04:33:49,How to train a pretrained binary file on my own corpus using gensim?,<nlp><models><gensim><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
17684,51418154,2018-07-19 08:45:36,,"<p>I'm training doc2vec, and using callbacks trying to see if alpha is decreasing over training time using this code:</p>

<pre><code>class EpochSaver(CallbackAny2Vec):
'''Callback to save model after each epoch.'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0

        os.makedirs(self.path_prefix, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = get_tmpfile(
            '{}_epoch{}.model'.format(self.path_prefix, self.epoch)
        )
        model.save(savepath)
        print(
            ""Model alpha: {}"".format(model.alpha), 
            ""Model min_alpha: {}"".format(model.min_alpha),
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch""
        )
        self.epoch += 1


def train():

    workers = multiprocessing.cpu_count()*4
    model = Doc2Vec(
        DocIter(),
        vec_size=600, alpha=0.03, min_alpha=0.00025, epochs=20,
        min_count=10, dm=1, hs=1, negative=0, workers=workers,
        callbacks=[EpochSaver(""./checkpoints"")]
    )
    print(
        ""HS"", model.hs, ""Negative"", model.negative, ""Epochs"", 
         model.epochs, ""Workers: "", model.workers, ""Model alpha: 
         {}"".format(model.alpha)
    )  
</code></pre>

<p>And while training I see that alpha is not changing over time. On each callback I see alpha = 0.03.<br>
Is it possible to check if alpha is decreasing? Or it really not decreasing at all during training? </p>

<p>One more question: 
How can I benefit from all my cores while training doc2vec?</p>

<p><a href=""https://i.stack.imgur.com/Bo1uh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bo1uh.jpg"" alt=""Loading of cores""></a></p>

<p>As we can see, each core is not loaded more than +-30%. </p>
",2018-07-20 14:08:41,2018-07-20 14:08:41,How to check via callbacks if alpha is decreasing? + How to load all cores during training?,<callback><gensim><multicore><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17693,51438277,2018-07-20 08:28:02,,"<p>I am trying to train a Doc2Vec word embedding on preprocessed paragraphs. I have removed punctuation, and have carried out tokenization, pos tag and chunking.</p>

<pre><code>import nltk
from nltk import word_tokenize, pos_tag, ne_chunk
from gensim.models.doc2vec import Doc2Vec

ne_tree = ne_chunk(pos_tag(word_tokenize(sent_pun)))

model = Doc2Vec(ne_tree)
</code></pre>

<p>I get the error ""AttributeError: 'Tree' object has no attribute 'words'"" when I run the Doc2Vec model. What should be done to correct this? Thank you.</p>
",,2018-07-20 09:24:53,AttributeError: 'Tree' object has no attribute 'words'. Doc2Vec error,<model><nltk><gensim><attributeerror><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
17694,51438425,2018-07-20 08:36:25,,"<p><strong>I am currently trying to classify text into 7 classes.</strong> Up to now, I have been able to reach a 90% precision score using Majority Voting (with SVM, Multinomial NB, Random Forest and KNN).</p>

<p><strong>I wanted to try to increase a little more this precision by using word embeddings</strong> and thus getting less dimensions for my samples. I use gensim word2vec to create my model and the NLTK list of stop-words and tokenizer:</p>

<pre><code>with open('data.pkl','r') as f:
    corpus=pickle.load(f)

with open('targets.pkl','r') as f:
    targets=pickle.load(f)

tokenized_corpus=[word_tokenize(anomaly) for anomaly in corpus]
stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're"", ""you've"", ""you'll"", ""you'd"", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', ""she's"", 'her', 'hers', 'herself', 'it', ""it's"", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', ""that'll"", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', ""don't"", 'should', ""should've"", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', ""aren't"", 'couldn', ""couldn't"", 'didn', ""didn't"", 'doesn', ""doesn't"", 'hadn', ""hadn't"", 'hasn', ""hasn't"", 'haven', ""haven't"", 'isn', ""isn't"", 'ma', 'mightn', ""mightn't"", 'mustn', ""mustn't"", 'needn', ""needn't"", 'shan', ""shan't"", 'shouldn', ""shouldn't"", 'wasn', ""wasn't"", 'weren', ""weren't"", 'won', ""won't"", 'wouldn', ""wouldn't""]
for i,anomaly in enumerate(tokenized_corpus):
    for w in anomaly:
        if w in stop_words:
            tokenized_corpus[i].remove(w)

model = gensim.models.Word2Vec(
        tokenized_corpus,
        window=5,
        size=100)

model.train(tokenized_corpus, total_examples=len(corpus), epochs=10)
</code></pre>

<p>The model seems fine, I get satisfying results when I use similarity between words.</p>

<p>I use this class to get a mean representation of my samples:</p>

<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        if len(word2vec)&gt;0:
            self.dim=len(word2vec[next(iter(word2vec))])
        else:
            self.dim=0

    def fit(self, X, y):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
                    or [np.zeros(self.dim)], axis=0) #moyenne des vecteurs des mots (a.word2vec[w])[ou 0 si il connait pas le mot] composant un lment de X
            for words in X
        ])
</code></pre>

<p>and </p>

<pre><code>w2v = dict(zip(model.wv.index2word, model.wv.syn0))
a=MeanEmbeddingVectorizer(w2v)    
X_transformed=a.transform(tokenized_corpus)
</code></pre>

<p>Finally, I build a common sklearn pipeline and let sklearn perform a GridSearchCV on my data:</p>

<pre><code>test_param_n_estimators=[i for i in range(1,50)]
parameters = {'clf2__n_estimators': test_param_n_estimators}

pip=Pipeline([['clf2',RandomForestClassifier()]])

gs_clf2 = GridSearchCV(pip, parameters,verbose=10,n_jobs=2)
gs_clf2 = gs_clf2.fit(X_transformed, targets)

print(gs_clf2.best_score_)
print(gs_clf2.best_params_)
</code></pre>

<p><strong>The issue is that I get random precision scores (around 0.5).</strong></p>

<p>Maybe dimensionality reduction is not always able to increase precision but I think I don't understand something or I did something wrong, <strong>do you have an idea of what is going wrong ?</strong></p>

<p>Thank you in advance</p>
",2018-07-20 13:05:31,2018-07-20 13:05:31,Word embedding decreasing classification precision,<scikit-learn><nlp><nltk><gensim><text-classification>,,,CC BY-SA 4.0,True,False,True,False,True
17712,51457515,2018-07-21 15:29:33,,"<p>I have several tables that have different column names which are mapped through ETL. There are a total of around 200 tables and 500 attributes, so the set is not massive.</p>

<p>Some column mappings are as follows:</p>

<pre><code>startDate EFT_DATE
startDate START_DATE
startDate entryDate 
</code></pre>

<p>As you can see the same column name can be mapped to different names across different tables. </p>

<p>I'm trying to solve the following problem :</p>

<p>Given two schemas I want to find matches between attribute names. </p>

<p>I was wondering if there is a way to leverage gensim to solve this problem similar to source-words from Google example. The challenge I'm facing is which dataset to use to train the model. Also I am wondering if there is another approach to solve the problem. </p>
",2018-07-21 19:34:41,2018-07-21 20:27:38,Attribute mapping using Machine learning,<machine-learning><database-design><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17722,51481553,2018-07-23 14:43:44,,"<p>I am trying to use the FastText's french pre-trained binary model (downloaded from the official <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">FastText's github page</a>). I need the <code>.bin</code> model and not the <code>.vec</code> word-vectors so as to approximate misspelled and out-of-vocabulary words.</p>
<p>However when I try to load said model, using:</p>
<pre><code>from gensim.models import FastText
model = FastText.load_fasttext_format('french_bin_model_path')
</code></pre>
<p>I get the following error:</p>
<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>
<p>What is surprising is that <em>it works just fine</em> when I try to load the <strong>english</strong> binary model.</p>
<p>I am running python 3.6 and gensim 3.5.0.</p>
<p>Any idea as of why it doesn't work with french vectors are welcome!</p>
",2020-06-20 09:12:55,2018-11-19 15:21:51,Error when loading FastText's french pre-trained model with gensim,<python><gensim><pre-trained-model><fasttext><french>,,,CC BY-SA 4.0,False,False,True,False,False
17733,51388707,2018-07-17 19:14:21,,"<p>We have Anaconda 4.3.1 installed on our hosts and recently we have installed several packages for data science use. All the imports were fine except for gensim.</p>

<p>I am getting ""Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so."" and getting out of python shell.</p>

<p>It sounds like a duplicate but the weird part is, when I import tensorflow or seaborn before importing gensim, I am not getting that error and gensim is being imported. I would also like to know if there is any dependency between these packages. And I do have the latest version of numpy which is 1.14.5. I have looked at various solutions proposed about installing few packages and uninstalling few. I would like to know the reason why we should be doing it before actually doing it.</p>
",,2018-09-07 06:11:24,Intel MKL FATAL ERROR: while trying to import gensim package,<python><tensorflow><anaconda><seaborn><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17736,51426107,2018-07-19 15:07:55,,"<p>I'm trying to build a Tf-Idf model that can score bigrams as well as unigrams using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">gensim</a>. To do this, I build a gensim dictionary and then use that dictionary to create bag-of-word representations of the corpus that I use to build the model. </p>

<p>The step to build the dictionary looks like this:</p>

<pre><code>dict = gensim.corpora.Dictionary(tokens)
</code></pre>

<p>where <code>token</code> is a list of unigrams and bigrams like this:</p>

<pre><code>[('restore',),
 ('diversification',),
 ('made',),
 ('transport',),
 ('The',),
 ('grass',),
 ('But',),
 ('distinguished', 'newspaper'),
 ('came', 'well'),
 ('produced',),
 ('car',),
 ('decided',),
 ('sudden', 'movement'),
 ('looking', 'glasses'),
 ('shapes', 'replaced'),
 ('beauties',),
 ('put',),
 ('college', 'days'),
 ('January',),
 ('sometimes', 'gives')]
</code></pre>

<p>However, when I provide a list such as this to <code>gensim.corpora.Dictionary()</code>, the algorithm reduces all tokens to bigrams, e.g.:</p>

<pre><code>test = gensim.corpora.Dictionary([(('happy', 'dog'))])
[test[id] for id in test]
=&gt; ['dog', 'happy']
</code></pre>

<p>Is there a way to generate a dictionary with gensim that includes bigrams? </p>
",,2019-01-28 05:47:48,How to build a gensim dictionary that includes bigrams?,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17754,51486374,2018-07-23 19:57:06,,"<p>I have this code that works for English language but does not work for Persian language</p>

<pre><code>from gensim.models import Word2Vec as wv
for sentence in sentences:
    tokens = sentence.strip().lower().split("" "")
    tokenized.append(tokens)
model = wv(tokenized
    ,size=5,
          min_count=1)
print('done2')
model.save('F:/text8/text8-phrases1')
print('done3')
print(model)
model = wv.load('F:/text8/text8-phrases1')

print(model.wv.vocab)
</code></pre>

<p>output</p>

<pre><code>&gt; '': &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB0B8&gt;,
&gt; '': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB160&gt;, '': &lt;gensim.models.keyedvectors.Vocab
&gt; object at 0x0000027716EEB198&gt;, '':
&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB1D0&gt;,
&gt; '': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB208&gt;, '': &lt;gensim.models.keyedvectors.Vocab
&gt; object at 0x0000027716EEB240&gt;, '':
&gt; &lt;gensim.models.keyedvectors.Vocab object at 0x0000027716EEB278&gt;,
&gt; '': &lt;gensim.models.keyedvectors.Vocab object at
&gt; 0x0000027716EEB2B0&gt;, ''
</code></pre>

<p>plesae take example with code
thanks</p>
",2018-07-24 13:03:55,2019-11-29 09:25:13,How to implement word embedding for persian language,<keras><nlp><persian><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17756,51429975,2018-07-19 18:53:50,,"<p>I try to use</p>

<pre><code>from gensim.sklearn_api import W2VTransformer
</code></pre>

<p>and get</p>

<pre><code>ImportError: No module named 'gensim.sklearn_api'
</code></pre>

<p>I used </p>

<pre><code>import gensim
import sklearn
from sklearn.base import BaseEstimator, TransformerMixin
</code></pre>

<p>and get the same.
In <a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html"" rel=""nofollow noreferrer"">sklearn_api.w2vmodel  Scikit learn wrapper for word2vec model</a> I could find no advice. 
How to install <code>gensim.sklearn_api</code>?</p>
",,2018-07-19 18:56:28,No module named 'gensim.sklearn_api',<python><scikit-learn><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
17758,51430560,2018-07-19 19:36:06,,"<p>I've been getting some irregular behavior from an LDA topic model program and right now, it seems like my file won't save the lda model it creates... I'm really not sure why.</p>

<p>Here's a code snippet, albeit it's going to take me more time before I could write code that's reproducible since I'm really just trying to load certain files I created beforehand.</p>

<pre><code>def naive_LDA_implementation(name_of_lda, create_dict=False, remove_low_freq=False):

    LDA_MODEL_PATH = ""lda_dir/"" + str(name_of_lda) +""/model_dir/"" # for some reason this location doesn't work entirely... and yes, I have made a directory in a the folder of this name.
    # This ends up saving the .state, .id2word, and .expEblogbeta.npy files... But normally when saving an lda model actually works, a fourth file is included that's to my understanding the model itself.
    # LDA_MODEL_PATH = ""models/"" # This is what I originally had as the location for LDA_MODEL_PATH. I was using a directory called models for multiple lda models. This no longer works.

    doc_df = getCorpus(name_of_lda, cleaned=True) # returns a dataframe containing a row for each text record and an extra column that contains the tokenized version of the text's post/string of words.
    dict_path = ""lda_dir/"" + str(name_of_lda) + ""/dict_of_tokens.dict""
    docs_of_tokens = convert_cleaned_tokens_entries(doc_df['cleaned_tokens'])
    if create_dict != False:
        doc_dict = corpora.Dictionary(docs_of_tokens) :
        if remove_low_freq==True:
            doc_dict.filter_extremes(no_below=5, no_above=0.6)
        doc_dict.save(dict_path)
        print(""Finished saving"") 
    else:
        doc_dict = corpora.Dictionary.load(dict_path)
doc_term_matrix = [doc_dict.doc2bow(doc) for doc in docs_of_tokens] # gives a unique id for each word in corpus_arr

Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=15, id2word = doc_dict, passes=20, chunksize=10000)
ldamodel.save(LDA_MODEL_PATH)
</code></pre>

<p>To put it sraightforwardly... I have no clue why permission is being denied when I try to save my lda model to a particular location. Right now even the original <code>models/</code> directory location is giving me ""permission denied"" with this error message. It's seeming like any and all directories I could use just... won't work. This is odd behavior and I can't really find asks that talk about this error in the same context. I have found posts of people getting this error message when they actually tried storing in locations that did not exist. But for me that isn't really a question.</p>

<p>When I first got this error... I actually started to wonder if it was because I had another lda topic model that I named topic_model_1. It was stored in the <code>models/</code> subdirectory. I started to wonder if the name was a potential cause, and changed it to <code>lda_model_topic_1</code> to see if that could change results... but nothing is working.</p>

<p>Even if you can't really figure out what solution applies to my situation (especially since right now I don't have reproducible code, I just have my work)... Can someone tell me what this error message means? When and why does it come up? Maybe that's a start.</p>

<pre><code>      Traceback (most recent call last):
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 679,
in save
    _pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
TypeError: file must have a 'write' attribute

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""text_mining.py"", line 461, in &lt;module&gt;
    main()
  File ""text_mining.py"", line 453, in main
    naive_LDA_implementation(name_of_lda=""lda_model_topic_1"", create_dict=True,
remove_low_freq=True)
  File ""text_mining.py"", line 411, in naive_LDA_implementation
    ldamodel.save(LDA_MODEL_PATH)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\models\ldamodel.py"",
line 1583, in save
    super(LdaModel, self).save(fname, ignore=ignore, separately=separately, *arg
s, **kwargs)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 682,
in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_prot
ocol=pickle_protocol)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 538,
in _smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 1337,
 in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
  File ""C:\Users\biney\Miniconda3\lib\site-packages\smart_open\smart_open_lib.py
"", line 181, in smart_open
    fobj = _shortcut_open(uri, mode, **kw)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\smart_open\smart_open_lib.py
"", line 287, in _shortcut_open
    return io.open(parsed_uri.uri_path, mode, **open_kwargs)
PermissionError: [Errno 13] Permission denied: 'lda_dir/lda_model_topic_1/model_
dir/'
</code></pre>
",,2018-07-20 21:41:25,gensim lda permission denied when I try to save my model,<python><text-mining><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
17764,51514825,2018-07-25 08:57:25,,"<p>I need to generate word2vec array for a dictionary of words. The dictionary looks something like this </p>

<pre><code>test={0: 'tench, Tinca tinca',
 1: 'goldfish, Carassius auratus',
 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',
 3: 'tiger shark, Galeocerdo cuvieri',
 4: 'hammerhead, hammerhead shark'}
</code></pre>

<p>The loop should go through each line, check if the word exists in the model, if yes then store the vector in an array otherwise check the next word in the line. If none of the words are present in the gensim model, then it should do nothing (array is initialised with zeros)
However if a word doesn't exist in the pre trained model, then it raises this exception:</p>

<blockquote>
  <p>KeyError: ""word 'Galeocerdo cuvieri' not in vocabulary""</p>
</blockquote>

<p>What should be the ideal loop that also has the exception in order to bypass the error raised?
This is my starting code:</p>

<pre><code> import gensim
 model = gensim.models.KeyedVectors.load_word2vec_format('/home/shikhar /Downloads/GoogleNews-vectors-negative300.bin',binary=True) 
 array=np.zeros((4,300)) 
 for i in test:
     synonyms=test[i].split(',')
</code></pre>
",,2018-07-25 09:47:44,word2vec for dictionary of words,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17767,51520031,2018-07-25 13:23:54,,"<p>I am familiar with the tfidf vectorizer.</p>

<p>However, in gensim it seems like tfidf is treated as a model on itself, just like LDA, LSI and others.</p>

<p>Why is this the case? Can't tfidf not just be used to vectorize and then to input in an LDA model for example?</p>

<p>Link to documentation: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a></p>
",,2019-10-21 07:53:31,Why is TFIDF seen as a model in Gensim,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17768,51520655,2018-07-25 13:52:14,,"<p>I am trying to download gensim pretrained word2vec models behind a proxy. I receive this error.</p>

<blockquote>
  <p>urllib.error.URLError: urlopen error [Errno 11004] getaddrinfo failed </p>
</blockquote>

<p>for the following code </p>

<pre><code>import gensim.downloader as api
api.info() 
</code></pre>

<p>I have already set proxy using </p>

<pre><code>set HTTPS_PROXY=https://username:xxxxxx@myproxy.com 
</code></pre>

<p>and have been successfully downloading packages using pip. Is there a way to add my proxy to gensim?  </p>
",,2018-07-25 13:52:14,Downloading gensim models behind a proxy,<python><python-3.x><gensim><http-proxy>,,,CC BY-SA 4.0,False,False,True,False,False
17773,51449841,2018-07-20 20:22:20,,"<p>All-</p>

<p>I would like to use the gensim library, but unfortunately I can't install it via pip due to the company's firewall. Any advice? Thank you in advance for any help or suggestions you can provide. </p>
",,2018-07-20 20:34:18,How to install gensim without pip (firewall issues),<python><installation><firewall><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17790,51523248,2018-07-25 16:00:06,,"<p>I have trained doc2vec model on 4 million records. I want to find most similar sentence to a new sentence i put in from my data but i am getting very bad results.</p>

<p>sample of data:</p>

<pre><code>Xolo Era (Black, 8 GB)(1 GB RAM).
Sugar C6 (White, 16 GB)(2 GB RAM).
Celkon Star 4G+ (Black &amp; Dark Blue, 4 GB)(512 MB RAM).
Panasonic Eluga I2 (Metallic Grey, 16 GB)(2 GB RAM).
Itel IT 5311(Champagne Gold).
Itel A44 Pro (Champagne, 16 GB)(2 GB RAM).
Nokia 2 (Pewter/ Black, 8 GB)(1 GB RAM).
InFocus Snap 4 (Midnight Black, 64 GB)(4 GB RAM).
Panasonic P91 (Black, 16 GB)(1 GB RAM).
</code></pre>

<p>Before passing this data i have done preprocessing which includes
1) Stop words removal.
2) special character and numeric value removal.
3) lowercase the data. 
I have also performed the same steps in testing process.</p>

<p>code which i used for training :</p>

<pre><code>sentences=doc2vec.TaggedLineDocument('training_data.csv') # i have used TaggedLineDocument which can generate label or tags for my data

max_epochs = 100
vec_size = 100
alpha = 0.025

model = doc2vec.Doc2Vec(vector_size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                dm =1,
                min_count=1)
model.build_vocab(sentences)
model.train(sentences, epochs=100, total_examples=model.corpus_count)
model.save('My_model.doc2vec')
</code></pre>

<p>well i am new to gensim and doc2vec so i have followed an example for training my model so please correct me if i have used wrong parameters.</p>

<p>on testing side</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec.load('My_model.doc2vec')
test = 'nokia pewter black gb gb ram'.split()
new_vector = model.infer_vector(test)
similar = model.docvecs.most_similar([new_vector]) 
print(similar) # It returns index of sentence and similarity score
</code></pre>

<p>for testing i have passed same sentences which are present in training data but model does not give related documents as similar document,for example i got ""<strong>lootmela tempered glass guard for micromax canvas juice</strong>"" as a most similar sentence to <strong>""nokia pewter black gb gb ram""</strong> this sentence with 0.80 as a similarity score.</p>

<pre><code>So my questions to you: 
1) Do i need to reconsider parameters for model training?
2) Training process is correct?
3) How to build more accurate model for similarity?
4) Apart from doc2vec what will be your suggestion for similarity (keeping in mind i have very large data so training and testing time should not be much longer)
</code></pre>

<p>Please forgive if question formatting is not good.</p>
",2018-07-25 16:38:36,2018-07-25 17:20:43,doc2vec inaccurate cosine similarity,<python><machine-learning><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17794,51560126,2018-07-27 14:30:30,,"<p>I have gensim pretrained  model  and I was trying to find most similar words using <code>model.most_similar('word')</code> So lets say, I have word named as 'politics' so what I have done is</p>
<pre><code>for i,q in model.most_similar('politics'):
    print (i) 
</code></pre>
<p>This gives me output as a list of words. But, same thing when I put it inside function as :</p>
<pre><code>def taxonomy(word):
    for i,q in model.most_similar(word):
        print (i)
</code></pre>
<p>when I used <code>taxonomy('politics')</code>, I get the error: <code>ValueError: cannot compute similarity with no input</code>. Is it something I did wrong?</p>
<h1>edit :</h1>
<p>How I can append the <code>i</code> into blank array with same name as my word. i.e for present array name should be <code>politics</code>, I've done this, but not working</p>
<pre><code>def taxonomy(word):
    word=[]
    for i,q in model.most_similar(word):
        word.append(i)
        return word
</code></pre>
",2020-06-20 09:12:55,2018-07-27 15:01:41,Python for loop not working inside user defined function,<python><python-3.x><function><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
17802,51602111,2018-07-30 20:52:57,,"<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
",2018-07-30 21:20:32,2018-07-30 22:53:27,Adding additional words in word2vec or Glove (maybe using gensim),<nlp><gensim><word2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
17803,51492778,2018-07-24 07:23:46,,"<p>I am trying to build a translation network using embedding and RNN. I have trained a Gensim Word2Vec model and it is learning word associations pretty well. However, I couldnt get my head around how to properly add the layer to a Keras model. (And how to do an inverse embedding for the output. But thats another question that had been answered: by default you cant.)</p>

<p>In Word2Vec, when you input a string, e.g. <code>model.wv[hello]</code>, you get a vector representation of the word. However, I believe that the <code>keras.layers.Embedding</code> layer returned by Word2Vec's <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.get_keras_embedding"" rel=""noreferrer"">get_keras_embedding()</a> takes a one-hot/tokenized input, instead of a string input. But the documentation provides no explanation on what the appropriate input is. <strong>I cannot figure out how to obtain the one-hot/tokenized vector of the vocabulary that has 1-to-1 correspondence with the Embedding layers input.</strong></p>

<p>More elaboration below:</p>

<p>Currently my workaround is to apply the embedding outside Keras before feeding it to the network. Is there any detriment in doing this? I will set the embedding to non-trainable anyway. So far I have noticed that memory use is extremely inefficient (like 50GB even before declaring the Keras model for a collection of 64-word-long sentences) having to load the padded inputs and the weights outside the model. Maybe generator can help.</p>

<p>The following is my code. Inputs are padded to 64-words long. The Word2Vec embedding has 300 dimensions. There are probably a lot of mistakes here due to repeated experimentation trying to make embedding work. Suggestions are welcome.</p>

<pre class=""lang-python prettyprint-override""><code>import gensim
word2vec_model = gensim.models.Word2Vec.load(word2vec.model"")
</code></pre>

<pre class=""lang-python prettyprint-override""><code>from keras.models import Sequential
from keras.layers import Embedding, GRU, Input, Flatten, Dense, TimeDistributed, Activation, PReLU, RepeatVector, Bidirectional, Dropout
from keras.optimizers import Adam, Adadelta
from keras.callbacks import ModelCheckpoint
from keras.losses import sparse_categorical_crossentropy, mean_squared_error, cosine_proximity

keras_model = Sequential()
keras_model.add(word2vec_model.wv.get_keras_embedding(train_embeddings=False))
keras_model.add(Bidirectional(GRU(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='tanh')))
keras_model.add(TimeDistributed(Dense(600, activation='tanh')))
# keras_model.add(PReLU())
# ^ For some reason I get error when I add Activation outside:
# int() argument must be a string, a bytes-like object or a number, not 'NoneType'
# But keras_model.add(Activation('relu')) works.
keras_model.add(Dense(source_arr.shape[1] * source_arr.shape[2]))
# size = max-output-sentence-length * embedding-dimensions to learn the embedding vector and find the nearest word in word2vec_model.wv.similar_by_vector() afterwards.
# Alternatively one can use Dense(vocab_size) and train the network to output one-hot categorical words instead.
# Remember to change Keras loss to sparse_categorical_crossentropy.
# But this wont benefit from Word2Vec. 

keras_model.compile(loss=mean_squared_error,
              optimizer=Adadelta(),
              metrics=['mean_absolute_error'])
keras_model.summary()
</code></pre>

<pre class=""lang-none prettyprint-override""><code>_________________________________________________________________ 
Layer (type)                 Output Shape              Param #   
================================================================= 
embedding_19 (Embedding)     (None, None, 300)         8219700   
_________________________________________________________________ 
bidirectional_17 (Bidirectio (None, None, 600)         1081800   
_________________________________________________________________ 
activation_4 (Activation)    (None, None, 600)         0         
_________________________________________________________________ 
time_distributed_17 (TimeDis (None, None, 600)         360600    
_________________________________________________________________ 
dense_24 (Dense)             (None, None, 19200)       11539200  
================================================================= 
Total params: 21,201,300 
Trainable params: 12,981,600 
Non-trainable params: 8,219,700
_________________________________________________________________
</code></pre>

<pre class=""lang-python prettyprint-override""><code>filepath=""best-weights.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')
callbacks_list = [checkpoint]
keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)
</code></pre>

<p>Which throws an error when I try to fit the model with text:</p>

<pre><code>Train on 8000 samples, validate on 2000 samples Epoch 1/100

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-865f8b75fbc3&gt; in &lt;module&gt;()
      2 checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')
      3 callbacks_list = [checkpoint]
----&gt; 4 keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)

~/virtualenv/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1040                                         initial_epoch=initial_epoch,
   1041                                         steps_per_epoch=steps_per_epoch,
-&gt; 1042                                         validation_steps=validation_steps)
   1043 
   1044     def evaluate(self, x=None, y=None,

~/virtualenv/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    197                     ins_batch[i] = ins_batch[i].toarray()
    198 
--&gt; 199                 outs = f(ins_batch)
    200                 if not isinstance(outs, list):
    201                     outs = [outs]

~/virtualenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2659                 return self._legacy_call(inputs)
   2660 
-&gt; 2661             return self._call(inputs)
   2662         else:
   2663             if py_any(is_tensor(x) for x in inputs):

~/virtualenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
   2612                 array_vals.append(
   2613                     np.asarray(value,
-&gt; 2614                                dtype=tensor.dtype.base_dtype.name))
   2615         if self.feed_dict:
   2616             for key in sorted(self.feed_dict.keys()):

~/virtualenv/lib/python3.6/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)
    490 
    491     """"""
--&gt; 492     return array(a, dtype, copy=False, order=order)
    493 
    494 

ValueError: could not convert string to float: 'hello'
</code></pre>

<p>The following is <a href=""https://rajmak.wordpress.com/2017/12/07/text-classification-classifying-product-titles-using-convolutional-neural-network-and-word2vec-embedding/"" rel=""noreferrer"">an excerpt from Rajmak</a> demonstrating how to use a tokenizer to convert words into the input of a Keras Embedding.</p>

<pre class=""lang-python prettyprint-override""><code>tokenizer = Tokenizer(num_words=MAX_NB_WORDS) 
tokenizer.fit_on_texts(all_texts) 
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

indices = np.arange(data.shape[0]) # get sequence of row index 
np.random.shuffle(indices) # shuffle the row indexes 
data = data[indices] # shuffle data/product-titles/x-axis

nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) 
x_train = data[:-nb_validation_samples]

word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)
</code></pre>

<blockquote>
  <p>Keras embedding layer can be obtained by Gensim Word2Vecsword2vec.get_keras_embedding(train_embeddings=False)method or constructed like shown below. The null word embeddings indicate the number of words not found in our pre-trained vectors (In this case Google News). This could possibly be unique words for brands in this context.</p>
</blockquote>

<pre class=""lang-python prettyprint-override""><code>from keras.layers import Embedding
word_index = tokenizer.word_index
nb_words = min(MAX_NB_WORDS, len(word_index))+1

embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if word in word2vec.vocab:
        embedding_matrix[i] = word2vec.word_vec(word)
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))

embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1
                            embedding_matrix.shape[1], # or EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

from keras.models import Sequential
from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation

model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.2))
model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))
model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))
model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(150,activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(3,activation='sigmoid'))

model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

model.summary()

model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>Here the <code>embedding_layer</code> is explicitly created using:</p>

<pre><code>for word, i in word_index.items():
    if word in word2vec.vocab:
        embedding_matrix[i] = word2vec.word_vec(word)
</code></pre>

<p>However, if we use <code>get_keras_embedding()</code>, the embedding matrix <em>is already constructed and fixed.</em> I do not know how each word_index in the Tokenizer can be coerced match the corresponding word in <code>get_keras_embedding()</code>'s Keras embedding input.</p>

<p><strong>So, what is the proper way to use Word2Vec's get_keras_embedding() in Keras?</strong></p>
",,2018-07-25 16:28:24,How to properly use get_keras_embedding() in Gensims Word2Vec?,<python><keras><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17805,51634656,2018-08-01 13:31:40,,"<p>I am trying to filter out tokens by their frequency using the filter_extremes function in Gensim (<a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/dictionary.html</a>). Specifically, I am interested in filtering out words that occur in ""Less frequent than no_below documents"" and ""More frequent than no_above documents"". </p>

<pre><code>id2word_ = corpora.Dictionary(texts)
print(len(id2word_))
id2word_.filter_extremes(no_above = 0.600)
print(len(id2word_))
</code></pre>

<p>The first print statement gives 11918 and the second print statement gives 3567. However, if I do the following:</p>

<pre><code>id2word_ = corpora.Dictionary(texts)
print(len(id2word_))
id2word_.filter_extremes(no_below = 0.599)
print(len(id2word_))
</code></pre>

<p>The first print statement gives 11918 (as expected) and the second gives 11406. Shouldn't <code>id2word_.filter_extremes(no_below = 0.599)</code> and <code>id2word_.filter_extremes(no_above = 0.600)</code> add up to the number of total words? However, 11406 +  3567 > 11918, so how come this sum exceeds the number of words in the corpus? That does not make sense since the filters should cover non-overlapping words, based off the explanation in the documentation. </p>

<p>If you have any ideas, I would really appreciate your input! Thanks!</p>
",2018-08-01 15:58:23,2020-07-05 09:59:51,Filtering tokens by frequency using filter_extremes in Gensim,<python><dictionary><text-processing><gensim><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
17808,51512064,2018-07-25 06:20:00,,"<p>I have a <code>word2vec</code> dataframe like this which saved from save_word2vec_format using Gensim under txt file. After using pandas to read this file. (Picture below). How to delete first row and make them as a index?
My txt file: <a href=""https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing</a>
<a href=""https://i.stack.imgur.com/5qS8O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5qS8O.png"" alt=""enter image description here""></a></p>
",2018-07-25 08:49:10,2018-07-25 08:49:10,Delete first column and then take them as a index pandas,<python><pandas><dataframe><nlp><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17822,51547315,2018-07-26 20:49:50,,"<p>I have an issue similar to the one discussed here - <a href=""https://stackoverflow.com/questions/40727093/gensim-word2vec-updating-word-embeddings-with-newcoming-data"">gensim word2vec - updating word embeddings with newcoming data</a></p>

<p>I have the following code that saves a model as <strong>text8_gensim.bin</strong></p>

<pre><code>sentences = word2vec.Text8Corpus('train/text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5,sg=0, window=8, iter=15, sample=1e-4,alpha=0.05,cbow_mean=1,negative=25)
model.save(""./savedModel/text8_gensim.bin"")
</code></pre>

<p>Here is the code that adds more data to the saved model (after loading it)</p>

<pre><code>fname=""savedModel/text8_gensim.bin""
model = word2vec.Word2Vec.load(fname)
model.epochs=15

#Custom words
docs = [""start date"", ""end date"", ""eft date"",""termination date""]
model.build_vocab(docs, update=True)
model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
model.wv.similarity('start','eft')
</code></pre>

<p>The model loads fine; however when I try to call <strong>model.wv.similarity</strong> function I get the following error</p>

<p><strong>KeyError: ""word 'eft' not in vocabulary""</strong></p>

<p>Am I missing something here?</p>
",,2018-07-26 21:30:41,gensim word2vec - update model data,<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17823,51514060,2018-07-25 08:18:11,,"<p>I have a word2vec dataframe like this which saved from save_word2vec_format using Gensim under txt file. After using pandas to read this file. (Picture below). How to delete the first word and make them as an index? I want to have a dataframe like one hot encoding vector dataframe. This is my txt file <a href=""https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1O206N93hPSmvMjwc0W5ATyqQMdMwhRlF/view?usp=sharing</a>
<a href=""https://i.stack.imgur.com/stOjk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/stOjk.png"" alt=""enter image description here""></a></p>
",2018-07-25 08:47:59,2018-07-25 08:50:22,Remove first word and then take the word as a index like one hot encode vector pandas,<pandas><dataframe><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17825,51655597,2018-08-02 14:03:27,,"<p>I have been using the sklearn.decomposition.LatentDirichletAllocation module  to explore a corpus of documents. After a number of iterations of training and adjusting the model (i.e. adding stopwords and synonyms, varying the number of topics), I am fairly happy and familiar with the distilled topics. As a next step I would like to apply the trained model to a new corpus. </p>

<p>Is it possible to apply the fitted model to a new set of documents to determine the topic distributions.</p>

<p>I know this is possible within the gensim library, where you can train a model:</p>

<pre><code>from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary

# Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

lda = LdaModel(common_corpus, num_topics=10)
</code></pre>

<p>And subsequently apply the trained model to a new corpus:</p>

<pre><code>Topic_distribtutions = lda[unseen_doc]
</code></pre>

<p>from: <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>

<p>How does one do this using the scikit-learn application of LDA?</p>
",2018-08-02 14:27:56,2018-08-02 15:38:18,sklearn LatentDirichletAllocation topic inference on new corpus,<python><scikit-learn><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
17849,51566257,2018-07-27 22:21:48,,"<p>I am now working on a project using Gensim.word2vec, and I am a total freshman for this field.</p>

<p>Actually I already got a model. Are there any way that I can get the similarity rank of a word for another word. For example, the top 2 most similar words for the word 'girl' is 'lady' and then 'woman'. Are there any functions I can use if i enter 'lady' is can return 1, if i enter 'woman' it can return 2?</p>

<p>Thanks!</p>
",,2020-08-31 19:00:35,Return the rank of word in Gensim Word2vec,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17856,51626028,2018-08-01 05:42:33,,"<p>I am trying to make a simple Text Summarization GUI that takes an input text and generates the summary. While the program is running absolutely fine, I am getting errors in converting it into exe file. Following are two the main programs for the GUI interface-</p>

<pre><code>import tkinter
import gensim
from tkinter import 
from gensim.summarization import summarize

#create windows object
root=Tk()
root.geometry(""1024x800"") #set the window size using .geometry command
root.title(""AUTOMATED TEXT SUMMARIZER"")
h1=Label(root,text=""AUTOMATED TEXT SUMMARIZER FOR ENGLISH"",font= 
(""arial"",12,""bold""),fg=""#26482B"").pack()

#set the label, like 'enter text'  
l1 = Label(root,text=""Enter Some Text"", font= 
(""arial"",10,""bold""),fg=""#26482B"").place(x=10,y=50)
#take some text as input in the textbox
inp = Text(root,width=100,height=30,bg=""white"",font=(""arial"",8,""bold""))
inp.place(x=10,y=90)
#set another label that indicates the results
l2 = Label(root,text=""Summary"", font= 
(""arial"",10,""bold""),fg=""#26482B"").place(x=720,y=50)


opt = Text(root,width=50,height=20,bg=""white"",font=(""arial"",10,""bold""))
opt.place(x=640,y=90)

def sum():
    txt = inp.get(""1.0"", END)
    output = summarize(txt, ratio=0.2)
    opt.insert(0.0, output)

        button_a = Button(root,text=""Generate 
         Summary"",width=30,height=2,bg=""#9B9D9C"",fg=""black"",font= 
    (""arial"",9,""bold""),command=sum)
 button_a.place(x=120,y=600)

def clear():
    inp.delete(""1.0"",END)
    opt.delete(""1.0"",END)

button_b= 
Button(root,text=""Clear"",width=18,height=2,bg=""#9B9D9C"",fg=""black"",font= 
(""arial"",9,""bold""),command=clear)
button_b.place(x=450,y=600)


root.mainloop()
</code></pre>

<p>This is the setup_file.py- </p>

<pre><code>import os
import cx_Freeze
from cx_Freeze import setup, Executable
import sys


os.environ['TCL_LIBRARY'] = ""C:/LOCAL_TO_PYTHON/PYTHON35-32/tcl/tcl86t.dll""
os.environ['TK_LIBRARY'] = ""C:/LOCAL_TO_PYTHON/PYTHON35-32/tcl/tk86t.dll""

includefiles = ['C:/Users/praty/AppData/Local/Programs/Python/Python36- 
32/DLLs/tcl86t.dll','C:/Users/praty/AppData/Local/Programs/Python/Python36- 
32/DLLs/tk86t.dll']
includes = []
packages = ['tkinter','gensim']
additional_mods = ['numpy.core._methods', 'numpy.lib.format']
build_exe_options = 
{'includes':includes,'includes':additional_mods,'packages':packages, 
'include_files':includefiles}

base = None

if sys.platform == 'win32':
    base = ""Win32GUI""
elif sys.platform == 'win64':
    base = 'Win64GUI'

setup(  name = 'text summarizer',
        version = '0.1',
        description = 'generate summary',
        options = {'build_exe': build_exe_options},
        executables = [Executable('Automated_Text_Summarizer.py', base=base)]
        )
</code></pre>

<p>This is the error I am getting:</p>

<p><img src=""https://i.stack.imgur.com/VKQbQ.png"" alt=""This the error related to the program""></p>
",2018-08-01 09:24:25,2018-08-01 09:24:25,tkinter for Text Summarization,<python><user-interface><tkinter><nlp><pycharm>,,,CC BY-SA 4.0,False,False,True,False,False
17858,51527411,2018-07-25 20:47:34,,"<p>I have documents with over 37M sentences and I'm using Gensim's Doc2Vec to train them. The model training works fine with smaller data sets, say 5M-10M records. However, when training on the full dataset, the process dies mostly at the ""resetting layer weights"" stage. Sometimes, it dies before.</p>

<p>I'm suspecting that it's a memory issue. I have 16GB of RAM with 4 cores. If it's indeed a memory issue, is there any way I can train the model in batches. From reading the documentation, it seems train() is useful in cases where the new documents don't have new vocabularies. But, this is not the case with my documents.</p>

<p>Any suggestions?</p>
",,2018-07-26 21:25:17,Gensim Doc2Vec training crashes with Killed: 9 error,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17865,51590459,2018-07-30 09:07:08,,"<p>I'm trying to do a clustering with word2vec and Kmeans, but it's not working.</p>
<p>Here part of my data:</p>
<pre><code>demain fera chaud  paris pas marseille
mauvais exemple ce n est pas un clich mais il faut comprendre pourquoi aussi
il y a plus de travail  Paris c est d ailleurs pour cette raison qu autant de gens&quot;,
mais s il y a plus de travail, il y a aussi plus de concurrence
s agglutinent autour de la capitale
</code></pre>
<p>Script:</p>
<pre><code>import nltk
import pandas
import pprint
import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn import metrics
from gensim.models import Word2Vec
from nltk.cluster import KMeansClusterer
from sklearn.metrics import adjusted_rand_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF

dataset = pandas.read_csv('text.csv', encoding = 'utf-8')

comments = dataset['comments']

verbatim_list = no_duplicate.values.tolist()

min_count = 2
size = 50
window = 4

model = Word2Vec(verbatim_list, min_count=min_count, size=size, window=window)

X = model[model.vocab]

clusters_number = 28
kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)

assigned_clusters = kclusterer.cluster(X, assign_clusters=True)

words = list(model.vocab)
for i, word in enumerate(words):  
    print (word + &quot;:&quot; + str(assigned_clusters[i]))

kmeans = cluster.KMeans(n_clusters = clusters_number)
kmeans.fit(X)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

clusters = {}
for commentaires, label in zip(verbatim_list, labels):
    try:
        clusters[str(label)].append(verbatim)
    except:
       clusters[str(label)] = [verbatim]
pprint.pprint(clusters)
</code></pre>
<p>Output:</p>
<blockquote>
<p>Traceback (most recent call last):</p>
<p>File &quot;kmwv.py&quot;, line 37, in </p>
<p>X = model[model.vocab]</p>
<p>AttributeError: 'Word2Vec' object has no attribute 'vocab'</p>
</blockquote>
<p>I need a clustering that works with word2vec, but every time I try something, I have this error. Is there any way to do a clustering with word2vec?</p>
",2020-06-20 09:12:55,2019-05-02 13:34:53,Clustering with word2vec and Kmeans,<python><python-3.x><cluster-analysis><k-means><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
17891,51593971,2018-07-30 12:27:15,,"<p>I want to label my documents with tags mapped to id attribute in database.
The ids can be for example also like this:</p>

<p>documents[0] is for example</p>

<pre><code>TaggedDocument(words=['blabla', 'request'], tags=[225616076])
</code></pre>

<p>For some reason, it is not able to build_vocabulary. Although I have only 33382 unique ids/tags with higher values, it does not matter, gensim writes that I have '225616077 tags' (in the log).  </p>

<pre><code>2018-07-30 12:07:59,271 : INFO : collecting all words and their counts
2018-07-30 12:07:59,273 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2018-07-30 12:07:59,330 : INFO : PROGRESS: at example #1000, processed 7974 words (314086/s), 1975 word types, 225616077 tags
2018-07-30 12:07:59,343 : INFO : PROGRESS: at example #2000, processed 15882 words (701054/s), 2794 word types, 225616077 tags
...

...  
2018-07-30 12:14:56,454 : INFO : estimated required memory for 6765 words and 20 dimensions: 19793760900 bytes
2018-07-30 12:14:56,457 : INFO : resetting layer weights

---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
in &lt;module&gt;()
----&gt; 1 model.build_vocab(documents)
</code></pre>

<p>How can I solve this problem? I do not want to start from 0 and then map it to the higher numbers (uselessly used compute time). I also tried it to tag it as strings (so the documents[0] is TaggedDocument(words=['blabla', 'request'], tags=['225616076'])) but it does not work either.</p>

<p>I am inspecting gensim's code but can not get to solution on my own.</p>
",,2018-07-30 16:57:10,Gensim tagging documents with big numbers,<python><gensim><topic-modeling><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17892,51594165,2018-07-30 12:38:19,,"<p>CBOW word2vec scheme look like this:</p>

<p><a href=""https://i.stack.imgur.com/gmgo6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gmgo6.png"" alt=""enter image description here""></a></p>

<p>How I can extract matrixes WI and WO from <code>gensim.models.word2vec.Word2Vec</code>?
I found only these fields in gensim w2v model:</p>

<p><code>gensim.models.word2vec.Word2Vec.trainables.syn1neg</code></p>

<p>and</p>

<p><code>gensim.models.word2vec.Word2Vec.vw.syn1neg.vectors</code></p>

<p>Can I make an assumption that <code>syn1neg</code> is WI, and WO = <code>vectors</code> - <code>syn1neg</code>?</p>

<p>Why this code</p>

<pre><code>sentences = [['car', 'tree', 'chip2'], ['chip1', 'sugar']]
model = Word2Vec(sentences, min_count=1, size = 5)
</code></pre>

<p>give <code>Word2Vec.trainables.syn1neg</code> matrix with zero elements only?</p>

<p>For 30MB dataset <code>Word2Vec.trainables.syn1neg</code> matrix also contain zero elements only, log is here:</p>

<p><a href=""https://pastebin.com/cKfxv2zz"" rel=""nofollow noreferrer"">gensim log</a></p>
",2018-08-01 14:55:47,2018-08-01 14:55:47,How I can extract matrixes WI and WO from gensim word2vec?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17899,51616074,2018-07-31 14:41:18,,"<p>Following <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">related question solution</a> I created docker container which loads GoogleNews-vectors-negative300 KeyedVector inside docker container and load it all to memory</p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
word_vectors.most_similar('stuff')
</code></pre>

<p>Also I have another Docker container which provides REST API which loads this model with </p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
</code></pre>

<p>And I observe that fully loaded container takes more than 5GB of memory and each gunicorn worker takes 1.7 GB of memory.</p>

<pre><code>CONTAINER ID        NAME                        CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS
acbfd080ab50        vectorizer_model_loader_1   0.00%               5.141GiB / 15.55GiB   33.07%              24.9kB / 0B         32.9MB / 0B         15
1a9ad3dfdb8d        vectorizer_vectorizer_1     0.94%               1.771GiB / 15.55GiB   11.39%              26.6kB / 0B         277MB / 0B          17
</code></pre>

<p>However, I expect that all this processes share same memory for KeyedVector, so it only takes 5.4 GB shared between all containers.</p>

<p>Have someone tried to achieve that and succeed?</p>

<p>edit: 
I tried following code snippet and it indeed share same memory across different containers.</p>

<pre><code>import mmap
from threading import Semaphore

with open(""data/GoogleNews-vectors-negative300.bin"", ""rb"") as f:
    # memory-map the file, size 0 means whole file
    fileno = f.fileno()
    mm = mmap.mmap(fileno, 0, access=mmap.ACCESS_READ)
    # read whole content
    mm.read()
    Semaphore(0).acquire()
    # close the map
    mm.close()
</code></pre>

<p>So the problem that <code>KeyedVectors.load(model_path, mmap='r')</code> don't share memory</p>

<p>edit2:
Studying gensim's source code I see that <code>np.load(subname(fname, attrib), mmap_mode=mmap)</code> is called to open memmaped file. Following code sample shares memory across multiple container.</p>

<pre><code>from threading import Semaphore

import numpy as np

data = np.load('data/native_format.bin.vectors.npy', mmap_mode='r')
print(data.shape)
# load whole file to memory
print(data.mean())
Semaphore(0).acquire()
</code></pre>
",2018-08-06 13:38:04,2018-08-07 08:49:54,Sharing memory for gensim's KeyedVectors objects between docker containers,<python><mmap><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17902,51680023,2018-08-03 20:59:43,,"<p>When I call <code>docvecs.most_similar</code> on a document, I am getting the error <code>AttributeError: 'list' object has no attribute 'shape'</code>:  </p>

<pre><code># load model from file
from gensim.models.doc2vec import Doc2Vec
model_doc2vec = Doc2Vec.load(""/path_to_file/doc2vec.bin"")

# attempt to get most similar documents from docvec
tokens = ""in space"".split()
new_vector = model_doc2vec.infer_vector(tokens)
sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )
</code></pre>

<p>which yields <code>AttributeError: 'list' object has no attribute 'shape'</code>.</p>

<p>I have a hunch this may have to do with numpy and gensim version compatibility.  I am using Python 3.6, numpy 1.14, and gensim 1.0.1.</p>

<p>Full error:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-37-220db2331e84&gt; in &lt;module&gt;()
----&gt; 1 sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )

~/doc2vec.py in most_similar(self, positive, negative, topn, clip_start, clip_end, indexer)
    436         there was chosen to be significant, such as more popular tag IDs in lower indexes.)
    437         """"""
--&gt; 438         self.init_sims()
    439         clip_end = clip_end or len(self.doctag_syn0norm)
    440 

~/doc2vec.py in init_sims(self, replace)
    419                         mode='w+', shape=self.doctag_syn0.shape)
    420                 else:
--&gt; 421                     self.doctag_syn0norm = empty(self.doctag_syn0.shape, dtype=REAL)
    422                 np_divide(self.doctag_syn0, sqrt((self.doctag_syn0 ** 2).sum(-1))[..., newaxis], self.doctag_syn0norm)
    423 

AttributeError: 'list' object has no attribute 'shape'
</code></pre>
",2018-08-03 21:25:34,2018-08-06 15:20:10,Gensim: calling docvecs.most_similar yields error,<python><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
17906,51632344,2018-08-01 11:39:43,,"<p>I have around 1000 pairs of sentences. Each pair consists of two sentences, one causing high CTR and one LOW. I want to create a mechanism to auto-produce sentences optimized for high CTR. When iterating the pairs, I can get a vector (using Spacy NLP) for each sentence. I take the vectors difference (Sent1.vector - Sent2.Vector) and then mean all the pairs using numpy mean. When I have the ""difference vector"" in hand, I want to add it to any given text and get a new sentence. any Ideas how to obtain this? Gensim most_similar only works on single words... Thanks</p>
",,2018-08-01 11:39:43,Spacy/Gensim - creating similar sentences,<gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
17940,51736907,2018-08-07 23:54:22,,"<p>I am trying to use word2vec for supervised classification of labelled classes. I have data containing sentences with their labels which I would like to train the word2vec model on, and then use word2vec (or possibly random forest classifier) in order to figure out the classes of unseen sentences. Here is what I have tried so far:</p>

<pre><code>import gensim
import numpy as np

class TokenizedSentence(object):

    def __init__(self, doc_list):
            self.doc_list = doc_list

    def __iter__(self):
            for t in (self.doc_list):
                    t = t.decode('utf-8')
                    yield gensim.utils.simple_preprocess(t)



tweets = [""a tweet"", ""another tweet"", ... , ""some tweet""]
labels = [1, 1, ... , 16]

training_data = TokenizedSentence(tweets)

model = gensim.models.Word2Vec(training_data, size=150, window=10, min_count=2, workers=10)
model.train(training_data, total_examples=model.corpus_count, epochs=200)
model.save(""w2vmodel"")
</code></pre>

<p>I am wondering how I need to go about doing this in order to include the labels that go along with each sentence, so that I can tell the model which label a sentence belongs to, and then be able to load the model in a separate file and classify unseen data into categories based on the supervised learning. Any help is appreciated!</p>
",,2018-08-07 23:54:22,Using word2vec for supervised classification,<python><gensim><word2vec><supervised-learning>,,,CC BY-SA 4.0,False,False,True,False,False
17942,51742332,2018-08-08 08:45:59,,"<p>I trained a word2vec model using gensim and I want to randomly select vectors from it, and find the corresponding word.
What is the best what to do so?</p>
",2018-08-08 09:29:37,2018-08-09 05:34:34,Randomly select vector in gensim word2vec,<python-3.x><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17947,51650896,2018-08-02 10:07:38,,"<p>I am trying to compute the accuracy of a Word2Vec model. This is my code:</p>

<pre><code>import gensim

vectors = gensim.models.KeyedVectors.load(""cbow_vectors.kv"", mmap='r')

questions = ""questions-words.txt""

analogy_scores = vectors.accuracy(questions, restrict_vocab=30000, case_insensitive=True)
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>C:\Users\\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\matutils.py:737:
  FutureWarning: Conversion of the second argument of issubdtype from
  <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be
  treated as <code>np.int32 == np.dtype(int).type</code>.   if
  np.issubdtype(vec.dtype, np.int):</p>
</blockquote>

<p>and then nothing happens :(
Anyone know how to fix this? </p>
",2018-08-02 11:31:21,2018-08-02 11:31:21,FutureWarning error when calculating accuracy of a Word2Vec model,<python><numpy><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17948,51651934,2018-08-02 10:57:51,,"<p>I am trying to allow users to enter a search term to find top 5 articles matching their search. I am in the process of comparing the results / performance for a variety of methods (gensim word2vec, doc2vec, nearest neighbour etc).</p>

<p>I have successfully created code to leverage standard similarity function in Spacy, however, as it loops through a massive list of documents appending the similarity score to a pandas df, it takes too long.</p>

<p>Is there a method to return the top 5 most similar documents without the loop and pandas append? The reason is that this method returns the most sensible top 5 documents compared to the others (the joy of the word embeddings!)</p>

<pre><code>#load relevant libraries
    import pandas as pd
    import numpy as np
    import spacy
#load spacy model
nlp=spacy.load('Z:\\en_core_web_lg-2.0.0')
#
#Get Doc Corpus
dfCorpus = pd.read_csv('z:\DocumentCorpus.csv', delimiter=',')
##get top 5 using spacy similarity function 
SearchStringCosine = nlp(input('Enter search term:'))
computed_similarities = []
for i in dfCorpus.CorpusInput_Unqiue:
   doc=nlp(i)
   computed_similarities.append((i, SearchStringCosine.similarity(doc)))
computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])
computed_similarities = pd.DataFrame(computed_similarities,columns=   ['CorpusInput_Unique','Score'])
print(computed_similarities[:5]) 
</code></pre>
",2018-08-02 11:05:56,2020-03-31 15:53:55,Python Spacy similarity without loop?,<python><python-3.x><machine-learning><similarity><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
17950,51798248,2018-08-11 09:18:16,,"<p>I've trained a Doc2Vec model in order to do a simple binary classification task, but I would also love to see which words or sentences weigh more in terms of contributing to the meaning of a given text. So far I had no luck finding anything relevant or helpful. Any ideas how could I implement this feature? Should I switch from Doc2Vec to more conventional methods like tf-idf?</p>
",,2019-09-03 21:04:39,How find the most decisive sentences or words in a document via Doc2Vec?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17955,51667852,2018-08-03 07:56:26,,"<p>i'm developing a LSTM - RNN text classification with Keras
This is my Code.</p>

<pre><code>import numpy as np
import csv
import keras
import sklearn
import gensim
import random
import scipy
from keras.preprocessing import text
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers.core import Dense , Dropout , Activation  , Flatten
from keras.layers.convolutional import Convolution1D, MaxPooling1D
from keras.layers import Embedding , LSTM
from sklearn import preprocessing
from sklearn.base import BaseEstimator
from sklearn.svm import LinearSVC , SVC
from sklearn.naive_bayes import MultinomialNB
from gensim.models.word2vec import Word2Vec
from gensim.models.doc2vec import Doc2Vec , TaggedDocument

# size of the word embeddings
embeddings_dim = 300

# maximum number of words to consider in the representations
max_features = 30000

# maximum length of a sentence
max_sent_len = 50

# percentage of the data used for model training
percent = 0.75

# number of classes
num_classes = 2

print ("""")
print (""Reading pre-trained word embeddings..."")
embeddings = dict( )
embeddings = gensim.models.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"" , binary=True) 

print (""Reading text data for classification and building representations..."")
data = [ ( row[""sentence""] , row[""label""]  ) for row in csv.DictReader(open(""test-data.txt""), delimiter='\t', quoting=csv.QUOTE_NONE) ]
random.shuffle( data )
train_size = int(len(data) * percent)
train_texts = [ txt.lower() for ( txt, label ) in data[0:train_size] ]
test_texts = [ txt.lower() for ( txt, label ) in data[train_size:-1] ]
train_labels = [ label for ( txt , label ) in data[0:train_size] ]
test_labels = [ label for ( txt , label ) in data[train_size:-1] ]
num_classes = len( set( train_labels + test_labels ) )
tokenizer = Tokenizer(num_words=max_features, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', lower=True, split="" "")
tokenizer.fit_on_texts(train_texts)
train_sequences = sequence.pad_sequences( tokenizer.texts_to_sequences( train_texts ) , maxlen=max_sent_len )
test_sequences = sequence.pad_sequences( tokenizer.texts_to_sequences( test_texts ) , maxlen=max_sent_len )
train_matrix = tokenizer.texts_to_matrix( train_texts )
test_matrix = tokenizer.texts_to_matrix( test_texts )
embedding_weights = np.zeros( ( max_features , embeddings_dim ) )
for word,index in tokenizer.word_index.items():
  if index &lt; max_features:
    try: embedding_weights[index,:] = embeddings[word]
    except: embedding_weights[index,:] = np.random.rand( 1 , embeddings_dim )
le = preprocessing.LabelEncoder( )
le.fit( train_labels + test_labels )
train_labels = le.transform( train_labels )
test_labels = le.transform( test_labels )
print(""Classes that are considered in the problem : "" + repr( le.classes_ ))


print(""-----WEIGHTS-----"")
print(embedding_weights.shape)

print (""Method = Stack of two LSTMs"")
np.random.seed(0)
model = Sequential()

model.add(Embedding(max_features, embeddings_dim, input_length=max_sent_len, mask_zero=True, weights=[embedding_weights] ))
model.add(Dropout(0.25))
model.add(LSTM(output_dim=embeddings_dim , activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))
model.add(Dropout(0.25))
model.add(LSTM(activation=""sigmoid"", units=embeddings_dim, recurrent_activation=""hard_sigmoid"", return_sequences=True))
model.add(Dropout(0.25))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')
else: model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])

model.summary()


model.fit(train_sequences, train_labels , epochs=30, batch_size=32)
</code></pre>

<p>My model is this:</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 50, 300)           9000000   
_________________________________________________________________
dropout_1 (Dropout)          (None, 50, 300)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 50, 300)           721200    
_________________________________________________________________
dropout_2 (Dropout)          (None, 50, 300)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 50, 300)           721200    
_________________________________________________________________
dropout_3 (Dropout)          (None, 50, 300)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 50, 1)             301       
_________________________________________________________________
activation_1 (Activation)    (None, 50, 1)             0         
=================================================================
Total params: 10,442,701
Trainable params: 10,442,701
Non-trainable params: 0
</code></pre>

<p>My error is:
Error when checking target: expected activation_1 to have 3 dimensions, but got array with shape (750, 1)</p>

<p>I try to reshape all my array, but i not found the solution.
Can someone help me??? thank you :D
Sorry for my bad english.</p>
",,2018-08-22 07:44:12,Text-Classification RNN - LSTM - Error checking target,<python><machine-learning><lstm><text-classification><rnn>,,,CC BY-SA 4.0,False,False,True,False,True
17959,51785296,2018-08-10 11:14:48,,"<p>I am trying to load a saved gensim lda mallet:</p>

<pre><code> ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=n_topics,id2word=id2word)
 ldamallet.save('ldamallet')
</code></pre>

<p>When testing this for a new query (with the original corpus and dictionary), everything seems fine for the first load. </p>

<pre><code>ques_vec = [dictionary.doc2bow(words) for words in data_words_list]
for i, row in enumerate(lda[ques_vec]):
    row = sorted(row, key=lambda x: (x[1]), reverse=True)
</code></pre>

<p>On executing the same code afterward, it is this error that pops up:</p>

<blockquote>
  <p>java.io.FileNotFoundException: /tmp/9f371_corpus.mallet (No such file
  or directory)
          at java.io.FileInputStream.open0(Native Method)
          at java.io.FileInputStream.open(FileInputStream.java:195)
          at java.io.FileInputStream.(FileInputStream.java:138)
          at cc.mallet.types.InstanceList.load(InstanceList.java:787)
          at cc.mallet.classify.tui.Csv2Vectors.main(Csv2Vectors.java:131)
  Exception in thread ""main"" java.lang.IllegalArgumentException:
  Couldn't read InstanceList from file /tmp/9f371_corpus.mallet
          at cc.mallet.types.InstanceList.load(InstanceList.java:794)
          at cc.mallet.classify.tui.Csv2Vectors.main(Csv2Vectors.java:131)
  Traceback (most recent call last):   File ""topic_modeling1.py"", line
  406, in 
      topic = get_label(text, id2word, first, ldamallet)   File ""topic_modeling1.py"", line 237, in get_label
      for i, row in enumerate(lda[ques_vec]):   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py"", line 308, in <strong>getitem</strong>
      self.convert_input(bow, infer=True)   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py"", line 256, in convert_input
      check_output(args=cmd, shell=True)   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/utils.py"",
  line 1806, in check_output
      raise error subprocess.CalledProcessError: Command '/home/user/sjha/projects/topic_modeling/mallet-2.0.8/bin/mallet
  import-file --preserve-case --keep-sequence --remove-stopwords
  --token-regex ""\S+"" --input /tmp/9f371_corpus.txt --output /tmp/9f371_corpus.mallet.infer --use-pipe-from
  /tmp/9f371_corpus.mallet' returned non-zero exit status 1.</p>
</blockquote>

<p>Contents of my <code>/tmp/</code> directory:</p>

<pre><code>/tmp/9f371_corpus.txt  /tmp/9f371_doctopics.txt /tmp/9f371_doctopics.txt.infer  /tmp/9f371_inferencer.mallet  /tmp/9f371_state.mallet.gz  /tmp/9f371_topickeys.txt
</code></pre>

<p>Also, it seems like the files <code>/tmp/9f371_doctopics.txt.infer</code> and <code>/tmp/9f371_corpus.txt</code> get modified every time I load the model. What could be the possible error source? Or is it some kind of bug in gensim's mallet wrapper?</p>
",,2019-07-09 20:43:17,Gensim mallet bug? Fails to load the saved model more than once,<python><gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
17963,51762199,2018-08-09 08:27:39,,"<p>help me to correct this or are there any other way to accomplish this task?</p>

<p><img src=""https://i.stack.imgur.com/OVvoK.png"" alt=""Snapshot of code snippet""></p>
",2018-08-09 09:06:56,2018-08-09 17:29:16,add more vocabulary to pretrained word2vec model,<machine-learning><nlp><data-mining><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17967,51800210,2018-08-11 13:40:14,,"<p>I'm training a Word2vec model using Gensim Word2vec on a well-known Wikipedia dump provided by Tobias Schnabel in the following link:
<a href=""http://www.cs.cornell.edu/~schnabts/eval/index.html"" rel=""nofollow noreferrer"">http://www.cs.cornell.edu/~schnabts/eval/index.html</a> (about 4GB).</p>

<p>I would like to understand how many epochs I should run the model for training until the model will be converged.</p>

<p>I added the following code:</p>

<pre><code> model = Word2Vec(size=self._number_of_dimensions_in_hidden_layer,
                    window=self._window_size,
                    min_count=3,
                    max_vocab_size=self._max_vocabulary_size,
                    sg=self._use_cbow,
                    seed=model_seed,
                    compute_loss=True,
                    iter=self._epochs)
    model.build_vocab(sentences)

    learning_rate = 0.025
    step_size = (learning_rate - 0.001) / self._epochs

    for i in range(self._epochs):
        end_lr = learning_rate - step_size
        trained_word_count, raw_word_count = model.train(sentences, compute_loss=True,
                                                         start_alpha=learning_rate,
                                                         end_alpha=learning_rate,
                                                         total_examples=model.corpus_count,
                                                         epochs=1)
        loss = model.get_latest_training_loss()
        print(""iter={0}, loss={1}, learning_rate={2}"".format(i, loss, learning_rate))
        learning_rate  *= 0.6


    model.save(model_name_path)
</code></pre>

<p>However I cannot see the model is converging:</p>

<pre><code>iter=0, loss=76893000.0, learning_rate=0.025
iter=1, loss=74870528.0, learning_rate=0.015
iter=2, loss=73959232.0, learning_rate=0.009
iter=3, loss=73605400.0, 
learning_rate=0.005399999999999999
iter=4, loss=73224288.0, 
learning_rate=0.0032399999999999994
iter=5, loss=73008048.0, 
learning_rate=0.0019439999999999995
iter=6, loss=72935888.0, 
learning_rate=0.0011663999999999997
iter=7, loss=72774304.0, 
learning_rate=0.0006998399999999999
iter=8, loss=72642072.0, 
learning_rate=0.0004199039999999999
iter=9, loss=72624384.0, 
learning_rate=0.00025194239999999993
iter=10, loss=72700064.0, 
learning_rate=0.00015116543999999996
iter=11, loss=72478656.0, 
learning_rate=9.069926399999997e-05
iter=12, loss=72486744.0, 
learning_rate=5.441955839999998e-05
iter=13, loss=72282776.0, 
learning_rate=3.2651735039999986e-05
iter=14, loss=71841968.0, 
learning_rate=1.9591041023999992e-05
iter=15, loss=72119848.0, 
learning_rate=1.1754624614399995e-05
iter=16, loss=72054544.0, 
learning_rate=7.0527747686399965e-06
iter=17, loss=71958888.0, 
learning_rate=4.2316648611839976e-06
iter=18, loss=71933808.0, 
learning_rate=2.5389989167103985e-06
iter=19, loss=71739256.0, 
learning_rate=1.523399350026239e-06
iter=20, loss=71660288.0, 
learning_rate=9.140396100157433e-07
</code></pre>

<p>I don't undersatnd why the loss function result is not reducing and stay quite constant around 71M. </p>
",,2018-08-11 17:30:11,Gensim Word2vec model is not converged,<gensim><word2vec><loss-function>,,,CC BY-SA 4.0,False,False,True,False,False
17971,51707594,2018-08-06 12:24:26,,"<p>I try to integrate gensim's <code>get_keras_embedding</code> into a Keras model.</p>

<p>First I do text preprocessing</p>

<ol>
<li><code>text_to_word_sequence</code> converts text into token sequence</li>
<li><code>w2v.vocab[tok].index</code> converts words into gensim's word indexes</li>
<li><code>pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, value=999999)</code> padding with some value</li>
</ol>

<p>Then I build a model</p>

<pre><code>embedding_layer = w2v.get_keras_embedding(train_embeddings=False)
...
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
...
x = Conv1D(100, kernel_size, activation='relu', padding='same', strides=1)(embedded_sequences)
</code></pre>

<p>Apparently the problem is with the padding value. The default value is 0 which is 'the' in gensim word2vec, if I use non-existing value it fails with the following error message. What is the right way to use padding with gensim word2vec?</p>

<pre><code>InvalidArgumentError (see above for traceback): indices[48,0] = 999999 is not in [0, 400000)
     [[Node: embedding_1/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_1/embeddings""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_1/embeddings/read, _arg_input_1_0_3)]]
</code></pre>

<p>I would appreciate any help.</p>
",2018-08-06 12:46:37,2018-08-06 12:46:37,How to use padding value with get_keras_embedding,<keras><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
17976,51747613,2018-08-08 13:09:31,,"<p>Given I got a word2vec model (by gensim), I want to get the rank similarity between to words.
For example, let's say I have the word ""desk"" and the most similar words to ""desk"" are:</p>

<blockquote>
  <ol>
  <li>table 0.64</li>
  <li>chair 0.61</li>
  <li>book 0.59</li>
  <li>pencil 0.52</li>
  </ol>
</blockquote>

<p>I want to create a function such that:</p>

<blockquote>
  <p>f(desk,book) = 3
  Since book is the 3rd most similar word to desk.
  Does it exists? what is the most efficient way to do this?</p>
</blockquote>
",2018-08-08 17:54:18,2018-08-08 17:54:18,Word2vec - get rank of similarity,<python><python-3.x><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
17997,51791964,2018-08-10 18:11:40,,"<p>While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?</p>
<p>Currently, I am getting the following issue.</p>
<p><strong>DeprecationWarning: Call to deprecated <code>most_similar</code> (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be treated as <code>np.int32 == np.dtype(int).type</code>.
if np.issubdtype(vec.dtype, np.int):</strong></p>
<p>Please help to curb this issue? Any help is appreciated.</p>
<p>The code what, I have tried is as follows.</p>
<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')
</code></pre>
",2020-09-29 09:45:14,2020-09-29 09:45:14,DeprecationWarning in Gensim `most_similar`?,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
17998,51792916,2018-08-10 19:23:44,,"<p>I have just been started learning about word embeddings and gensim and I tried this <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">code</a> 
. In this article during the visualisation it says we need PCA to convert high-dimensional vectors into low-dimensions. Now we have a parameter ""size"" in Word2Vec method, so why can't we set that size equals to 2 rather using PCA. 
So, I tried to do this and compare both graphs (one with 100 size and other with 2 as size) and got very different result. Now I am confused that what this ""size"" depicts? How the size of vectors affect this?</p>

<p><a href=""https://i.stack.imgur.com/B2AkA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B2AkA.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 100 as size.</p>

<p><a href=""https://i.stack.imgur.com/Wf4vM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wf4vM.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 2 as size.</p>
",,2018-08-10 20:23:23,"What is the parameter ""size"" means in gensim.model.Word2Vec(sentence, size)?",<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18006,51754634,2018-08-08 19:54:03,,"<p>I am currently running LDA on my dataset. The dataset consists of approximately 60k Documents. The documents are approximately as long as a Wikipedia article. 
I ran LDA for 10,20,30...,70 Topics fine. For 80 and more Topics, all coefficients turn to nan. I tried every combination of numpy and gensim version on two different computers but I get the same result on both.
I have absolutely no idea how to solve this problem. Please let me know what info you need.</p>

<p>Since you asked for a code snippet here is the line for the lda:</p>

<p><code>lda = gensim.models.ldamulticore.LdaMulticore(corpus, 
    id2word=dictionary, num_topics=80, chunksize=1800, passes=20, 
    workers=1, eval_every=1, iterations=1000)</code></p>

<p>I changed the number of workers. Normally I used 3 or 4.</p>
",2018-08-11 14:50:05,2018-08-11 14:50:05,Gensim LDA All coefficients are NaN for more than 70 topics,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18031,51817146,2018-08-13 07:24:14,,"<p>I'm a very new student of doc2vec and have some questions about document vector.
What I'm trying to get is a vector of phrase like 'cat-like mammal'.
So, what I've tried so far is by using doc2vec pre-trained model, I tried the code below</p>

<pre><code>import gensim.models as g
model = ""path/pre-trained doc2vec model.bin""
m = g. Doc2vec.load(model)
oneword = 'cat'
phrase = 'cat like mammal'
oneword_vec = m[oneword]
phrase_vec = m[phrase_vec]
</code></pre>

<p>When I tried this code, I could get a vector for one word 'cat', but not 'cat-like mammal'.
Because word2vec only provide the vector for one word like 'cat' right? (If I'm wrong, plz correct me)
So I've searched and found infer_vector() and tried the code below</p>

<pre><code>phrase = phrase.lower().split(' ')
phrase_vec = m.infer_vector(phrase)
</code></pre>

<p>When I tried this code, I could get a vector, but every time I get different value when I tried
    phrase_vec = m.infer_vector(phrase) 
Because infer_vector has 'steps'.</p>

<p>When I set steps=0, I get always the same vector.
    phrase_vec = m.infer_vector(phrase, steps=0)</p>

<p>However, I also found that document vector is obtained from averaging words in document.
like if the document is composed of three words, 'cat-like mammal', add three vectors of 'cat', 'like', 'mammal', and then average it, that would be the document vector. (If I'm wrong, plz correct me)</p>

<p>So here are some questions.</p>

<ol>
<li>Is it the right way to use infer_vector() with 0 steps to getting a vector of phrase?</li>
<li>If it is the right averaging vector of words to get document vector, is there no need to use infer_vector()?</li>
<li>What is a model.docvecs for?</li>
</ol>
",2018-08-13 17:38:51,2018-08-13 17:38:51,Doc2vec - About getting document vector,<word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18033,51900688,2018-08-17 17:54:43,,"<p>Spacy has great parsing capacities and it's API is very intuitive for the most part. Is there any way from the Spacy API to fine tune its word embedding models? In particular, I would like to keep Spacy's tokens and give them a vector when possible.</p>

<p>The only thing I've come across for now is to train the embeddings using gensim (but then I wouldn't know how to load the embeddings from spacy to gensim) and then load then back to spacy, as in: <a href=""https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/"" rel=""nofollow noreferrer"">https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/</a>. This doesn't help for the first part: training on spacy tokens.</p>

<p>Any help appreciated.</p>
",2018-09-14 21:23:59,2018-09-14 21:23:59,Fine tune spaCy's word embeddings,<python-3.x><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
18034,51903087,2018-08-17 21:27:50,,"<p>So I have made an AnnoyIndexer and am running some most_similar queries to find the nearest neighbours of some vectors in a 300dimensional vector space. This is the code for it:</p>

<pre><code>def most_similar(self, vector, num_neighbors):
    """"""Find the approximate `num_neighbors` most similar items.
    Parameters
    ----------
    vector : numpy.array
        Vector for word/document.
    num_neighbors : int
        Number of most similar items
    Returns
    -------
    list of (str, float)
        List of most similar items in format [(`item`, `cosine_distance`), ... ]
    """"""

    ids, distances = self.index.get_nns_by_vector(
        vector, num_neighbors, include_distances=True)

    return [(self.labels[ids[i]], 1 - distances[i] / 2) for i in range(len(ids))]
</code></pre>

<p>I am wondering why the returned values for the distances are all taken away from 1 and then divided by 2? Surely after doing that, largest/smallest distances are all messed up?</p>
",,2018-08-17 22:12:21,Understanding the most_similar method for an AnnoyIndexer in gensim.similarities.index,<python><nlp><gensim><word2vec><annoy>,,,CC BY-SA 4.0,False,False,True,False,False
18058,51945520,2018-08-21 09:23:24,,"<p>Hello Community Members,</p>

<p>At present, I am implementing the Word2Vec algorithm.</p>

<p>Firstly, I have extracted the data (sentences), break and split the sentences into tokens (words), remove the punctuation marks and store the tokens in a single list. The list basically contain the words. Then I have calculated the frequency of words and then computed it occurrences in terms of frequency. It results a list. </p>

<p>Next, I am trying to load the model using gensim. However, I am facing a problem. The problem is about <code>the word is not in the vocabulary</code>. The code snippet, whatever I have tried is as follows.</p>

<pre><code>import nltk, re, gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2= (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't', 's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop=sw+sw1+sw2
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()
def word_embedding():
    for i in range(len(words)):
        model = Word2Vec(words, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
        model.init_sims(replace = True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word , score)
word_embedding()
</code></pre>

<p>Note: I am using Python 3.7 in Windows OS. From the <code>syntax of gensim</code>, it is suggested to use sentences and split into tokens and apply the same to build and train the model. My question is that how to apply the same to a corpus with single list containing only words. I have specified the words also using list, i.e. [words], during the training of the model.       </p>
",2018-08-21 09:25:28,2018-08-21 17:22:36,'word' not in Vocabulary in a corpus with words shown in a single list only in gensim library,<python-3.x><nltk><gensim><word2vec><nltk-book>,,,CC BY-SA 4.0,True,False,True,False,False
18077,51888879,2018-08-17 05:25:30,,"<p>I have trained my LDA model and saved it on a disk.</p>

<pre><code>`lda_model_tfidf = gensim.models.LdaMulticore(self.corpus_tfidf, num_topics=4, id2word=dictionary, passes=1, workers=3)`
</code></pre>

<p>Method used for saving the data:</p>

<p><code>lda_model_tfidf.save(""ldamodel"")</code></p>

<p>Output after saving the model : 1)ldamodel file 2)ldamodelexpElogbeta.npy 3)ldamodel.id2wor 4)ldamodel.state</p>

<p>Now when I try update the model after training on the new corpus, the code runs continuously.</p>

<pre><code>`lda.update(corpus)`
</code></pre>

<p>I need help in updating the model after training on the new corpus! </p>
",2018-08-17 14:52:45,2018-08-17 14:52:45,lda.update(corpus) -Updating the model by incrementally training on the new corpus runs for a very long time,<machine-learning><analytics><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18120,51985536,2018-08-23 12:11:20,,"<p>I am using gensim's doc2vec implementation and I have a few thousand documents tagged with four labels.</p>

<pre><code>yield TaggedDocument(text_tokens, [labels])
</code></pre>

<p>I'm training a Doc2Vec model with a list of these <strong>TaggedDocument</strong>s. However, I'm not sure how to infer the tag for a document that was not seen during training. I see that there is a infer_vector method which returns the embedding vector. But how can I get the most likely label from that?</p>

<p>An idea would be to infer the vectors for every label that I have and then calculate the cosine similarity between these vectors and the vector for the new document I want to classify. Is this the way to go? If so, how can I get the vectors for each of my four labels?</p>
",2018-08-23 12:18:34,2018-08-23 19:11:05,gensim doc2vec - How to infer label,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18122,51988701,2018-08-23 14:56:05,,"<p>We are trying to implement a word vector model for the set of words given below.</p>

<pre><code>stemmed = ['data', 'appli', 'scientist', 'mgr', 'microsoft', 'hire', 'develop', 'mentor', 'team', 'data', 'scientist', 'defin', 'data', 'scienc', 'prioriti', 'deep', 'understand', 'busi', 'goal', 'collabor', 'across', 'multipl', 'group', 'set', 'team', 'shortterm', 'longterm', 'goal', 'act', 'strateg', 'advisor', 'leadership', 'influenc', 'futur', 'direct', 'strategi', 'defin', 'partnership', 'align', 'efficaci', 'broad', 'analyt', 'effort', 'analyticsdata', 'team', 'drive', 'particip', 'data', 'scienc', 'bi', 'commun', 'disciplin', 'microsoftprior', 'experi', 'hire', 'manag', 'run', 'team', 'data', 'scientist', 'busi', 'domain', 'experi', 'use', 'analyt', 'must', 'experi', 'across', 'sever', 'relev', 'busi', 'domain', 'util', 'critic', 'think', 'skill', 'conceptu', 'complex', 'busi', 'problem', 'solut', 'use', 'advanc', 'analyt', 'larg', 'scale', 'realworld', 'busi', 'data', 'set', 'candid', 'must', 'abl', 'independ', 'execut', 'analyt', 'project', 'help', 'intern', 'client', 'understand']
</code></pre>

<p>We are using this code:</p>

<pre><code>import gensim
model = gensim.models.FastText(stemmed, size=100, window=5, min_count=1, workers=4, sg=1)
model.wv.most_similar(positive=['data'])
</code></pre>

<p>However, we are getting this error:</p>

<pre><code>KeyError: 'all ngrams for word data absent from model'
</code></pre>
",2019-01-25 12:43:36,2019-01-25 12:43:36,Implementing Word to vector model using Gensim,<python-3.x><machine-learning><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18123,51851193,2018-08-14 23:55:08,,"<p>I've seen some posts say that the average of the word vectors perform better in some tasks than the doc vectors learned through PV_DBOW. What is the relationship between the document's vector and the average/sum of its words' vectors? Can we say that vector <b><i>d</i></b>
 is approximately equal to the average or sum of its word vectors? 
Thanks!</p>
",,2018-08-15 03:59:17,Is doc vector learned through PV-DBOW equivalent to the average/sum of the word vectors contained in the doc?,<machine-learning><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18124,51854220,2018-08-15 06:56:45,,"<p>I trained a gensim Word2Vec model.
Let's say I have a certain vector and I want the find the word it represents - what is the best way to do so?</p>

<p>Meaning, for a specific vector:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>I want to get a word:</p>

<pre><code> 'computer' = model.vec2word(vec)
</code></pre>
",2018-08-15 07:23:55,2020-09-17 00:34:34,word2vec - find a word by a specific vector,<python-3.x><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18130,52038651,2018-08-27 11:48:18,,"<p>What can cause loss from <code>model.get_latest_training_loss()</code> increase on each epoch?  </p>

<p>Code, used for training: </p>

<pre><code>class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0

        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch))
        model.save(savepath)
        print(
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch ... "", sep=""\n""
            )
        if os.path.isfile(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))):
            print(""Previous model deleted "")
            os.remove(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))) 
        self.epoch += 1
        print(""Model loss:"", model.get_latest_training_loss())

    def train():

        ### Initialize model ###
        print(""Start training Word2Vec model"")

        workers = multiprocessing.cpu_count()/2

        model = Word2Vec(
            DocIter(),
            size=300, alpha=0.03, min_alpha=0.00025, iter=20,
            min_count=10, hs=0, negative=10, workers=workers,
            window=10, callbacks=[EpochSaver(""./checkpoints"")], 
            compute_loss=True
    )     
</code></pre>

<p>Output: </p>

<p>Losses from epochs (1 to 20): </p>

<pre><code>Model loss: 745896.8125
Model loss: 1403872.0
Model loss: 2022238.875
Model loss: 2552509.0
Model loss: 3065454.0
Model loss: 3549122.0
Model loss: 4096209.75
Model loss: 4615430.0
Model loss: 5103492.5
Model loss: 5570137.5
Model loss: 5955891.0
Model loss: 6395258.0
Model loss: 6845765.0
Model loss: 7260698.5
Model loss: 7712688.0
Model loss: 8144109.5
Model loss: 8542560.0
Model loss: 8903244.0
Model loss: 9280568.0
Model loss: 9676936.0
</code></pre>

<p>What am I doing wrong?</p>

<p>Language arabian. 
As input from DocIter - list with tokens. </p>
",2019-12-19 00:06:53,2019-12-19 00:06:53,"Loss does not decrease during training (Word2Vec, Gensim)",<python><gensim><word2vec><loss>,,,CC BY-SA 4.0,False,False,True,False,False
18132,51956153,2018-08-21 20:21:09,,"<p>I used this code, <a href=""https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/"" rel=""nofollow noreferrer"">https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/</a>, to find topic coherence for a dataset. When I tried this code with the same number of topics, I got new values after each running. For example, for the number of topics =10, I got the following value after 2 running:</p>

<p>First Run for the number of topics =10
Coherence Score CV_1:  0.31230269562327095</p>

<p>Coherence Score UMASS_1:  -3.3065236823786064</p>

<p>Second Run the number of topics =10
Coherence Score CV_2:  0.277016662550274</p>

<p>Coherence Score UMASS_2:  -3.6146150653617743</p>

<p>What is the reason? In this unstable case, how we can trust this library? The highest coherence value changed as well.  </p>
",2019-04-14 22:09:30,2019-04-14 22:09:30,Gensim LDA: coherence values not reproducible between runs,<gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
18133,51958011,2018-08-21 23:30:50,,"<p>Here is my use case: </p>

<p>HR department provide <code>job description</code>(free text) and set of <code>resumes</code>(plain text), and the ask is to come up with salience score based on job description relevance.</p>

<p>The <code>job description</code> consists of skills required and minimum qualification. I was considering <strong>Doc2Vec</strong> but bit confused <strong>how should I train the model</strong>? </p>

<ol>
<li>If I collate all job descriptions, and create corpus, querying profile text will give incorrect results. </li>
<li>Moreover, job requisitions are transient in nature and a profile might match with a expired requisition.</li>
</ol>

<p>Since each job description is exclusive, shall I create a trained model for each job? Or if there's any better framework, please advise.</p>

<p>Please see the code below:</p>

<pre><code>'
import os
from gensim import corpora, models, similarities

path = &lt;working_dir&gt;

os.chdir(path)

with open('Dir int Strategy.txt') as f:
    job_desc_text = f.read()

with open('Jeannine.txt') as f:
    candidate1_text = f.read()

with open('Penny.txt') as t:
    candidate2_text = t.read()

with open('Omar.txt') as z:
    candidate3_text = z.read()


with open('Kyle.txt') as p:
    candidate4_text = p.read()

documents = [candidate1_text, candidate2_text, candidate3_text,candidate4_text]   
stoplist = set('for a of the and to in'.split())   
documents_split_texts = [[word for word in document.lower().split() if  word not in stoplist]
    for document in documents]   
dictionary = corpora.Dictionary(documents_split_texts)   
dictionary.save('/tmp/deerwester.dict')        

corpus = [dictionary.doc2bow(text) for text in documents_split_texts]   
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)     

tfidf = models.TfidfModel(corpus)      

query_vector = job_desc_text    
query_vector = dictionary.doc2bow(query_vector.lower().split())

index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=699)     

sims = index[tfidf[query_vector]]            
'
</code></pre>
",2018-08-27 18:48:07,2018-08-27 18:48:07,Using Doc2Vec to find salience score for resumes based on job description,<nlp><gensim><doc2vec><information-extraction>,,,CC BY-SA 4.0,False,False,True,False,False
18142,52080365,2018-08-29 14:47:33,,"<p>I have downloaded a <code>.bin</code> FastText model, and I use it with <code>gensim</code> as follows:</p>

<pre><code>model = FastText.load_fasttext_format(""cc.fr.300.bin"")
</code></pre>

<p>I would like to continue the training of the model to adapt it to my domain. After checking <a href=""https://github.com/facebookresearch/fastText/pull/423"" rel=""noreferrer"">FastText's Github</a> and the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/fasttext.py#L685"" rel=""noreferrer"">Gensim documentation</a> it seems like it is <strong>not</strong> currently feasible appart from using this person's proposed <a href=""https://github.com/ericxsun/fastText"" rel=""noreferrer"">modification</a> (not yet merged). </p>

<p>Am I missing something?</p>
",,2019-07-08 02:55:06,Continue training a FastText model,<python><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18150,52061585,2018-08-28 15:33:05,,"<p>I am trying to train the <code>Gensim Word2Vec</code> model by:</p>

<pre><code>X = train['text']    

model_word2vec = models.Word2Vec(X.values, size=150)
model_word2vec.train(X.values, total_examples=len(X.values), epochs=10)
</code></pre>

<p>after the training, I get a small vocabulary (<code>model_word2vec.wv.vocab</code>) of length <code>74</code> containing only the alphabet's letters.</p>

<p>How could I get the right vocabulary?</p>

<p><strong>Update</strong></p>

<p>I tried this before:</p>

<pre><code>tokenizer = Tokenizer(lower=True)
tokenized_text = tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

model_word2vec.train(sequence, total_examples=len(X.values), epochs=10
</code></pre>

<p>but I got the same wrong vocabulary size.</p>
",2018-08-29 01:37:35,2018-08-29 01:37:35,Wrong length for Gensim Word2Vec's vocabulary,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18158,52102961,2018-08-30 18:01:55,,"<p>I have a question around measuring/calculating topic coherence for LDA models built in scikit-learn. </p>

<p>Topic Coherence is a useful metric for measuring the human interpretability of a given LDA topic model. Gensim's <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""noreferrer"">CoherenceModel</a> allows Topic Coherence to be calculated for a given LDA model (several variants are included). </p>

<p>I am interested in leveraging <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"" rel=""noreferrer"">scikit-learn's LDA</a> rather than <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer"">gensim's LDA</a> for ease of use and documentation (<em>note: I would like to avoid using the gensim to scikit-learn wrapper i.e. actually leverage sklearns LDA</em>). From my research, there is seemingly no scikit-learn equivalent to Gensims CoherenceModel. </p>

<p>Is there a way to either:</p>

<p><strong>1</strong> - Feed scikit-learns LDA model into gensims CoherenceModel pipeline, either through manually converting the scikit-learn model into gensim format or through a scikit-learn to gensim wrapper (I have seen the wrapper the other way around) to generate Topic Coherence?</p>

<p>Or</p>

<p><strong>2</strong> - Manually calculate topic coherence from scikit-learns LDA model and CountVectorizer/Tfidf matrices?</p>

<p>I have done quite a bit of research on implementations for this use case online but havent seen any solutions. The only leads I have are the documented equations from scientific literature.</p>

<p>If anyone has any knowledge on any similar implementations, or if you could point me in the right direction for creating a manual method for this, that would be great. Thank you!</p>

<p>*Side note: I understand that perplexity and log-likelihood are available in scikit-learn for performance measurements, but these are not as predictive from what I have read.</p>
",2018-08-30 19:49:03,2019-04-14 15:45:50,LDA Topic Model Performance - Topic Coherence Implementation for scikit-learn,<scikit-learn><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
18171,52009779,2018-08-24 18:11:54,,"<p>I found it is not explicit in usage  </p>

<pre><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""]]

model = Word2Vec(min_count=1)
model.build_vocab(sentences)  # prepare the model vocabulary
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors
(1, 30)
</code></pre>

<p>the sentences whether should contain the old corpus?</p>
",2018-08-24 20:59:18,2018-08-24 21:10:06,Is it necessary to mix old corpus and new corpus in updating word2vec model?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18183,52139386,2018-09-02 17:21:53,,"<p>I have this code and I have list of article as dataset. Each raw has an article. </p>

<p>I run this code:</p>

<pre><code>import gensim    
docgen = TokenGenerator( raw_documents, custom_stop_words )    
# the model has 500 dimensions, the minimum document-term frequency is 20    
w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)    
print( ""Model has %d terms"" % len(w2v_model.wv.vocab) )    
w2v_model.save(""w2v-model.bin"")    
# To re-load this model, run    
#w2v_model = gensim.models.Word2Vec.load(""w2v-model.bin"")    
    def calculate_coherence( w2v_model, term_rankings ):
        overall_coherence = 0.0
        for topic_index in range(len(term_rankings)):
            # check each pair of terms
            pair_scores = []
            for pair in combinations(term_rankings[topic_index], 2 ):
                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )
            # get the mean for all pairs in this topic
            topic_score = sum(pair_scores) / len(pair_scores)
            overall_coherence += topic_score
        # get the mean score across all topics
        return overall_coherence / len(term_rankings)

import numpy as np    
def get_descriptor( all_terms, H, topic_index, top ):    
    # reverse sort the values to sort the indices    
    top_indices = np.argsort( H[topic_index,:] )[::-1]    
    # now get the terms corresponding to the top-ranked indices    
    top_terms = []    
    for term_index in top_indices[0:top]:    
        top_terms.append( all_terms[term_index] )    
    return top_terms    
from itertools import combinations    
k_values = []    
coherences = []    
for (k,W,H) in topic_models:    
    # Get all of the topic descriptors - the term_rankings, based on top 10 terms
    term_rankings = []    
    for topic_index in range(k):
        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )

    # Now calculate the coherence based on our Word2vec model
    k_values.append( k )
    coherences.append( calculate_coherence( w2v_model, term_rankings ) )
    print(""K=%02d: Coherence=%.4f"" % ( k, coherences[-1] ) )
</code></pre>

<p>I face with this error:</p>

<pre><code>raise KeyError(""word '%s' not in vocabulary"" % word)
</code></pre>

<p>KeyError: u""word 'business' not in vocabulary""</p>

<p>The original code works great with their data set. </p>

<p><a href=""https://github.com/derekgreene/topic-model-tutorial"" rel=""nofollow noreferrer"">https://github.com/derekgreene/topic-model-tutorial</a></p>

<p>Could you help what this error is?</p>
",2018-09-02 19:15:44,2018-09-02 19:15:44,"Gensim: raise KeyError(""word '%s' not in vocabulary"" % word)",<python><nlp><gensim><word2vec><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18208,52012579,2018-08-24 22:49:37,,"<p>Instead of having construct from one single document ('mycorpus.txt'), How can i frame dictionary from multiple documents (Each one going to be 25 MB in file size with 10,000 Files) and please be aware that i am trying ""to construct the dictionary without loading all texts into memory"" via gensim</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; from six import iteritems
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
&gt;&gt;&gt; stop_ids = [dictionary.token2id[stopword] for stopword in stoplist
&gt;&gt;&gt;             if stopword in dictionary.token2id]
&gt;&gt;&gt; once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]
&gt;&gt;&gt; dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once
&gt;&gt;&gt; dictionary.compactify()  # remove gaps in id sequence after words that were removed
&gt;&gt;&gt; print(dictionary)
</code></pre>
",,2018-08-26 22:51:07,Gensim Contruct the dictionary without loading all texts into memory gensim,<python><bigdata><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18222,52144567,2018-09-03 06:56:40,,"<p>I have a large corpus of words extracted from the documents. In the corpus are words which might mean the same.
For eg: ""command"" and ""order"" means the same, ""apple"" and ""apply"" which does not mean the same.</p>

<p>I would like to merge the similar words, say ""command"" and ""order"" to ""command"".
I have tried to use word2vec but it doesn't check for semantic similarity of words(it ouputs good similarity for apple and apply since four characters in the words are the same). And when I try using wup similarity, it gives good similarity score if the words have matching synonyms whose results are not that impressive.</p>

<p>What could be the best approach to reduce semantically similar words to get rid of redundant data and merge similar data?</p>
",,2018-09-03 09:01:12,How to reduce semantically similar words?,<python-2.7><gensim><word2vec><text-analysis><redundancy>,,,CC BY-SA 4.0,False,False,True,False,False
18233,52125136,2018-09-01 05:26:57,,"<p>I followed the steps in gensim Python <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> to train wikipedia on LDA model, now I want to compare arbitary article from cnn.com for example with the trained data, what do I need to do next? Suppose this article is in txt file? </p>
",2018-09-01 05:53:43,2018-09-02 16:50:53,Training LDA on Wikipedia corpus to tag arbitary article?,<python><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
18235,52126539,2018-09-01 08:53:27,,"<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>
",2018-09-01 09:25:02,2019-08-06 11:30:57,Using pretrained gensim Word2vec embedding in keras,<python><keras><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18247,52072855,2018-08-29 08:26:50,,"<p>I found a warning about <code>word2vec.similarity()</code> as follows:</p>

<pre><code>&gt;d:\python\lib\site-packages\gensim\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.
  if np.issubdtype(vec.dtype, np.int):
</code></pre>

<p>what can I do to remove this warning?</p>
",2018-08-29 08:31:29,2019-04-11 07:36:17,how to remove gensim warning about use Word2vec gensim\matutils.py:737,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18250,52077016,2018-08-29 12:04:58,,"<p>I created an LDA model for some text files using gensim package in python. I want to get topic's distributions for the learned model. Is there any method in gensim ldamodel class or a solution to get topic's distributions from the model?
For example, I use the coherence model to find a model with the best cohrence value subject to the number of topics in range 1 to 5. After getting the best model I use get_document_topics method (thanks <a href=""https://stackoverflow.com/users/6256482/kenhbs"">kenhbs</a>) to get topic distribution in the document that used for creating the model.</p>

<pre><code>id2word = corpora.Dictionary(doc_terms)

bow = id2word.doc2bow(doc_terms)

max_coherence = -1
best_lda_model = None

for num_topics in range(1, 6):

    lda_model = gensim.models.ldamodel.LdaModel(corpus=bow, num_topics=num_topics)

    coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=doc_terms,dictionary=id2word)

    coherence_value = coherence_model.get_coherence()

    if coherence_value &gt; max_coherence:
        max_coherence = coherence_value
        best_lda_model = lda_model
</code></pre>

<p>The best has 4 topics</p>

<pre><code>print(best_lda_model.num_topics)

4
</code></pre>

<p>But when I use get_document_topics, I get less than 4 values for document distribution.</p>

<pre><code>topic_ditrs = best_lda_model.get_document_topics(bow)

print(len(topic_ditrs))

3
</code></pre>

<p>My question is: For best lda model with 4 topics (using coherence model) for a document, why get_document_topics returns fewer topics for the same document? why some topics have very small distribution (less than 1-e8)?</p>
",2018-09-03 10:35:00,2019-12-31 11:21:32,Extracting Topic distribution from gensim LDA model,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18273,52220514,2018-09-07 10:24:05,,"<p>I would like to pass the <code>X_train_word2vec</code> vector as input to <code>Gensim Word2Vec</code> model.
The vector type is <code>numpy.ndarray</code>, at example:</p>

<pre><code>X_train_word2vec[9] = array([   19,     7,     1, 20120,     2,     1,   856,   233,   671,
       1,  1208,  6016,     2,    32,     0,     0,     0,     0, ....)]
</code></pre>

<p>When I run this code:</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
model_word2vec.train(X_train_word2vec,total_examples=X_train_word2vec.shape[0], epochs=10)
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U11') dtype('&lt;U11') dtype('&lt;U11')</code></p>

<p>I have read <a href=""https://stackoverflow.com/questions/44527956/python-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtype"">this</a> post, where the issue is due to different data types in the input array but, in my case, I have all the data of the same type: <code>int</code>.</p>

<p><strong>Update:</strong>
The code before <code>model_Word2Vec</code>:</p>

<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

seq_max_len = 50
X_seq = pad_sequences(sequenza, maxlen=seq_max_len,padding='post',truncating='post',dtype=int)

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_seq, y_cat, test_size=0.2, random_state=123)
</code></pre>
",2018-09-07 10:39:47,2018-09-07 15:29:15,TypeError: ufunc 'add' did not contain a loop with signature matching types dtype,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18292,52184439,2018-09-05 11:49:19,,"<p>I have 50,000k files - that have a combined total of 162 million words. I wanted to do topic modelling using Gensim similar to this tutorial <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">here</a></p>

<p>So, LDA requires one to tokenize the documents into words and then create a word frequency dictionary. </p>

<p>So, I have these files read into a pandas dataframe (The 'content' column has the text) and do the following to create a list of the texts.<a href=""https://i.stack.imgur.com/R42dr.png"" rel=""nofollow noreferrer"">image of dataframe attached here</a></p>

<p><code>texts = [[word for word in row[1]['content'].lower().split() if word not in stopwords]
         for row in df.iterrows()]</code></p>

<p>However, I have been running into a memory error, because of the large word count. </p>

<p>I also tried the TokenVectorizer in Python. I had got a memory error for this too.  </p>

<p><code>def simple_tokenizer(str_input):
    words = re.sub(r""[^A-Za-z0-9\-]"", "" "", str_input).lower().split()
    return words</code></p>

<p><code>vectorizer = TfidfVectorizer(
    use_idf=True, tokenizer=simple_tokenizer, stop_words='english')
X = vectorizer.fit_transform(df['content'])</code></p>

<p>How do I handle tokenizing these really long documents in a way it can be processed for LDA Analysis? </p>

<p>I have an i7, 16GB Desktop if that matters. </p>

<p><strong>EDIT</strong></p>

<p>Since Python was unable to store really large lists. I actually rewrote the code, to read each file (originally stored as HTML), convert it to text, create a text vector, append it to a list, and then sent it to the LDA code. It worked!</p>
",2018-09-06 12:24:38,2018-09-06 12:24:38,Handling Memory Error when dealing with really large number of words (>100 million) for LDA analysis,<python><out-of-memory><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
18294,52116717,2018-08-31 13:38:43,,"<p>I used an LDA over a corpus of 60000 entries and I got a good result. But inserting 200 more lines in this corpus and relaunching the LDA, I have quite different topics. However, 200 lines do not represent 1% of the corpus. Normally, the results should not change. I've looked for something about the sensitivity and stability of the LDA models and I've seen that they're just pretty sensitive on the parameter level ... does anyone know anything about this?</p>
<p>Here this script:</p>
<pre><code>import pandas
import mglearn
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

text = pandas.read_csv('pretraitement_janaina_modif.csv', encoding = 'utf-8')
text_list = text.values.tolist()

vector = CountVectorizer()
X = vector.fit_transform(text_list)

lda_model = LatentDirichletAllocation(n_components = 30, learning_method = &quot;batch&quot;, max_iter = 25, random_state = 0)
document_topics = lda_model.fit_transform(X)
sorting = np.argsort(lda_model.components_, axis = 1)[:, ::-1]
feature_names = np.array(vector.get_feature_names())

topics = mglearn.tools.print_topics(topics = range(30), feature_names = feature_names, sorting = sorting, topics_per_chunk = 5, n_words = 10)

print(topics)
</code></pre>
<p><strong>I have as a first result this list of topics:</strong></p>
<blockquote>
<p>Topic 0: no order mail received back always share refund answer more</p>
<p>Topic 1: cancel order wishes possible wish would wish to advance error items deceived</p>
<p>Topic 2: keep current informed delivery order informed product inform face number</p>
<p>Topic 3: faulty wooden box present box side level painting defects box</p>
</blockquote>
<p><strong>But when I add some few lines in the corpus, the topics change:</strong></p>
<blockquote>
<p>Topic 0: big top metal big size damaged product chandelier canape support</p>
<p>Topic 1: receipt ordered article product misses well order not send back only</p>
<p>Topic 2: faulty wooden box present box side level painting defects box</p>
<p>Topic 3: order payment amount spend transfer made pay bank come well</p>
</blockquote>
<p>What I don't understand is how this algorithm can be so sensitive and not stability and change so much when we add a few lines...</p>
",2020-06-20 09:12:55,2018-08-31 15:37:39,Sensitivity and stability of the LDA models,<python><scikit-learn><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,True
18304,52131011,2018-09-01 18:23:27,,"<p>I want to understand how <code>pip install -e &lt;local project path&gt;</code> works. <a href=""https://pip.pypa.io/en/stable/reference/pip_install/#usage"" rel=""nofollow noreferrer"">The references</a> tell the <code>&lt;local project path&gt;</code> should have a setup.py file but doesn't mention anything else. </p>

<p>Does the <code>-e</code> flag executes setup.py file? Or are there some other steps involved?</p>

<p>Also, I see <a href=""https://github.com/RaRe-Technologies/gensim/issues/1519#issuecomment-339700695"" rel=""nofollow noreferrer"">projects</a> using the command like this: <code>pip install -e .[test]</code>. What does <code>.[test]</code> mean? Is it value placeholder that is defined somewhere? Is it a folder name? Or is it specific to how I write my <code>setup.py</code>? I couldn't find such example usage in the <a href=""https://github.com/RaRe-Technologies/gensim/issues/1519#issuecomment-339700695"" rel=""nofollow noreferrer""><code>pip install</code> references</a>.</p>
",,2018-09-01 18:23:27,pip - Installing dependencies with <local project path>. Using `pip install -e`,<python-3.x><pip><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18325,52252119,2018-09-10 06:22:37,,"<p>For Skip-gram word2vec training samples are obtained as follows:</p>

<pre><code>Sentence: The fox was running across the maple forest
</code></pre>

<p>The word <code>fox</code> give next pairs for training:</p>

<pre><code>fox-run, fox-across, fox-maple, fox-forest
</code></pre>

<p>and etc. for every word. CBOW w2v use reverse approach:</p>

<pre><code>run-fox, across-fox, maple-fox, forest-fox
</code></pre>

<p>or for <code>forest</code> word:</p>

<pre><code>fox-forest, run-forest, across-forest, maple-forest
</code></pre>

<p>So we get all the pairs. What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library, if we do not specify the target word when training in the CBOW-mode? In both cases all pairs of words are used, or not?</p>
",,2018-09-10 19:24:58,What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library?,<python><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18332,52170784,2018-09-04 16:47:18,,"<p>I have gone through the other threads where its specified that in LDA the memory is proportional to <strong>numberOfTerms*numberOfTopics</strong> . In my case  I have two datasets. 
In dataset A I have 250K Documents and around 500K terms here I am easily able to run for ~ 500 Topics. But in dataset B I have around 2 Million documents and 500K terms(we got here after some filtering) but here I am only able to run till 50 topics above that it throws memory exception.</p>

<p>So just wanted to understand if only number of terms and topics matter for memory why number of documents is causing this problem and is there any quick workaround which can avoid this.</p>

<p><strong>Note :</strong> I know corpus can be wrapped around as an iteratable as specified in <a href=""https://stackoverflow.com/questions/35609171/memory-efficient-lda-training-using-gensim-library"">memory-efficient-lda-training-using-gensim-library</a> but lets assume I already loaded the corpora in memory because of some other restrictions I have of keeping input data in different format so it can be run on different platforms for different algorithms. The point is I am able to run it for some lesser number of Topics after loading whole corpora in memory. So is there any workaround which can help it run for more number of topics. For example I was thinking adjusting chunksize might help but that didn't work.</p>
",,2018-09-04 16:47:18,LDA Gensim OOM Exception because of large corpus,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18333,52171195,2018-09-04 17:17:04,,"<p>There are two ways to load in pretrained word embeddings, those who are compiled in C and the other in python. I have self trained embeddings in python which are loaded in with:</p>

<p><code>model = gensim.models.Word2Vec.load('transcript-vectors.pickle')</code></p>

<p>But when I go to load them into a word dictionary ( the same way I would with a pretrained embeddings from a third party ) it errors out since it does not have the same methods as the other load.</p>

<pre><code>embeddings_index = dict()
for word in model.wv.vocab:
    embeddings_index[word] = model.word_vec(word)
print('Loaded %s vectors' % len(embeddings_index))
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-94-c1e5d21d49af&gt; in &lt;module&gt;()
  1 embeddings_index = dict()
  2 for word in model.wv.vocab:
----&gt; 3     embeddings_index[word] = model.word_vec(word)
  4 print('Loaded %s vectors' % len(embeddings_index))

AttributeError: 'Word2Vec' object has no attribute 'word_vec'
</code></pre>
",,2018-09-04 17:46:25,Gensim Self Trained embedding load,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18337,52286330,2018-09-12 01:44:58,,"<p>I am working with Gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method <code>model.docvecs.most_similar(""file"")</code> , I always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.<br/><br/>
<strong>Here is the code for training the model</strong><br/></p>

<pre><code>model = gensim.models.Doc2Vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.Doc2Vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim
</code></pre>

<p><br/> 
<strong>this is the output result</strong></p>

<blockquote>
<p>
[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]
</p>
</blockquote>

<p>what am I doing wrong? how could I improve the accuracy of results?</p>
",,2018-09-12 17:23:05,Inaccurate similarities results by doc2vec using gensim library,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18353,52364632,2018-09-17 09:30:43,,"<p>According to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a>: </p>

<blockquote>
  <p>It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.</p>
</blockquote>

<p>So we can try from the supplied demo script:</p>

<pre><code>+ ../bin/word-analogy ../data/text8-vector.bin
Enter three words (EXIT to break): paris france berlin

Word: paris  Position in vocabulary: 198365

Word: france  Position in vocabulary: 225534

Word: berlin  Position in vocabulary: 380477

                                              Word              Distance
------------------------------------------------------------------------
                                           germany      0.509434
                                          european      0.486505
</code></pre>

<p>Please note that <code>paris france berlin</code> is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in <code>Gensim</code> and try to compute the vectors myself. For example:</p>

<pre><code>&gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(BIGDATA, binary=True)
&gt;&gt;&gt; v = word_vectors['paris'] - word_vectors['france'] + word_vectors['berlin']
&gt;&gt;&gt; word_vectors.most_similar(np.array([v]))
[('berlin', 0.7331711649894714), ('paris', 0.6669869422912598), ('kunst', 0.4056406617164612), ('inca', 0.4025722146034241), ('dubai', 0.3934606909751892), ('natalie_portman', 0.3909246325492859), ('joel', 0.3843030333518982), ('lil_kim', 0.3784593939781189), ('heidi', 0.3782389461994171), ('diy', 0.3767407238483429)]
</code></pre>

<p>So, what is the word analogy actually doing? How should I reproduce it?</p>
",2018-09-17 09:39:55,2018-09-17 20:09:53,What is the operation behind the word analogy in Word2vec?,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18357,52385474,2018-09-18 11:21:59,,"<p>I'm trying to segregate the topics using lda's topic modeling. </p>

<p>Here, I'm able to fetch the top 10 keywords for each topic. Instead of getting only top 10 keywords, I'm trying to fetch all the keywords from each topic.</p>

<p>Can anyone please suggest me regarding the same...</p>

<h1>My Code:</h1>

<pre><code>from gensim.models import ldamodel
import gensim.corpora;
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;
from sklearn.decomposition import LatentDirichletAllocation
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)


def load_data(filename):
    reviews = list()
    labels = list()
    with open(filename, encoding='utf-8') as file:
        file.readline()
        for line in file:
            line = line.strip().split(' ',1)
            labels.append(line[0])
            reviews.append(line[1])

    return reviews
data = load_data('/Users/abc/dataset.txt')
#print(""Data:"" , data)

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print (""Topic %d:"" % (topic_idx))
        print ("" "".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))


no_features = 1000

tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tf = tf_vectorizer.fit_transform(data)
tf_feature_names = tf_vectorizer.get_feature_names()

no_topics = 5

lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)
no_top_words = 10


display_topics(lda, tf_feature_names, no_top_words)
</code></pre>
",,2018-09-18 11:21:59,How to get all the keywords based on topic using topic modeling?,<python><scikit-learn><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
18373,52277384,2018-09-11 13:43:28,,"<p>I build two word embedding (word2vec models) using <code>gensim</code> and save it as (word2vec1 and word2vec2) by using the <code>model.save(model_name)</code> command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as <code>a</code>). </p>

<p>How to compute the degree of similarity (<code>cosine-similarity or similarity</code>) of the extracted top word (say 'a'), for the two word2vec models? Does <code>most_similar()</code> will work in this case efficiently? </p>

<p>I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?</p>

<p>Any idea is deeply appreciated.</p>
",2018-09-11 16:11:07,2018-09-12 01:15:31,Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models,<python-3.x><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18383,52297260,2018-09-12 14:12:38,,"<p>PyCharm can't find gensim that is listed in ""anaconda list"". in anaconda list I can see gensim but it does not exist in project interpreter!!!
I'm using paython version 3.7. I have no problem with other libraries, the problem is just in gensim installation </p>

<p><img src=""https://i.stack.imgur.com/PcrFs.png"" alt=""anaconda list""></p>

<p><img src=""https://i.stack.imgur.com/pHN8L.png"" alt=""project interpreter list""></p>

<p>If anyone has any advice, I'd appreciate it! Thanks.</p>
",2018-09-12 14:31:18,2018-09-12 14:31:18,'gensim' is not recognized in pycharm,<python><python-3.x><pycharm><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18388,52348079,2018-09-15 19:21:22,,"<p>i read other questioned which are asked about this error earlier.but still i am not getting where i am making a mistake.when i call the function i got this error. i am very new in this forum ,any help in solving my problem will be appreciated.here is my code</p>

<pre><code>def lda_train(self, documents):
        # create dictionary
        dictionary= corpora.Dictionary(documents)
        dictionary.compactify()
        dictionary.save(self.DICTIONARY_FILE)  # store the dictionary, for future reference
        print (""============ Dictionary Generated and Saved ============"")

        ############# Create Corpus##########################

        corpus = [dictionary.doc2bow(text) for text in documents]
        print('Number of unique tokens: %d' % len(dictionary))
        print('Number of documents: %d' % len(corpus))
        return dictionary,corpus

def compute_coherence_values(dictionary,corpus,documents,  limit, start=2, step=3):
        num_topics = 10
        coherence_values = []
        model_list = []
        for num_topics in range(start, limit, step):
            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary, num_topics=num_topics,  random_state=100, alpha='auto')
            model_list.append(model)
            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
            coherence_values.append(coherencemodel.get_coherence())
        return model_list, coherence_values
</code></pre>

<p>when i call this function in main by using this code:</p>

<pre><code>if __name__ == '__main__':
    limit=40
    start=2
    step=6
    obj = LDAModel()
    lda_input = get_lda_input_from_corpus_folder('./dataset/TRAIN')
    dictionary,corpus =obj.lda_train(lda_input)
    model_list, coherence_values = obj.compute_coherence_values(dictionary=dictionary,corpus=corpus, texts=lda_input,  start=2, limit=40, step=6)
</code></pre>

<p>i receive an error message :</p>

<pre><code> model_list, coherence_values=obj.compute_coherence_values(dictionary=dictionary,corpus=corpus, texts=lda_input,  start=2, limit=40, step=6) 
TypeError: compute_coherence_values() got multiple values for argument 'dictionary'
</code></pre>
",2018-09-15 19:29:06,2018-09-16 10:31:24,TypeError: got multiple values for argument 'dictionary',<python><python-3.x><typeerror><lda>,,,CC BY-SA 4.0,False,False,True,False,False
18406,52425323,2018-09-20 12:36:04,,"<p>I checked unsupervised clsutering on gensim, fasttext, sklearn but did not find any documentation where I can cluster my text data using unsupervised learn without mentioning numbers of cluster to be identified</p>

<p>for example in sklearn KMneans clustering </p>

<pre><code>km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)
</code></pre>

<p>Where I have to provide n_clusters. </p>

<p>In my case, I have text and it should be automatically identify numbers of clusters in it and cluster the text. Any reference article or link much appreciated. </p>
",2018-09-21 10:37:41,2018-09-21 10:37:41,Is there any unsupervised clustering technique which can identify numbers clusters itself?,<tensorflow><scikit-learn><gensim><unsupervised-learning><fasttext>,,,CC BY-SA 4.0,False,False,True,False,True
18421,52391572,2018-09-18 17:11:22,,"<p>I am trying to use a <strong>text corpus file</strong> (<em>One sentence by line</em>) to extarct <strong>words co-occurrence</strong> from it in order to use them in a later traitement. So how can i extract word(statistical) co-occurrence from large corpus file using <strong>gensim</strong> and how to use them later ?  </p>
",2018-09-18 21:20:35,2018-09-18 21:20:35,Gensim: How to extract words co-occurrence?,<python><nlp><gensim><word2vec><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
18424,52397065,2018-09-19 02:14:15,,"<p>I would like to load a file for only it's extension name in gensim. </p>

<p>A normal code would be this:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(""news.bin"")
</code></pre>

<p>But I would like it to auto open any file with "".bin"".</p>

<p>Example:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(***I would like to change this part to only load any .bin***)
</code></pre>

<p>.bin files:</p>

<p>It can be ""news.bin"", ""file.bin"" or ""guess.bin"". As long as it load only the extension. Thank you.</p>
",,2018-09-19 02:29:22,load a file with only its extension name,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18429,52414249,2018-09-19 21:09:48,,"<p>I'm running a VM on Google Cloud Platform using Jupyter notebook with word2vec models. I have the following code snippet:</p>

<pre><code>from gensim.models import Word2Vec
amazon_word2vec = Word2Vec(model, min_count=1, size=100)
</code></pre>

<p>And it results in the error:</p>

<pre><code>AttributeError: module 'boto' has no attribute 'plugin'
</code></pre>

<p>What is the solution to the above problem?</p>
",2018-12-01 14:10:19,2018-12-01 14:10:19,AttributeError: module 'boto' has no attribute 'plugin',<python><google-cloud-platform><jupyter-notebook><google-compute-engine><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18431,52415929,2018-09-20 00:42:39,,"<p>I wish to use a variable like a model that has trained certain sentence for gensim. 
Example, I use gensim word2vec to train a sentence to find its vectors in another function and save it to a variable called ""model"".</p>

<p>Then I create another function to get the the vectors of each word and return it.</p>

<p>My code is as follow:</p>

<pre><code>def gen(sentence):
    model = gensim.models.word2vec.Word2Vec([sentence],min_count=1, workers=1, size=3)
    ....
    ....
    return ...

def name(sentence):
    for word in sentence:
        print(model.wv[word])
    return
</code></pre>

<p>Is there a way to pass the variable from function def ""gen"" named ""model"" to function def ""name""? I tried to use self but its not working. I wanted to just call the variable because I dont want to build another extra function just to link it.</p>
",,2018-09-20 01:09:28,Is it possible to call a variable from another function in another function?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18433,52379317,2018-09-18 05:07:41,,"<p>I would like to ask more about Word2Vec:</p>

<p>I am currently trying to build a program that check for the embedding vectors for a sentence. While at the same time, I also build a feature extraction using sci-kit learn to extract the lemma 0, lemma 1, lemma 2 from the sentence.</p>

<p>From my understanding;</p>

<p>1) Feature extractions : Lemma 0, lemma 1, lemma 2
2) Word embedding: vectors are embedded to each character (this can be achieved by using gensim word2vec(I have tried it))</p>

<p>More explanation:</p>

<p>Sentence = ""I have a pen"".
Word = token of the sentence, for example, ""have""</p>

<p>1) Feature extraction</p>

<p>""I have a pen"" --> lemma 0:I, lemma_1: have, lemma_2:a.......lemma 0:have, lemma_1: a, lemma_2:pen and so on.. Then when try to extract the feature by using one_hot then will produce:</p>

<pre><code>[[0,0,1],
[1,0,0],
[0,1,0]]
</code></pre>

<p>2) Word embedding(Word2vec)</p>

<p>""I have a pen"" ---> ""I"", ""have"", ""a"", ""pen""(tokenized) then word2vec from gensim will produced matrices for example if using window_size = 2 produced:</p>

<pre><code>[[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345],
[0.31235,0.31345]
]
</code></pre>

<p><strong>The floating and integer numbers are for explanation purpose and original data should vary depending on the sentence. These are just dummy data to explain.*</strong></p>

<p>Questions:</p>

<p>1) Is my understanding about Word2Vec correct? If yes, what is the difference between feature extraction and word2vec?
2) I am curious whether can I use word2vec to get the feature extraction embedding too since from my understanding, word2vec is only to find embedding for each word and not for the features. </p>

<p>Hopefully someone could help me in this.</p>
",,2018-09-18 17:52:20,Word2Vec is it for word only in a sentence or for features as well?,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18449,52401067,2018-09-19 08:06:31,,"<p>i am able to extract topics from LDA model using gensim. when i print topics ,it is displaying topics with 10 number of words by defaults. i want to show 15 words in one topic.i tried to change it but still i am getting 10 words per topic.how can i change this default behavior?</p>

<p>here is the code:</p>

<pre><code>for n, topic in model.show_topics(num_topics=-1, num_words=15,formatted=False):
                topic = [word for word, _ in topic]
                cm = CoherenceModel(topics=[topic], texts=documents, dictionary=dictionary, window_size=10)
                coherence_values[n] = cm.get_coherence()
            top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)
            result.append((model, top_topics))
</code></pre>

<p>and for printing the topics:</p>

<pre><code>pprint([lm.show_topic(topicid) for topicid, c_v in top_topics[:8]])
</code></pre>
",,2018-09-19 09:01:46,how to change default number_words in LDA,<python><python-3.x><nlp><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18450,52402693,2018-09-19 09:34:49,,"<p>How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?</p>

<p>What are the best ways to generate high quality word embedding for non - english words .</p>

<p>Words may include (samsung-galaxy-s9)</p>
",,2018-10-17 14:21:49,Non English Word Embedding from English Word Embedding,<tensorflow><nlp><gensim><word-embedding><chainer>,,,CC BY-SA 4.0,False,False,True,False,False
18451,52525990,2018-09-26 21:02:56,,"<p>I am new to deep learning. I am trying to make very basic LSTM network on word embedding feature. I have written the following code for the model but I am unable to run it.    </p>

<pre><code>from keras.layers import Dense, LSTM, merge, Input,Concatenate
from keras.layers.recurrent import LSTM
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten


max_sequence_size = 14
classes_num = 2

LSTM_word_1 = LSTM(100, activation='relu',recurrent_dropout = 0.25, dropout = 0.25)
lstm_word_input_1 = Input(shape=(max_sequence_size, 300))
lstm_word_out_1 = LSTM_word_1(lstm_word_input_1)


merged_feature_vectors = Dense(50, activation='sigmoid')(Dropout(0.2)(lstm_word_out_1))

predictions = Dense(classes_num, activation='softmax')(merged_feature_vectors)

my_model = Model(input=[lstm_word_input_1], output=predictions)
print my_model.summary()
</code></pre>

<p>The error I am getting is <code>ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (3019, 300)</code>. On searching, I found that people have used <code>Flatten()</code> which will compress all the 2-D features (3019,300) for the dense layer. But I am unable to fix the issue. </p>

<p>While explaining, kindly let me know how do the dimension work out.</p>

<p>Upon request:</p>

<p>My X_training had dimension issues, so I am providing the code below to clear out the confusion,</p>

<pre><code>def makeFeatureVec(words, model, num_features):
    # Function to average all of the word vectors in a given
    # paragraph
    #
    # Pre-initialize an empty numpy array (for speed)
    featureVec = np.zeros((num_features,),dtype=""float32"")
    #
    nwords = 0.
    #
    # Index2word is a list that contains the names of the words in
    # the model's vocabulary. Convert it to a set, for speed
    index2word_set = set(model.wv.index2word)
    #
    # Loop over each word in the review and, if it is in the model's
    # vocaublary, add its feature vector to the total
    for word in words:
        if word in index2word_set:
            nwords = nwords + 1.
            featureVec = np.add(featureVec,model[word])
    #
    # Divide the result by the number of words to get the average
    featureVec = np.divide(featureVec,nwords)
    return featureVec
</code></pre>

<p>I think the following code is giving 2-D numpy array as I am initializing it that way</p>

<pre><code>def getAvgFeatureVecs(reviews, model, num_features):
    # Given a set of reviews (each one a list of words), calculate
    # the average feature vector for each one and return a 2D numpy array
    #
    # Initialize a counter
    counter = 0.
    #
    # Preallocate a 2D numpy array, for speed
    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=""float32"")

    for review in reviews:

       if counter%1000. == 0.:
           print ""Question %d of %d"" % (counter, len(reviews))

       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \
           num_features)

       counter = counter + 1.
    return reviewFeatureVecs


def getCleanReviews(reviews):
    clean_reviews = []
    for review in reviews[""question""]:
        clean_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, remove_stopwords=True ))
    return clean_reviews
</code></pre>

<p>My objective is just to use gensim pretrained model for LSTM on some comments that I have.</p>

<pre><code>trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )
</code></pre>
",2018-09-27 07:27:23,2018-09-27 09:40:08,LSTM network on pre trained word embedding gensim,<python><machine-learning><deep-learning><lstm><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18458,52486070,2018-09-24 19:26:58,,"<p>I am using the following code to get the ordered list of user posts.</p>

<pre><code>model = doc2vec.Doc2Vec.load(doc2vec_model_name)
doc_vectors = model.docvecs.doctag_syn0
doc_tags = model.docvecs.offset2doctag

for w, sim in model.docvecs.most_similar(positive=[model.infer_vector('phone_comments')], topn=4000):
        print(w, sim)
        fw.write(w)
        fw.write("" ("")
        fw.write(str(sim))
        fw.write("")"")
        fw.write(""\n"")

fw.close()
</code></pre>

<p>However, I am also getting the vector <code>""phone comments""</code> (that I use to find nearest neighbours) in like 6th place of the list. Is there any mistake I do in the code? or is it a issue in Gensim (becuase the vector cannot be a neighbour of itself)?</p>

<p><strong>EDIT</strong></p>

<p>Doc2vec model training code</p>

<pre><code>######Preprocessing
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^1-9a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key.replace(' ', '_')
    docs.append(analyzedDocument(words, tags.split(' ')))

sentences = []  # Initialize an empty list of sentences
######Get n-grams
#Get list of lists of tokenised words. 1 sentence = 1 list
for item in docs:
    sentences.append(item.words)

#identify bigrams and trigrams (trigram_sentences_project) 
trigram_sentences_project = []
bigram = Phrases(sentences, min_count=5, delimiter=b' ')
trigram = Phrases(bigram[sentences], min_count=5, delimiter=b' ')

for sent in sentences:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)

paper_count = 0
for item in trigram_sentences_project:
    docs[paper_count] = docs[paper_count]._replace(words=item)
    paper_count = paper_count+1

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 5, workers = 4, iter = 20)

#Save the trained model for later use to take the similarity values
model_name = user_defined_doc2vec_model_name
model.save(model_name)
</code></pre>
",2018-09-25 03:55:50,2018-09-25 03:55:50,Why Gensim most similar in doc2vec gives the same vector as the output?,<nlp><data-mining><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18460,52488877,2018-09-25 00:10:37,,"<p>Using Gensim's Doc2Vec how would I find the distance between a <code>Doctag</code> and an <code>infer_vector()</code>?</p>

<p>Many thanks</p>
",,2018-09-25 01:11:51,Finding the distance between 'Doctag' and 'infer_vector' with Gensim Doc2Vec?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18462,52469352,2018-09-23 19:04:52,,"<p>Edit: I'm asking this because I've spent over 40 hours experimenting with these packages, and feel as though I've gotten nowhere.</p>

<p>I'm pretty new to Python. I've done a RandomForestClassifier model successfully at my organization and the model is in production, but neural nets are beyond my current comprehension.</p>

<p>I'm working on a text classification problem in Python. I had 243 samples (rows) that are taken from 25 job postings. I have one column that is the string sentence, and one column that is the job posting that it came from. </p>

<p>I'm gunning for a promotion at work, and thought this would be a neat opportunity to learn about neural networks. (I'm not going for a data scientist-type role, this just fascinates me.) Each sample is one ""job duty"" from a job posting, and each ""document"" is a job posting. One job posting could have multiple duties that are nearly identical, every job posting should have 2-3 identical (abstracted) duties, and ultimately, I assume there will be 15-20 clusters of ""duties"" from my 25 job postings. </p>

<p>Essentially, my desired output is to classify each row (regardless of which job posting it came from; I don't think my document column is relevant) to n clusters. I don't expect labels for my clusters. </p>

<p>I've cleaned my 243 samples; removing punctuation and stopwords, and have it in a dataframe.</p>

<p>The packages I've experimented with so far are Keras, doc2vec, word2vec, nltk, and Soundex</p>

<ol>
<li><p>Is there a way to cluster my samples (unsupervised) without training data?</p></li>
<li><p>Do I need to upload a corpus to train? Does a corpus by default have classification labels? </p></li>
<li><p>What is the simplest (willing to sacrifice accuracy) to get n clusters out of 243 samples (I will go through the contents of each cluster and determine the label for the cluster post-processing)</p></li>
</ol>

<p>Just some vaguely directional guidance would really help me.</p>
",2018-09-23 19:20:06,2018-09-23 19:55:41,"Python NLP, Neural Network, text clustering",<python><tensorflow><keras><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
18488,52460163,2018-09-22 19:31:45,,"<p>I'm using the pre-trained model from Google to do word embedding. What kind of string should I give to the model to receive and array full of zeros? I need to pad some sentences and I need an array of zeros to embed the padding. Maybe should I create the array of zeros instead of the model?</p>
",,2018-09-22 19:31:45,Which Pad token in Google-News Word2vec Pre-trained Model,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18498,52514911,2018-09-26 09:54:01,,"<p>I have gensim installed in my system. I did the summarization with gensim. NOw I want to  find the similarity between the sentence and it showing an error. sample code is given below. I have downloaded the Google news vectors.</p>

<pre><code>from gensim.models import KeyedVectors

#two sample sentences
s1 = 'the first sentence'
s2 = 'the second text'
#model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)
model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)
#calculate distance between two sentences using WMD algorithm
distance = model.wmdistance(s1, s2)

print ('distance = %.3f' % distance)
</code></pre>

<blockquote>
  <p>Error#################################################</p>
  
  <p>****Traceback (most recent call last):   File ""/home/abhi/Desktop/CHiir/CLustering &amp;
  summarization/.idea/FInal_version/sentence_embedding.py"", line 7, in
  
      model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz',
  binary=True) NameError: name 'gensim' is not defined****</p>
</blockquote>
",,2019-01-27 07:04:25,NameError: name 'gensim' is not defined (doc2vec similarity),<similarity><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18540,52584376,2018-10-01 03:56:34,,"<p>I am able to build the model using the built-in lee_background corpus. But when I try to compare using most_similar method, I get an error.</p>

<pre><code>lee_train_file = '/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor'

train_corpus=list()
with open(lee_train_file) as f:
    for i, line in enumerate(f):
        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=48, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.wv.vocab['penalty'].count
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

line=""""""
dummy text here...
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line) )

model.docvecs.most_similar(inferred_vector, topn=3)
</code></pre>

<p>I tried this with list(inferred_vector) but still getting an error.</p>

<blockquote>
  <p>TypeError: 'numpy.float32' object is not iterable</p>
</blockquote>

<p>I am trying to compare the dummy text with the corpus and find if the entry already exist in the data file.</p>

<hr>

<p>Update:
Instead of list(inferred_vector) I need to use [inferred_vector]. This has solved my problem. But ever-time I run this code, I get different similar documents. How is this possible?</p>

<pre><code>line=""""""
The national executive of the strife-torn Democrats last night appointed little-known West Australian senator Brian Greig 
as interim leader--a shock move likely to provoke further conflict between the party's senators and its organisation. 
In a move to reassert control over the party's seven senators, the national executive last night rejected Aden Ridgeway's 
bid to become interim leader, in favour of Senator John, a supporter of deposed leader Natasha Stott Despoja and an outspoken 
gay rights activist.
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line))

model.docvecs.most_similar([inferred_vector], topn=5)
</code></pre>

<p>Sometimes I get this list and the list keeps changing everytime I run the code even if there is no change in the model.</p>

<pre><code>[(151, 0.5980586409568787),
 (74, 0.5736572742462158),
 (106, 0.5714541077613831),
 (249, 0.5695925951004028),
 (209, 0.5642371773719788)]

[(249, 0.5727256536483765),
 (151, 0.5725511312484741),
 (74, 0.5711895823478699),
 (106, 0.5583171248435974),
 (292, 0.5491517782211304)]
</code></pre>

<p>As a matter of fact, the first line in training corpus is 99% similar to this line because only 1 word is changed. Surprisingly the document_id 1 is nowhere in the top 5 list.</p>
",2018-10-01 04:36:58,2018-10-01 05:42:01,compare documents using most similar method,<nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18541,52612455,2018-10-02 16:19:18,,"<p>I have the following lstm model:</p>

<pre><code>class LSTM_model():

def __init__(self):
    w2v_model = gensim.models.Word2Vec(sentences, size=150, window=10, min_count=2, workers=10)
    pretrained_weights = w2v_model.wv.syn0
    vocab_size, emdedding_size = pretrained_weights.shape
    self.w2v_model = w2v_model
    self.keras_lstm_model = Sequential()
    self.keras_lstm_model.add(Embedding(input_dim = vocab_size, output_dim = emdedding_size, weights = [pretrained_weights]))
    self.keras_lstm_model.add(LSTM(units = emdedding_size))
    self.keras_lstm_model.add(Dense(units = vocab_size))
    self.keras_lstm_model.add(Activation('sigmoid'))
    self.keras_lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['mae','acc'])
</code></pre>

<p>My aim is to predict probablity of a given word in a context.
I have list of sentences and I want to train this model: </p>

<pre><code>   def train_lstm_model(self, sentences):

        sentences_as_indexes = []


        filtered_sentences = list(filter(lambda x : all([w in self.w2v_model.wv.vocab for w in x]) , sentences)) #filter to take only sentences with no OOV words
        for sentence in filtered_sentences:
            if all([w in self.w2v_model.wv.vocab for w in sentence]): #todo use filter
                indexes_row = []
                for word in sentence:

                    idx = self.w2v_model.wv.vocab.get(word).index
                    indexes_row.append(idx)

                sentences_as_indexes.append(indexes_row)
        X = pd.DataFrame([sentence[:-1] for sentence in sentences_as_indexes])
        y = pd.DataFrame([sentence[-1] for sentence in sentences_as_indexes])
        print(datetime.datetime.now(), "": Fitting LSTM model , size of X is "", X.shape)

        self.keras_lstm_model.fit(X, y) #HERE the error
</code></pre>

<p>Where sentences is list of around 1M sentences (my training set).
I Get the following error in the fit():</p>

<blockquote>
  <p>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[7,38] = -2147483648 is not in [0, 694415)
       [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@training/Adam/Assign_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]</p>
</blockquote>

<p>I though this might be because I have data that is OOV but as you can see - I filtered the sentences to take only those with all the word in the vocab.</p>

<p>What may cause this error?</p>

<p>Thanks!</p>
",2018-10-09 08:57:29,2018-10-09 08:57:29,tensorflow.python.framework.errors_impl.InvalidArgumentError: in KERAS LSTM model,<python><tensorflow><keras><nlp><lstm>,,,CC BY-SA 4.0,False,False,True,False,False
18564,52752804,2018-10-11 05:28:03,,"<p>I am trying to load the fasttext file to use it as word embedding first time. I have this:</p>

<pre><code>KeyedVectors.load_word2vec_format(binary_file_path,
binary=True, encoding='utf-8', unicode_errors='ignore')
</code></pre>

<p>I also tried what is described here: <a href=""https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"">https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim</a>
Still same results
I have downloaded the .bin file from kaggle (<a href=""https://www.kaggle.com/kambarakun/fasttext-pretrained-word-vectors-english"" rel=""nofollow noreferrer"">https://www.kaggle.com/kambarakun/fasttext-pretrained-word-vectors-english</a>)
But still I am having the issue:
<code>'utf8' codec can't decode byte 0xba in position 0: invalid start byte</code>
I want to use only the .bin file and not .vec file as it takes less time.</p>
",2018-10-11 14:24:10,2018-10-11 14:24:10,Fasttext UnicodeDecode issue,<python><machine-learning><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18574,52734146,2018-10-10 06:43:12,,"<p>I'm working on a NLP application, where I have a corpus of text files. I would like to create word vectors using the <strong>Gensim word2vec algorithm</strong>. </p>

<p>I did a 90% training and 10% testing split. I trained the model on the appropriate set, but I would like to assess the accuracy of the model on the testing set.</p>

<p>I have surfed the internet for any documentation on accuracy assessment, but I could not find any methods that allowed me to do so. Does anyone know of a function that does accuracy analysis?</p>

<p>The way I processed my test data was that I extracted all the sentences from the text files in the test folder, and I turned it into a giant list of sentences. After that, I used a function that I though was the right one (turns out it wasn't as it gave me this error: <strong>TypeError: don't know how to handle uri</strong>). Here is how I went about doing this:</p>

<pre><code>test_filenames = glob.glob('./testing/*.txt')

print(""Found corpus of %s safety/incident reports:"" %len(test_filenames))

test_corpus_raw = u""""
for text_file in test_filenames:
    txt_file = open(text_file, 'r')
    test_corpus_raw += unicode(txt_file.readlines())
print(""Test Corpus is now {0} characters long"".format(len(test_corpus_raw)))

test_raw_sentences = tokenizer.tokenize(test_corpus_raw)

def sentence_to_wordlist(raw):
    clean = re.sub(""[^a-zA-Z]"","" "", raw)
    words = clean.split()
    return words

test_sentences = []
for raw_sentence in test_raw_sentences:
    if len(raw_sentence) &gt; 0:
        test_sentences.append(sentence_to_wordlist(raw_sentence))

test_token_count = sum([len(sentence) for sentence in test_sentences])
print(""The test corpus contains {0:,} tokens"".format(test_token_count))


####### THIS LAST LINE PRODUCES AN ERROR: TypeError: don't know how to handle uri 
texts2vec.wv.accuracy(test_sentences, case_insensitive=True)
</code></pre>

<p>I have no idea how to fix this last part. Please help. Thanks in advance!</p>
",,2019-07-29 09:11:07,Word2vec Gensim Accuracy Analysis,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18579,52693004,2018-10-07 21:18:10,,"<p>I have a sample of ~60,000 documents.  We've hand coded 700 of them as having a certain type of content.  Now we'd like to find the ""most similar"" documents to the 700 we already hand-coded.  We're using gensim doc2vec and I can't quite figure out the best way to do this.</p>

<p>Here's what my code looks like:</p>

<pre><code>cores = multiprocessing.cpu_count()

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
        epochs=10, workers=cores, dbow_words=1, train_lbls=False)

all_docs = load_all_files() # this function returns a named tuple
random.shuffle(all_docs)
print(""Docs loaded!"")
model.build_vocab(all_docs)
model.train(all_docs, total_examples=model.corpus_count, epochs=5)
</code></pre>

<p>I can't figure out the right way to go forward.  Is this something that doc2vec can do?  In the end, I'd like to have a ranked list of the 60,000 documents, where the first one is the ""most similar"" document.</p>

<p>Thanks for any help you might have!  I've spent a lot of time reading the gensim help documents and the various tutorials floating around and haven't been able to figure it out.</p>

<p>EDIT: I can use this code to get the documents most similar to a short sentence:</p>

<pre><code>token = ""words associated with my research questions"".split()
new_vector = model.infer_vector(token)
sims = model.docvecs.most_similar([new_vector])
for x in sims:
    print(' '.join(all_docs[x[0]][0]))
</code></pre>

<p>If there's a way to modify this to instead get the documents most similar to the 700 coded documents, I'd love to learn how to do it!</p>
",2018-10-07 21:30:30,2018-10-22 19:04:54,Doc2Vec: Similarity Between Coded Documents and Unseen Documents,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18586,52618479,2018-10-03 01:28:32,,"<p>I would like to print each vector of a word by using gensim word2vec.</p>

<p>Here is my code:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
a = [[""man"", ""eater"", ""king""]]
model = Word2Vec(a, size =100, window=1, min_count=1)
model.build_vocab(a, update=True)
model.train(a, total_examples=1, epochs=1)

"""""" I know that I could use:
    for x in a:
        print(model.wv[x])

    This is not I intended because I wanted to know whether is there a possibility
    to loop each element in a list and return it into the model.wv[element] by printing 
    every each element by using only 1 line of code""""""

#For example my intended way
print(model.wv[x for x in a])
</code></pre>

<p>However, this is list comprehensive method and it wont work. I also tried </p>

<pre><code>print(model.wv[lambda x:x, a])
</code></pre>

<p>But still dont work. Could anyone please tell me how can I print every each word vector 
without using the for loop way? I just want it to be 1 line for the print part.
If it is still not clear, please let me know.  </p>
",,2018-10-03 01:39:31,loop each element and print word2vec vectors based on the element,<python><printing><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18591,52662133,2018-10-05 09:10:21,,"<p>I am having problem while running this code in Windows10</p>

<pre><code>word2vec = gensim.models.Word2Vec.load(""word2vec.gensim"")
</code></pre>

<p>and getting this error</p>

<pre><code>word2vec = gensim.models.Word2Vec.load(""word2vec.gensim"")
Traceback (most recent call last):
File ""&lt;ipython-input-2-4557a9b0bbb0&gt;"", line 1, in &lt;module&gt;
word2vec = gensim.models.Word2Vec.load(""word2vec.gensim"")

File ""C:\Users\abhis\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", 
line 1312, in load
model = super(Word2Vec, cls).load(*args, **kwargs)

File ""C:\Users\abhis\Anaconda3\lib\site- 
packages\gensim\models\base_any2vec.py"", line 1244, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)

File ""C:\Users\abhis\Anaconda3\lib\site-
packages\gensim\models\base_any2vec.py"", line 603, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)

File ""C:\Users\abhis\Anaconda3\lib\site-packages\gensim\utils.py"", line 422, 
in load
obj = unpickle(fname)

File ""C:\Users\abhis\Anaconda3\lib\site-packages\gensim\utils.py"", line 
1358, in unpickle
with smart_open(fname, 'rb') as f:

File ""C:\Users\abhis\Anaconda3\lib\site- 
packages\smart_open\smart_open_lib.py"", line 181, in smart_open
fobj = _shortcut_open(uri, mode, **kw)

File ""C:\Users\abhis\Anaconda3\lib\site- 
packages\smart_open\smart_open_lib.py"", line 301, in _shortcut_open
return open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)

FileNotFoundError: [Errno 2] No such file or directory: 'word2vec.gensim'
</code></pre>
",2018-10-06 00:57:36,2018-10-06 00:57:36,Error in word embedding with word2vec in windows,<python><filepath><word2vec><python-os>,,,CC BY-SA 4.0,False,False,True,False,False
18596,52757683,2018-10-11 10:20:39,,"<p>Hello I have some word2vec models generated using Word2Vec java implementation in <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec"" rel=""nofollow noreferrer"">DL4J</a> and saved by calling </p>

<pre><code>writeWord2VecModel(Word2Vec vectors, String path)
</code></pre>

<p>The output of that is a zip file that contains a bunch of txt files.
I can successfully load and use the model in DL4j using </p>

<pre><code>Word2Vec readWord2VecModel(String path)
</code></pre>

<p>I am now trying to read that model in python, using <code>gensim</code></p>

<pre><code>import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('file_path, binary=False)
</code></pre>

<p>But I get the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0 in position 10: invalid continuation byte
</code></pre>

<p>I also tried with binary=True and get same results.</p>

<p>If I extract the model generated by DL4J I get the following files:</p>

<p><a href=""https://i.stack.imgur.com/zMowD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zMowD.png"" alt=""List Of Files""></a></p>

<p>Is there a way to read that model in python <code>genism</code>?</p>
",,2018-10-11 18:15:45,Loading DL4J trained Word2Vec Model into gensim,<python><gensim><word2vec><dl4j>,,,CC BY-SA 4.0,False,False,True,False,False
18606,52724444,2018-10-09 15:22:01,,"<p>I am new to ML field and trying my hands on creating a model which will predict semantic similarity between two sentences.
I am using following approach:</p>

<p>1.Using word2vec model in gensim package vectorise each word present in the sentences in question</p>

<p>2.Calculate the average vector for all words in every sentence/document </p>

<pre><code>import numpy as np
from scipy import spatial

index2word_set = set(model.wv.index2word)

def avg_feature_vector(sentence, model, num_features, index2word_set):
    words = sentence.split()
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    for word in words:
        if word in index2word_set:
            n_words += 1
            feature_vec = np.add(feature_vec, model[word])
    if (n_words &gt; 0):
        feature_vec = np.divide(feature_vec, n_words)
    return feature_vec
</code></pre>

<p>3.Next calculate cosine similarity between these two average vectors</p>

<pre><code>s1_afv = avg_feature_vector('this is a sentence', model=model, 
num_features=300, index2word_set=index2word_set)
s2_afv = avg_feature_vector('this is also sentence', model=model, 
num_features=300, index2word_set=index2word_set)
sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)
print(sim)
</code></pre>

<p>Reference stackoverflow question:
<a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>

<p>Help needed for the following challenge:</p>

<p>As I want to create a model which would predict semantic similarity between two sentences, I am not quite sure about:</p>

<p>1.Which model would be best suited for this problem</p>

<p>2.Next more importantly how to train that model? </p>

<p>Should I create a matrix where each row will contain two sentences: 
sen1 and sen2 and I would vectorise them and calculate cosine similarity(as per the above mentioned approach)</p>

<p>Then for training data:</p>

<p>X_Train: avg vectors for sen1 and sen2 and their cosine similarity value</p>

<p>y_Train(prediction) : a set of binary values(1 or similar if cosine similarity > 0.7 and 0 otherwise)</p>

<p>I am quite confused whether my approach is correct and how to put a proper approach in the form of a working codebase.</p>

<p>Internet and materials available online are my only teachers to learn ML; thus requesting your guidance in help clearing my gap in understanding and help in coming up with a good working model for my problem.</p>
",,2020-02-07 11:42:14,Need help in creating an appropriate model to predict semantic similarity between two sentences,<python><machine-learning><nlp><data-modeling><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18616,52707075,2018-10-08 17:07:49,,"<p>I am training a doc2vec model with multiple tags, so it includes the typical doc ""ID"" tag and then it also contains a label tag ""Category 1."" I'm trying to graph the results such that I get the doc distribution in a 2d (using LargeVis) but am able to color different tags. My problem is that the vectors the model returns exceed the number of training observations by 5 making difficult to align the original tags with the vectors: </p>

<pre><code>In[1]: data.shape 
Out[1]: (17717,5)
</code></pre>

<p>Training the model on 100 parameters  </p>

<pre><code>In[2]: model.docvecs.doctag_syn0.shape
Out[2]: (17722,100) 
</code></pre>

<p>I have no idea whether the 5 additional observations shift the order of the vectors or whether they're just appended to the end. I want to avoid using string tags for the doc IDs because I am preparing this code to use on a much larger dataset.
I found an explanation in a google group <a href=""https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0</a>
which explained that using multiple tags per doc can result in this type of output. However, I haven't been able to find a way to avoid or correct it in any forum or documentation. </p>
",,2018-10-08 18:37:04,Cannot align graph because multiple tag doc2vec returning more items in doctag_syn0 than there are in the training data,<python><machine-learning><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18617,52686366,2018-10-07 07:28:40,,"<p>I train and save a gensim word2vec model:</p>

<pre><code>W2V_MODEL_FN = r""C:\Users\models\w2v.model""

model = Word2Vec(X, size=150, window=3, min_count=2, workers=10)
model.train(X, total_examples=len(X), epochs=50)
model.save(W2V_MODEL_FN)
</code></pre>

<p>And then:</p>

<pre><code>w2v_model = Word2Vec.load(W2V_MODEL_FN)
</code></pre>

<p>On one enviroment it works perfectly but in another I get the error:</p>

<blockquote>
  <p>{AttributeError}Can't get attribute 'Word2VecKeyedVectors' on  module
  'gensim.models.keyedvectors' from
  'C:\Users\Anaconda3_New\envs\ISP_env\lib\site-packages\gensim\models\keyedvectors.py'></p>
</blockquote>

<p>So I guess it might be a package version issue?</p>

<p>But I couldn't figure what it is.
Any ideas?</p>

<p>Thanks!</p>
",,2020-06-05 17:46:41,Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.keyedvectors' >,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18626,52645618,2018-10-04 11:31:10,,"<p>I have trained word2Vec model on a huge corpus using gensim. Then i have tried to find gender bias in the word2vec model. After this I am neutralizing the bias for few specific professional words. Referred the following link for gender de-biasing
<a href=""https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html"" rel=""nofollow noreferrer"">https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html</a>
On de-biasing or neutralizing a word, new vector of the specific word is returned. Is it possible to take such new vectors of existing words and retrain the previous model.
Please let me know if there are any techniques of doing so.
Thanks and Regards
AIMC</p>
",,2018-10-04 11:31:10,retrain existing word2vec model with new word vectors,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18631,52743468,2018-10-10 15:13:01,,"<p>I'm trying to train a gensim sgns model and in the process I measure the loss during which I'm calculating as  </p>

<pre><code>loss = model.running_training_loss / model.corpus_count, 
</code></pre>

<p>however, I noticed that if I change my worker thread I get different losses keeping all other parameters same. Especially if I keep my worker thread a 1 I get a really high loss and If I increase threads I get less loss. An instance</p>

<pre><code>thread  loss
worker=1  20.40519721
worker=10   2.714875407
worker=16  1.239528453
</code></pre>
",,2018-10-10 17:38:48,effect of increase worker thread in gensim word2vec,<multithreading><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18640,52762875,2018-10-11 14:42:14,,"<p>I had been trying to keep an output of topic modeling stable by using mallet as a library in gensim. However, I found out that mallet can set random-seed but I do not see any parameter in gensim to set it.  </p>
",,2019-04-19 21:08:20,How can I set random-seed of topic model using mallet in gensim?,<python><gensim><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
18646,52725193,2018-10-09 16:08:38,,"<p>I did some research and found that gensim has a script to convert glove to word2vec <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/scripts/glove2word2vec.py"" rel=""nofollow noreferrer"">GLove2Wrod2Vec</a>. I am looking to do the opposite.</p>

<p>Is there any simple way to convert using gensim or any other library</p>
",2018-10-09 18:40:03,2018-10-19 11:53:53,How to convert word2vec to glove format,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18651,52876014,2018-10-18 14:12:45,,"<p>I am trying to find best hyperparameters for my trained doc2vec gensim model which takes a document as an input and create its document embeddings. My train data consists of text documents but it doesn't have any labels. i.e. I just have 'X' but not 'y'.</p>

<p>I found some questions here related to what I am trying to do but all of the solutions are proposed for supervised models but none for unsupervised like mine.</p>

<p>Here is the code where I am training my doc2vec model:</p>

<pre><code>def train_doc2vec(
    self,
    X: List[List[str]],
    epochs: int=10,
    learning_rate: float=0.0002) -&gt; gensim.models.doc2vec:

    tagged_documents = list()

    for idx, w in enumerate(X):
        td = TaggedDocument(to_unicode(str.encode(' '.join(w))).split(), [str(idx)])
        tagged_documents.append(td)

    model = Doc2Vec(**self.params_doc2vec)
    model.build_vocab(tagged_documents)

    for epoch in range(epochs):
        model.train(tagged_documents,
                    total_examples=model.corpus_count,
                    epochs=model.epochs)
        # decrease the learning rate
        model.alpha -= learning_rate
        # fix the learning rate, no decay
        model.min_alpha = model.alpha

    return model
</code></pre>

<p>I need suggestions on how to proceed and find best hyperparameters for my trained model using GridSearch or any suggestions about some other technique. Help is much appreciated.</p>
",2018-10-18 14:35:55,2018-10-23 08:00:51,GridSearch for doc2vec model built using gensim,<machine-learning><gensim><grid-search><doc2vec><hyperparameters>,,,CC BY-SA 4.0,False,False,True,False,False
18665,52817087,2018-10-15 12:44:07,,"<p>With this Gensim example in github, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> it provides examples at the end to find simalarities with phrases or keywords, like 'lady gaga' or 'machine learning'. However am looking to find similarity with actual document in plain text file, could this be done? and how can I do it? suppose text file is located on my local laptop in txt format.</p>
",2018-10-15 13:26:00,2018-10-16 02:11:40,Document similarity with doc2vec,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18683,52895284,2018-10-19 15:13:31,,"<p>I am using gensim LDA for topic modelling.
I need to get <strong>the topic distribution of a corpus</strong>, not the individual documents.
Let say I have 1000 documents, which belongs to 10 different categories (let say 100 docs for each category).
After training the LDA model overall 1000 documents, then I want to see what are <strong>the dominant topics</strong> of each category. The following image illustrates my dataset and aim.</p>

<h2><a href=""https://i.stack.imgur.com/BRqsB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRqsB.png"" alt=""enter image description here""></a></h2>

<p>So far I can think of two approaches, but I am not sure either is sane, I will be happy to know if there is a better way of doing it.</p>

<p>In the first approach, I can concatenate the documents of each category into one large document. So there will be only 10 large documents, hence for each document, I will be able to retrieve its topic distribution. </p>

<p>Another approach might be getting the topic distribution of all document, without concatenating documents. Hence for each category, we will have 100 documents topic distributions. To get the dominant topics for each category, I may sum the probability of each topic, and get only a few highest scored topics.
I am not sure any of this approaches are right, what would you suggest?</p>
",,2018-10-21 17:50:12,"Overall topic distribution of a corpus, not individual documents",<nlp><data-science><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18687,52861807,2018-10-17 18:55:52,,"<p>I am trying to make a model with the Gensim library. I am using python 3 and Spyder. I also want to incorporate the wiki corpus. The code is shown below:</p>

<pre><code>enter code hereimport os
import sys
import bz2
import logging
import multiprocessing
import gensim

SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))
DATA_PATH   = os.path.join(SCRIPT_PATH, 'data/')
MODEL_PATH  = os.path.join(SCRIPT_PATH, 'model/')

DICTIONARY_FILEPATH = os.path.join(DATA_PATH, 'wiki-english_wordids.txt.bz2')
WIKI_DUMP_FILEPATH = os.path.join(DATA_PATH, 'enwiki-latest-pages- 
articles.xml.bz2')

if __name__ == '__main__':

# Check if the required files have been downloaded
if not WIKI_DUMP_FILEPATH:
    print('Wikipedia articles dump could not be found..')
    print('Please see README.md for instructions!')
    sys.exit()


# Get number of available cpus
cores = multiprocessing.cpu_count()

if not os.path.exists(MODEL_PATH):
    os.makedirs(MODEL_PATH)

# Initialize logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

if not os.path.isfile(DICTIONARY_FILEPATH):
    logging.info('Dictionary has not been created yet..')
    logging.info('Creating dictionary (takes about 9h)..')

    # Construct corpus
    wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH)

    # Remove words occuring less than 20 times, and words occuring in more
    # than 10% of the documents. (keep_n is the vocabulary size)
    wiki.dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=100000)

    # Save dictionary to file
    wiki.dictionary.save_as_text(DICTIONARY_FILEPATH)
    del wiki

# Load dictionary from file
dictionary = gensim.corpora.Dictionary.load_from_text(DICTIONARY_FILEPATH)

# Construct corpus using dictionary
wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH, dictionary=dictionary)

class SentencesIterator:
    def __init__(self, wiki):
        self.wiki = wiki

    def __iter__(self):
        for sentence in self.wiki.get_texts():
            yield list(map(lambda x: x.decode('utf-8'), sentence))

# Initialize simple sentence iterator required for the Word2Vec model
sentences = SentencesIterator(wiki)

logging.info('Training word2vec model..')
model = gensim.models.Word2Vec(sentences=sentences, size=300, min_count=1, window=5, workers=cores)

# Save model
logging.info('Saving model..')
model.save(os.path.join(MODEL_PATH, 'word2vec.model'))
logging.info('Done training word2vec model!')
</code></pre>

<p>But I am getting the following error:</p>

<pre><code>File ""C:/Users/elli/.spyder-py3/temp.py"", line 60, in &lt;lambda&gt;
yield list(map(lambda x: x.decode('utf-8'), sentence))

AttributeError: 'str' object has no attribute 'decode'
</code></pre>

<p>This code was from github from this link:
<a href=""https://github.com/LasseRegin/gensim-word2vec-model/blob/master/train.py"" rel=""nofollow noreferrer"">https://github.com/LasseRegin/gensim-word2vec-model/blob/master/train.py</a>.</p>

<p>I suspect this should be something simple to sort. Could you please advise?</p>
",2018-10-17 19:02:29,2018-11-01 18:41:05,Gensim Doc2Vec Exception AttributeError: 'str' object has no attribute 'decode',<python><python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18691,52914701,2018-10-21 11:14:26,,"<p>I'm try to get started with the gensim library. My goal is pretty simple. I want to use the keywords extraction provided by gensim on a german text. Unfortunately, i'm failing hard.</p>

<p>Gensim comes with a keywords extraction build in, it is build on TextRank. While the results look good on english text, it seems not to work on german. I simple installed gensim via pypi and used it out of the box. Well such AI Products are usually driven by a model. My guess is that gensim comes with a english model. A word2vec model for german is available on a <a href=""https://github.com/devmount/GermanWordEmbeddings"" rel=""nofollow noreferrer"">github page</a>.</p>

<p>But here i'm stuck, i can't find a way how the summarization module of gensim, which provides the <a href=""https://radimrehurek.com/gensim/summarization/keywords.html"" rel=""nofollow noreferrer"">keywords function</a> i'm looking for, can work with a external model.</p>

<p>So the basic question is, how do i load the german model and get keywords from german text?</p>

<p>Thanks</p>
",,2018-10-21 13:10:32,"Gensim Keywords, how to load a german model?",<nlp><keyword><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18694,52919299,2018-10-21 19:57:40,,"<p>So, I'm messing around with gensim and I've got it to print the top 5 topics and popular nouns associated with the topics (this was done using the example here <a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi"">Topic Distribution and clustering using LDA</a>). I'm working with 51 documents in my case. I'm having difficulty getting my last two clusters to work as I keep receiving a ""list index out of range"" error. I'm completely clueless about what changes I could make to fix my clusters. The method I attempted using if and else conditions gave an incorrect first cluster (you'll spot it commented out). Where exactly am I going wrong?</p>

<pre><code>    from gensim import corpora, models, similarities
from itertools import chain

# list of tokenised nouns from the noun documents
nounTokens = []

for index, row in df_Data.iterrows():
    nounTokens.append(df_Data.iloc[index]['Noun Tokens'])

# create a dictionary using noun Tokens
id2word = corpora.Dictionary(nounTokens)

# creates the bag of word corpus
mm = [id2word.doc2bow(noun) for noun in nounTokens]

# trains lda models
lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=5, update_every=1, chunksize=10000, passes=1)

# prints the topics of the corpus
for topics in lda.print_topics():
    print(topics)
print

lda_corpus = lda[mm]

# search for scores of all the words under each topic for all documents
scores = list(chain(*[[score for topic_id, score in topic] 
                      for topic in [doc for doc in lda_corpus]]))
# calculating the avg sum of all the probabilities to ensure we have a valid threshold.
threshold = sum(scores)/len(scores)
print(threshold)
print
# cluster1 = []
# cluster2 = []
# cluster3 = []

# for i,j in zip(lda_corpus, noun_Docs):
#     if len(i) &gt; 0:
#         if i[0][1] &gt; threshold:
#             cluster1.append(j)
#     elif len(i)&gt;1:
#         if i[1][1] &gt; threshold:
#             cluster2.append(j)
#     elif len(i) &gt; 2:
#         if i[2][1] &gt; threshold:
#             cluster3.append(j)

cluster1 = [j for i, j in zip(lda_corpus, noun_Docs) if i[0][1] &gt; threshold]
cluster2 = [j for i, j in zip(lda_corpus, noun_Docs) if i[1][1] &gt; threshold]
cluster3 = [j for i, j in zip(lda_corpus, noun_Docs) if i[2][1] &gt; threshold]
# for i,j in zip(lda_corpus, noun_Docs):
#     print(i)

print(cluster1)
# print(cluster2)
# print(cluster3)
</code></pre>
",,2018-10-21 19:57:40,Topic distribution: Problem with clustering my documents using LDA,<python><nltk><gensim><lda>,,,CC BY-SA 4.0,True,False,True,False,False
18696,52785462,2018-10-12 18:48:56,,"<p>While implementing <code>Fasttext</code> in Python 3.7, I am facing an unexpected scenario related to <code>Exception in thread</code>, which leads to </p>

<blockquote>
  <p>NoneType' object is not subscriptable</p>
</blockquote>

<p>The error (screenshot) of full stack trace is as follows: 
<a href=""https://i.stack.imgur.com/MhNgz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MhNgz.png"" alt=""enter image description here""></a></p>

<p>What exactly is this issue in  gensim python?</p>

<p>The code I have tried:</p>

<pre><code>import nltk, re
import string
from collections import Counter 
from string import punctuation
from nltk.tokenize import word_tokenize
from nltk.corpus import gutenberg, stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import FastText

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    #remove punctuation from each word
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])

    stop=sw+sw1
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
freq_count()

def intialize_word_embedding():
    model = FastText([words], size = 100, sg = 1, window = 5, min_count = 5, workers = 4)
    model.train([words], total_examples=len(words), epochs=10)
    model.init_sims(replace=True)
    model_name = ""mcft""
    model.save(model_name)
    print(len(model.wv.vocab))
intialize_word_embedding()
def load_model():
    model = FastText.load('mcft')
    similarities = model.wv.most_similar('hamlet')
    for word, score in similarities:
        print(word , score)
    print(model.wv.similarity('hamlet', 'king'))
load_model()
</code></pre>

<p>Note: The model works perfectly, when I comment the </p>

<pre><code>model.train([words], total_examples=len(words), epochs=10)`
</code></pre>

<p>line in the above shown code.</p>
",2018-10-13 08:44:10,2018-10-13 08:44:10,gensim error : 'NoneType' object is not subscriptable during training in Fasttext,<python><python-3.x><nltk><gensim><fasttext>,,,CC BY-SA 4.0,True,False,True,False,False
18701,52866407,2018-10-18 03:06:43,,"<p>I am working on data-set with more than 100,000 records.
This is how the data looks like:</p>

<pre><code>email_id    cust_id campaign_name
123         4567     World of Zoro
123         4567     Boho XYz
123         4567     Guess ABC
234         5678     Anniversary X
234         5678     World of Zoro
234         5678     Fathers day
234         5678     Mothers day
345         7890     Clearance event
345         7890     Fathers day
345         7890     Mothers day
345         7890     Boho XYZ
345         7890     Guess ABC
345         7890     Sale
</code></pre>

<p>I am trying to understand the campaign sequence and predict the next possible campaign for the customers.</p>

<p>Assume I have processed my data and stored it in 'camp'.</p>

<p>With Word2Vec-</p>

<pre><code>from gensim.models import Word2Vec

model = Word2Vec(sentences=camp, size=100, window=4, min_count=5, workers=4, sg=0)
</code></pre>

<p>The problem with this model is that it accepts tokens and spits out text-tokens with probabilities in return when looking for similarities.</p>

<p>Word2Vec accepts this form of input- </p>

<pre><code>['World','of','Zoro','Boho','XYZ','Guess','ABC','Anniversary','X'...]
</code></pre>

<p>And gives this form of output -</p>

<pre><code> model.wv.most_similar('Zoro')
[Guess,0.98],[XYZ,0.97]
</code></pre>

<p>Since I want to predict campaign sequence, I was wondering if there is anyway I can give below input to the model and get the campaign name in the output</p>

<p>My input to be as - </p>

<pre><code>[['World of Zoro','Boho XYZ','Guess ABC'],['Anniversary X','World of 
Zoro','Fathers day','Mothers day'],['Clearance event','Fathers day','Mothers 
day','Boho XYZ','Guess ABC','Sale']]
</code></pre>

<p>Output - </p>

<pre><code>model.wv.most_similar('World of Zoro')
[Sale,0.98],[Mothers day,0.97]
</code></pre>

<p>I am also not sure if there is any functionality within the Word2Vec or any similar algorithms which can help predicting campaigns for individual users.</p>

<p>Thank you for your help.</p>
",,2018-10-18 08:51:48,Sequence Models Word2vec,<machine-learning><nlp><sequence><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18702,52840791,2018-10-16 17:16:54,,"<p>I have installed gensim for Windows by the command:</p>

<blockquote>
  <p>conda install -c anaconda gensim</p>
</blockquote>

<p>I have an environment py35. I'm trying to run a Python script using the import statement: <code>from gensim.models import Word2Vec</code> where there is an error <code>ImportError: No module named 'gensim'</code>. Any idea what to be done?</p>
",,2018-10-17 11:23:40,ImportError: No module named 'gensim',<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18703,52842474,2018-10-16 19:11:06,,"<p>I am trying to build doc2vec model, using gensim + sklearn to perform sentiment analysis on short sentences, like comments, tweets, reviews etc.</p>

<p>I downloaded <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow noreferrer"">amazon product review data set</a>, <a href=""https://www.kaggle.com/c/twitter-sentiment-analysis2"" rel=""nofollow noreferrer"">twitter sentiment analysis data set</a> and <a href=""https://www.kaggle.com/utathya/imdb-review-dataset"" rel=""nofollow noreferrer"">imbd movie review data set</a>.</p>

<p>Then combined these in 3 categories, positive, negative and neutral.</p>

<p>Next I trinaed gensim doc2vec model on the above data so I can obtain the input vectors for the classifying neural net.</p>

<p>And used sklearn LinearReggression model to predict on my test data, which is about 10% from each of the above three data sets.</p>

<p>Unfortunately the results were not good as I expected. Most of the tutorials out there seem to focus only on one specific task, 'classify amazon reviews only' or 'twitter sentiments only', I couldn't manage to find anything that is more general purpose.</p>

<p>Can some one share his/her thought on this? </p>
",,2018-10-16 21:35:58,Data set for Doc2Vec general sentiment analysis,<dataset><artificial-intelligence><gensim><sentiment-analysis><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
18704,52845345,2018-10-16 23:35:08,,"<p>I am trying to implement the following code:</p>

<pre><code>import os
os.environ.update({'MALLET_HOME':r'c:/mallet-2.0.8/'})

mallet_path = 'C:\\mallet-2.0.8\\bin\\mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow, num_topics=20, id2word=dictionary)
</code></pre>

<p>However, I keep getting this error: </p>

<blockquote>
  <p>CalledProcessError: Command 'C:\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\Joshua\AppData\Local\Temp\98094d_corpus.txt --output C:\Users\Joshua\AppData\Local\Temp\98094d_corpus.mallet' returned non-zero exit status 1.</p>
</blockquote>

<p>I previously was able to execute this code on my laptop with the same directories yet it does not execute on my PC (where I am currently running python). </p>

<p>Could someone please let me know what I am doing wrong?</p>
",,2019-04-02 17:54:16,LDA Mallet CalledProcessError,<python-3.x><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
18714,52827465,2018-10-16 03:14:54,,"<p>I am using <code>gensim</code> to do LDA on a corpus of arXiv abstracts in the category <a href=""https://arxiv.org/list/stat.ML/recent"" rel=""nofollow noreferrer"">stats.ML</a></p>

<p>My problem is that there is a lot of overlap between the topics (whether I pick 5, 10, or 50 topics). Every topic has a distribution of words like ""model"" ""algorithm"", or ""problem."" How can topics be considered differentiable if so many of them prominently feature the same terms? </p>

<p>Using pyLDAvis was instructive for me . This is the distribution for topic #3:
<a href=""https://i.stack.imgur.com/tLWCn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tLWCn.png"" alt=""topics with lambda equal one""></a></p>

<p>But when I turn down <code>lambda = 0.08</code>, the actual nature of the topic emerges (ML in medical applications):
<a href=""https://i.stack.imgur.com/rHyWn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rHyWn.png"" alt=""topics with low lambda""></a></p>

<p>So my question is, how could I uncover these distinctive terms in the course of training my LDA model (without pyLDAvis)? And also, does the performance (as opposed to interpret-ability) of the model improve if I can get it to ignore these common, non-discriminating terms?</p>

<p>I have several ideas to try but would like more guidance:</p>

<ul>
<li>Filtering the 50 most common terms from my dictionary. While I think it helped a bit, I'm not sure if it's the right approach</li>
<li>Tweaking <code>eta</code> parameter in <code>gensim.models.LdaModel</code></li>
</ul>

<p>My goal is ultimately to take a new document and do word coloring on it based on which words relate to which topics, and also offer the documents most similar to the input document.</p>

<p>I am pretty new with <code>gensim</code>, and this is my first SO question, so if I'm totally off-base with something, please let me know ;-). Thank you</p>
",,2018-10-16 03:14:54,How to limit LDA topics to terms that are distinct?,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
18746,52938047,2018-10-22 21:32:25,,"<p>hope you're having a good day.</p>
<p>I am trying to instantiate a Doc2Vec model by implementing the following code:</p>
<pre><code>model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)
model_dbow.build_vocab([x for x in tqdm(all_data)])
</code></pre>
<p>However, this returns the following error:</p>
<blockquote>
<p>C:\Users\Joshua\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.</p>
<p>100%|| 10177/10177 [00:00&lt;00:00, 322714.39it/s]</p>
</blockquote>
<p>I tried to resolve it by installing the Microsoft Visual C++ compiler as noted here: <a href=""https://wiki.python.org/moin/WindowsCompilers"" rel=""nofollow noreferrer"">https://wiki.python.org/moin/WindowsCompilers</a></p>
<p>However, it does not seem to help. I also tried to uninstall and re-install Gensim as was advised in the UserWarning but that also didn't help.</p>
<p>Any suggestions? I am unfamiliar with using C compilers so maybe I'm missing something here.</p>
<p>This is what it should look like if it works; refer to line 109 in the notebook: <a href=""https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb"" rel=""nofollow noreferrer"">https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb</a></p>
<p>Thank you in advance.</p>
",2020-06-20 09:12:55,2019-05-03 22:55:14,Doc2Vec C-compiler User Warning,<python-3.x><visual-c++><nlp><anaconda><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18757,53037373,2018-10-29 00:39:53,,"<pre><code>import gensim
corpus = [[""a"",""b"",""c""],[""a"",""d"",""e""],[""a"",""f"",""g""]]
from gensim.corpora import Dictionary
dct = Dictionary(corpus)
print(dct)
dct.filter_extremes(no_below=1)
print(dct)
</code></pre>

<p>When I ran the code above, my output was -</p>

<pre><code>Dictionary(7 unique tokens: ['a', 'b', 'c', 'd', 'e']...)
Dictionary(6 unique tokens: ['b', 'c', 'd', 'e', 'f']...)
</code></pre>

<p>I supposed that since 'a' occurs in two documents, it should not be removed. However, this is not the case. Am I missing something?</p>
",2018-10-29 00:50:53,2018-10-29 00:57:02,Misunderstanding the use of filter_extreme in gensim,<python><python-2.7><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18763,52979374,2018-10-24 23:42:34,,"<p>I have just over 100k word embeddings which I created using gensim, originally each containing 200 dimensions. I've been trying to visualize them within tensorboard's projector but I have only failed so far.
My problem is that tensorboard seems to freeze while computing PCA. At first, I left the page open for 16 hours, imagining that it was just too much to be calculated, but nothing happened. At this point, I started to try and test different scenarios just in case all I needed was more time and I was trying to rush things. The following is a list of my testing so far, all of which failed at the same spot, computing PCA:</p>

<ul>
<li>I plotted only 10 points of 200 dimensions;</li>
<li>I retrained my gensim model so that I could reduce its dimensionality to 100;</li>
<li>Then I reduced it to 10;</li>
<li>Then to 2;</li>
<li>Then I tried plotting only 2 points, i.e. 2 two dimensional points;</li>
</ul>

<p>I am using Tensorflow 1.11;
You can find my last saved tensor flow session <a href=""https://drive.google.com/open?id=10Cnzc2RH9nDFUYrf51r6d8d0jsl0-H6Y"" rel=""nofollow noreferrer"">here</a>, would you mind trying it out?</p>

<p>I am still a beginner, therefore I used a couple tutorial to get me started; I used <a href=""https://github.com/sudharsan13296/visualise-word2vec/blob/master/Word2vec%20Embeddings.ipynb"" rel=""nofollow noreferrer"">Sud Harsan</a> work so far.</p>

<p>Any help is much appreciated. Thanks.</p>

<hr>

<p>Updates:</p>

<p>A) I've found someone else <a href=""https://stackoverflow.com/questions/44054907/tensorfboard-embeddings-hangs-with-computing-pca"">dealing with the same problem</a>; I tried the solution provided, but it didn't change anything. </p>

<p>B) I thought it could have something to do with my installation, therefore I tried uninstalling tensorflow and installing it back; no luck. I then proceeded to create a new environment dedicated to tensorflow and that also didn't work. </p>

<p>C) Assuming there was something wrong with my code, I ran <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">tensorflow's basic embedding tutorial</a> to check if I could open its projector's results. And guess what?! I still can't go past ""Calculating PCA""</p>

<p>Now, I did visit <a href=""https://projector.tensorflow.org"" rel=""nofollow noreferrer"">the online projector example</a> and that loads perfectly. </p>

<p>Again, Any help would be more than appreciated. Thanks!</p>
",2018-11-02 20:03:05,2019-01-22 19:38:28,Tensorboard projector will compute PCA endlessly,<tensorflow><pca><gensim><tensorboard>,,,CC BY-SA 4.0,False,False,True,False,False
18770,52893017,2018-10-19 13:08:32,,"<p>Beginner here. </p>

<p>I have a large body of .txt files that I want to train a Doc2Vec model on. However, I am having trouble importing the data into python in a usable way.</p>

<p>To import data, I have used:</p>

<pre><code>docLabels = []
docLabels = [f for f in listdir(PATH TO YOU DOCUMENT FOLDER) if 
f.endswith(.txt)]
data = []
for doc in docLabels:
    data.append(open(PATH TO YOU DOCUMENT FOLDER + doc).read()) `
</code></pre>

<p>However, with this, I get a ""list"", which I can do no further work with. I cannot seem to find how to import text files in a way they can be used with NLTK / doc2vec anywhere on SO or in tutorials.</p>

<p>Help would be greatly appreciated. Thank you!</p>
",2018-10-19 13:21:41,2018-10-19 16:03:37,gensim Doc2Vec: Getting from txt files to TaggedDocuments,<python><gensim><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
18782,52941179,2018-10-23 04:28:28,,"<p>I am using python gensim to create word2vec for my 93 million sentences. However, when I train my model, I am getting three files as output with extensions .bin.trainables.syn1neg.npy and .bin.wv.vectors.npy in addition to .bin. I went through the answer provided here: <a href=""https://stackoverflow.com/q/47173538/9526057"">Why are multiple model files created in gensim word2vec?</a> which gives reasoning of why this happens. However I would like to know if there is a way to convert these files into a normal single bin file?</p>
",,2018-10-23 09:52:50,gensim creates files with extension .bin.trainables.syn1neg.npy and .bin.wv.vectors.npy in addition to .bin,<python-2.7><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18784,52947708,2018-10-23 11:17:20,,"<p>I am making use of the HDP implementation by Gensim to infer the topics of a dataset, but I have a question regarding the truncation level. </p>

<p>Is there a way to infer the most appropriate truncation level? I have noticed that the final number of topics is dependent on the value for truncation level selected. </p>
",2018-10-23 13:32:21,2018-10-23 13:32:21,Hierarchical Dirichlet Process - Inferring Truncation Level,<lda><topic-modeling><dirichlet><hdp>,,,CC BY-SA 4.0,False,False,True,False,False
18786,52982761,2018-10-25 06:33:39,,"<p>I want to train word2vec and fasttext to get vectors for a specific dataset that I have.</p>

<p>What should my model take as input?</p>

<p>My file is like this:</p>

<pre><code>Customer_4: I want to book a ticket to New York.
Agent_9: Okay, when do you want the tickets for
Customer_4: hmm, wait a sec
Agent_9: Sure
Customer_4: When is the least expensive to fly
</code></pre>

<p>Now, How should I prepare my data for word2vec to run? Does the word2vec model take inter sentence similaarity into account, i.e. should i not prepare the corpus sentence wise.</p>
",,2018-10-28 23:51:00,How to prepare data for word2vec in gensim and fasttext?,<python><machine-learning><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18797,53004827,2018-10-26 08:42:37,,"<p>I understand that you treat the paragraph ID as a new word in doc2vec (DM approach, left on the figure) during training. The training output is the context word. After a model is trained, suppose I want to get 1 embedding given a new document. </p>

<p>Do I feed each word to the network and then average it to get the embedding? Or is there another way? </p>

<p>I can feed this to gensim, but I am trying to understand how it works. </p>

<p><a href=""https://i.stack.imgur.com/t7slV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t7slV.png"" alt=""enter image description here""></a></p>
",,2019-04-06 12:39:48,Doc2vec predictions - do we average the words or what is the paragraph ID for a new paragraph?,<nlp><word2vec><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18817,52989568,2018-10-25 12:42:57,,"<p>I'm a little bit confused about the comments to alpha in the documentation of LDA (Gensim).</p>

<p>In the ""regular"" Gensim LdaModel it says that if one sets alpha = 'asymmetric', Gensim uses a ""fixed normalized asymmetric prior of 1.0 / topicno"" (topicno is num_topics, right?!). But why it is called asymmetric? Isn't that the symmetric case? (see <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a>)</p>

<p>And whats the default number for alpha used by Mallet? 50? If so, why? As far as i know one should choose some value &lt;1 to get good results.
(see <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/wrappers/ldamallet.html</a>)</p>
",,2018-10-25 12:42:57,LDA Gensim/Mallet documentation on alpha,<gensim><lda><mallet><dirichlet>,,,CC BY-SA 4.0,False,False,True,False,False
18820,53030121,2018-10-28 09:37:13,,"<p>I am using Google-App-Engine standard (Not flex) Enviroment with Python2.7, and I need to load some pre-trained models (Gensim's Word2vec and Keras's LSTM).</p>

<p>I need to load it once (since it very slow - takes around 1.5 seconds) and keep it in faster access for several hours.</p>

<p>What is the best &amp; fastest way to do so? </p>

<p>Thanks!</p>
",,2018-10-28 13:48:45,How to load files to Google-App-Engine in standard enviroment,<python><google-app-engine><keras><gensim><google-app-engine-python>,,,CC BY-SA 4.0,False,False,True,False,False
18846,53015716,2018-10-26 20:13:05,,"<p>I am pretty new at topic modeling and Gensim. So, I am still trying to understand many of concepts. I am trying to run gensim's LDA model on my corpus that contains around 25,446,114 tweets. I created a streaming corpus and id2word dictionary using gensim. I am using num_topics = 100, chunk size = 85000 (loading 85000 tweets at a time)</p>

<p>I am using 
Gensim : 3.5.0
Numpy: 1.15.3</p>

<p>Here is the link to corpus and id2word dictionary: <a href=""https://drive.google.com/drive/folders/1FrJ8gJbiDqp3VC5syOjRVcQPcESdYOYa?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1FrJ8gJbiDqp3VC5syOjRVcQPcESdYOYa?usp=sharing</a></p>

<p>I don't know what I am doing wrong or how to solve this. The topic diff first hits inf and then nan , and I start getting same topic. 
Please help !!</p>

<p>Here is the code: </p>

<pre><code>import pprint
import logging
import gensim
logging.basicConfig(filename='gensim.log',
                    format=""%(asctime)s:%(levelname)s:%(message)s"",
                    level=logging.INFO)
corpus = gensim.corpora.MmCorpus('disasterTweets.mm')
id2word = gensim.corpora.Dictionary.load('disasterTweets.dict')
id2word.filter_tokens(bad_ids=[id2word.token2id['eofeofeof']])
print('eofeofeof' in id2word.token2id)

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       chunksize=85000,
                                       num_topics=100)
pprint.pprint(lda_model.print_topics())
</code></pre>

<p>Here are the errors I am receiving: </p>

<pre><code>/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log
 diff = np.log(self.expElogbeta)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:690: RuntimeWarning: overflow encountered in add
  sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:700: RuntimeWarning: invalid value encountered in multiply
  sstats *= self.expElogbeta
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:690: RuntimeWarning: overflow encountered in add
  sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:700: RuntimeWarning: invalid value encountered in multiply
  sstats *= self.expElogbeta
Process ForkPoolWorker-30:
Traceback (most recent call last):
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap
    self.run()
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/pool.py"", line 105, in worker
    initializer(*initargs)
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamulticore.py"", line 333, in worker_e_step
    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py"", line 725, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py"", line 662, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 287500 is out of bounds for axis 1 with size 287500
</code></pre>
",,2018-10-28 23:48:08,Gensim LDA model topic diff resulting in nan,<python><python-3.x><numpy><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
18872,53142322,2018-11-04 15:23:48,,"<p>I tried to reproduce <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">this tutorial</a> on my local machine to get used to gensim fasttext functionalities. Fasttext and gensim libraries are correctly installed. By calling the train method of gensim fasttext wrapper</p>

<pre><code>model_wrapper = FT_wrapper.train(ft_home, lee_train_file)
</code></pre>

<p>I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
&lt;ipython-input-19-0815ab031d23&gt; in &lt;module&gt;()
      3 
      4 # train the model
----&gt; 5 model_wrapper = FT_wrapper.train(ft_home, lee_train_file)
      6 
      7 print(model_wrapper)

~/anaconda3/lib/python3.6/site-packages/gensim/models/deprecated/fasttext_wrapper.py in train(cls, ft_path, corpus_file, output_file, model, size, alpha, window, min_count, word_ngrams, loss, sample, negative, iter, min_n, max_n, sorted_vocab, threads)
    240             cmd.append(str(value))
    241 
--&gt; 242         utils.check_output(args=cmd)
    243         model = cls.load_fasttext_format(output_file)
    244         cls.delete_training_files(output_file)

~/anaconda3/lib/python3.6/site-packages/gensim/utils.py in check_output(stdout, *popenargs, **kwargs)
   1795     try:
   1796         logger.debug(""COMMAND: %s %s"", popenargs, kwargs)
-&gt; 1797         process = subprocess.Popen(stdout=stdout, *popenargs, **kwargs)
   1798         output, unused_err = process.communicate()
   1799         retcode = process.poll()

~/anaconda3/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    707                                 c2pread, c2pwrite,
    708                                 errread, errwrite,
--&gt; 709                                 restore_signals, start_new_session)
    710         except:
    711             # Cleanup if the child failed starting.

~/anaconda3/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)
   1342                         if errno_num == errno.ENOENT:
   1343                             err_msg += ': ' + repr(err_filename)
-&gt; 1344                     raise child_exception_type(errno_num, err_msg, err_filename)
   1345                 raise child_exception_type(err_msg)
   1346 

PermissionError: [Errno 13] Permission denied: '/Users/marcomattioli/fastText'
</code></pre>

<p>Note that I have <strong>-rwxr-xr-x</strong> rights on the fasttext executable. Any help appreciated how to fix this.</p>
",,2019-11-05 08:32:16,Gensim fasttext wrapper returns permission error 13 while model training,<file-permissions><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18901,53076731,2018-10-31 05:15:59,,"<p>What I am trying to do is get a score of the likelihood of a search/query term in a single document/text/paragraph.</p>

<p>The score should tell how much the text is talking about the query term.</p>

<p>Here is what I have tried but failed:</p>

<pre><code>def score(text_data,query):

    texts = [str(doc).encode('utf-8').lower().split() for doc in text_data]

    dictionary = Dictionary(texts)
    corpus = [dictionary.doc2bow(line) for line in texts]

    tfidf_model = TfidfModel(corpus)

    query_vec = dictionary.doc2bow(query.lower().split())
    query_vec = tfidf_model[query_vec]

    index = similarities.MatrixSimilarity(tfidf_model[corpus])

    x = tfidf_model[query_vec]
    sims = index[x]
    score = list(sims)

    return score
</code></pre>

<p>I only need 1 value of the likelihood of the search term in the text data, between 0 to 1. What am I doing incorrectly ?</p>
",2019-03-07 15:20:52,2019-03-07 15:20:52,Text similarity score using a single query on a single document in Gensim,<python><gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
18905,53183341,2018-11-07 03:51:37,,"<p>I am having trouble converting a fast FastText vector back to a word.
Here is my python code: </p>

<pre><code>from gensim.models import KeyedVectors
en_model = KeyedVectors.load_word2vec_format('wiki.en/wiki.en.vec')
vect = en_model.get_vector(""turtles"")
</code></pre>

<p>How can I take the vector (especially an arbitrary vector with the proper dimensions) and have it spit out a word?</p>
",,2018-11-07 04:06:36,Converting Fasttext vector to word,<python><nlp><data-science><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
18909,53187257,2018-11-07 10:05:24,,"<p>I'm pretty new to MySQL, Gensim, and Word2Vec, and I'm still learning how to use by working on my personal project.</p>

<p>I have data that I got by doing web scrapping so it's not hard coded.
(I used Instagram account to get hashtag data from several post, so my data is 
Instagram hashtags)</p>

<p>I'm trying to use that data in this code below:</p>

<pre><code>import pymysql.cursors
import re
from gensim.models import Word2Vec

# Connect to the database
connection = pymysql.connect(host=secrets[0],
user=username,
password=password,
db='test',
charset='charsetExample',
cursorclass=pymysql.cursors.DictCursor)

try:
    # connection to database
    with connection.cursor() as cursor:
    # cursor is iterator / 'Select' - caption is column 
     # post is the table 
     cursor.execute(""SELECT caption FROM posts LIMIT 1000"")
     data = cursor.fetchall()
     # list of captions
      captions = [d['caption'].lower() for d in data]
     # hashtags = [re.findall(r""#([A-Za-z_0-9]+)"", caption) for caption in captions]
    # hashtags = [hashtag for hashtag in hashtags if hashtag != []]
    model = Word2Vec(captions, min_count=1)
    model = Word2Vec(hashtags) 
    res = model.wv.most_similar(""fitness"")

    print(captions)
    print(res)

finally:
    connection.close()
</code></pre>

<p>This is the part that I'm working on and not really sure how to do:</p>

<pre><code>res = model.wv.most_similar(""fitness"")
</code></pre>

<p>For now I was trying to use <code>most_similar()</code> method to see how it works.
What I'm trying to do is in the <code>most_similar(""value"")</code> I want to use my data 
which will be each hashtags that I got by scrapping the Instagram website as the value. </p>

<p>Thank you!</p>
",2018-11-07 12:55:55,2018-11-07 13:15:24,How to use scrapped data from website to Word2vec Gensim,<python><mysql><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18913,53159443,2018-11-05 17:38:13,,"<p>I have a list of sentences and I want to perform some action on two sentences each time, but not for al of the sentences.</p>

<p>for example:</p>

<pre><code>list= [""aaaaa"",""bbbbb"",""ccccc"",""ddddd"",""eeeee""]
similarity_a-d = sim(""aaaaa"",""ddddd"")
similarity_a-e = sim(""aaaaa"",""eeeee"")
similarity_b-d = sim(""bbbbb"",""ddddd"")
similarity_b-e = sim(""bbbbb"",""eeeee"")
similarity_c-d = sim(""ccccc"",""ddddd"")
similarity_c-e = sim(""ccccc"",""eeeee"")
</code></pre>

<p>That's what I tried:</p>

<pre><code>similarity={}
for i,vec_lda_topic in enumerate(vec_lda_topics)[:numOfUSs]:
    for j,vec_lda_topic in enumerate(vec_lda_topics)[numOfUSs:]:
        similarity[""sim{0}-{1}"".format(i,j)] = gensim.matutils.cossim(vec_lda_topics[i], vec_lda_topics[j])
        print('similarity between docs ', i, ' and ',j,': ', similarity[""sim{0}-{1}"".format(i,j)])
</code></pre>

<p>and receive the following error:</p>

<pre><code>TypeError: 'enumerate' object is not subscriptable
</code></pre>

<p>And besides the error, maybe there is a better way to do this?</p>
",2018-11-05 17:45:28,2018-11-05 17:45:28,nested loop over list and dynamically create variables,<python><python-3.x><for-loop><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18914,53160763,2018-11-05 19:16:19,,"<p>I am working on a text classification use case. The text is basically contents of legal documents, for example, companies annual reports, W9 etc. So there are 10 different categories and 500 documents in total. Therefore 50 documents per category. So the dataset consists of 500 rows and 2 columns, 1st column consisting of text and 2nd column is the Target. </p>

<p>I have built a basic model using TF-IDF for my textual features. I have used Multinomial Naive Bayes, SVC, Linear SGD, Multilayer Perceptron, Random Forest. These models are giving me an F1-score of approx 70-75%.</p>

<p>I wanted to see if creating word-embedding will help me improve the accuracy. I trained the word vectors using gensim Word2vec, and fit the word vectors through the same ML models as above, but I am getting a score of about 30-35%. I have a very small dataset and lot of categories, is that the problem? Is it the only reason, or there is something I am missing out? </p>
",2018-11-05 20:02:46,2019-06-04 18:37:54,Word embeddings perform poorly for text classification,<nlp><word2vec><text-classification><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
18928,53130024,2018-11-03 09:26:21,,"<p>I am trying to train <code>Gensim</code> library's <code>word2vec</code> on <code>NLTK</code> <code>Brown Corpus</code>, But facing issues while setting the path to the corpus</p>

<p><strong>Code</strong></p>

<pre><code>from gensim.models import word2vec

sentences = word2vec.BrownCorpus('/nltk_data/corpora/brown')

model = word2vec.Word2Vec(sentences, min_count=1)
</code></pre>

<p><strong>Error</strong></p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory: '/nltk_data/corpora/brown'</p>
</blockquote>

<p>I have checked, Brown corpus data is present at the above path.</p>

<p>I know there is one another way to train Gensim word2vec on NLTK Brown Corpus as follows, But I want to know why the above method doesn't work</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import brown

sentences = brown.sents()

model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Feel free to drop any thoughts</p>
",,2018-11-03 09:26:21,Trouble training Gensim word2vec on NLTK Brown Corpus,<python><nltk><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
18930,53130738,2018-11-03 11:10:38,,"<p>When running the below code. this Python 3.6, latest Gensim library in Jupyter</p>

<pre><code>for model in models:
       print(str(model))
       pprint(model.docvecs.most_similar(positive=[""Machine learning""], topn=20))
</code></pre>

<p>[1]: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a><img src=""https://i.stack.imgur.com/wI25F.png"" alt=""enter image description here""></p>
",2018-11-04 03:31:10,2018-11-05 14:09:27,"Gensim example, TypeError:between str and int error",<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18939,53192902,2018-11-07 15:46:16,,"<p>I am using online LDA to perform some topic modeling task. I am using the core code based on the paper Original Online LDA paper: Hoffman, Blei and Bach, ""Online Learning for Latent Dirichlet Allocation."" NIPS, 2010. and the code is available at  : <a href=""https://github.com/blei-lab/onlineldavb"" rel=""nofollow noreferrer"">https://github.com/blei-lab/onlineldavb</a>. </p>

<p>I am using a train set of ~167000 documents. The code generates lambda files as output which I use to generate the topics(<a href=""https://github.com/wellecks/online_lda_python"" rel=""nofollow noreferrer"">https://github.com/wellecks/online_lda_python</a> , printtopics.py).But I am not sure how I can use it to find topics on new test data ( similar to model.get_document_topics in gensim ). 
Please help to resolve my confusion. </p>
",2018-11-09 05:01:25,2018-11-09 11:04:01,Using online LDA to predict on test data,<python><algorithm><lda><topic-modeling><dirichlet>,,,CC BY-SA 4.0,False,False,True,False,False
18958,53247197,2018-11-11 08:57:45,,"<p>I want to measure the similarity between sentences. Can I use sklearn and Euclidean Distance to measure the semantic similarity between sentences. I read about Cosine similarity also. Can someone explain the difference of those to measures and what is the best approach to use?</p>
",,2018-11-14 13:07:54,Does Euclidean Distance measure the semantic similarity?,<scikit-learn><gensim><euclidean-distance><cosine-similarity><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,True
18967,53158087,2018-11-05 16:11:24,,"<p>I have a problem when using word2vec and lstm, the code is:</p>

<pre><code>def input_transform(string):
    words=jieba.lcut(string)
    words=np.array(words).reshape(1,-1)
    model=Word2Vec.load('lstm_datamodel.pkl')
    combined=create_dictionaries(model,words)
    return combined

def lstm_predict(string):
    print ('loading model......')
    with open('lstm_data.yml', 'r') as f:
        yaml_string = yaml.load(f)
    model = model_from_yaml(yaml_string)

    print ('loading weights......')
    model.load_weights('lstm_data.h5')
    model.compile(loss='binary_crossentropy',
              optimizer='adam',metrics=['accuracy'])
    data=input_transform(string)
    data.reshape(1,-1)
    #print data
    result=model.predict_classes(data)
    if result[0][0]==1:
        print (string,' positive')
    else:
        print (string,' negative')
</code></pre>

<p>and the error is:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Python36\lib\site-packages\gensim\models\word2vec.py"", line 1312, in load
model = super(Word2Vec, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\base_any2vec.py"", line 1244, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\base_any2vec.py"", line 603, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 423, in load
obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 211, in &lt;module&gt;
lstm_predict(string)
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 191, in lstm_predict
data=input_transform(string)
File ""C:/GitHub/reviewsentiment/veclstm.py"", line 177, in input_transform
model=Word2Vec.load('lstm_datamodel.pkl')
File ""C:\Python36\lib\site-packages\gensim\models\word2vec.py"", line 1323, in load
return load_old_word2vec(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 153, in load_old_word2vec
old_model = Word2Vec.load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 1618, in load
model = super(Word2Vec, cls).load(*args, **kwargs)
File ""C:\Python36\lib\site-packages\gensim\models\deprecated\old_saveload.py"", line 88, in load
obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'enter code here
</code></pre>

<p>I am sorry for including so much code. </p>

<p>This is my first time to ask on StackOverflow, and I have tried my very best to find the answer on my own, but failed. So can you help me? Thank you very much!</p>
",2018-11-05 22:10:03,2018-11-06 13:19:34,How do I get word2vec to load a string? problem'dict' object has no attribute '_load_specials',<python-3.x><lstm><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18968,53195906,2018-11-07 18:49:45,,"<p>I'm working on project using Word2vec and gensim,</p>

<pre><code>model = gensim.models.Word2Vec(
    documents = 'userDataFile.txt',
    size=150,
    window=10,
    min_count=2,
    workers=10)
model = gensim.model.Word2Vec.load(""word2vec.model"")
model.train(documents, total_examples=len(documents), epochs=10)
model.save(""word2vec.model"")
</code></pre>

<p>this is the part code that I have at the moment, and I'm getting this error below </p>

<blockquote>
<pre><code>Traceback (most recent call last):
File ""C:\Users\User\Desktop\InstaSubProject\templates\HashtagData.py"", line

37, in &lt;module&gt;
workers=10)
TypeError: __init__() got an unexpected keyword argument 'documents'
</code></pre>
</blockquote>

<p><code>UserDataFile.txt</code> is the file that I stored output result data that I got from web scrapping.</p>

<p>I'm not really sure what I need to fix here.</p>

<p>Thank you in advance !</p>
",2018-11-07 19:07:52,2018-11-07 19:07:52,"Getting ""__init__() got an unexpected keyword argument 'document'"" this error in python I'm working with Word2Vec and gensim",<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18972,53227410,2018-11-09 14:16:51,,"<p>import gensim</p>

<p>Load Google's pre-trained Word2Vec model.</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

embed = model.get_keras_embedding()

print(embed.weights)
</code></pre>

<p>output: []</p>

<p>How to print the weights of embedding layer in Keras, currently it is printing empty list?</p>
",2018-11-09 14:29:10,2018-11-09 14:29:10,How to print the weights of Keras embedding?,<keras><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
18980,53249919,2018-11-11 14:54:07,,"<p>i want to get the cosine similarity between sentences. I have tested doc2vec with gensim and trained it with only few sentences given in the code. But I want to train my model using a text document that have one sentence per each line. How can I use a document with sentences?</p>
",,2018-11-11 15:33:01,How to import a document with sentences to train a doc2vec model?,<python><gensim><cosine-similarity><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
18985,53281744,2018-11-13 13:10:21,,"<p>I am supposed to do some exercises with python glove, most of it doesn't give me any problems but now i am supposed to find the 5 most similar words to ""norway - war + peace"" from the ""glove-wiki-gigaword-100"" package. But when i run my code it just says that the 'word' is not in the vocabulary. Now I'm guessing that this is some kind of formatting, but i don't know how to use it.</p>

<pre><code>import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")  # download the model and return as object ready for use

bests = model.most_similar(""norway - war + peace"", topn= 5)

print(""5 most similar words to 'norway - war + peace':"")

for best in bests:
    print(best)
</code></pre>
",,2018-11-13 13:25:11,glove most similar to multiple words,<python><nlp><gensim><glove>,,,CC BY-SA 4.0,False,False,True,False,False
18988,53265028,2018-11-12 15:12:41,,"<p>I have a code that converts word to vector. Below is my code:</p>

<pre><code># word_to_vec_demo.py

from gensim.models import word2vec
import logging

logging.basicConfig(format='%(asctime)s : \
%(levelname)s : %(message)s', level=logging.INFO)

sentences = [['In', 'the', 'beginning', 'Abba','Yahweh', 'created', 'the',
'heaven', 'and', 'the', 'earth.', 'And', 'the', 'earth', 'was',
'without', 'form,', 'and', 'void;', 'and', 'darkness', 'was',
'upon', 'the', 'face', 'of', 'the', 'deep.', 'And', 'the',
'Spirit', 'of', 'Yahweh', 'moved', 'upon', 'the', 'face',  'of',
'the', 'waters.']]

model = word2vec.Word2Vec(sentences, size=10, min_count=1)

print(""Vector for \'earth\' is: \n"")
print(model.wv['earth'])

print(""\nEnd demo"")
</code></pre>

<p>The output is </p>

<pre><code>Vector for 'earth' is: 

[-0.00402722  0.0034133   0.01583795  0.01997946  0.04112177  0.00291858
-0.03854967  0.01581967 -0.02399057  0.00539708]
</code></pre>

<p>Is it possible to encode from array of vector to words? If yes, how will I implement it in Python?</p>
",2018-11-13 06:17:04,2018-11-13 06:17:04,Python - Data Encoding Vector To Word,<python><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
18997,53301916,2018-11-14 13:56:33,,"<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>

<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>

<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>
",2018-11-14 19:21:24,2018-11-16 07:12:51,Python/Gensim - What is the meaning of syn0 and syn0norm?,<python><deep-learning><nlp><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19003,53320951,2018-11-15 13:50:25,,"<p>I am trying to perform LDATransformer using gensim api and then I want to get the topic words only using following code:</p>

<pre><code>from gensim.sklearn_api.ldamodel import LdaTransformer
   print(""Loading docs for lda input..."")
   docs = get_lda_input_from_corpus_folder(CORPUS_PATH)
    print(""Topic modeling using LdaTransformer.."")
    dictionary = Dictionary(docs)
    corpus = [dictionary.doc2bow(text) for text in docs]
    model =  LdaTransformer(id2word=dictionary, num_topics=n_topics,iterations=lda_n_iter,random_state=n_random)
    model.fit(corpus)
    print(""\nTopical words:"")
    print(""-"" * 20)
    topic_word = model.topic_word_
    n_top_words = 8
 for i, topic_dist in enumerate(topic_word):
     topic_words = np.argsort(topic_dist)[:-(n_top_words+1):-1]
     print('Topic {}: {}'.format(i, ' '.join(topic_words)))
</code></pre>

<p>but when I print these topic words i have received an error as:</p>

<pre><code>AttributeError: 'LdaTransformer' object has no attribute 'topic_word_'
</code></pre>

<p>any other method which I can use to extract words from this model?</p>
",,2018-11-15 13:50:25,how to display topic words using sklearn api in gensim,<python-3.x><scikit-learn><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,True
19014,53232894,2018-11-09 20:32:30,,"<p>Folks,</p>

<p>I have searched Google for different type of papers/blogs/tutorials etc but haven't found anything helpful. I would appreciate if anyone can help me. <strong>Please note that I am not asking for code step-by-step but rather an idea/blog/paper or some tutorial.</strong>   </p>

<p>Here's my problem statement:  </p>

<blockquote>
  <p>Just like sentiment analysis is used for identifying positive and
  negative tone of a sentence, I want to find whether a sentence is
  forward-looking (future outlook) statement or not.</p>
</blockquote>

<p>I do not want to use bag of words approach to sum up the number of forward-looking words/phrases such as <em>""going forward""</em>, ""<em>in near future</em>"" or ""<em>In 5 years from now</em>"" etc. I am not sure if word2vec or doc2vec can be used. Please enlighten me.  </p>

<p>Thanks. </p>
",,2018-11-10 07:39:21,Unsupervised sentiment Analysis using doc2vec,<nlp><gensim><word2vec><sentiment-analysis><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19017,53343027,2018-11-16 17:52:59,,"<p>I am trying to use word2vec from gensim but I get this warning on running: 
 - C:\Users\user1PycharmProjects\FirstTest\venv\lib\site-packages\gensim\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
 - C:\Users\user1\PycharmProjects\FirstTest\venv\lib\site-packages\gensim\models\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.
  ""C extension not loaded, training will be slow. ""</p>

<p>I installed (&amp; configure path): </p>

<ul>
<li>mingw32-gcc-ada-bin</li>
<li>mingw32-gcc-fortran-bin</li>
<li>mingw32-gcc-g++-bin</li>
<li>mingw32-gcc-objc-bin</li>
</ul>

<p>I sure that I have a compiler (tested with a C script), but I don't know why I can't use the fast version of gensim !!!
I run my script on windows 10 with python3. </p>

<p>Thank you</p>
",,2019-04-22 18:27:40,"Gensim on windows: C extension not loaded, training will be slow",<python><pip><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19035,53286476,2018-11-13 17:24:55,,"<p>I am using Glove, <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Gensim-word2vec</a>, module and I can use it to return the similarity score between entities such as <code>'man'</code> and <code>'woman'</code> will return <code>0.89034</code>. But is there a way to return the semantic relationship between two entities? For example given the word <code>'people'</code> and a <code>'location'</code>, the result should be the relationship <code>'lives_in'</code>?</p>

<p>I can do something like: </p>

<pre><code>print(model.most_similar(positive=['king', 'woman'], negative=['man']))
</code></pre>

<p>Output is:</p>

<pre><code>[('queen', 0.775162398815155), ('prince', 0.6123066544532776), ('princess', 0.6016970872879028), ('kings', 0.5996100902557373), ('queens', 0.565579891204834), ('royal', 0.5646308660507202), ('throne', 0.5580971240997314), ('Queen', 0.5569202899932861), ('monarch', 0.5499411821365356), ('empress', 0.5295248627662659)]
</code></pre>

<p>Desired output:</p>

<pre><code>[(is_a, 0.3223), (same_as, 0349230), (people, 0302432) ...]
</code></pre>
",2018-11-13 18:27:15,2018-11-14 10:17:13,Is there a way to get the relationship from 'GloVe' word2vec?,<relationship><semantic-web><gensim><word2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
19050,53313575,2018-11-15 06:23:52,,"<p>I'm new to python and I need to construct a LDA project. After doing some preprocessing step, here is my code:</p>

<pre><code>dictionary = Dictionary(docs)
corpus = [dictionary.doc2bow(doc) for doc in docs]

from gensim.models import LdaModel
num_topics = 10
chunksize = 2000
passes = 20
iterations = 400
eval_every = None
temp = dictionary[0]
id2word = dictionary.id2token
model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       alpha='auto', eta='auto', \
                       random_state=42, \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
</code></pre>

<p>I want to get a topic distribution of docs, all of the document and get 10 probability of topic distribution, but when I use:</p>

<pre><code>get_document_topics = model.get_document_topics(corpus)
print(get_document_topics)
</code></pre>

<p>The output only appear </p>

<pre><code>&lt;gensim.interfaces.TransformedCorpus object at 0x000001DF28708E10&gt;
</code></pre>

<p>How do I get a topic distribution of docs?</p>
",2018-11-15 06:45:12,2018-11-15 08:41:01,How to get document_topics distribution of all of the document in gensim LDA?,<python-3.x><gensim><lda><topic-modeling><probability-distribution>,,,CC BY-SA 4.0,False,False,True,False,False
19052,53406593,2018-11-21 06:46:19,,"<p>I'm trying to run a simple LDA model on Gensim:</p>

<pre><code>from gensim import corpora
#text_data here is a list of tokens
dictionary = corpora.Dictionary(text_data)
corpus = [dictionary.doc2bow(text) for text in text_data]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=5, id2word=dictionary, passes=15)
</code></pre>

<p>and getting the following error:</p>

<pre><code>File "".../anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py"", line 465, in __init__
    self.random_state = utils.get_random_state(random_state)

  File "".../anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 84, in get_random_state
    return np.random.mtrand._rand

AttributeError: module 'numpy.random' has no attribute 'mtrand'
</code></pre>

<p>I have tried <code>from numpy import random</code> but doesn't seem to work, and not really sure why this error is thrown. </p>

<p>Any suggestions?</p>

<p>EDIT: Restarting Anaconda and Spyder fixed the issue. </p>
",2018-11-21 22:07:55,2018-11-21 22:07:55,AttributeError from gensim running LDA model,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
19055,53347009,2018-11-17 00:17:53,,"<pre><code>import gensim
from gensim.models import word2vec
mainmodel = gensim.models.Word2Vec.load('C:/Users/user/2330.model')
data = open('C:/Users/user/vec_2330_segement.txt','r',encoding = 'utf-8')
data=data.read()
</code></pre>

<p>Above is loading work2vec model and list of data 
Data is a huge of chinese vocabulary list ( have been cut in jieba , about 100 MegaByte ) but have two comma ',' between each whole document(news) , like :
["""" """" """" """" """" ,, """" """" """" ,, ""...""]
Have total 40,000 documents.
so , I hope what I see in result ""doc_all"".</p>

<p>FOR example(each vector in documents are 20 Dimensions) </p>

<p>0.12345 , 1.23456 , -2.34567 , ... , -3.45678 </p>

<p>... </p>

<p>... </p>

<p>0.12345 , -1.23456 , 2.34567 , ... , 3.45678</p>

<p>(for 40000 rows) </p>

<pre><code>data=data.read()
doc_all = []
doc_sent = []
for index in data:
    doc_sent.append(mainmodel.wv[index]),
    try:
        (doc_all if data[index] == ',').append(np.sum(doc_sent,axis=0)),        
    except KeyError:
        continue
len(doc_all)
</code></pre>

<p>which I can't believe is :
Why there comes an error from syntax , 
I have search lot of relate documents in flow but have no same situation ,
instead my code grammar is same  ,
have anything that I suppose to notice but I didn't ?</p>

<p>======Edit====== 
here is the tracebook (sorry for late) 
I guess it's need an else to break 
or... not ?</p>

<pre><code>File ""&lt;ipython-input-3-c55175ca5d96&gt;"", line 9
(doc_all if data[index] == ',').append(np.sum(doc_sent,axis=0)),
                              ^
SyntaxError: invalid syntax
</code></pre>
",2018-11-17 05:47:46,2018-11-17 05:47:46,out put a model.wv with an if condition in a list,<list><if-statement><syntax-error><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19057,53353978,2018-11-17 17:57:54,,"<p>I am new to using word embedding and want to know how i can project my model in Tensorflow. I was looking at the tensorflow website and it only accepts tsv file (vector/metadata), but don't know how to generate the required tsv files. I have tried looking it up and can't find any solutions regrading this. Will I try saving my model in a tsv file format, will i need to do some transformations? Any help will be appreciated.</p>

<p>I have saved my model as the following files, and just load it up when I need to use it:</p>

<blockquote>
  <p>word2vec.model</p>
  
  <p>word2vec.model.wv.vectors.npy</p>
</blockquote>
",,2018-11-17 18:44:41,How to project my word2vec model in Tensorflow,<tensorflow><gensim><word2vec><tensorboard>,,,CC BY-SA 4.0,False,False,True,False,False
19072,53368915,2018-11-19 05:41:32,,"<p>In each tiny step of doc2vec training process, it takes a word and its neighbors within certain length(called window size). The neighbors are summed up, averaged, or concated, and so on and so on.</p>

<p>My question is, what if the window exceed the boundary of a certain doc, like 
<a href=""https://i.stack.imgur.com/Iy5e0.png"" rel=""nofollow noreferrer"">this</a></p>

<p>Then how are the neighbors summed up, averaged, or concated? Or they are just simply discarded? </p>

<p>I am doing some nlp work and most doc in my dataset are quite short. Appeciate for any idea.</p>
",,2018-11-19 17:18:14,Genisim doc2vec: how is short doc processed?,<machine-learning><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19098,53376459,2018-11-19 14:11:13,,"<p>I have a model based on <code>doc2vec</code> trained on multiple documents. I would like to use that model to infer the vectors of another document, which I want to use as the corpus for comparison. So, when I look for the most similar sentence to one I introduce, it uses this new document vectors instead of the trained corpus.
Currently, I am using the <code>infer_vector()</code> to compute the vector for each one of the sentences of the new document, but I can't use the <code>most_similar()</code> function with the list of vectors I obtain, it has to be <code>KeyedVectors</code>.</p>

<p>I would like to know if there's any way that I can compute these vectors for the new document that will allow the use of the <code>most_similar()</code> function, or if I have to compute the similarity between each one of the sentences of the new document and the sentence I introduce individually (in this case, is there any implementation in Gensim that allows me to compute the cosine similarity between 2 vectors?).</p>

<p>I am new to Gensim and NLP, and I'm open to your suggestions.</p>

<p>I can not provide the complete code, since it is a project for the university, but here are the main parts in which I'm having problems.</p>

<p>After doing some pre-processing of the data, this is how I train my model:</p>

<pre><code>documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_data)]
assert gensim.models.doc2vec.FAST_VERSION &gt; -1

cores = multiprocessing.cpu_count()

doc2vec_model = Doc2Vec(vector_size=200, window=5, workers=cores)
doc2vec_model.build_vocab(documents)
doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=30)
</code></pre>

<p>I try to compute the vectors for the new document this way:</p>

<pre><code>questions = [doc2vec_model.infer_vector(line) for line in lines_4]
</code></pre>

<p>And then I try to compute the similarity between the new document vectors and an input phrase:</p>

<pre><code>text = str(input('Me: '))

tokens = text.split()

new_vector = doc2vec_model.infer_vector(tokens)

index = questions[i].most_similar([new_vector])
</code></pre>
",2018-11-19 14:28:52,2018-11-20 13:33:34,Gensim Doc2vec model: how to compute similarity on a corpus obtained using a pre-trained doc2vec model?,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19110,53430997,2018-11-22 12:26:22,,"<p>I have a <code>pandas</code> dataframe containing descriptions. I would like to cluster descriptions based on meanings usign <code>CBOW</code>. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using <code>gensim</code> as so:</p>

<pre><code>from gensim.models import Word2Vec

vocab = pd.concat((df['description'], df['more_description']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
</code></pre>

<p>I am however a bit confused now on how to replace the full sentences from my <code>df</code> with document vectors of equal dimensions.</p>

<p>For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though <code>gensim</code>, so that I could say something like this:</p>

<pre><code>df['description'].apply(model.vectorize)
</code></pre>
",,2018-11-23 09:16:07,How to sentence embed from gensim Word2Vec embedding vectors?,<python-3.x><gensim><word2vec><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19121,53417171,2018-11-21 17:02:37,,"<p>I use gensim LDA topic modelling to find topics for each document and to check the similarity between documents by comparing the received topics vectors.
Each document is given a different number of matching topics, so the comparison of the vector (by cosine similarity) is incorrect because vectors of the same length are required.</p>

<p>This is the related code:</p>

<pre><code>lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=3, passes=1, random_state=47)

#---------------Calculating and Viewing the topics----------------------------
vec_bows = [dictionary.doc2bow(filtered_text.split()) for filtered_text in filtered_texts]

vec_lda_topics=[lda_model_bow[vec_bow] for vec_bow in vec_bows]

for id,vec_lda_topic in enumerate(vec_lda_topics):
    print ('document ' ,id, 'topics: ', vec_lda_topic)
</code></pre>

<p>The output vectors is:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(2, 0.93666667)]
document  2 topics:  [(2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>As you can see, each vector has a different length, so it is not possible to perform cosine similarity between them.</p>

<p>I would like the output to be:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(1, 0.0), (2, 0.93666667), (3, 0.0)]
document  2 topics:  [(1, 0.0), (2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>Any ideas how to do it? tnx</p>
",,2018-11-21 19:02:42,fixed-size topics vector in gensim LDA topic modelling for finding similar texts,<python><gensim><lda><topic-modeling><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
19127,53503049,2018-11-27 15:34:45,,"<p>I have already trained gensim doc2Vec model, which is finding most similar documents to an unknown one.</p>

<p>Now I need to find the similarity value between two unknown documents (which were not in the training data, so they can not be referenced by doc id)</p>

<pre><code>d2v_model = doc2vec.Doc2Vec.load(model_file)

string1 = 'this is some random paragraph'
string2 = 'this is another random paragraph'

vec1 = d2v_model.infer_vector(string1.split())
vec2 = d2v_model.infer_vector(string2.split())
</code></pre>

<p>in the code above vec1 and vec2 are successfully initialized to some values and of size - 'vector_size'</p>

<p>now looking through the gensim api and examples I could not find method that works for me, all of them are expecting TaggedDocument</p>

<p>Can I compare the feature vectors value by value and if they are closer => the texts are more similar?</p>
",2018-11-27 18:07:13,2020-02-19 16:12:55,Measure similarity between two documents using Doc2Vec,<python><machine-learning><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19136,53467414,2018-11-25 12:25:40,,"<p>I am trying to calculate similarity between two documents which are comprised of more than thousands sentences.</p>

<p>Baseline would be calculating cosine similarity using BOW.</p>

<p>However, I want to capture more of semantic difference between documents.</p>

<p>Hence, I built word embedding and calculated documents similarity by generating document vectors by simply averaging all the word vectors in each of documents and measure cosine similarity between these documents vectors. </p>

<p>However, since the size of each input document is rather big, the results I get from using the method above are very similar to simple BOW cosine similarity.</p>

<p>I have two questions, </p>

<p>Q1. I found gensim module offers soft cosine similarity. But I am having hard time understanding the difference from the methods I used above, and I think it may not be the mechanism to calculate similarity between million pairs of documents.</p>

<p>Q2. I found Doc2Vec by gensim would be more appropriate for my purpose. But I recognized that training Doc2Vec requires more RAM than I have (32GB) (the size of my entire documents is about 100GB). Would there be any way that I train the model with small part(like 20GB of them) of entire corpus, and use this model to calculate pairwise similarities of entire corpus?
If yes, then what would be the desirable train set size, and is there any tutorial that I can follow?</p>
",,2019-01-09 18:30:41,"Python Calculating similarity between two documents using word2vec, doc2vec",<python><similarity><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19148,53473868,2018-11-26 01:58:55,,"<p>I have a time series dataset. therefore, for each time period I trained a word2vec model and realigned the models.</p>

<p>However, when I try to load the aligned word2vec models as follows I get the below mentioned error.</p>

<pre><code>#Load model
model = word2vec.Word2Vec.load('model_1970')
</code></pre>

<p>Error:</p>

<pre><code>train_words_pow += wv.vocab[wv.index2word[word_index]].count**power
KeyError: 'ctrx'
</code></pre>

<p>Is there a way to resolve this error? :)</p>

<p>I have attached a sample trained word2vec model that gives error for testing purposes</p>

<p>Link: <a href=""https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing</a></p>

<p>EDIT:
Mention below is the log of my program.</p>

<pre><code>2018-11-30 14:23:43,897 : INFO : loading Word2Vec object from model_1970
2018-11-30 14:23:43,961 : INFO : loading wv recursively from model_1970.wv.* with mmap=None
2018-11-30 14:23:43,965 : INFO : loading vectors from model_1970.wv.vectors.npy with mmap=None
2018-11-30 14:23:44,005 : INFO : setting ignored attribute vectors_norm to None
2018-11-30 14:23:44,009 : INFO : loading vocabulary recursively from model_1970.vocabulary.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading trainables recursively from model_1970.trainables.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading syn1neg from model_1970.trainables.syn1neg.npy with mmap=None
2018-11-30 14:23:44,053 : INFO : setting ignored attribute cum_table to None
2018-11-30 14:23:44,053 : INFO : loaded model_1970
Reloaded modules: __mp_main__
Traceback (most recent call last):

  File ""&lt;ipython-input-3-3b9230dacba9&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/Emi/Desktop/code/word2vec_distance.py', wdir='C:/Users/Emi/Desktop/code')

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 668, in runfile
    execfile(filename, namespace)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Emi/Desktop/code/word2vec_distance.py"", line 26, in &lt;module&gt;
    model_1 = word2vec.Word2Vec.load(word2vec_model_name_1)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 975, in load
    return super(Word2Vec, cls).load(*args, **kwargs)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py"", line 631, in load
    model.vocabulary.make_cum_table(model.wv)  # rebuild cum_table from vocabulary

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 1383, in make_cum_table
    train_words_pow += wv.vocab[wv.index2word[word_index]].count**power

KeyError: 'cmnx'
</code></pre>
",2018-11-30 04:00:39,2018-11-30 04:00:39,How to load an aligned word2vec model in Gensim?,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19151,53493465,2018-11-27 05:46:53,,"<p>Based on 37,000 article texts, I implemented <code>LDA mallet</code> topic modeling. Each article was properly categorized and the dominant topic of each was determined.</p>

<p>Now I want to create a dataframe that shows <strong>each topic's percentages for each article</strong>, in Python.</p>

<p>I want the data frame to look like this:</p>

<hr>

<pre><code>no |      Text     | Topic_Num_1 | Topic_Num_2 | .... | Topic_Num_25
01 | article text1 |   0.7529    |   0.0034    | .... | 0.0011
02 | article text2 |   0.3529    |   0.0124    | .... | 0.0001
</code></pre>

<p>....
(37000 x 27 row)</p>

<p>How would I do this?</p>

<p>+</p>

<p>All the code I've been doing is based on the following site. </p>

<p><a href=""http://machinelearningplus.com/nlp/topic-modeling-gensim-python"" rel=""nofollow noreferrer"">http://machinelearningplus.com/nlp/topic-modeling-gensim-python</a></p>

<p>How can I see the all probability list of the topics of every single article?</p>
",2018-11-28 05:13:36,2018-11-28 05:13:36,How to construct a dataframe with LDA in Python,<python><dataframe><lda>,,,CC BY-SA 4.0,False,False,True,False,False
19161,53570547,2018-12-01 11:59:55,,"<p>I'm using Python, Django, IDE PyCharm. I imported Gensim for my project, however, seems like Django does not recognize it. Here are my versions: <strong>gensim-3.6.0 numpy-1.15.4 scipy-1.1.0 python 3.7.1</strong></p>

<p>The following error is:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Me/PycharmProjects/sma-capstone-2018/sma_core/brinfluence/lib/doc2vec.py"", line 4, in &lt;module&gt;
    import gensim
  File ""C:\Users\Me\venv\lib\site-packages\gensim\__init__.py"", line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""C:\Users\Me\venv\lib\site-packages\gensim\parsing\__init__.py"", line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File ""C:\Users\Me\venv\lib\site-packages\gensim\parsing\preprocessing.py"", line 40, in &lt;module&gt;
    from gensim import utils
  File ""C:\Users\Me\venv\lib\site-packages\gensim\utils.py"", line 40, in &lt;module&gt;
    import scipy.sparse
  File ""C:\Users\Me\venv\lib\site-packages\scipy\sparse\__init__.py"", line 229, in &lt;module&gt;
    from .csr import *
  File ""C:\Users\Me\venv\lib\site-packages\scipy\sparse\csr.py"", line 15, in &lt;module&gt;
    from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
",2019-05-21 02:39:06,2019-05-21 02:39:06,Django doesn't recognize Gensim,<python><django><numpy><pycharm><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19174,53449019,2018-11-23 15:06:20,,"<p>My questioon is about cossim usage.</p>

<p>I have this fragment of a very big fuction:</p>

<pre><code>for elem in lList:
    temp = []
    try:
        x = dict(np.ndenumerate(np.asarray(model[elem])))
    except:
        if x not in embedDict.keys():
            x = np.random.uniform(low=0.0, high=1.0, size=300)
            embedDict[elem] = x
        else:
            x  =  dict(np.ndenumerate(np.asarray(embedDict[elem])))

    for w in ListWords:
        try:
            y =  dict(np.ndenumerate(np.asarray(model[w])))
        except:
            if y not in embedDict.keys():
                y = np.random.uniform(low=0.0, high=1.0, size=300)
                embedDict[w] = y
            else:
                y =  dict(np.ndenumerate(np.asarray(embedDict[w])))

        temp.append(gensim.matutils.cossim(x,y))
</code></pre>

<p>I get the following exception:</p>

<pre class=""lang-none prettyprint-override""><code>File ""./match.py"", line 129, in getEmbedding
    test.append(gensim.matutils.cossim(x,y))
  File ""./Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/matutils.py"", line 746, in cossim
    vec1, vec2 = dict(vec1), dict(vec2)
TypeError: cannot convert dictionary update sequence element #0 to a sequence
</code></pre>

<p>Can you please help me and explain to me what this exception means? </p>
",2018-11-23 15:07:23,2019-01-29 00:40:18,how to use cosssim in gensim,<python><python-2.7><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19188,53575141,2018-12-01 21:18:40,,"<p>Hello I am new in word2vec so I was trying a simple program to read file and get the vec of each word, but there's something wrong with the tokenization process, as word2vec takes into account each letter not word!</p>

<p>for instance my file contains ""hello this is my first trial""</p>

<pre><code>from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize


F = open('testfile')
f=F.read()
doc= word_tokenize(f)
print(f)

print(doc)

model = Word2Vec(doc,min_count=1)

# summarize the loaded model
print(model)

words = list(model.wv.vocab)
print(model['hello'])
</code></pre>

<p>I get an error that hello is not in the vocab, but when i use a letter 'h' it works</p>
",2018-12-01 21:24:08,2018-12-02 19:45:46,Trying to use word2Vec on file but not working?,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
19200,53534262,2018-11-29 07:58:10,,"<p>I use three txt file to do a LDA project
I try to separate these three txt file with two way 
The difference among the process is:</p>

<pre><code>docs = [[doc1.split(' ')], [doc2.split(' ')], [doc3.split(' ')]]
docs1 = [[''.join(i)] for i in re.split(r'\n{1,}', doc11)] + [[''.join(e)] for e in re.split(r'\n{1,}', doc22)] + [[''.join(t)] for t in re.split(r'\n{1,}', doc33)]    
dictionary = Dictionary(docs)
dictionary1 = Dictionary(docs1)
corpus = [dictionary.doc2bow(doc) for doc in docs]
corpus1 = [dictionary.doc2bow(doc) for doc in docs1]
</code></pre>

<p>And the document number is</p>

<pre><code>len(corpus)
len(corpus1)
3
1329
</code></pre>

<p>But the lda model create a rubbish result in <code>corpus</code> but a relatively good result in <code>corpus1</code></p>

<p>I use this model to train the document</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                    id2word=id2word,
                                    num_topics=10, 
                                    random_state=100,
                                    update_every=1,
                                    chunksize=100,
                                    passes=10,
                                    alpha='auto',
                                    per_word_topics=True)
</code></pre>

<p>The difference in the two model is the document number, everything else is the same</p>

<p>Why LDA create such a different result in this two model?</p>
",,2018-12-02 05:57:03,How will the document number affect the result of Gensim LDA?,<python-3.x><text-mining><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
19206,53587960,2018-12-03 05:29:29,,"<p>Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?</p>

<p>But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?</p>
",,2018-12-03 20:47:17,"What is the meaning of ""size"" of word2vec vectors [gensim library]?",<python><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19220,53626582,2018-12-05 06:42:51,,"<p>I built a text classification model for imbalanced class classification data. Instead of using keras word vector, i used embedding by using googlenews word2vec vector as the baseline in the embedding layer.</p>

<pre><code>import pandas as pd
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D, Bidirectional, LSTM, Input, concatenate, Conv1D, GlobalMaxPooling1D, BatchNormalization


from keras.optimizers import SGD, Adam
from sklearn.model_selection import train_test_split

from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
import keras.backend as K
from keras import backend as K
from keras import metrics

import numpy as np
from itertools import chain
from collections import Counter
from sklearn.utils import shuffle

import nltk
import gensim
from gensim.models import KeyedVectors

from sklearn.utils import class_weight


dat = pd.read_csv('/home/data.csv',encoding='latin',delimiter='\t')

dat = shuffle(dat)
dat.reset_index(drop=True,inplace=True)
</code></pre>

<p>Since this is a class imbalance problem, i used f1 metric.</p>

<pre><code>def f1_metric(y_true, y_pred):
    def recall(y_true, y_pred):
        """"""Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """"""
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        """"""Precision metric.

        Only computes a batch-wise average of precision.

        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        """"""
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
</code></pre>

<p>I processed text and created word vector as below</p>

<pre><code>def preprocess(dat):
    return [nltk.word_tokenize(row) for row in dat]

x_train, x_test, y_train, y_test= train_test_split(dat.text,dat.labels,test_size=0.20)

X = preprocess(x_train)
model = KeyedVectors.load_word2vec_format('/home/user/Downloads/GoogleNews-vectors-negative300.bin', binary=True,limit=100000)
</code></pre>

<p>I use this function to convert an array of text to number values from word2vec model.</p>

<pre><code>def word2idx(word):
    return model.wv.vocab[word].index

vocab_size, emdedding_size = model.wv.syn0.shape
pretrained_weights = model.wv.syn0
print(vocab_size, emdedding_size)
100000 300
</code></pre>

<p>I created the matrix</p>

<pre><code>max_sentence_len = 50
train_x = np.zeros([len(X), max_sentence_len], dtype=np.int32)
</code></pre>

<p>And replace 0 with index values from word2vec model against corresponding 
tokenized words, upto maximum of 50 words.</p>

<pre><code>for i in range(len(X)):
    for j in range(len(X[i])):
        try:
            train_x[i][j] = word2idx(X[i][j])
        except:
            pass
</code></pre>

<p>I computed class weights using sklearn function since this is a class imbalance problem.</p>

<pre><code>class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)
</code></pre>

<p>This is the function to create multiConvnet model.</p>

<pre><code>def model_architecture(vocab_size,emdedding_size,pretrained_weights):

    # vector-space embedding: 
    n_dim = 64
    n_unique_words = 5000 
    max_review_length = 50
    pad_type = trunc_type = 'pre'
    drop_embed = 0.2 

    # convolutional layer architecture:
    n_conv_1 = n_conv_2 = n_conv_3 = n_conv_4= 256
    k_conv_1 = 3
    k_conv_2 = 2
    k_conv_3 = 4
    k_conv_4 = 5

    # dense layer architecture: 
    n_dense = 256
    dropout = 0.2

    input_layer = Input(shape=(max_review_length,), dtype='int16', name='input') # supports integers +/- 32.7k

#    embedding_layer = Embedding(n_unique_words, n_dim, input_length=max_review_length, name='embedding')(input_layer)
    embedding_layer = Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights], name='embedding')(input_layer)
    drop_embed_layer = SpatialDropout1D(drop_embed, name='drop_embed')(embedding_layer)

    conv_1 = Conv1D(n_conv_1, k_conv_1, activation='relu', name='conv_1')(drop_embed_layer)
    maxp_1 = GlobalMaxPooling1D(name='maxp_1')(conv_1)

    conv_2 = Conv1D(n_conv_2, k_conv_2, activation='relu', name='conv_2')(drop_embed_layer)
    maxp_2 = GlobalMaxPooling1D(name='maxp_2')(conv_2)

    conv_3 = Conv1D(n_conv_3, k_conv_3, activation='relu', name='conv_3')(drop_embed_layer)
    maxp_3 = GlobalMaxPooling1D(name='maxp_3')(conv_3)

    concat = concatenate([maxp_1, maxp_2, maxp_3])

    dense_layer = Dense(n_dense, activation='relu', name='dense')(concat)
    drop_dense_layer = Dropout(dropout, name='drop_dense')(dense_layer)
    dense_2 = Dense(64, activation='relu', name='dense_2')(drop_dense_layer)
    dropout_2 = Dropout(dropout, name='drop_dense_2')(dense_2)

    predictions = Dense(units=1, activation='sigmoid', name='output')(dropout_2)
    model = Model(input_layer, predictions)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_metric])
    return model
</code></pre>

<p>My model is below</p>

<pre><code>mod_keras = model_architecture(vocab_size,emdedding_size,pretrained_weights)

mod_keras.fit(train_x,y_train,batch_size=32,epochs=2,verbose=1,validation_split=0.2,class_weight=class_weights)
</code></pre>

<p>when i run this, i am getting below error.</p>

<pre><code>Train on 287895 samples, validate on 71974 samples
Epoch 1/2
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-25-fcb6fa008311&gt; in &lt;module&gt;
----&gt; 1 mod_Access.fit(train_x,y_train_Access,batch_size=32,epochs=2,verbose=1,validation_split=0.2,class_weight=class_weights)

~/.local/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1037                                         initial_epoch=initial_epoch,
   1038                                         steps_per_epoch=steps_per_epoch,
-&gt; 1039                                         validation_steps=validation_steps)
   1040 
   1041     def evaluate(self, x=None, y=None,

~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    197                     ins_batch[i] = ins_batch[i].toarray()
    198 
--&gt; 199                 outs = f(ins_batch)
    200                 outs = to_list(outs)
    201                 for l, o in zip(out_labels, outs):

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2713                 return self._legacy_call(inputs)
   2714 
-&gt; 2715             return self._call(inputs)
   2716         else:
   2717             if py_any(is_tensor(x) for x in inputs):

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
   2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
   2674         else:
-&gt; 2675             fetched = self._callable_fn(*array_vals)
   2676         return fetched[:len(self.outputs)]
   2677 

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-&gt; 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: indices[26,0] = -3338 is not in [0, 100000)
     [[{{node embedding/embedding_lookup}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@training/Adam/Assign_2""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding/embeddings/read, embedding/Cast, training/Adam/gradients/embedding/embedding_lookup_grad/concat/axis)]]
</code></pre>

<p>I did read this post <a href=""https://stackoverflow.com/questions/42567398/invalidargumenterror-see-above-for-traceback-indices1-10-is-not-in-0-10"">InvalidArgumentError (see above for traceback): indices[1] = 10 is not in [0, 10)</a></p>

<p>As per this post i need to set vocabulary. In my case, this is exactly what i have done by using the parameter <code>vocab_size</code>.</p>
",,2018-12-05 06:42:51,Keras google word2vec CNN model InvalidArgumentError,<python><keras><deep-learning><word2vec><word-embedding>,,,CC BY-SA 4.0,True,False,True,False,True
19226,53536021,2018-11-29 09:45:04,,"<p>I am currently using custom corpus that wields Tagged Documents</p>

<pre><code>class ClassifyCorpus(object):
    def __iter__(self):
        with open(train_data) as fp:
            for line in fp:
                splt = line.split(':')
                id = splt[0]
                text = splt[1].replace('\n', '')
                yield TaggedDocument(text.split(), [id])
</code></pre>

<p>Looking at the source code of Brown Corpus, is see that it just reads from directory and handles the tagging of the documents for me.</p>

<p>I tested it and didn't see improvements in the training speed.</p>
",,2018-11-29 12:58:10,Why use TaggedBrownCorpus when training gensim doc2vec,<python><gensim><corpus><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19237,53628382,2018-12-05 08:51:11,,"<p>Is there a way to find similar docs like we do in word2vec</p>

<p>Like:</p>

<pre><code>  model2.most_similar(positive=['good','nice','best'],
    negative=['bad','poor'],
    topn=10)
</code></pre>

<p>I know we can use infer_vector,feed them to have similar ones, but I want to feed many positive and negative examples as we do in word2vec.</p>

<p>is there any way we can do that! thanks !</p>
",,2018-12-07 01:07:31,Find similarity with doc2vec like word2vec,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19245,53610331,2018-12-04 10:03:24,,"<p>I've got a problem with online updating my Word2Vec model.</p>

<p>I have a document and build model by it. But this document can update with new words, and I need to update vocabulary and model in general.</p>

<p>I know that in gensim 0.13.4.1 we can do this</p>

<p>My code:</p>

<pre><code>model = gensim.models.Word2Vec(size=100, window=10, min_count=5, workers=11, alpha=0.025, min_alpha=0.025, iter=20)
model.build_vocab(sentences, update=False)

model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)

model.save('model.bin')
</code></pre>

<p>And after this I have new words. For e.x.:</p>

<pre><code>sen2 = [['absd', 'jadoih', 'sdohf'], ['asdihf', 'oisdh', 'oiswhefo'], ['a', 'v', 'b', 'c'], ['q', 'q', 'q']]

model.build_vocab(sen2, update=True)
model.train(sen2, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>What's wrong and how can I solve my problem?</p>
",,2018-12-04 10:33:13,Online updating Word2Vec,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19272,53613789,2018-12-04 13:12:53,,"<p>In this study author have found out that, Word2Vec generates the two kinds of embeddings(IN &amp; OUT).</p>

<p><a href=""https://arxiv.org/abs/1602.01137"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1602.01137</a></p>

<p>Well, you can easily get that using syn1 attribute in gensim word2vec. But in the case of gensim fastText, the syn1 do exists but as the concept of fastText is sub-word based, it's not possible to get a vector for word from output matrix by matching the indexes. Do you know any other way around to calculate vector using output matrix??</p>
",,2018-12-05 01:14:12,How I can get vector from output matrix in FastText ?,<word2vec><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
19278,53616003,2018-12-04 15:15:05,,"<p>I train my doc2vec model:</p>

<pre><code>data = [""Sentence 1"",
        ""Sentence 2"",
        ""Sentence 3"",
        ""Sentence 4""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags[str(i)]) 
                              for i, _d in enumerate(data)]
</code></pre>

<p>training part:</p>

<pre><code>model = Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, 
                min_alpha=0.025, iter=20)

model.build_vocab(tagged_data, update=False)

model.train(tagged_data,epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>Save model:</p>

<pre><code>model.save(""d2v.model"")
</code></pre>

<p>And it's work. Than I want to add some sentence to my vocabulary and model. E.x.:</p>

<pre><code>new_data = [""Sentence 5"",
            ""Sentence 6"",
            ""Sentence 7""]
new_tagged_data= 
[TaggedDocument(words=word_tokenize(_d.lower()),tags[str(i+len(data))]) 
                for i,_d in enumerate(new_data)]
</code></pre>

<p>And than update model:</p>

<pre><code>model.build_vocab(new_tagged_data, update=True)

model.train(new_tagged_data, 
            epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>But it doesn't work. Jupiter urgently shut down and no answer. I use the same way with word2vec model and it works!</p>

<p>What can be a problem with this?</p>
",,2018-12-04 20:25:41,Doc2Vec online training,<python><python-3.x><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19283,53648464,2018-12-06 09:43:24,,"<p>I am using Relaxed Word Mover's Distance in the package <code>text2vec</code> to compute the distance between documents, so as to identify the most similar document for each target document.  Word vectors are compiled using <code>FastText</code> available in the pacakage <code>gensim</code> in Python.  The length of the documents can vary from one word to over 50 words.  Some documents are duplicated in the corpus.  I assume that the distance between these duplicated should be very short and the value across different pair of the same documents should be the same.  However, what I observe is that the distance of these identical pair can vary from close to 0 to over 1, and some other less relevant documents are even concluded to be closer than these identical pair.  The command I use is as follows:</p>

<pre><code>library(text2vec)
tokens = word_tokenizer(tolower(data$item))

v = create_vocabulary(itoken(tokens))

v = prune_vocabulary(v, term_count_min = 12, term_count_max = 1000000)
it = itoken(tokens)

# Use our filtered vocabulary
vectorizer = vocab_vectorizer(v)

dtm = create_dtm(it, vectorizer)
tcm = create_tcm(it, vectorizer, skip_grams_window = 50)

#word vectors from FastText
wv_fasttext&lt;-as.data.frame(wv_fasttext)
rownames(wv_fasttext) &lt;- wv_fasttext[, 'na']

wv_fasttext$name&lt;- NULL
wv_fasttext&lt;- data.matrix(wv_fasttext, rownames.force = TRUE)

rwmd_model = RWMD$new(wv)

rwmd_distance = dist2(dtm[1:1000,], dtm[1:1000,], method = rwmd_model, norm 
= 'none')
</code></pre>

<p>Is there any problem with the above model? </p>
",2018-12-06 10:16:37,2018-12-06 10:16:37,Relaxed Word Mover's Distance in R,<python><r><gensim><wmd><text2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19293,53618906,2018-12-04 17:59:28,,"<p>I am trying to apply the word2vec model implemented in the library gensim 3.6 in python 3.7, Windows 10 machine. I have a list of sentences (each sentences is a list of words) as an input to the model after performing preprocessing.</p>

<p>I have computed the results (obtaining 10 most similar words of a given input word using <code>model.wv.most_similar</code>) in <code>Anaconda's Spyder</code> followed by <code>Sublime Text</code> editor. </p>

<p>But, I am getting different results for the same source code executed in two editors.</p>

<p>Which result should <strong>I need to choose and Why</strong>?</p>

<p>I am specifying the screenshot of the results obtained by running the same code in both spyder and sublime text. The input word for which I need to obtain 10 most similar word is <code>#universe#</code></p>

<p>I am really confused how to choose the results, on what basis? Also, I have started learning Word2Vec recently.</p>

<p>Any suggestion is appreciated.</p>

<p>Results Obtained in Spyder:</p>

<p><a href=""https://i.stack.imgur.com/v87AW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v87AW.png"" alt=""enter image description here""></a></p>

<p>Results Obtained using Sublime Text:
<a href=""https://i.stack.imgur.com/JPN0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JPN0X.png"" alt=""enter image description here""></a></p>
",2018-12-05 10:39:46,2018-12-05 10:39:46,Different results of Gensim Word2Vec Model in two editors for the same source code in same environment and platform?,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19307,53621737,2018-12-04 21:31:56,,"<p>How is it possible to retrieve the n most frequent words from a Gensim <code>word2vec</code> model? As I understand, the frequency and count are not the same, and I therefore can't use the <code>object.count()</code> method.  </p>

<p>I need to produce a list of the n most frequent words from my <code>word2vec</code> model. </p>

<p>Edit: </p>

<p>I've tried the following: </p>

<pre class=""lang-py prettyprint-override""><code>w2c = dict()
for item in model.wv.vocab:
   w2c[item]=model.wv.vocab[item].count
w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))
w2cSortedList = list(w2cSorted.keys())
</code></pre>

<p>My initial guess was to use code above, but this implements the count method. I'm not sure if this represents the most frequent words.</p>
",2020-03-05 20:56:33,2020-03-05 20:56:33,Gensim (word2vec) retrieve n most frequent words,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19312,53694381,2018-12-09 16:36:00,,"<p>I want to learn bigrams from a corpus using gensim, and then just print the bigrams learned. i've not seen an example that does this.
help appreciated</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream)

# how can I print all bigrams learned and just the bigrams, including ""new_york"" and ""human computer"" ?enter code here
</code></pre>
",2018-12-09 16:43:54,2020-02-13 20:34:21,print bigrams learned with gensim,<python><gensim><n-gram><topic-modeling><phrase>,,,CC BY-SA 4.0,False,False,True,False,False
19318,53767024,2018-12-13 17:17:43,,"<p>Recently I switched to gensim 3.6 and the main reason was the optimized training process, which streams the training data directly from file, thus avoiding the GIL performance penalties.</p>

<p>This is how I used to trin my doc2vec:</p>

<pre><code>training_iterations = 20
d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
d2v.build_vocab(corpus)

for epoch in range(training_iterations):
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.iter)
    d2v.alpha -= 0.0002
    d2v.min_alpha = d2v.alpha
</code></pre>

<p>And it is classifying documents quite well, only draw back is that when it is trained CPUs are utilized at 70%</p>

<p>So the new way:</p>

<pre><code>corpus_fname = ""spped.data""
save_as_line_sentence(corpus, corpus_fname)

# Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()

# Train models using all cores
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores, dm=0, vector_size=200, epochs=50)
</code></pre>

<p><strong>Now all CPUs are utilized at 100%</strong></p>

<p>but the model is performing very poorly.
According to the documentation, I should not use the train method also, I should use only epoch count and not iterations, also the min_aplpha and aplha values should not be touched.</p>

<p>The configuration of both Doc2Vec looks the same to me so is there an issue with my new set up or configuration, or there is something wrong with the new version of gensim?</p>

<p>P.S I am using the same corpus in both cases, also I tried epoch count = 100, also with smaller numbers like 5-20, but I had no luck</p>

<p><strong>EDIT</strong>: First model was doing 20 iterations 5 epoch each, second was doing 50 epoch, so having the second model make 100 epochs made it perform even better, since I was no longer managing the alpha by myself.</p>

<p>About the second issue that popped up: when providing file with line documents, the doc ids were not always corresponding to the lines, I didn't manage to figure out what could be causing this, it seems to work fine for small corpus, If I find out what I am doing wrong I will update this answer.</p>

<p>The final configuration for corpus of size 4GB looks like this</p>

<pre><code>    d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
    d2v.build_vocab(corpus)
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=100)
</code></pre>
",2019-01-08 17:27:59,2019-01-08 17:27:59,Gensim doc2vec file stream training worse performance,<nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19325,53748921,2018-12-12 18:07:38,,"<p>I have a question about topic modeling via python and gensim library: when I run the following code, it works well and comes up with the related topics but I want to see each topic per document listed in the .csv file but it shuffles. For example, 1st topic is from the 2nd document but the 2nd topic is from the 1st document, and the 3rd is from the 3rd one. And when I run the same code, it shuffles again. How can I fix that and get topics per document or/and linked topics directly to the Ids or authors of the documents that could be listed in the 1st column?</p>

<h1>Codes:</h1>

<h1>Step 1:</h1>

<pre><code> import nltk
 import csv
 import re
 import nltk.corpus
 import gensim
 from gensim import corpora
 from nltk.corpus import stopwords
 from nltk.stem.wordnet import WordNetLemmatizer
 import string
</code></pre>

<h1>Step 2: Loading data and Processing</h1>

<pre><code> doc_complete = open('/home/erdal/Desktop/big_data/abstract1.csv', 'r').readlines()
 stop = set(stopwords.words('english'))
 exclude = set(string.punctuation)
 lemma = WordNetLemmatizer()
 def clean(doc):
    stop_free = "" "".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
    doc_clean = [clean(doc).split() for doc in doc_complete]
    print(doc_clean)
    dictionary = corpora.Dictionary(doc_clean)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)
    print(ldamodel.print_topics(num_topics=3, num_words=3))
</code></pre>

<h1>Step 3: Printing out the topics</h1>

<pre><code>  myData = ldamodel.print_topics(num_topics=3, num_words=3)
  myFile = open('/home/erdal/Desktop/big_data/all_data_1_topics.csv', 'w')
  with myFile:
   writer = csv.writer(myFile)
   writer.writerows(myData)
   print(""Writing complete"")
</code></pre>

<h1>Topics:</h1>

<pre><code> [(0, '0.036*""learning"" + 0.036*""student"" + 0.026*""intergroup""'), (1, '0.005*""abstract"" + 0.005*""significant"" + 0.005*""using""'), (2, '0.042*""clickers"" + 0.027*""motivation"" + 0.027*""student""')]
</code></pre>
",2018-12-12 22:33:55,2018-12-12 22:33:55,"Topic Modeling, Gensim, Python, Getting Topic Models According to fixed IDs or Linked Data",<python-3.x><gensim><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
19331,53455834,2018-11-24 06:43:55,,"<p>I want to predict a score for each sentence in a text. I have written this test method:</p>

<pre><code>def test(sent):
    # Predict for a given sentence
    if sent != """":
        input, seq_lengths, target = make_variables([sent], [])
        output = classifier(input, seq_lengths)
        pred = output.data.max(1, keepdim=True)[1]
        score = pred.cpu().numpy()[0][0]
        print(""The sentence is:"",sent, ""The score is:"", score)
        return


    print(""evaluating trained model ..."")
    total_mse=0

    for sents, scores in test_loader:
        input, seq_lengths, target = make_variables(sents, scores)
        output = classifier(input, seq_lengths)
        pred = output.data.max(1, keepdim=True)[1]
        error=mean_squared_error(pred,target.data.view_as(pred.float()))
        total_mse +=error
    print("" **********  Total MSE is   **********"",total_mse)
    return
</code></pre>

<p>In a part of main method, I have:</p>

<pre><code># Testing
test("""")
# Testing for a given sample _a sentence_
test(""For instance, wolves prey on moose, which are too big for coyotes."")
</code></pre>

<p>But I received this error:</p>

<blockquote>
  <p>Error Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 59, in testPartExecutor
      yield   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 601, in run
      testMethod()   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) Exception: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging &lt;&lt; -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging &lt;&lt; ---------------------</p>
  
  <p>E
  ====================================================================== ERROR: mahsa_rnn_sent_classification.test
  ---------------------------------------------------------------------- Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) TypeError: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging &lt;&lt; -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging &lt;&lt; ---------------------</p>
  
  <p>---------------------------------------------------------------------- Ran 1 test in 0.004s</p>
  
  <p>FAILED (errors=1)</p>
</blockquote>

<p>I think that test() has its argument 'sent'. How can I correct this error?</p>
",2018-11-24 09:19:29,2018-11-24 09:19:29,TypeError: test() missing 1 required positional argument,<python><pycharm><pytest><text-mining><nose>,,,CC BY-SA 4.0,False,False,True,False,False
19340,53683918,2018-12-08 15:20:49,,"<p>I got  freebase-vectors-skipgram1000-en.bin.gz
 from <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a>
Then rename it to 
 freebase-vectors-skipgram1000-en.bin
 and used it in my code</p>

<pre><code>import gensim.models.keyedvectors as word2vec
gvc='freebase-vectors-skipgram1000-en.bin'
model=word2vec.KeyedVectors.load_word2vec_format(gvc,binary=True)
</code></pre>

<p>It gives me this error</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
</code></pre>

<p>I will be grateful for any help regarding this issue.</p>
",2018-12-08 15:32:54,2018-12-08 17:20:09,Load Freebase using python,<python-3.x><utf-8><word2vec><freebase>,,,CC BY-SA 4.0,False,False,True,False,False
19344,53788106,2018-12-14 23:21:10,,"<p>My gensim model is like this:</p>

<pre><code>class MyCorpus(object):
    parametersList = []
    def __init__(self,dictionary):
       self.dictionary=dictionary
    def __iter__(self):
        #for line in open('mycorpus.txt'):
        for line in texts:
            # assume there's one document per line, tokens separated by whitespace
            yield self.dictionary.doc2bow(line[0].lower().split())




if __name__==""__main__"":
    texts=[['human human interface computer'],
             ['survey user user computer system system system response time'],
             ['eps user interface system'],
             ['system human system eps'],
             ['user response time'],
             ['trees'],
             ['graph trees'],
             ['graph minors trees'],
             ['graph minors minors survey survey survey']]


    dictionary = corpora.Dictionary(line[0].lower().split() for line in texts)

    corpus= MyCorpus(dictionary)
</code></pre>

<p>The frequency of each token in each document is automatically evaluated.</p>

<p>I also can define the tf-idf model and access the tf-idf statistic for each token in each document.</p>

<pre><code>model = TfidfModel(corpus)
</code></pre>

<p><strong>However, I have no clue how to count (memory-friendly) the number of documents that a given word arise. How can I do that?</strong> [Sure... I can use the values of tf-idf and document frequency to evaluate it... However, I would like to evaluate it directly from some counting process]</p>

<p>For instance, for the first document, I would like to get somenthing like</p>

<pre><code>[('human',2), ('interface',2), ('computer',2)]
</code></pre>

<p>since each token above arises twice in each document.</p>

<p>For the second.</p>

<pre><code>[('survey',2), ('user',3), ('computer',2),('system',3), ('response',2),('time',2)]
</code></pre>
",2018-12-14 23:46:59,2018-12-15 10:06:36,Gensim-python: Is there a simple way to get the number of times a given token arise in all documents?,<nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19345,53728556,2018-12-11 16:36:43,,"<p>I would like to train a model with Gensim using news texts from electronic newspapers (in pdf format). What is the best way to extract texts from pdf files and to process the texts ready for training? Any sample codes?</p>
",,2018-12-12 08:20:55,Extracting texts from pdf files for building a model with Gensim,<python-3.x><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19355,53697450,2018-12-09 22:47:38,,"<p>the version of python is 3.6
I tried to execute my code but, there are still some errors as below:</p>

<p>Traceback (most recent call last):</p>

<blockquote>
  <p>File
  ""C:\Users\tmdgu\Desktop\NLP-master1\NLP-master\Ontology_Construction.py"",
  line 55, in 
      , binary=True)</p>
  
  <p>File ""E:\Program
  Files\Python\Python35-32\lib\site-packages\gensim\models\word2vec.py"",
  line 1282, in load_word2vec_format
      raise DeprecationWarning(""Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead."")</p>
  
  <p>DeprecationWarning: Deprecated. Use
  gensim.models.KeyedVectors.load_word2vec_format instead.</p>
</blockquote>

<p>how to fix the code? or is the path to data wrong?</p>
",2018-12-10 01:58:24,2018-12-10 21:55:36,Error for word2vec with GoogleNews-vectors-negative300.bin,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19375,53742400,2018-12-12 11:49:30,,"<p>A couple of years ago, a previous developer for my team wrote the following Python code calling word2vec, passing in a training file and the location of an output file. He worked on Linux. I have been asked to get this running on a Windows machine. Bearing in mind <em>I know next to no Python</em>, I have installed Gensim which I'm guessing implements word2vec now, but do not know how to rewrite the code to use the library rather than the executable which it doesnt seem possible to compile on a Windows box. Could someone help me update this code please?</p>

<pre><code>#!/usr/bin/env python3

import os
import csv
import subprocess
import shutil

from gensim.models import word2vec

def train_word2vec(trainFile, output):
    # run word2vec:
    subprocess.run([""word2vec"", ""-train"", trainFile, ""-output"", output,
                    ""-cbow"", ""0"", ""-window"", ""10"", ""-size"", ""100""],
                   shell=False)
    # Remove some invalid unicode:
    with open(output, 'rb') as input_,\
         open('%s.new' % output, 'w') as new_output:
        for line in input_:
            try:
                print(line.decode('utf-8'), file=new_output, end='')
            except UnicodeDecodeError:
                print(line)
                pass
    shutil.move('%s.new' % output, output)

def main():
    train_word2vec(""c:/temp/wc/test1_BigF.txt"", ""c:/temp/wc/test1_w2v_model.txt"")

if __name__ == '__main__':
    main()
</code></pre>
",2018-12-12 14:06:48,2018-12-19 12:54:14,How to run word2vec on Windows using gensim,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19380,53852041,2018-12-19 13:15:16,,"<p>I am using LDA for topic modelling but unfortunately my data is heavily skewed. I have documents from 10 different categories and would like each category to equally contribute to the LDA topics. </p>

<p>However, each category has a varying number of documents (one category for example holds more than 50% of the entire documents, while several categories hold only 1-2% of the documents).</p>

<p>What would be the best approach to assign weights to these categories, so they equally contribute to my topics? If I run the LDA without doing so, my topics will be largely based on the category, which holds over 50% of the documents in the corpus. I am exploring up-sampling but would prefer a solution that directly assigns weight in LDA.</p>
",,2018-12-19 13:15:16,Assign more weight to certain documents within the corpus - LDA - Gensim,<python-3.x><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
19382,53852871,2018-12-19 14:03:18,,"<p>I am using Gensim's Doc2vec to train a model, and I use the infer_vector to infer the vector of a new document to compare the similarity document of the model. However, reusing the same document can have very different results. This way there is no way to accurately evaluate similar documents.<br>
The search network mentions that infer_vector has random characteristics, so each time a new text vector is produced, it will be different.<br>
Is there any way to solve this problem?</p>

<pre><code>model_dm =pickle.load(model_pickle)

inferred_vector_dm = model_dm.infer_vector(i)  

simsinput =model_dm.docvecs.most_similar([inferred_vector_dm],topn=10)
</code></pre>
",2018-12-19 16:04:58,2018-12-19 17:05:09,How to improve the reproducibility of Doc2vec cosine similarity,<python-3.x><nlp><gensim><similarity><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19394,53837088,2018-12-18 16:15:30,,"<p>I used gensim to build a word2vec embedding of my corpus.
Currently I'm converting my (padded) input sentences to the word vectors using the gensim model.
This vectors are used as input for the model.</p>

<pre><code>model = Sequential()
model.add(Masking(mask_value=0.0, input_shape=(MAX_SEQUENCE_LENGTH, dim)))
model.add(Bidirectional(
    LSTM(num_lstm, dropout=0.5, recurrent_dropout=0.4, return_sequences=True))
)
...
model.fit(training_sentences_vectors, training_labels, validation_data=validation_data)
</code></pre>

<p>Are there any drawbacks using the word vectors directly without a keras embedding layer?</p>

<p>I'm also currently adding additional (one-hot encoded) tags to the input tokens by concatenating them to each word vector, does this approach make sense?</p>
",,2018-12-20 15:29:05,Embedding vs inserting word vectors directly to input layer,<keras><deep-learning><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19396,53839662,2018-12-18 19:15:56,,"<p>I am implementing word2vec in gensim, on a corpus with nested lists (collection of tokenized words in sentences of sentences form) with 408226 sentences (lists) and a total of 3150546 words or tokens. </p>

<p>I am getting a meaningful results (in terms of the similarity between two words using model.wv.similarity) with the  chosen values of 200 as size, window as 15, min_count as 5, iter as 10 and alpha as 0.5. All are lemmatized words and these all are input to models with vocabulary as 32716.</p>

<p>The results incurred from default alpha value, size, window and dimensions are meaningless for me based on the used data in computing the similarity values. However higher value of alpha as 0.5 gives me some meaningful results in terms of inducing meaningful similarity scores between two words. However, when I calculate the top n similar words, it's again meaningless. Does I need to change the entire parameters used in the initial training process.</p>

<p>I am still unable to reveal the exact reason, why the model behaves good with such a higher alpha value in computing the similarity between two words of the used corpus, whereas it's meaningless while computing the top n similar words with scores for an input word. Why is this the case?</p>

<p>Does it is diverging towards optimal solution. How to check this?</p>

<p>Any idea why is it the case is deeply appreciated.</p>

<p>Note: I'm using Python 3.7 on Windows machine with anaconda prompt and giving input to the model from a file.</p>

<p>This is what I have tried.</p>

<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.models import Word2Vec
import ast

path = ""F:/Folder/""
def load_data():
    global Sentences
    Sentences = []
    for file in ['data_d1.txt','data_d2.txt']:
        with open(path + file, 'r', encoding = 'utf-8') as f1:
           Sentences.extend(ast.literal_eval(*f1.readlines()))
load_data()

def initialize_word_embedding():
    model = Word2Vec(Sentences, size = 200, window = 15, min_count = 5, iter = 10, workers = 4)
    print(model)
    print(len(model.wv.vocab))
    print(model.wv.similarity(w1 = 'structure', w2 = '_structure_'))
    similarities = model.wv.most_similar('system')
    for word, score in similarities:
        print(word , score)

initialize_word_embedding()    
</code></pre>

<p>The example of Sentences list is as follows:</p>

<blockquote>
  <p>[['<em>scientist</em>', '<em>time</em>', '<em>comet</em>', '<em>activity</em>', '<em>sublimation</em>', '<em>carbon</em>', '<em>dioxide</em>', '<em>nears</em>', '<em>ice</em>', '<em>system</em>'], ['<em>inconsistent</em>', '<em>age</em>', '<em>system</em>', '<em>year</em>', '<em>size</em>', '<em>collision</em>'], ['intelligence', 'system'], ['example', 'application', 'filter', 'image', 'motion', 'channel', 'estimation', 'equalization', 'example', 'application', 'filter', 'system']] </p>
</blockquote>

<p>The <code>data_d1.txt</code> and <code>data_d2.txt</code> is a nested list (list of lists of lemmatized tokenized words). I have preprocessed the raw data and save it in a file. Now giving the same as input. For computing the lemmatizing tokens, I have used the popular WordNet lemmatizer. </p>

<p>I need the word-embedding model to calculate the similarity between two words and computing the most_similar words of a given input word. I am getting some meaningful scores for the <code>model.wv.similarity()</code> method, whereas in calculating the <code>most_similar()</code> words of a word (say, <code>system</code> as shown in above). I am not getting the desired results. </p>

<p>I am guessing the model is getting diverged from the global minima, with the use of high alpha values. </p>

<p>I am confused what should be the dimension size, window for inducing some meaningful results, as there is no such rules regarding how to compute the the size and window. </p>

<p>Any suggestion is appreciated. The size of total sentences and words are specified above in the question. </p>

<p>Results what I am getting without setting alpha = 0.5</p>

<blockquote>
  <p>Edit to Recent Comment:</p>
</blockquote>

<p>Results:</p>

<blockquote>
  <p>Word2Vec(vocab=32716, size=200, alpha=0.025)</p>
</blockquote>

<p>The similarity between <code>set</code> and <code>_set_</code> is : <code>0.000269373188960656</code>
which is meaningless for me as it is very very less in terms of accuracy, But, I am a getting 71% by setting alpha as 0.5, which seems to be meaningful for me as the word <code>set</code> is same for both the domains. </p>

<p>Explanation: The word <code>set</code> should be same for both the domains (as I am comparing the data of two domains with same word). Don't get confused with word <code>_set_</code>, this is because the word is same as set, I have injected a character <code>_</code> at start and end to distinguish the same for two different domains. </p>

<p>The top 10 words along with scores of <code>_set_</code> are:</p>

<pre><code>_niche_ 0.6891741752624512
_intermediate_ 0.6883598566055298
_interpretation_ 0.6813371181488037
_printer_ 0.675414502620697
_finer_ 0.6625382900238037
_pertinent_ 0.6620787382125854
_respective_ 0.6619025468826294
_converse_ 0.6610435247421265
_developed_ 0.659270167350769
_tent_ 0.6588765382766724
</code></pre>

<p>Whereas, the top 10 words for set are:</p>

<pre><code>cardinality 0.633270263671875
typereduction 0.6233855485916138
zdzisaw 0.619156002998352
crisp 0.6165326833724976
equivalenceclass 0.605925977230072
pawlak 0.6058803200721741
straight 0.6045454740524292
culik 0.6040038466453552
rin 0.6038737297058105
multisets 0.6035065650939941
</code></pre>

<p>Why the <code>cosine similarity value</code> is 0.00 for the word <code>set</code> for two different data.</p>
",2018-12-19 23:37:32,2018-12-19 23:37:32,Some diverging issues of Word2Vec in Gensim using high alpha values,<python-3.x><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19406,53815402,2018-12-17 12:36:39,,"<p>I just want to know the effect of the value of alpha in gensim <code>word2vec</code> and <code>fasttext</code> word-embedding models? I know that alpha is the <code>initial learning rate</code> and its default value is <code>0.075</code> form Radim blog.</p>

<p>What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.</p>

<p>Why this imprecise result for same data with two popular models with different precision? Does the value of <code>alpha</code> plays such a crucial role during building of the model?</p>

<p>Any suggestion is appreciated.</p>
",2018-12-17 12:49:10,2018-12-17 18:47:42,Value of alpha in gensim word-embedding (Word2Vec and FastText) models?,<python-3.x><gensim><word2vec><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
19408,53821303,2018-12-17 18:44:42,,"<p>I am working on the 20newsgroup dataset using Python. After using CountVectorizer on it and then using the gensim api for augmented term frequency. I tried fitting it but am getting this error.</p>

<p>Here is my code:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2000)
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
X_train_counts = count_vect.fit_transform(twenty_train.data)
from gensim.sklearn_api import TfIdfTransformer
model = TfIdfTransformer(smartirs='atn')
tfidf_aug = model.fit_transform(X_train_counts())
</code></pre>

<p>After running the above code I get this error: </p>

<p><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]</code></p>

<p>After using getnz() at the end like this.</p>

<pre><code>tfidf_aug = model.fit_transform(X_train_counts().getnnz())
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: 'int' object is not iterable</code></p>
",2019-01-19 08:49:08,2019-01-19 08:49:08,Augmented Frequency on 20newsgroup dataset.TypeError: 'int' object is not iterable,<python><scikit-learn><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
19418,53765598,2018-12-13 15:54:49,,"<p>I trained Gensim W2V model on 500K sentences (around 60K) words and I want to calculate the perplexity.</p>

<ol>
<li>What will be the best way to do so?</li>
<li>for 60K words, how can I check what will be a proper amount of data?</li>
</ol>

<p>Thanks</p>
",,2018-12-13 16:00:59,Calculate perplexity of word2vec model,<python><nlp><gensim><word2vec><language-model>,,,CC BY-SA 4.0,False,False,True,False,False
19434,53791972,2018-12-15 11:34:03,,"<p>I have a pair of word and semantic types of those words. I am trying to compute the relatedness measure between these two words using semantic types, for example: word1=king, type1=man, word2=queen, type2=woman
we can use gensim word_vectors.most_similar to get 'queen' from 'king-man+woman'. However, I am looking for similarity measure between vector represented by 'king-man+woman' and 'queen'.</p>

<p>I am looking for a solution to above (or)
way to calculate vector that is representative of 'king-man+woman' (and)
calculating similarity between two vectors using vector values in gensim (or)
 way to calculate simple mean of the projection weight vectors(i.e king-man+woman)</p>
",2018-12-15 11:44:35,2018-12-15 20:37:47,Similarity measure using vectors in gensim,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19438,53796806,2018-12-15 20:04:58,,"<p>I am using gensim's Doc2vec to learn features from news articles. I can successfully train my documents. However, I struggle to retrieve the document vectors from the model for further processing. </p>

<p>Example code (directly taken <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#usage-examples"" rel=""nofollow noreferrer"">from gensim's documentation</a>):</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.test.utils import common_texts

documents = [TaggedDocument((doc), [i]) for i, doc in enumerate(common_texts)]
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
</code></pre>

<p>This correctly trains without error. </p>

<p>If I try use <code>model.docvecs</code> directly or iterate over it like so:</p>

<pre><code>for vector in model.docvecs:
    print(vector)
</code></pre>

<p>I get this error:</p>

<pre><code>KeyError: ""tag '9' not seen in training corpus/invalid""
</code></pre>

<p>What is the reason for this and how can I fix this?</p>

<p>Thanks in advance!</p>
",,2018-12-15 20:18:28,"Gensim Doc2vec  KeyError: ""tag not seen in training corpus/invalid""",<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19449,53862627,2018-12-20 04:59:23,,"<p>I have a word2vec model using pre-trained GoogleNews-vectors-negative300.bin. The model works fine and I can get the similarities between the two words. For example:</p>

<pre><code>word2vec.similarity('culture','friendship')

0.2732939
</code></pre>

<p>Now, I want to use list elements instead of the words. For example, suppose that I have a list which its name is ""tag"". and the first two elements in the first row are culture and friendship. So, tag[0,0]= culture, and tag[0,1]=friendship.
I use the following code which gives me an error:</p>

<pre><code>word2vec.similarity(tag[0,0],tag[0,1])
</code></pre>

<p>the ""tag"" list is a <code>numpy.ndarray</code></p>

<p>the error is:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 992, in similarity
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 337, in __getitem__
    return self.get_vector(entities)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 455, in get_vector
    return self.word_vec(word)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 452, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word ' friendship' not in vocabulary""
</code></pre>
",2018-12-20 05:16:46,2018-12-20 05:17:38,How to use a list in word2vec.similarity,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19455,53930617,2018-12-26 10:16:48,,"<p>I have list of co-occurences and I want to train word2vec model with my own customized loss_function. </p>

<p>What is the best way to approach this?</p>

<ol>
<li>Is it possible to set gensim Word2Vec model with my own function? </li>
<li>If not, is there an example to an implementation with keras?</li>
<li>If not, must I define everything totally from scratch?</li>
</ol>

<p>Thanks!</p>
",,2018-12-26 10:16:48,Customizg loss function in Word2vec,<keras><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19465,53800830,2018-12-16 09:10:49,,"<p>Here is my code :</p>

<pre><code>data = pd.read_csv('asscsv2.csv', encoding = ""ISO-8859-1"", error_bad_lines=False);
data_text = data[['content']]
data_text['index'] = data_text.index
documents = data_text
</code></pre>

<p>It looks like</p>

<pre><code>print(documents[:2])
                                              content  index
 0  Pretty extensive background in Egyptology and ...      0
 1  Have you guys checked the back end of the Sphi...      1
</code></pre>

<p>And I define a preprocess function by using gensim </p>

<pre><code>stemmer = PorterStemmer()
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            result.append(lemmatize_stemming(token))
    return result
</code></pre>

<p>And when I use this function:</p>

<pre><code>processed_docs = documents['content'].map(preprocess)
</code></pre>

<p>It appears </p>

<pre><code>TypeError: decoding to str: need a bytes-like object, float found
</code></pre>

<p>How to encode my csv file to byte-like object or how to avoid this kind of error?</p>
",2018-12-16 09:13:56,2019-07-11 06:14:19,How to avoid decoding to str: need a bytes-like object error in pandas?,<python><python-3.x><pandas><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
19489,53885591,2018-12-21 13:28:12,,"<p>How to convert pretrained fastText vectors to gensim model?
I need predict_output_word method.</p>

<p>import gensim
from gensim.models import Word2Vec 
from gensim.models.wrappers import FastText</p>

<p>model_wiki = gensim.models.KeyedVectors.load_word2vec_format(""wiki.ru.vec"")
model3 = Word2Vec(sentences=model_wiki)</p>

<blockquote>
  <p>TypeError                                 Traceback (most recent call
  last)  in 
  ----> 1 model3 = Word2Vec(sentences=model_wiki)  # train a model from the corpus</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, size, alpha, window,
  min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs,
  negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule,
  sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
      765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
      766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
  --> 767             fast_version=FAST_VERSION)
      768 
      769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, workers, vector_size,
  epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed,
  hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss,
  fast_version, **kwargs)
      757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
      758 
  --> 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
      760             self.train(
      761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in build_vocab(self, sentences, corpus_file, update, progress_per,
  keep_raw_vocab, trim_rule, **kwargs)
      934         """"""
      935         total_words, corpus_count = self.vocabulary.scan_vocab(
  --> 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
      937         self.corpus_count = corpus_count
      938         self.corpus_total_words = total_words</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in scan_vocab(self, sentences, corpus_file, progress_per, workers,
  trim_rule)    1569             sentences = LineSentence(corpus_file)<br>
  1570 
  -> 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)    1572     1573 
  logger.info(</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in _scan_vocab(self, sentences, progress_per, trim_rule)    1538<br>
  vocab = defaultdict(int)    1539         checked_string_types = 0
  -> 1540         for sentence_no, sentence in enumerate(sentences):    1541             if not checked_string_types:    1542<br>
  if isinstance(sentence, string_types):</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/keyedvectors.py
  in <strong>getitem</strong>(self, entities)
      337             return self.get_vector(entities)
      338 
  --> 339         return vstack([self.get_vector(entity) for entity in entities])
      340 
      341     def <strong>contains</strong>(self, entity):</p>
  
  <p>TypeError: 'int' object is not iterable</p>
</blockquote>
",2018-12-21 13:35:41,2018-12-21 14:16:01,How to convert pretrained fastText vectors to gensim model,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19495,53901258,2018-12-23 04:44:30,,"<p>I am using gensim for topic modeling. After training the lda model I call get_document_topics on a new document to get the topic distribution. However, for some documents, the return value is an empty list. Here is my code. Any idea what could have gone wrong?</p>

<pre><code> topic_vector = [ x[1] for x in self.ldamodel.get_document_topics(new_doc_bow , minimum_probability=
0.0, per_word_topics=False)]
</code></pre>
",,2018-12-26 23:08:24,get_document_topics return an empty list.,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
19505,53867257,2018-12-20 10:53:37,,"<p>I have a word2vec model and I want to change it by adding some additional data beside the occurrence of the word itself.</p>

<p>For example:</p>

<p>Category (out of predefined 50), POS etc.</p>

<p>I thought of two ways to do it:</p>

<ol>
<li>Just concat the metadata to the word. (so that the word ""desk"" will be coded as ""desk-furniture-Noun""</li>
<li>The better way in my opinion: Create a new loss function that will be a function of the co-occurrences of the word itself, the co-occurrences of the category, the co-occurrences POS, etc.</li>
</ol>

<p>So my questions are:
1. What will be a better way?
2. How can I create a new loss function and optimize it in Word2Vec? Can I just pass a parameter to Gensim's Word2Vec or do I need to build a new Word2vec model from scratch?</p>
",,2018-12-20 10:53:37,Adding metadata to words in word2vec,<nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19507,53971240,2018-12-29 16:22:00,,"<p>I have a pre-trained word embedding with vectors of different norms, and I want to normalize all vectors in the model. I am doing it with a for loop that iterates each word and normalizes its vector, but the model us huge and takes too much time. Does gensim include any way to do this faster? I cannot find it.</p>

<p>Thanks!!</p>
",,2018-12-29 18:29:15,Normalize vectors in gensim model,<nlp><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
19510,53941291,2018-12-27 07:24:25,,"<p>Pretrained models of English and other language wikipedia are available here... </p>

<p><a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a></p>

<p>What is the difference between 100d and 500d in case of English wikipedia? </p>

<p>And what does these parameters mean to training (window=5, iteration=10, negative=15)</p>
",,2018-12-27 08:45:12,Wikipedia model training parameters,<machine-learning><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19538,53929657,2018-12-26 08:53:22,,"<p>Hi Im using Gensim to find similarity between documents to do so I make TF-IDF of documents and calculate cosine similarity. when I have new document I can calculate similarity of this document with previous documents using index[tfidf[vec]] but in this way TF-IDF doesnt update and new words does not consider in similarity calculation is there any solution to update TF-IDF quickly without recalculating whole matrix or what is the best solution for my problem?</p>
",2019-01-12 10:55:25,2020-08-08 20:50:54,Updating TF-IDF using Gensim,<python><gensim><similarity><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
19543,53998446,2019-01-01 19:47:37,,"<p>Following reproducible script is used to compute the accuracy of a Word2Vec classifier with the <code>W2VTransformer</code> wrapper in gensim:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)

# Set random seed
np.random.seed(0)

# Tokenize text
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
# Get labels
y_train = data.label

train_input = [x[0] for x in X_train]

# Train W2V Model
model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1)
clf.fit(model.transform(train_input), y_train)

text_w2v = Pipeline(
    [('features', model),
     ('classifier', clf)])

score = text_w2v.score(train_input, y_train)
score
</code></pre>

<blockquote>
  <p>0.80000000000000004</p>
</blockquote>

<p>The problem with this script is that it <strong>only</strong> works when <code>train_input = [x[0] for x in X_train]</code>, which essentially is always the first word only. 
Once change to <code>train_input = X_train</code> (or <code>train_input</code> simply substituted by <code>X_train</code>), the script returns:</p>

<blockquote>
  <p>ValueError: cannot reshape array of size 10 into shape (10,10)</p>
</blockquote>

<p>How can I solve this issue, i.e. how can the classifier work with more than one word of input?</p>

<p><strong>Edit:</strong></p>

<p>Apparently, the W2V wrapper can't work with the variable-length train input, as compared to D2V. Here is a working D2V version:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess, lemmatize
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')

np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
score = accuracy_score(y_train,y_pred)
print(score)
</code></pre>
",2019-01-02 09:13:48,2019-01-02 09:36:36,W2VTransformer: Only works with one word as input?,<scikit-learn><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,True
19550,53955958,2018-12-28 09:02:04,,"<p>How to get most frequent context words from pretrained fasttext model?</p>

<p>For example:
For word 'football' and corpus <code>[""I like playing football with my friends""]</code></p>

<p>Get list of context words: <code>['playing', 'with','my','like']</code></p>

<p>I try to use 
<code>model_wiki = gensim.models.KeyedVectors.load_word2vec_format(""wiki.ru.vec"")
model.most_similar("""")</code></p>

<p>But it's not satisfied for me</p>
",2018-12-28 09:37:09,2019-02-26 21:25:21,How to get list of context words in Gensim,<python><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
19552,53963743,2018-12-28 20:02:24,,"<p>I've generated a PySpark Word2Vec model like so:</p>

<pre><code>from pyspark.ml.feature import Word2Vec

w2v = Word2Vec(vectorSize=100, minCount=1, inputCol='words', outputCol = 'vector')
model = w2v.fit(df)
</code></pre>

<p>(The data that I used to train the model on isn't relevant, what's important is that its all in the right format and successfully yields a <code>pyspark.ml.feature.Word2VecModel</code> object.)</p>

<p>Now I need to convert this model to a Gensim Word2Vec model. How would I go about this?</p>
",,2018-12-29 06:51:33,Convert PySpark ML Word2Vec model to Gensim Word2Vec model,<pyspark><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19560,53968214,2018-12-29 09:09:09,,"<p>I'm working on a reviews dataset. The problem is to fetch the important(number of times the same feature reviewed) positive and negative features of that specific product from the reviews.</p>

<p>Ex: <code>some xyz car</code></p>

<p><strong>positive:</strong> Great mileage, good looking, spacious etc</p>

<p><strong>Negative:</strong> Poor power, bad performance, software problems etc</p>

<p>Thing is to extract the best and worst things about the product!</p>

<p>Until now I've used gensim's doc2vec to find the top positive and negative sentence. The results are not so good and because it gets similar sentences with structure, not similar feathers it holds.</p>
",2018-12-29 10:15:35,2018-12-30 06:41:19,Feature extraction NLP,<python><machine-learning><nlp><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19578,54089804,2019-01-08 10:26:54,,"<p>Following code is used to preprocess text with a custom lemmatizer function:</p>

<pre><code>%%time
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from gensim.utils import simple_preprocess, lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

def preprocessor(s):
    result = []
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=2):
        result.append(token.decode('utf-8').split('/')[0])
    return result

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')

%%time
X_train, X_test, y_train, y_test = train_test_split([preprocessor(x) for x in data.text],
                                                    data.label, test_size=0.2, random_state=0)
#10.8 seconds
</code></pre>

<p>Question: 
<strong>Can the speed of the lemmatization process be improved?</strong> </p>

<p>On a large corpus of about 80,000 documents, it currently takes about two hours. The <code>lemmatize()</code> function seems to be the main bottleneck, as a gensim function such as <code>simple_preprocess</code> is quite fast.</p>

<p>Thanks for your help!</p>
",,2019-01-08 17:55:53,Improving the speed of preprocessing,<gensim><lemmatization>,,,CC BY-SA 4.0,False,False,True,False,True
19581,54003616,2019-01-02 09:03:17,,"<p>I use following gensim wrapper to train a word-vector model:</p>

<pre><code>import numpy as np
import pandas as pd
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)
# Set random seed
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

model.wv.vocab
</code></pre>

<p>However, once I try to access the trained model, i.e. <code>model.wv.vocab</code>, it outputs the error:</p>

<blockquote>
  <p>AttributeError: 'W2VTransformer' object has no attribute 'wv'</p>
</blockquote>

<p>Can I somehow access the vocabulary and other model parameters, or is this not possible with the wrapper?</p>

<pre><code>Current workaround: 

from gensim.models.doc2vec import TaggedDocument
from gensim.models.doc2vec import Doc2Vec

#Defining model without wrapper
documents = data.apply(lambda r: TaggedDocument(words=simple_preprocess(r['text'], min_len=2), tags=[r.label]), axis=1)
d2v = Doc2Vec(documents, window=2, vector_size=10, min_count=1, seed=0)
d2v.wv.vocab
</code></pre>
",2019-01-02 09:52:17,2019-01-02 22:37:36,Accessing model in gensim wrapper,<model><wrapper><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
19586,54005055,2019-01-02 10:54:48,,"<p>I just came across an error when trying to apply a cross-validation for a paragraph vector model: </p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(size=10, min_count=1, iter=5, seed=1)
clf = LogisticRegression(random_state=0)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

pipeline.fit(X_train, y_train)

score = pipeline.score(X_train, y_train)
print(""Score:"", score) # This works
cval = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=3)
print(""Cross-Validation:"", cval) # This doesn't work
</code></pre>

<blockquote>
  <p>KeyError: 0</p>
</blockquote>

<p>I experimented by replacing <code>X_train</code> in <code>cross_val_score</code> with <code>model.transform(X_train)</code> or <code>model.fit_transform(X_train)</code>. Also, I tried the same with raw input data (<code>data.text</code>), instead of pre-processed text. I suspect that something must be wrong with the format of <code>X_train</code> for the cross-validation, as compared to the <code>.score</code> function for Pipeline, which works just fine. I also noted that the <code>cross_val_score</code> worked with <code>CountVectorizer()</code>.</p>

<p>Does anyone spot the mistake?</p>
",,2019-01-02 11:04:00,Cross-validation for paragraph-vector model,<scikit-learn><transform><cross-validation><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
19595,54078386,2019-01-07 16:40:36,,"<p>I have downloaded Google's pretrained word embeddings as a binary file here (GoogleNews-vectors-negative300.bin.gz). I want to be able to filter the embedding based on some vocabulary. </p>

<p>I first tried loading the bin file as a KeyedVector object, and then creating a dictionary that uses its vocabulary along with another vocabulary as a filter. However, it takes a long time.</p>

<pre><code>  # X is the vocabulary we are interested in 
  embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors- 
  negative300.bin.gz', binary=True) 

  embeddings_filtered = dict((k, embeddings[k]) for k in X if k in list(embeddings.wv.vocab.keys()))
</code></pre>

<p>It takes a very long time to run. I am not sure if this is the most efficient solution. Should I filter it out in the <code> load_word2vec_format </code> step first?</p>
",2019-01-07 16:45:58,2019-01-07 17:16:25,Filtering Word Embeddings from word2vec,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19599,53989210,2018-12-31 15:52:48,,"<p>I'm running this code</p>

<pre><code>from gensim.summarization import summarize
text = ""In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones "" + \
       ""daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando),""  + \
       ""the head of the Corleone Mafia family, is known to friends and associates as Godfather. ""  + \
       ""He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors ""  + \
       ""because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding "" + \
       "" day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician ""  + \
       ""and acquaintance of the Don, whose daughter was brutally beaten by two young men because she""  + \
       ""refused their advances; the men received minimal punishment from the presiding judge. "" + \
       ""The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's"" + \
       ""nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, "" + \
       ""a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees "" + \
       ""to have his men punish the young men responsible (in a non-lethal manner) in return for "" + \
        ""future service if necessary.""

print summarize(text)
</code></pre>

<p>It runs perfectly fine for the first time. But after that it shows me following error until I restart the kernel in spyder:</p>

<pre><code>File ""/home/taha/.local/lib/python2.7/site -packages/scipy/sparse/compressed.py"", line 50, in __init__ from .coo import coo_matrix
</code></pre>

<p><code>SystemError: Parent module 'scipy.sparse' not loaded, cannot perform relative import</code></p>

<p>I am using ubuntu 18.04</p>
",2018-12-31 17:35:23,2019-06-25 10:53:49,"How to fix ""Relative import error"" in python (gensim.summarization)",<python><gensim><summarization>,,,CC BY-SA 4.0,False,False,True,False,False
19602,54131612,2019-01-10 15:11:13,,"<p>I am struggling to implement FastText (<a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a>) into a Pipeline that iterates over different vectorizers. More particular, I can't get cross-validation scores. Following code is used:</p>

<pre><code>%%time
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess
from gensim.sklearn_api.ftmodel import FTTransformer
np.random.seed(0)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split(data.text, data.label, random_state=0)
w2v_texts = [simple_preprocess(doc) for doc in X_train]

models = [FTTransformer(size=10, min_count=0, seed=42)]
classifiers = [LogisticRegression(random_state=0)]

for model in models:

    for classifier in classifiers:

        model.fit(w2v_texts)
        classifier.fit(model.transform(X_train), y_train)

        pipeline = Pipeline([
                ('vec', model),
                ('clf', classifier)
            ])

        print(pipeline.score(X_train, y_train))
        #print(model.gensim_model.wv.most_similar('kirk'))

        cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=5)
</code></pre>

<blockquote>
  <p>KeyError: 'all ngrams for word  ""Machine learning can be useful
  branding sometimes"" absent from model'</p>
</blockquote>

<p><strong>How can the problem be solved?</strong> </p>

<p>Sidenote: My other pipelines with <code>D2VTransformer</code> or <code>TfIdfVectorizer</code> work just fine. Here, I can simply apply <code>pipeline.fit(X_train, y_train)</code> after defining the pipeline, instead of the two fits as shown above. It seems like <a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a> doesn't integrate so well with other given vectorizers?  </p>
",,2019-01-10 16:45:52,FastText: Can't get cross_validation,<scikit-learn><cross-validation><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
19606,54060506,2019-01-06 10:11:56,,"<p>following script is used to lemmatize a given input column with text:</p>

<pre><code>%%time
import pandas as pd
from gensim.utils import lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

data = pd.read_csv('https://pastebin.com/raw/0SEv1RMf')

def lemmatization(s):
    result = []
    # lowercase, tokenize, remove stopwords, len&gt;3, lemmatize
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=3):
        result.append(token.decode('utf-8').split('/')[0])
    # print(len(result)) &lt;- This didn't work.
    return result

X_train = data.apply(lambda r: lemmatization(r['text']), axis=1)
print(X_train)
</code></pre>

<p><strong>Question:</strong></p>

<p><strong>How can I print the progress of the lemmatization progress?</strong></p>
",2019-01-08 10:35:11,2019-01-08 10:35:11,Show progress in lemmatization,<gensim><lemmatization>,,,CC BY-SA 4.0,False,False,True,False,False
19631,54063448,2019-01-06 16:15:09,,"<p>I am using Gensim LDA for the topic modelling. I am using pandas DataFrame for the processing. but I am getting an error</p>

<blockquote>
  <p>TypeError: decoding to str: need a bytes-like object, Series found</p>
</blockquote>

<p>I need to process data using Pandas only, input data is like (one row)</p>

<pre><code> PMID           Text
12755608    The DNA complexation and condensation properties
12755609    Three proteins namely protective antigen PA edition
12755610    Lecithin retinol acyltransferase LRAT catalyze
</code></pre>

<p>My code is</p>

<pre><code>data = pd.read_csv(""h1.csv"", delimiter = ""\t"")
data = data.dropna(axis=0, subset=['Text'])
data['Index'] = data.index
data[""Text""] = data['Text'].str.replace('[^\w\s]','')
data.head()

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token):
            result.append(lemmatize_stemming(token))
    return result


input_data = data.Text.str.strip().str.split('[\W_]+')
print('\n\n tokenized and lemmatized document: ')
print(preprocess(input_data))
</code></pre>
",2019-01-08 14:41:07,2020-08-29 02:03:32,Error in Data Processing in Gensim LDA using Pandas Dataframe,<python><pandas><dataframe><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
19636,54085239,2019-01-08 04:31:48,,"<p>I am trying to convert a KeyedVector word2vec object to a tsv file. Here is my code:</p>

<pre><code>wv_embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)
</code></pre>

<p>Would you loop through each of the embeddings and save them to a tsv file?</p>
",,2019-01-28 23:48:28,Converting KeyedVector to a tsv file,<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19658,54187308,2019-01-14 18:38:17,,"<p>I installed Gensim in python3</p>

<p>when I call gensim I got this error. Can someone help?</p>

<pre><code>&gt;&gt;&gt; import gensim


AttributeError: 'tuple' object has no attribute 'type'
</code></pre>
",2019-01-14 21:11:44,2019-01-14 21:11:44,Gensim installation with python3,<python-3.5><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19673,54213078,2019-01-16 08:37:46,,"<p>I have a large language corpus and I use sklearn tfidf vectorizer and gensim Doc2Vec to compute language models. My total corpus has about 100,000 documents and I realized that my Jupyter notebook stops computing once I cross a certain threshold. I guess that the memory is full after applying the grid-search and cross-validation steps.</p>

<p>Even following example script already stops for Doc2Vec at some point:</p>

<pre><code>%%time
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.externals import joblib

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.sklearn_api import D2VTransformer

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess

np.random.seed(1)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split([simple_preprocess(doc) for doc in data.text],
                                                    data.label, random_state=1)

model_names = [
               'TfidfVectorizer',
               'Doc2Vec_PVDM',
              ]

models = [
    TfidfVectorizer(preprocessor=' '.join, tokenizer=None, min_df = 5),
    D2VTransformer(dm=0, hs=0, min_count=5, iter=5, seed=1, workers=1),
]

parameters = [
              {
              'model__smooth_idf': (True, False),
              'model__norm': ('l1', 'l2', None)
              },
              {
              'model__size': [200],
              'model__window': [4]
              }
              ]

for params, model, name in zip(parameters, models, model_names):

    pipeline = Pipeline([
      ('model', model),
      ('clf', LogisticRegression())
      ])

    grid = GridSearchCV(pipeline, params, verbose=1, cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)
    print(grid.best_params_)

    cval = cross_val_score(grid.best_estimator_, X_train, y_train, scoring='accuracy', cv=5, n_jobs=-1)
    print(""Cross-Validation (Train):"", np.mean(cval))

print(""Finished."")
</code></pre>

<p><strong>Is there a way to ""stream"" each line in a document, instead of loading the full data into memory? Or another way to make it more memory efficient? I read a few articles on the topic but could not discover any that included a pipeline example.</strong></p>
",2019-01-31 19:06:12,2019-01-31 19:06:12,Streaming corpus to a vectorizer in a pipeline,<scikit-learn><streaming><gensim><corpus>,,,CC BY-SA 4.0,False,False,True,False,True
19687,54196106,2019-01-15 09:35:24,,"<p>Why when I call Numpy, Scipy, Gensim with python3 in linux I have the following error?</p>

<pre><code>&gt;import gensim
_concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
AttributeError: 'tuple' object has no attribute 'type'
</code></pre>
",2019-01-15 11:49:44,2019-01-17 01:27:20,"Error with calling Numpy, Scipy, Gensim in python3",<python><python-3.x><numpy><scipy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19695,54165109,2019-01-13 00:26:23,,"<p>I'm vectorizing words on a few different corpora with Gensim and am getting results that are making me rethink how Word2Vec functions. My understanding was that Word2Vec was deterministic, and that the position of a word in a vector space would not change from training to training. If ""My cat is running"" and ""your dog can't be running"" are the two sentences in the corpus, then the value of ""running"" (or its stem) seems necessarily fixed.</p>

<p>However, I've found that that value indeed does vary across models, and words keep changing where they are on a vector space when I train the model. The differences are not always hugely meaningful, but they do indicate the existence of some random process. What am I missing here?</p>
",2019-01-13 00:33:25,2019-01-14 20:43:08,What is the stochastic aspect of Word2Vec?,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19697,54228660,2019-01-17 03:37:37,,"<p>I am using cosine similarity function in gensim module, which is similarities.SparseMatrixSimilarity(). And I want to get similarities between all index documents. The method have an attribute:index, but I don't know what are stored in it.</p>

<pre><code>    sim = similarities.SparseMatrixSimilarity(
        self.tfidf_vectors, num_features=self.featurenum)
    sim.save(path + '/model/train_index.index')
    print(sim.index.shape)
    print(sim.index.toarray().shape)
</code></pre>

<p>len(self.tfidf.vectors) is 9117, but sim.index.shape is (9117, 143807) and sim.index.toarray().shape is also (9117, 143807). I guess it should be (9117,9117). What is in sim.index ?</p>
",,2019-07-10 02:29:13,What is stored in similarities.SparseMatrixSimilarity().index,<gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
19709,54287822,2019-01-21 10:21:29,,"<p>I want to extract potential sentences from news articles which can be part of article summary.</p>

<p>Upon spending some time, I found out that this can be achieved in two ways,</p>

<ol>
<li>Extractive Summarization (Extracting sentences from text and clubbing them)</li>
<li>Abstractive Summarization (internal language representation to generate more human-like summaries)</li>
</ol>

<p>Reference: <a href=""https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/"" rel=""nofollow noreferrer"">rare-technologies.com</a></p>

<p>I followed <a href=""http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html"" rel=""nofollow noreferrer"">abigailsee's Get To The Point: Summarization with Pointer-Generator Networks</a> for summarization which was producing good results with the pre-trained model but it was abstractive.</p>

<p><strong>The Problem:</strong>
Most of the extractive summarizers that I have looked so far(PyTeaser, PyTextRank and Gensim) are not based on Supervised learning but on Naive Bayes classifier, tfidf, POS-tagging, sentence ranking based on keyword-frequency, position etc., which don't require any training. </p>

<p>Few things that I have tried so far to extract potential summary sentences.</p>

<ul>
<li>Get all sentences of articles and label summary sentences as 1 and 0 for all others</li>
<li>Clean up the text and apply stop word filters</li>
<li>Vectorize a text corpus using Tokenizer <code>from keras.preprocessing.text import Tokenizer</code> with Vocabulary size of 20000 and pad all sequences to average length of all sentences.</li>
<li>Build a Sqequential keras model a train it.</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>model_lstm = Sequential()
model_lstm.add(Embedding(20000, 100, input_length=sentence_avg_length))
model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model_lstm.add(Dense(1, activation='sigmoid'))
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>This is giving very low accuracy ~0.2</p>

<p>I think this is because the above model is more suitable for positive/negative sentences rather than summary/non-summary sentences classification.   </p>

<p>Any guidance on approach to solve this problem would be appreciated.</p>
",2019-01-22 02:04:01,2019-01-22 16:41:14,Supervised Extractive Text Summarization,<python><keras><nlp><nltk><text-extraction>,,,CC BY-SA 4.0,True,False,True,False,False
19713,54215456,2019-01-16 10:52:02,,"<p>Problem :</p>

<p>Im using glove pre-trained model with vectors to retrain my model with a specific domain say #cars, after training I want to find similar words within my domain but I got words not in my domain corpus, I believe it's from glove's vectors. </p>

<pre><code>model_2.most_similar(positive=['spacious'],    topn=10)

[('bedrooms', 0.6275501251220703),
 ('roomy', 0.6149100065231323),
 ('luxurious', 0.6105825901031494),
 ('rooms', 0.5935696363449097),
 ('furnished', 0.5897485613822937),
 ('cramped', 0.5892841219902039),
 ('courtyard', 0.5721820592880249),
 ('bathrooms', 0.5618442893028259),
 ('opulent', 0.5592212677001953),
 ('expansive', 0.555268406867981)]
</code></pre>

<p>Here I expect something like leg-room, car's spacious features mentioned in the domain's corpus. How can we exclude the glove vectors while having similar vectors?</p>

<p>Thanks  </p>
",2019-01-17 07:01:51,2019-01-17 07:01:51,Gensim pretrained model similarity,<python><vector><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19720,54202117,2019-01-15 15:38:25,,"<p>I have two versions of python: python2, python3. </p>

<p>When I install gensim it goes to python2
 I install it with the following comand</p>

<pre><code>sudo pip3 install --upgrade gensim
</code></pre>

<p>how can I install it in python3?</p>
",2020-05-07 03:08:30,2020-05-07 03:08:30,Install Gensim in python3,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19721,54186233,2019-01-14 17:15:22,,"<p>I am generating a Doc2Vec embedding of a Pandas DataFrame by following the guidance provided <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
import gensim.models.doc2vec
from collections import OrderedDict
import pandas as pd
import numpy as np

cube_embedded =  # pandas cube
# convert the cube to documents
alldocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(cube_embedded.values.tolist())]

# train models
simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
]

for d2v_model in simple_models:
    d2v_model.build_vocab(alldocs)
    d2v_model.train(alldocs, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)

models_by_name = OrderedDict((str(d2v_model), d2v_model) for d2v_model in simple_models)
models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])
models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])
</code></pre>

<p>Given a document vector V, if I try to infer the most similar documents to the document vector V from a ConcatenatedDocvecs model, I get the following error:</p>

<pre><code>V = np.random.rand(200)
models_by_name['dbow+dmc'].docvecs.most_similar([V])

AttributeError: 'ConcatenatedDocvecs' object has no attribute 'most_similar'
</code></pre>

<p>Of course, I cannot use the simple models to infer similar documents as the produced vector embeddings have size 100 (and not 200 as the concatenated vectors do).</p>

<p>How can I get the list of most similar documents to a document vector from a ConcatenatedDocvecs model?</p>
",2019-01-14 17:22:07,2019-01-15 00:28:37,Doc2Vec: infer most similar vector from ConcatenatedDocvecs,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19739,54289583,2019-01-21 12:03:10,,"<p>I am trying to train a word2vec model using gensim. This is the line I am using:</p>

<pre><code>model = Word2Vec(training_texts, size=50, window=5, min_count=1, workers=4, max_vocab_size=20000)
</code></pre>

<p>Where training_texts is a list of lists of strings representing words. The corpora I am using has 8924372 sentences with 141,985,244 words and 1,531,477 unique words. After training, only 15642 words are present in the model:</p>

<pre><code>len(list(model.wv.vocab))
# returns 15642
</code></pre>

<p>Shouldn't the model have 20,000 words, as specified max_vocab_size? Why is it missing most of the training words?</p>

<p>Thanks!!</p>
",2019-01-21 16:35:51,2019-01-22 08:41:49,Missing words when training word2vec model,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19755,54277363,2019-01-20 14:18:14,,"<p>I've trained a word2vec model not for English but for an Asian language 'Sinhala'. in the later phase, I'm going to use this trained model to get the sentence similarities in order to detect plagiarism in Sinhala documents.
Please explain to me how to measure the accuracy of the trained model.I'm a university student. I have no previous knowledge of these things.</p>
",,2019-01-22 05:20:40,How to measure the accuracy of Word2vec model Trained on another language?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19757,54279792,2019-01-20 12:49:14,,"<p>Recently, I am implementing an algorithm from a paper that I will be using in my master's work, but I've come across some problems regarding the time it is taking to perform some operations.</p>

<p>Before I get into details, I just want to add that my data set comprehends roughly 4kk entries of data points.</p>

<p>I have two lists of tuples that I've get from a framework (<a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">annoy</a>) that calculates cosine similarity between a vector and every other vector in the dataset. The final format is like this:</p>

<pre><code>[(name1, cosine), (name2, cosine), ...]
</code></pre>

<p>Because of the algorithm, I have two of that lists with the same names (first value of the tuple) in it, but two different cosine similarities. What I have to do is to sum the cosines from both lists, and then order the array and get the top-N highest cosine values.</p>

<p>My issue is: is taking too long. My actual code for this implementation is as following:</p>

<pre><code>def topN(self, user, session):
    upref = self.m2vTN.get_user_preference(user)
    spref = self.sm2vTN.get_user_preference(session)

    # list of tuples 1
    most_su = self.indexer.most_similar(upref, len(self.m2v.wv.vocab))
    # list of tuples 2
    most_ss = self.indexer.most_similar(spref, len(self.m2v.wv.vocab))

    # concat both lists and add into a dict
    d       = defaultdict(int)      
    for l, v in (most_ss + most_su): 
        d[l] += v  


     # convert the dict into a list, and then sort it
    _list    = list(d.items())
    _list.sort(key=lambda x: x[1], reverse=True)

    return [x[0] for x in _list[:self.N]]
</code></pre>

<p>How do I make this code faster? I've tried using threads but I'm not sure if it will make it faster. Getting the lists is not the problem here, but the concatenation and sorting is.</p>

<p>Thanks! English is not my native language, so sorry for any misspelling.</p>
",,2019-01-22 05:38:56,How do I speedup adding two big vectors of tuples?,<machine-learning><python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19758,54223836,2019-01-16 19:15:16,,"<p>I am trying to read a zipped file (.gz) in python and I am having a trouble.</p>

<pre><code>import gzip
import gensim
import logging
import os

logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s',
    level=logging.INFO)

data_file = r""C:\\PythonProgs\\OpinRank.gz""

def show_file_contents(input_file):
    with gzip.open(input_file, 'rb') as f:
        for i, line in enumerate(f):
            print(line)
            break


show_file_contents(data_file)
</code></pre>

<p>I expected read the zip file but actually give me this error :</p>

<p>FileNotFoundError: [Errno 2] No such file or directory: 'C:\PythonProgs\OpinRank.gz'</p>
",2019-01-16 23:36:35,2019-01-16 23:36:35,How to read zip text file?,<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19762,54348402,2019-01-24 14:01:22,,"<p>I am creating document vectors with a trained FastText model on my computer. Gensim's FastText, as far as I know, doesn't have an option to create document vectors (better known as Paragraph Vectors [PV]). Therefore I have calculated them manually by taking the average of the sum of the words available in a document. This task alone doesn't take as much time.</p>

<p>If I want to append several other numerical features to the calculated PV, 5 millionen docs take about 30 minutes to create. I thought this process could be improved by splitting the work onto several cores on my computer with the multiprocessing library of Python, which works right now but only to a certain extent.</p>

<p>There were a few problems I had to solve before getting to this stage. Since I am using the Jupyter Notebook to execute the code I had to place some methods in a seperate Python script to be able to use mulitprocessing in a Jupyter Notebook. This is the code that's available in the Jupyter Notebook. It imports the module ""m_helpers"" which has the methods to
create the document vectors:</p>

<pre><code>import multiprocessing
import m_helpers

# Define number of workers.
num_processes = 3

if __name__ == ""__main__"":
    # This pool spawns several processes to built the
    # document vectors with the FastText model
    with multiprocessing.Pool(processes = num_processes, 
                              initializer = m_helpers.init_vars, 
                              initargs = (fasttext_model, vars_df)) as pool:
        results = pool.map(m_helpers.create_docvecs,
                           data_df.itertuples(name = False), 
                           chunksize = 512)
        output = [x for x in results]

    # Print length of output to see whether everything was processed
    print(""Length of output (document vectors): {0}"".format(len(output)))
</code></pre>

<p>m_helpers.py:</p>

<pre><code>fasttext_model = None
vars_df = None

def init_vars(model, df):
    global fasttext_model
    fasttext_model = model
    global vars_df
    vars_df = df

def create_docvecs(data):
    word_vectors = [fasttext_model.wv[word] for word in data[-1].str.split()]
    document_vector = sum(word_vectors) / len(word_vectors)
    feature_vector = vars_df.loc[data[0], :].values
    # Further code to combine both vectors
    return document_vector
</code></pre>

<p>I have a computer with 6 cores / 12 threads. However I can make this code work only for 3 cores. Using more cores always results in an error caused by using up all the memory (RAM). I think this is caused by all those copies of objects for each process. </p>

<p>Now there seems to be ways to create a shared memory for all processes to access. There is a dataframe I am iterating over to access the text data. The method that is called for all processes uses an other dataframe and the fasttext model. All of those objects are read only to create the PV and append values from the other dataframe. I could merge the text dataframe and feature dataframe before. However, I would still need to share at least the fasttext_model object. So the question would be, how to do it? Is it possible at all? I have read several questions regarding this problem on stackoverflow but I couldn't make much out of it. Maybe I need to use something different than Pool?</p>
",,2019-01-24 14:01:22,Shared memory for several read-only objects with multiprocessing Pool,<python><multiprocessing><jupyter-notebook><shared-memory><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19768,54243797,2019-01-17 20:27:57,,"<p>I am using gensim to create Word2Vec models trained on large text corpora. I have some models based on StackExchange data dumps. I also have a model trained on a corpus derived from English Wikipedia. </p>

<p>Assume that a vocabulary term is in both models, and that the models were created with the same parameters to Word2Vec. Is there any way to combine or add the vectors from the two separate models to create a single new model that has the same word vectors that would have resulted if I had combined both corpora initially and trained on this data?</p>

<p>The reason I want to do this is that I want to be able to generate a model with a specific corpus, and then if I process a new corpus later, I want to be able to add this information to an existing model rather than having to combine corpora and retrain everything from scratch (i.e. I want to avoid reprocessing every corpus each time I want to add information to the model). </p>

<p>Are there builtin functions in gensim or elsewhere that will allow me to combine models like this, adding information to existing models instead of retraining?</p>
",,2020-05-16 11:17:12,Combining/adding vectors from different word2vec models,<python><gensim><word2vec><training-data><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
19769,54385850,2019-01-27 07:05:34,,"<p>I've imported all the packages I need</p>

<pre><code>from gensim import corpora
from gensim import models
from gensim.models import LdaModel
from gensim.models import TfidfModel
from gensim.models import CoherenceModel
</code></pre>

<p>and then I need to run the LdaMallet model so I import them like this</p>

<pre><code>from gensim.models.wrappers import LdaMallet
</code></pre>

<p>when run the code below, I've got some <code>Namerror</code>:</p>

<pre><code>mallet_path = 'mallet-2.0.8/bin/mallet' # update this path

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=20, id2word=dictionary)
</code></pre>

<p>Error occurred:</p>

<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-22-1c656d4f8c21&gt; in &lt;module&gt;()
      1 mallet_path = 'mallet-2.0.8/bin/mallet' # update this path
      2 
----&gt; 3 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=20, id2word=dictionary)

NameError: name 'gensim' is not defined
</code></pre>

<p>I thought I've imported all the things that I need, and the lda model ran well before I tried to use mallet. So what's the problem?</p>
",2019-01-27 07:16:56,2019-01-27 07:16:56,NameError: name 'gensim' is not defined,<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
19776,54339299,2019-01-24 04:19:44,,"<p>I have tons of text data on multiple web pages about the product I am interested to sell to customers. I tried using pre-trained fasttext word embedding trained on Wikipedia and it didn't give me good results for the classification task. Probably because the text data on the website contains lots of technical details and its different from text data in wikipedia. So I would like to do some kind of transfer learning of word embedding keeping the pretrained fasttext word embedding as base.  </p>

<ol>
<li>How can I train my own custom word embedding on these web pages using Keras?</li>
<li>How can I initialize the custom word embedding with fasttext pre-trained embedding and train? Will this initialization really help in giving better word embedding?</li>
</ol>

<p>I would prefer a solution using Keras for training the word embedding.<br>
I know Embedding has trainable=True option not sure how I can use it.</p>

<pre><code>Embedding(voc_size, emb_dim, weights=[embedding_matrix], input_length, trainable=True)
</code></pre>

<p>Which framework due to recommend for this Keras or Gensim and why?</p>
",2019-01-24 08:53:36,2019-01-24 15:45:52,How to train my own custom word embedding on web pages?,<python><tensorflow><keras><deep-learning><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
19807,54431187,2019-01-29 23:28:46,,"<p>I'm running the following python script on a large dataset (around 100 000 items). Currently the execution is unacceptably slow, it would probably take a month to finish at least (no exaggeration). Obviously I would like it to run faster.</p>

<p>I've added a comment belong to highlight where I think the bottleneck is. I have written my own database functions which are imported.  </p>

<p>Any help is appreciated!</p>

<pre><code># -*- coding: utf-8 -*-
import database
from gensim import corpora, models, similarities, matutils
from gensim.models.ldamulticore import LdaMulticore
import pandas as pd
from sklearn import preprocessing



def getTopFiveSimilarAuthors(author, authors, ldamodel, dictionary):
    vec_bow = dictionary.doc2bow([researcher['full_proposal_text']])
    vec_lda = ldamodel[vec_bow]

    # normalization
    try:
        vec_lda = preprocessing.normalize(vec_lda)
    except:
        pass

    similar_authors = []

    for index, other_author in authors.iterrows():
        if(other_author['id'] != author['id']):
            other_vec_bow = dictionary.doc2bow([other_author['full_proposal_text']])

            other_vec_lda = ldamodel[other_vec_bow]
            # normalization
            try:
                other_vec_lda = preprocessing.normalize(vec_lda)
            except:
                pass

            sim = matutils.cossim(vec_lda, other_vec_lda)
            similar_authors.append({'id': other_author['id'], 'cosim': sim})
    similar_authors = sorted(similar_authors, key=lambda k: k['cosim'], reverse=True)
    return similar_authors[:5]


def get_top_five_similar(author, authors, ldamodel, dictionary):
    top_five_similar_authors = getTopFiveSimilarAuthors(author, authors, ldamodel, dictionary)
    database.insert_top_five_similar_authors(author['id'], top_five_similar_authors, cursor)

connection = database.connect()
authors = []
authors = pd.read_sql(""SELECT id, full_text FROM author WHERE full_text IS NOT NULL;"", connection)

# create the dictionary
dictionary = corpora.Dictionary([authors[""full_text""].tolist()])

# create the corpus/ldamodel
author_text = []

for text in author_text['full_text'].tolist():
    word_list = []
    for word in text:
        word_list.append(word)
        author_text.append(word_list)

corpus = [dictionary.doc2bow(text) for text in author_text]
ldamodel = LdaMulticore(corpus, num_topics=50, id2word = dictionary, workers=30)

#BOTTLENECK: the script hangs after this point. 
authors.apply(lambda x: get_top_five_similar(x, authors, ldamodel, dictionary), axis=1)
</code></pre>
",,2019-01-30 00:51:24,Gensim LDA Multicore Python script runs much too slow,<python><mysql><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,True
19812,54262583,2019-01-18 23:09:39,,"<p>I've loaded pretrained word2vec embeddings into a python dictionary of the form</p>

<p><code>{word: vector}</code></p>

<p>As an example, an element of this dictionary is</p>

<p><code>w2v_dict[""house""] = [1.1,2.0, ... , 0.2]</code></p>

<p>I would like to load this model into Gensim (or a similar library) so that I can find euclidean distances between embeddings.</p>

<p>I understand that pretrained embeddings typically come in a .bin file which can be loaded into Gensim. But if I only have a dictionary of this form, how would I load the vectors into a model?</p>
",2019-01-21 12:32:32,2019-11-09 06:41:57,Load word2vec dictionary into gensim,<nlp><gensim><word2vec><spacy><word-embedding>,,,CC BY-SA 4.0,False,True,True,False,False
19818,54432791,2019-01-30 03:02:01,,"<p>I am running LDAMulticore from the python gensim library, and the script cannot seem to create more than one thread. Here is the error:</p>

<pre><code>  Traceback (most recent call last):
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 114, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 97, in worker
    initializer(*initargs)
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamulticore.py"", line 333, in worker_e_step
    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamodel.py"", line 725, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamodel.py"", line 655, in inference
    ids = [int(idx) for idx, _ in doc]
TypeError: 'int' object is not iterable
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 812, in __bootstrap_inner
    self.run()
  File ""/usr/lib64/python2.7/threading.py"", line 765, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 325, in _handle_workers
    pool._maintain_pool()
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 229, in _maintain_pool
    self._repopulate_pool()
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 222, in _repopulate_pool
    w.start()
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 130, in start
    self._popen = Popen(self)
  File ""/usr/lib64/python2.7/multiprocessing/forking.py"", line 121, in __init__
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
</code></pre>

<p>I'm creating my LDA model like this:</p>

<pre><code>ldamodel = LdaMulticore(corpus, num_topics=50, id2word = dictionary, workers=3)
</code></pre>

<p>I have actually asked another question about this script, so the full script can be found here: </p>

<p><a href=""https://stackoverflow.com/questions/54431187/gensim-lda-multicore-python-script-runs-much-too-slow"">Gensim LDA Multicore Python script runs much too slow</a></p>

<p>If it's relevant, I'm running this on a CentOS server. Let me know if I should include any other information. </p>

<p>Any help is appreciated!</p>
",,2019-01-30 07:19:06,gensim.LDAMulticore throwing exception:,<python><gensim><multicore><lda>,,,CC BY-SA 4.0,False,False,True,False,False
19821,54395907,2019-01-28 05:16:50,,"<p>I want to use pretrained embedding model (fasttext) in a pyspark application.</p>

<p>So if I broadcast the file (.bin), the following exception is thrown:
 Traceback (most recent call last):</p>

<pre><code>cPickle.PicklingError: Could not serialize broadcast: OverflowError: cannot serialize a string larger than 2 GiB
</code></pre>

<p>Instead, I tried to use <code>sc.addFile(modelpath)</code> where <code>modelpath=path/to/model.bin</code> as following:</p>

<p>i create a file called fasttextSpark.py</p>

<pre><code>import gensim
from gensim.models.fasttext import FastText as FT_gensim
# Load model (loads when this library is being imported)
model = FT_gensim.load_fasttext_format(""/project/6008168/bib/wiki.en.bin"")

# This is the function we use in UDF to predict the language of a given msg
def get_vector(msg):
    pred = model[msg]
    return pred
</code></pre>

<p>and testSubmit.sh:</p>

<pre><code>#!/bin/bash
#SBATCH -N 2
#SBATCH -t 00:10:00
#SBATCH --mem 20000
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task 32
module load python/2.7.14
source ""/project/6008168/bib/ENV2.7.14/bin/activate""
module load spark/2.3.0
spark-submit /project/6008168/bib/test.py
</code></pre>

<p>and the test.py:</p>

<pre><code>from __future__ import print_function
import sys
import time
import math
import csv
import datetime
import StringIO
import pyspark
import gensim
from operator import add
from pyspark.sql import *
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
from gensim.models.fasttext import FastText as FT_gensim
appName = ""bib""
modelpath = ""/project/6008168/bib/wiki.en.bin""
conf = (SparkConf()
         .setAppName(appName)
         .set(""spark.executor.memory"", ""12G"")
         .set(""spark.network.timeout"", ""800s"")
         .set(""spark.executor.heartbeatInterval"", ""20s"")
         .set(""spark.driver.maxResultSize"", ""12g"")
         .set(""spark.executor.instances"", 2)
         .set(""spark.executor.cores"", 30)
         )
sc = SparkContext(conf = conf)
#model = FT_gensim.load_fasttext_format(modelpath)
sc.addFile(modelpath)
sc.addPyFile(""/project/6008168/bib/fasttextSpark.py"")

# Import our custom fastText language classifier lib
import fasttextSpark
print (""nights = "", fasttextSpark.get_vector(""nights""))
print (""done"")
</code></pre>

<p>Now, each node will have a copy of the pretrained dataset. Some words are out of vocabulary so each time I'm facing such words I want to create for it a random but fixed vector and add the word and its vector to a dictionary. </p>

<p>So, How I can maintain such a dictionary in each node?</p>

<p>Indeed, suppose my rdd is as following my_rdd = (id, sentence) and I want to find the embedding vector of the sentence by summing up the vectors of its words. How many times the embedding model will be loaded. For example:</p>

<p>suppose <code>rdd=(""id1"", ""motorcycle parts"")</code>, does my implementation load the model two times: one for motorcycle and one for parts? if yes, my approach is inefficacce? In this case what it should be the best approaches to be applied?</p>
",2019-02-24 13:21:19,2019-02-24 13:21:19,How i can maintain a temporary dictionary in a pyspark application?,<python><apache-spark><pyspark><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
19822,54395916,2019-01-28 05:18:12,,"<p>I'm using the gensim python library to work on small corpora (around 1500 press articles each time). Let say I'm interested in creating clusters of articles relating the same news.</p>

<p>So for each corpus of articles I've tokenized, detected collocations, stemmed and then fed a little dictionary (around 20k tokens) I've passed though a TFIDF model.</p>

<p>Finally I've used the TFIDF corpus to build a LSI model of the corpus and with the help of the document similarity functions of gensim I was able to get very good results.</p>

<p>But I was curious and made some coherence checking of the LSI with:</p>

<pre><code>lsi_topics = [[word for word, prob in topic] for topicid, topic in 
lsi.show_topics(formatted=False)]
lsi_coherence = CoherenceModel(topics=lsi_topics[:10], texts=corpus, dictionary=dictionary, window_size=10).get_coherence()
logger.info(""lsi coherence: %.3f"" % lsi_coherence)
</code></pre>

<p>And I always get values around 0.45 which could seem pretty weak.</p>

<p>So I was wondering how to interpret this coherence value? And does this value make sense when you only need similarity of documents in the index to the index itself (so the queries are a full document from the corpus)?</p>

<p>Edit: I tried different things for text preprocessing such as splitting each document in real sentences before feeding the Phrases class, generating bigrams, trigrams or removing accents or not and in some cases I was able to get a coherence value around 0.55 so at least I guess it can help finding the most efficient way to process raw datas...</p>
",2019-01-30 03:41:31,2019-01-30 03:41:31,"What is a ""good"" value for LSI topic coherence?",<python><gensim><topic-modeling><latent-semantic-indexing>,,,CC BY-SA 4.0,False,False,True,False,False
19835,54362442,2019-01-25 09:34:43,,"<p>My goal is to read the lines inside a file and to replace all special characters like french characters (, , , ...) by normal characters (a, e, c, ...)</p>

<p>I work with Python 3 and in the documentation of gensim, the example works with a simple sentence like : deaccent("") but not with lines I read from a file
At this time, I just get """" and not ""aec"" with my code</p>

<pre><code>from gensim.utils import deaccent

def getTextFromFile(filename):
    with open(filename) as file:
        text = [line.rstrip() for line in file.readlines()]
    file.close()
    for line in text:
        print(deaccent(line))
    return text
</code></pre>

<p>My file contains : </p>

<p>I would like to get : aec</p>
",,2019-01-25 19:33:53,How to use properly deaccent method from gensim?,<python><string><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19840,54318701,2019-01-23 01:15:31,,"<p>I'd like to create a big gensim dictionary for french language to try getting better results in topic detection, similarities between texts and other things like that.
So I've planned to use a wikipedia dump and process it the following way:</p>

<ol>
<li>Extract each article from frwiki-YYYYMMDD-pages-articles.xml.bz2 (Done)</li>
<li>Tokenize each article (basically converting the text to lowercases, removing stop words and non-word characters) (Done)</li>
<li>Train a Phrases model on the articles to detect collocation.</li>
<li>Stem the resulting tokens in each article.</li>
<li>Feed the dictionary with the new corpus (one stemmed-collocated-tokenized article per line)</li>
</ol>

<p>Because of the very large size of the corpus, I don't store anything in memory and access the corpus via smart_open but it appears gensim Phrases model is consuming too much RAM to complete the third step.</p>

<p>Here is my sample code:</p>

<pre><code>corpus = smart_open(corpusFile, ""r"")
phrases = gensim.models.Phrases()
with smart_open(phrasesFile, ""wb"") as phrases_file:
    chunks_size = 10000
    texts, i = [], 0
    for text in corpus:
        texts.append(text.split())
        i += 1
        if i % chunks_size == 0:
            phrases.add_vocab(texts)
            texts = []
    phrases.save(phrases_file)
corpus.close()
</code></pre>

<p>Is there a way to complete the operation without freezing my computer or will I have to train the Phrases model only on a subset of my corpus?</p>
",,2019-01-23 05:03:32,How to train a Phrases model from a huge corpus of articles (wikipedia)?,<python><nltk><gensim><collocation>,,,CC BY-SA 4.0,True,False,True,False,False
19852,54400712,2019-01-28 11:10:47,,"<p>I have found so many similar questions none of them answer my problem can Someone help me . I have two legal documents I need to find which are contextually same or have same meaning what should be my approach. I thought of use something with LSTM wherever I see i get people having one or two sentences to compare . I want to do it for lot of docs and find out which of them are similar cannot get my head around how to begin my task  </p>
",,2019-02-04 19:10:42,How can I find semantically similar paragraph in two different text files (two documents),<nlp><gensim><recurrent-neural-network>,,,CC BY-SA 4.0,False,False,True,False,False
19857,54422810,2019-01-29 14:03:15,,"<p>I'm pretty new to Gensim and I'm trying to train my first model using word2vec model. I see that all the parameters are pretty straightforward and easy to understand, however I don't know how to track the loss of the model to see the progress. Also, I would like to be able to get the embeddings after each epoch so that I can also <em>show</em> that the predictions also get more <em>logical</em> with after each epoch. How can I do that?</p>

<p>OR, is it better to train for <em>iter=1</em> each time and save the loss and embeddings after each epoch? Sounds not too efficient.</p>

<p>Not much to show with the code but still posting it below:</p>

<pre><code>model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999)
</code></pre>
",,2019-01-29 14:43:54,Tracking loss and embeddings in Gensim word2vec model,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19865,54508420,2019-02-03 22:55:31,,"<p>My goal is to classify email, so each email have to correspond to a specific category like sport, clothes and so on.</p>

<p>My idea is to use a Recurrent Neural Network with one layer based on embedding (by word2vec). I'm using the followed schema ""emojify v2"" with some changing:</p>

<p><a href=""https://github.com/enggen/Deep-Learning-Coursera/blob/master/Sequence%20Models/Week2/Emojify/Emojify%20-%20v2.ipynb"" rel=""nofollow noreferrer"">https://github.com/enggen/Deep-Learning-Coursera/blob/master/Sequence%20Models/Week2/Emojify/Emojify%20-%20v2.ipynb</a></p>

<p>and the word2vec pretrained model downloaded from:
<a href=""http://hlt.isti.cnr.it/wordembeddings/"" rel=""nofollow noreferrer"">http://hlt.isti.cnr.it/wordembeddings/</a></p>

<p>The problem is that this neural network does not work properly. In fact, trying to train the network on only 2 categories (20 sample). I get an accuracy of 50-60% on the training set that is of course too low.</p>

<p>Do you have any hint to improve my result?</p>

<p>What could be the problem?</p>

<ul>
<li>a low number of samples?</li>
<li>a low number of nodes?</li>
<li>using loss='categorical_crossentropy'?</li>
<li>using optimizer='adam'?</li>
<li>the text of the email is too long?</li>
<li>the lenght of each email is different. This could be a problem?</li>
</ul>

<p>I'm using keras (tensorflow).</p>

<p>edit1: insert code</p>

<blockquote>
<pre><code>import numpy as np
import emoji
import matplotlib.pyplot as plt
import numpy as np
</code></pre>
</blockquote>

<pre><code># utiliti che serviranno in seguito
import csv
import numpy as np
import emoji
import pandas as pd
from sklearn.metrics import confusion_matrix

def read_glove_vecs(glove_file):
    with open(glove_file, 'r') as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)

        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map

def get_categories(filename):
    categories_code = []
    categories_name = []

    with open(filename, newline='', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            category_code = row[1]; 
            if (search(category_code, categories_code) == False):
                categories_code.append(row[1])
                categories_name.append(row[2])

        Y = np.asarray(categories_code)
        Z = np.asarray(categories_name)

    return Y, Z

def search(category, categories):
    for c in categories:
        if (c == category):
            return True
    return False   

def softmax(x):
    """"""Compute softmax values for each sets of scores in x.""""""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

def read_csv_for_email(filename):
    name = []
    text = []
    category = []
    category_code = []

    with open(filename, newline='', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            name.append(row[0])
            category_code.append(row[1])
            category.append(row[2])
            text.append(row[3])

        X = np.asarray(name)
        Y = np.asarray(category_code)
        Z = np.asarray(category)
        W = np.asarray(text)

    return X, Y, Z, W

def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)]
    return Y

# converto in one hot vector
def my_convert_to_one_hot(Y, values):
    #print(Y)
    #print(values)
    col = len(values)
    row = len(Y)
    #print(col)
    #print(row)
    R = np.zeros((row, col))
    for res in range(row):
        for i in range(col):
            #print(Y[res])
            #print(values[i])
            if Y[res] == values[i]:
                X = [0]*col;
                X[i] = 1;
                R[res] = np.array(X)
    return R;             


def print_predictions(X, pred):
    print()
    for i in range(X.shape[0]):
        print(X[i], label_to_emoji(int(pred[i])))


def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):

    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)

    df_conf_norm = df_confusion / df_confusion.sum(axis=1)

    plt.matshow(df_confusion, cmap=cmap) # imshow
    #plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(df_confusion.columns))
    plt.xticks(tick_marks, df_confusion.columns, rotation=45)
    plt.yticks(tick_marks, df_confusion.index)
    #plt.tight_layout()
    plt.ylabel(df_confusion.index.name)
    plt.xlabel(df_confusion.columns.name)


def predict(X, Y, W, b, word_to_vec_map):
    """"""
    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.

    Arguments:
    X -- input data containing sentences, numpy array of shape (m, None)
    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)

    Returns:
    pred -- numpy array of shape (m, 1) with your predictions
    """"""
    m = X.shape[0]
    pred = np.zeros((m, 1))

    for j in range(m):                       # Loop over training examples

        # Split jth test example (sentence) into list of lower case words
        words = X[j].lower().split()

        # Average words' vectors
        avg = np.zeros((50,))
        for w in words:
            avg += word_to_vec_map[w]
        avg = avg/len(words)

        # Forward propagation
        Z = np.dot(W, avg) + b
        A = softmax(Z)
        pred[j] = np.argmax(A)

    print(""Accuracy: ""  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))


    return pred

import tensorflow as tf
sess = tf.Session()
from keras.layers import Input, Dense, Activation

import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)

# GRADED FUNCTION: sentences_to_indices

def sentences_to_indices(X, word_to_index, max_len):
    """"""
    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.
    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). 

    Arguments:
    X -- array of sentences (strings), of shape (m, 1)
    word_to_index -- a dictionary containing the each word mapped to its index
    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. 

    Returns:
    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)
    """"""

    m = X.shape[0]                                   # number of training examples

    ### START CODE HERE ###
    # Initialize X_indices as a numpy matrix of zeros and the correct shape ( 1 line)
    X_indices = np.zeros((m, max_len))

    for i in range(m):                               # loop over training examples

        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.
        sentence_words = X[i].lower().split()

        # Initialize j to 0
        j = 0

        # Loop over the words of sentence_words
        for w in sentence_words:
            # Set the (i,j)th entry of X_indices to the index of the correct word.
            if w in word_to_index:
                if (j &gt;= maxLen):
                    continue
                X_indices[i, j] = word_to_index[w]
                # Increment j to j + 1
                j = j + 1

    ### END CODE HERE ###

    return X_indices

from gensim.models import Word2Vec
model = Word2Vec.load('/home/ale/Scrivania/emoji_ita/embedding/glove_WIKI') # glove model

def getMyModels (model):
    word_to_index = dict({})
    index_to_word = dict({})
    word_to_vec_map = dict({})
    for idx, key in enumerate(model.wv.vocab):
        word_to_index[key] = idx
        index_to_word[idx] = key
        word_to_vec_map[key] = model.wv[key]
    return word_to_index, index_to_word, word_to_vec_map

code_train, category_train, category_code_train, text_train = read_csv_for_email('/home/ale/Scrivania/TestClassificazione/dataText2Categories.csv')
#FIXME
X_test, Y_test, Z_test, text_test = read_csv_for_email('C:\Users\Alessio\Desktop\3Febbraio\dataText2Categories.csv')

word_to_index, index_to_word, word_to_vec_map = getMyModels(model)

# EMBEDDING LAYER

# GRADED FUNCTION: pretrained_embedding_layer
# emb_dim define dimensionality of your GloVe word vectors (= 50)
def pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim):
    """"""
    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.

    Arguments:
    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    embedding_layer -- pretrained layer Keras instance
    """"""

    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)

    ### START CODE HERE ###
    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros((vocab_len, emb_dim))

    # Set each row ""index"" of the embedding matrix to be the word vector representation of the ""index""th word of the vocabulary
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. 
    embedding_layer = Embedding(vocab_len, emb_dim)
    ### END CODE HERE ###

    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the ""None"".
    embedding_layer.build((None,))

    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])

    return embedding_layer

# MODELLO EMOJIFY2

def Classifier(input_shape, word_to_vec_map, word_to_index, emb_dim, num_activation):
    """"""
    Function creating the Emojify-v2 model's graph.

    Arguments:
    input_shape -- shape of the input, usually (max_len,)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    model -- a model instance in Keras
    """"""

    ### START CODE HERE ###
    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
    sentence_indices = Input(shape=input_shape, dtype=np.int32)

    # Create the embedding layer pretrained with GloVe Vectors (1 line)
    embedding_layer =  pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim)

    # Propagate sentence_indices through your embedding layer, you get back the embeddings
    embeddings = embedding_layer(sentence_indices)   

    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a batch of sequences.
    X = LSTM(128, return_sequences=True)(embeddings)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X trough another LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a single hidden state, not a batch of sequences.
    X = LSTM(128)(X)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
    X = Dense(num_activation, activation='softmax')(X)
    # Add a softmax activation
    X =  Activation('softmax')(X)

    # Create Model instance which converts sentence_indices into X.
    model = Model(sentence_indices, X)

    ### END CODE HERE ###

    return model

def max_text_lenght(filename):
    max_len = 0;
    max_id = 0;

    with open(filename, newline='', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            text = row[3]; 
            current_lenght = len(text);
            if (current_lenght &gt; max_len):
                max_len = current_lenght;
                max_id = row[0]
    #print(max_id)
    return max_len

def create_values_list():
    z, y = get_categories('C:\Users\Alessio\Desktop\3Febbraio\dataText2Categories.csv')
    return y;

values = create_values_list()
num_activation = len(values)
maxLen = 300
print(maxLen)
model = Classifier((maxLen,), word_to_vec_map, word_to_index, maxLen, num_activation)
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

Y_train_indices = sentences_to_indices(text_train, word_to_index, maxLen)

Z_train_oh = my_convert_to_one_hot(category_code_train, values)
print(Z_train_oh)

model.fit(Y_train_indices, Z_train_oh, epochs = 60, batch_size = 32, shuffle=True)

model.save('text_classification.h5')
</code></pre>
",2019-02-04 08:45:28,2019-02-04 08:45:28,Email classification using word2vec,<tensorflow><keras><word2vec><recurrent-neural-network><text-classification>,,,CC BY-SA 4.0,False,False,True,False,True
19873,54472367,2019-02-01 03:14:39,,"<p>I tried saving a word2vec model that I had trained with gensim like so:</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec(sentences, parameters)
model.save('modelfile.model')
</code></pre>

<p>Now when I try <code>Word2Vec.load('modelfile.model')</code>, I get:</p>

<pre><code>ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
</code></pre>

<p>Can post the full traceback if it helps.</p>
",,2019-02-01 04:08:23,Can't load saved gensim word2vec model,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19875,54476634,2019-02-01 09:34:42,,"<p>I am trying to feed sentence lists sequentially into gensim.models.Word2Vec, But it generates the TypeError: '_Token' object is not iterable. What should I do?</p>

<pre><code>    embedding_model= Word2Vec()
    for index, sentence_list in df.iterrows():
        embedding_model = Word2Vec(sentence_list, size=100, window=5, min_count=2, workers=2)
        embedding_model.train(tokenized_contents, total_examples=len(tsentence_list), epochs=10)
</code></pre>
",,2019-02-01 10:57:26,word2vec error: '_Token' object is not iterable,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19883,54492390,2019-02-02 11:00:22,,"<p>I cannot find anything about the default values about the parameters for gensim fasttext <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"" rel=""nofollow noreferrer"">here</a></p>

<p>Or are they the same as for the original Facebook fasttext implementation?</p>
",2019-02-02 13:24:38,2019-02-03 06:58:06,What are the defaults for gensim's fasttext?,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
19891,54323245,2019-01-23 08:49:50,,"<p>I'm trying to load a saved word2vec model. The file on loading throws error: cannot reshape array of size 974897576 into shape (6828798,300)</p>

<p>I've created a word2vec model by initialising 6828798 words to random vectors, by feeding a list of list of my vocabulary words <code>[['a', 'b', 'c']]</code> to gensim and running the model.</p>

<p>new_titles_grp is a pandas df, with synon_title (str) and averages (np.array.shape=(300,)) columns</p>

<pre><code>sentences = [new_titles_grp['synon_title'].tolist()]
model_gensim = word2vec.Word2Vec(sentences, min_count=1, size=300)
</code></pre>

<p>Then, i'm replacing all the vectors with my pre-computed vector values, (all are (300,) dimensions) with</p>

<pre><code>for ind, row in new_titles_grp.iterrows():
    model_gensim.wv.syn0[model_gensim.wv.vocab[row['synon_title']].index] = row['averages']

model_gensim.init_sims(replace=True) #doing this to normalize the values
</code></pre>

<p>The model works well when called in the same notebook</p>

<pre><code>model_gensim.most_similar(get_job_title('Data Science'), topn=20)
Out[26]: 
[('Senior Data Scientist', 0.9982969760894775),
 ('DATA SCIENTIST', 0.9951386451721191),
 ('Data Scientist Analyst', 0.9940630197525024),
 ('Associate Data Scientist', 0.9907373189926147),
 ('Lead Data Scientist', 0.9906764030456543),
 ('Data Scientist Machine Learning Engineer', 0.9891946315765381),
 ('Data Scientist Specialist', 0.9885110855102539),
 ('Data Analyst Scientist', 0.988399863243103),
 ('Data Scientist III', 0.9873133301734924),
 ('Senior Data Scientist Machine Learning', 0.9868470430374146),
 ('Research Data Scientist', 0.98624187707901),
 ('Data Scientist Engineer', 0.9855831861495972),
 ('Data Scientist Intern', 0.9855802059173584),
 ('Machine Learning Specialist', 0.9842516779899597),
 ('Principal Data Scientist', 0.9835132360458374),
 ('Senior Manager Data Scientist', 0.9829919338226318),
 ('Machine Learning Engineer', 0.9829450249671936),
 ('Data Scientist IV', 0.9827083349227905),
 ('Data Analytics Engineer', 0.9825129508972168),
 ('Senior Data Scientist Consultant', 0.9823513031005859)]
</code></pre>

<p>On loading it in different notebook is throwing this error, I thought the numpy array dimensions are not same, which is not the case, all are 300 (0-Dimension) vectors. Please help!!</p>

<p>EDIT:
I'm saving it using <code>model_gensim.save(SAVE_PATH+'/jd_w2v.bin')</code></p>

<p>Also, using </p>

<p><code>model_gensim.wv.save_word2vec_format('/dbfs/FileStore/tables/talent_lemma_collection/w2v_models_v_1/jd_w2v.bin', binary=True)</code></p>

<p>I'm able to save them (apparently)</p>

<p>Loading it using</p>

<pre><code>model_titles = gensim.models.Word2Vec.load(jd_titles_path)
</code></pre>

<p>Error: </p>

<pre><code>ValueError: cannot reshape array of size 974897576 into shape (6828798,300)
</code></pre>

<p>and</p>

<pre><code>model_titles = KeyedVectors.load_word2vec_format(datapath(jd_titles_path), binary=True, unicode_errors='ignore')
</code></pre>

<p>Error:</p>

<pre><code>ValueError: string size must be a multiple of element size
</code></pre>
",2019-01-24 05:54:24,2019-01-24 05:54:24,"Unable to load a saved gensim model, cannot reshape array of size 974897576 into shape (6828798,300)",<python-3.x><numpy><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19924,54587140,2019-02-08 06:35:50,,"<p>I'm working on an NLP problem and my goal is to be able to pass my data into sklearn's algos after having used Word2Vec via Python's Gensim Library. The underlying problem I am trying to solve is binary classification of a series of tweets. To do so I am modifying the code in <a href=""https://github.com/halidebey/PyCon2018/blob/master/analysis.py"" rel=""nofollow noreferrer"">this</a> git repo.</p>

<p>Here is part of the code relating to tokenization:</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
input_file[""tokens""] = input_file[""text""].apply(tokenizer.tokenize)
all_words = [word for tokens in input_file[""tokens""] for word in tokens]
sentence_lengths = [len(tokens) for tokens in input_file[""tokens""]]
vocabulary = sorted(set(all_words))
</code></pre>

<p>Now here is the part where I use Gensim's sklearn-api to try to vectorize my tweets:</p>

<pre><code>from sklearn.model_selection import train_test_split
from gensim.test.utils import common_texts
from gensim.sklearn_api import W2VTransformer
text = input_file[""text""].tolist()
labels = input_file[""label""].tolist()
X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2,random_state=40)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_w2v = model.fit(common_texts).transform(X_train)
</code></pre>

<p>This results in the following error:</p>

<pre><code>KeyError: ""word 'Great seeing you again, don't be a stranger!' not in vocabulary""
</code></pre>

<p>It seems that part of the issue is that Gensim is expecting to be fed one word at a time and instead it is getting entire tweets.</p>

<p>X_train is of type list, here are the first three elements of the list:</p>

<pre><code>[""Great seeing you again, don't be a stranger!"",
 ""Beautiful day here in sunny Prague. Not a cloud in the sky"",
 "" pfft! i wish I had a laptop like that""]
</code></pre>

<p><strong>Update</strong></p>

<p>In order to remedy this, I have tried the following:</p>

<pre><code>X_train_list = []
for sentence in X_train:
word_list = sentence.split(' ')
while("""" in word_list): 
    word_list.remove("""") 
X_train_list.append(word_list)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_tfidf = model.fit(common_texts).transform(X_train_list)
</code></pre>

<p>This produces the following error:</p>

<pre><code>KeyError: ""word 'here' not in vocabulary""
</code></pre>

<p>To be honest, this one blows my mind! How a common word like 'here' is not in the vocabulary is beyond me. Also wondering if tweets with stray letters will throwing errors, I imagine the weird jumbles of letters that often pass for words will cause similar issues.</p>
",2019-02-08 08:26:08,2019-02-08 08:26:08,Parsing a List of Tweets in Order to Utlize Gensim Word2Vec,<python><nlp><gensim><word2vec><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,True
19931,54626897,2019-02-11 08:56:14,,"<p>I've got a question about gensim <strong>Word2Vec</strong> and documentation doesn't help me.</p>

<p>For example in my block of text I have some sentences like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;
         ...
</code></pre>

<p>And in some time I have a new sentence like:</p>

<pre><code>&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;
</code></pre>

<p>How can I detect this situation? (of course Word4 is in dictionary too)</p>

<p>My solutions:
1). I tried to find most similar words for each and see - if the next word is in this is - OK, otherwise - I can find Word4. I mean I will do:</p>

<pre><code>model.most_similar('&lt;Word_i&gt;')
or
model.similar_by_vector('&lt;Word_i&gt;')
</code></pre>

<p>And in top of answer list I will get Word_i+1. But it doesn't work!
Because I thought that the words in the sentence after training will have quite similar coordinates and in top list Word_i+1 will be for Word_i.
But it's wrong. When I checked this solution and trained by all corpus of text I had a situation when Word_2 wasn't in top list for Word_1! My explanation that not the near words have quite similar coordinates, but words with contextual proximity have quite similar coordinates, it's not the same..</p>

<p>2). So my second solution is using <strong>doesnt_match()</strong>, which takes a list of words, and reports the one word which is furthest from the average of all the words.</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt; &lt;Word4&gt;'.split()))
</code></pre>

<p>And yes - in this case the answer will be Word4! (so I detect this word)
But if I do it with:</p>

<pre><code>print(model.doesnt_match('&lt;Word1&gt; &lt;Word2&gt; &lt;Word3&gt;'.split()))
</code></pre>

<p>The answer will be Word2 (for example). And if I again will explore top words for Word1 and Word3 I won't see Word2 in this lists, but this sentence (Word1 Word2 Word3) is normal.</p>

<p>So how can I detect it?</p>
",2019-02-11 09:12:33,2019-02-11 13:32:57,How Word2Vec works? Python,<python><python-3.x><machine-learning><nlp><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19936,54554922,2019-02-06 13:36:46,,"<p>--python3</p>

<p>I have created a word2vec model using gensim libraries and saved it in local disk. I want to upload that file to my s3 bucket. I have successfully created the word2vec model using gensim, but while uploading it to my bucket I get an error.</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>some <a href=""https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s"">links</a> suggest encoding for avoiding such error. Is that applicable to the word2vec vector model that is created? If so what type of encoding I have to do? If not is there any other way that I could upload the file. 
Here is my code to upload a file to my s3 bucket</p>

<pre><code>import boto 
from boto.s3.key import Key
from os.path import expanduser

def upload_file(aws_access_key_id, aws_secret_access_key, bucket_name, bucket_folders, path_to_file, file_name, job_id):
    try: 
        conn = boto.connect_s3(aws_access_key_id, aws_secret_access_key)
    except Exception as error: 
        #LOGGER.info( ""Cannot upload %s job vector to aws due to connection error in aws"")
        #LOGGER.exception(error)
        print(""connection error"")

    if conn:
        bucket = conn.get_bucket(bucket_name)
        check_file_in_bucket = bucket_folders + file_name
        if bucket.lookup(check_file_in_bucket): 
            # deleting the existing file on server
            (bucket.lookup(check_file_in_bucket)).delete()
        k = Key(bucket)
        k.key = check_file_in_bucket

        upload_file = path_to_file + file_name
        try:

            if os.path.isfile(upload_file):
                print(""file present"")
                upload_file = open(upload_file, 'r+')
                try:
                    size = os.fstat(upload_file.fileno()).st_size
                except:
                    # Not all file objects implement fileno(),
                    # so we fall back on this
                    file.seek(0, os.SEEK_END)
                    size = file.tell()
                sent = k.set_contents_from_file(upload_file, rewind=True)
                # Rewind for later use
                upload_file.seek(0)
                if sent == size:
                    #LOGGER.info(""jobvector model for %s has been sucessfully uploaded"", job_id)
                    print("" It worked"")
                else:
                    #LOGGER.info(""job vector model for %s has not sucessfully uploaded"", job_id)
                    print(""Try again"")
        except Exception as error: 
            #LOGGER.info(""Cannot upload %s job vector model as file not found in local disk"")
            #LOGGER.exception(error)
            print(""file not found in local disk"")
    return 0

if __name__ == '__main__':
    MODEL_FOLDER = expanduser(""~"") + '/modelsdata/job_vectors/'
    BUCKET_FOLDER = 'w2v_model/jobvectors/'
    BUCKET_NAME = 'test-voip'
    aws_access_key_id = CONFIG[""aws-s3""][""key_id""]
    aws_secret_access_key = CONFIG[""aws-s3""][""key_access""]
    upload_file(aws_access_key_id,aws_secret_access_key,\
        BUCKET_NAME, BUCKET_FOLDER, MODEL_FOLDER, '237091_model', 6789)
</code></pre>

<p>I have tried uploading a 'wav' file to my s3 bucket and it was sucessful with the above code.I am having issue with </p>
",,2019-02-06 13:36:46,uploading word2vec model in AWS gives error,<python><amazon-web-services><amazon-s3><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19941,54568930,2019-02-07 08:15:14,,"<p>Im writing a script that takes a website url and downloads it using beautiful soup. It then uses gensim.summarization to summarize the text but I keep getting ValueError(""input must have more than one sentence"") even thought the text has more than one sentence. The first section of the script works that downloads the text but I cant get the second part to summarize the text.</p>

<pre><code>import bs4 as bs
import urllib.request
from gensim.summarization import summarize
from gensim.summarization.textcleaner import split_sentences

#===========================================

print(""(Insert URL)"")
url = input()
sauce = urllib.request.urlopen(url).read()
soup = bs.BeautifulSoup(sauce,'lxml')

#===========================================

print(soup.title.string)
with open (soup.title.string + '.txt', 'wb') as file:
    for paragraph in soup.find_all('p'):
        text = paragraph.text.replace('.', '.\n')
        text = split_sentences(text)
        text = summarize(str(text))
        text = text.encode('utf-8', 'ignore')

#===========================================

        file.write(text+'\n\n'.encode('utf-8'))
</code></pre>

<p>It should create a .txt file with the summarized text in it after the script is run in whatever folder the .py file is located</p>
",,2019-02-07 09:12:34,"How to fix 'ValueError(""input must have more than one sentence"")' Error",<python-3.x><beautifulsoup><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
19946,54521323,2019-02-04 17:28:50,,"<p>I have protein sequences and want to do doc2vec. My goal is to have one vector for each sentence/sequence.</p>

<p>I have 1612 sentences/sequences and 30 classes so the label is not unique and many documents share the same labels.</p>

<p>So when I first tried doc2vec, it gave my just 30 vectors which is the number of unique labels. Then I decided to have multiple tags to get a vector for each sentence.</p>

<p>When I did this I ended up having more vectors than my sentences. Any explanations what might have gone wrong?</p>

<p><a href=""https://i.stack.imgur.com/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>

<p><a href=""https://i.stack.imgur.com/MVLN3.jpg"" rel=""nofollow noreferrer"">Screenshot of corpus</a></p>

<p><code>tagged = data.apply(lambda r: TaggedDocument(words=(r[""A""]), tags=[r.label,r.id]), axis=1)</code></p>

<p><code>print(len(tagged))</code></p>

<p><code>1612</code></p>

<p><code>sents = tagged.values</code></p>

<p><code>model = Doc2Vec(sents, size=5, window=5, iter=20, min_count = 0)</code></p>

<p><code>sents.shape</code></p>

<p><code>(1612,)</code></p>

<p><code>model.docvecs.vectors_docs.shape</code></p>

<p><code>(1643,5)</code></p>

<p><a href=""https://i.stack.imgur.com/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>
",2019-02-04 18:45:41,2019-02-04 19:40:49,I get more vectors than my documents size - gensim doc2vec,<python><tags><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19951,54537417,2019-02-05 15:09:35,,"<p>I am trying to rewrite algorithm that basically takes a input text file and compares with different documents and results the similarities.</p>

<p>Now I want to print output of unmatched words and output a new textile with unmatched words. </p>

<p>From this code, ""hello force"" is the input and is checked against the raw_documents and prints out rank for matched document between 0-1(word ""force"" is matched with second document and ouput gives more rank to second document but ""hello"" is not in any raw_document i want to print unmatched word ""hello"" as not matched ), But what i want is to print unmatched input word that was not matched with any of the raw_document </p>

<pre><code>import gensim
import nltk

from nltk.tokenize import word_tokenize

raw_documents = [""I'm taking the show on the road"",
                 ""My socks are a force multiplier."",
             ""I am the barber who cuts everyone's hair who doesn't cut their own."",
             ""Legend has it that the mind is a mad monkey."",
            ""I make my own fun.""]

gen_docs = [[w.lower() for w in word_tokenize(text)]
            for text in raw_documents]

dictionary = gensim.corpora.Dictionary(gen_docs)

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)
s = 0
for i in corpus:
    s += len(i)
sims = gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                      num_features=len(dictionary))
query_doc = [w.lower() for w in word_tokenize(""hello force"")]

query_doc_bow = dictionary.doc2bow(query_doc)

query_doc_tf_idf = tf_idf[query_doc_bow]
result = sims[query_doc_tf_idf]
print result
</code></pre>
",2019-02-06 08:35:32,2019-02-06 08:35:32,How can I find and print unmatched/dissimilar words from the documents(dataset)?,<python><dictionary><nltk><gensim><nltk-trainer>,,,CC BY-SA 4.0,True,False,True,False,False
19960,54592261,2019-02-08 12:14:45,,"<p>I've got a problem/question with <strong>Word2Vec</strong></p>

<p>As I understand: let's train a model on a corpus of text (in my way it's a corpus ~2 Gb size)
Let's take one line from this text and calculate a vector of this line (line's vector = sum of words vectors). It will be smth. like this:</p>

<pre><code>for w in words:
    coords += model[w]
</code></pre>

<p>Than let's calculate length of this vector. With standard library as:</p>

<pre><code>import numpy as np
vectorLen = np.linalg.norm(coords)
</code></pre>

<p>Why do we need Word2Vec? Yes, for converting words to vectors <strong>AND</strong> contextual proximity (near words that are found and words that are close in meaning have similar coordinates)!</p>

<p>And what I want (what I am waiting) - if I will take some line of the text and add some word from the dictionary which is not typical for this line, than again calculate length of this vector, I will get quite different value that if I will calculate only vector of this line without adding some uncharacteristic words to this line from dictionary.</p>

<p>But in fact - the values of this vectors (before adding word(s) and after) are quite the similar! Moreover - they are practically the same! Why am I getting this result? 
If I understand right for the line the coordinates of words will quite the same (contextual proximity), but new words will have rather different coordinates and it should affect to result (vector length of line with new words)!</p>

<p>E.x. it's my W2V model settings:</p>

<pre><code>#Word2Vec model

model = gensim.models.Word2Vec(
    sg=0,
    size=300,
    window=3,
    min_count=1,
    hs=0,
    negative=5,
    workers=10,
    alpha=0.025,
    min_alpha=0.025,
    sample=1e-3,
    iter=20
)

#prepare the model vocabulary
model.build_vocab(sentences, update=False)

#train model
model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>OR this:</p>

<pre><code>#Word2Vec model

model = gensim.models.Word2Vec(
    sg=1,
    size=100,
    window=10,
    min_count=1,
    hs=0,
    negative=5,
    workers=10,
    alpha=0.025,
    min_alpha=0.025,
    seed=7,
    sample=1e-3,
    hashfxn=hash,
    iter=20
)

#prepare the model vocabulary
model.build_vocab(sentences, update=False)
</code></pre>

<p>What's the problem? And how can I get necessary result? </p>
",2019-02-08 12:28:05,2019-02-08 13:46:29,Python. Gensim Wrod2vec. Words similarity,<python><python-3.x><machine-learning><nlp><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19966,54559615,2019-02-06 17:44:35,,"<p>When I start training my word2vec model, I am presented with the warning</p>

<blockquote>
  <p>consider setting layer size to a multiple of 4 for greater performance</p>
</blockquote>

<p>That sounds neat, but I can't find any reference to a <code>layer</code> argument or similar in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the documentation</a>. </p>

<p>So how can I increase the layer size, and how can I determine a good value?</p>
",,2019-02-06 18:24:41,Layer size in gensim's word2vec,<python><python-3.x><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19968,54627037,2019-02-11 09:05:05,,"<p>I am trying to rewrite algorithm that basically takes a input text file and compares with different documents and results the similarities.</p>

<p>Now I want to print output of unmatched words and output a new textile with unmatched words.</p>

<p>From this code, ""hello force"" is the input and is checked against the raw_documents and prints out rank for matched document between 0-1(word ""force"" is matched with second document and ouput gives more rank to second document but ""hello"" is not in any raw_document i want to print unmatched word ""hello"" as not matched ), But what i want is to print unmatched input word that was not matched with any of the raw_document</p>

<pre><code>import gensim
import nltk

from nltk.tokenize import word_tokenize

raw_documents = [""I'm taking the show on the road"",
                 ""My socks are a force multiplier."",
             ""I am the barber who cuts everyone's hair who doesn't 
cut their own."",
             ""Legend has it that the mind is a mad monkey."",
            ""I make my own fun.""]

gen_docs = [[w.lower() for w in word_tokenize(text)]
            for text in raw_documents]

dictionary = gensim.corpora.Dictionary(gen_docs)

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)
s = 0
for i in corpus:
    s += len(i)
sims =gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                  num_features=len(dictionary))
query_doc = [w.lower() for w in word_tokenize(""hello force"")]

query_doc_bow = dictionary.doc2bow(query_doc)

query_doc_tf_idf = tf_idf[query_doc_bow]
result = sims[query_doc_tf_idf] 
print result
</code></pre>
",,2019-02-11 14:27:55,How can I find and print unmatched/dissimilar words from the documents?,<python><scikit-learn><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,True
19976,54580260,2019-02-07 18:48:10,,"<p>I am unsure how I should use the most_similar method of gensim's Word2Vec. Let's say you want to test the tried-and-true example of: <em>man stands to king as woman stands to X</em>; find X. I thought that is what you could do with this method, but from the results I am getting I don't think that is true.</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">The documentation</a> reads:</p>

<blockquote>
  <p>Find the top-N most similar words. Positive words contribute
  positively towards the similarity, negative words negatively.</p>
  
  <p>This method computes cosine similarity between a simple mean of the
  projection weight vectors of the given words and the vectors for each
  word in the model. The method corresponds to the word-analogy and
  distance scripts in the original word2vec implementation.</p>
</blockquote>

<p>I assume, then, that <code>most_similar</code> takes the positive examples and negative examples, and tries to find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones. Is that correct?</p>

<p>Additionally, is there a method that allows us to map the relation between two points to another point and get the result (cf. the man-king woman-X example)?</p>
",,2019-02-07 20:17:27,Understanding gensim word2vec's most_similar,<python><python-3.x><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
19992,54684338,2019-02-14 06:23:03,,"<p>I'm running tfidf model in python.</p>

<pre><code>texts=[**tokenized words**]
dictionary = corpora.Dictionary(texts)
corpus = list(map(dictionary.doc2bow,texts))
test_model = models.TfidfModel(corpus)
corpus_tfidf = test_model[corpus]  
</code></pre>

<p>And it returns the output which gives some patterns of values to the exact same word.
For example, I chose the word ""AAA"".</p>

<pre><code>         key          score
0       ""AAA""       1
2323    ""AAA""       0.896502
4086    ""AAA""       0.844922 
</code></pre>

<p>Why do they have every different value even though they are exact same.</p>
",2019-02-14 09:22:21,2019-02-14 19:37:48,tfidf oucomes are different for the exact same word,<python><pandas><gensim><tf-idf><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
19994,54684552,2019-02-14 06:42:33,,"<p>After training an LDA model on gensim LDA model i converted the model to a with the gensim mallet via the <code>malletmodel2ldamodel</code> function provided with the wrapper. Before and after the conversion the topic word distributions are quite different. The mallet version returns very rare topic word distribution after conversion.</p>

<pre><code>ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=13, id2word=dictionary)
model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)
model.save('ldamallet.gensim')

dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')
corpus = pickle.load(open('corpus.pkl', 'rb'))
lda_mallet = gensim.models.wrappers.LdaMallet.load('ldamallet.gensim')
import pyLDAvis.gensim
lda_display = pyLDAvis.gensim.prepare(lda_mallet, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)
</code></pre>

<p><a href=""https://i.stack.imgur.com/jOPUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jOPUf.png"" alt=""Mallet Implementation after using &lt;code&gt;malletmodel2ldamodel&lt;/code&gt; ""></a></p>

<p>Here is the output from gensim original implementation:</p>

<p><a href=""https://i.stack.imgur.com/t2zpV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t2zpV.png"" alt=""Here is the output from gensim original implementation""></a></p>

<p>I can see there was a bug around this issue which has been fixed with the previous versions of gensim. I am using gensim=3.7.1</p>
",,2020-03-18 18:26:39,Issue with topic word distributions after malletmodel2ldamodel in gensim,<gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
20008,54673964,2019-02-13 15:37:30,,"<p>I have a DataFrame that has a text column. I am splitting the DataFrame into two parts based on the value in another column. One of those parts is indexed into a gensim similarity model. The other part is then fed into the model to find the indexed text that is most similar. This involves a couple of search functions to enumerate over each item in the indexed part. With the toy data, it is fast, but with my real data, it is much too slow using <code>apply</code>. Here is the code example:</p>

<pre><code>import pandas as pd
import gensim
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

d = {'number': [1,2,3,4,5], 'text': ['do you like python', 'do you hate python','do you like apples','who is nelson mandela','i am not interested'], 'answer':['no','yes','no','no','yes']}
df = pd.DataFrame(data=d)

df_yes = df[df['answer']=='yes']

df_no = df[df['answer']=='no']
df_no = df_no.reset_index()

docs = df_no['text'].tolist()
genDocs = [[w.lower() for w in word_tokenize(text)] for text in docs]
dictionary = gensim.corpora.Dictionary(genDocs)
corpus = [dictionary.doc2bow(genDoc) for genDoc in genDocs]
tfidf = gensim.models.TfidfModel(corpus)
sims = gensim.similarities.MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))

def search(row):
    query = [w.lower() for w in word_tokenize(row)]
    query_bag_of_words = dictionary.doc2bow(query)
    query_tfidf = tfidf[query_bag_of_words]
    return query_tfidf

def searchAll(row):
    max_similarity = max(sims[search(row)])
    index = [i for i, j in enumerate(sims[search(row)]) if j == max_similarity]
    return max_similarity, index

df_yes = df_yes.copy()

df_yes['max_similarity'], df_yes['index'] = zip(*df_yes['text'].apply(searchAll))
</code></pre>

<p>I have tried converting the operations to dask dataframes to no avail, as well as python multiprocessing. How would I make these functions more efficient? Is it possible to vectorize some/all of the functions? </p>
",2019-02-13 16:02:56,2019-02-14 06:29:37,Make Python Gensim Search Functions Efficient,<python><python-3.x><pandas><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
20022,54655604,2019-02-12 17:28:57,,"<p>I would like to use pre-trained embeddings in my neural network architecture. The pre-trained embeddings are trained by gensim. I found <a href=""https://stackoverflow.com/a/49802495/1150683"">this informative answer</a> which indicates that we can load pre_trained models like so:</p>

<pre><code>import gensim
from torch import nn

model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors)
emb = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors))
</code></pre>

<p>This seems to work correctly, also on 1.0.1. My question is, that I don't quite understand what I have to feed into such a layer to utilise it. Can I just feed the tokens (segmented sentence)? Do I need a mapping, for instance token-to-index? </p>

<p>I found that you can access a token's vector simply by something like</p>

<pre><code>print(weights['the'])
# [-1.1206588e+00  1.1578362e+00  2.8765252e-01 -1.1759659e+00 ... ]
</code></pre>

<p>What does that mean for an RNN architecture? Can we simply load in the tokens of the batch sequences? For instance:</p>

<pre><code>for seq_batch, y in batch_loader():
    # seq_batch is a batch of sequences (tokenized sentences)
    # e.g. [['i', 'like', 'cookies'],['it', 'is', 'raining'],['who', 'are', 'you']]
    output, hidden = model(seq_batch, hidden)
</code></pre>

<p>This does not seem to work so I am assuming you need to convert the tokens to its index in the final word2vec model. Is that true? I found that you can get the indices of words by using the word2vec model's <code>vocab</code>:</p>

<pre><code>weights.vocab['world'].index
# 147
</code></pre>

<p>So as an input to an Embedding layer, should I provide a tensor of <code>int</code> for a sequence of sentences that consist of a sequence of words? Example use with dummy dataloader (cf. example above) and dummy RNN welcome.</p>
",2019-02-12 19:32:26,2019-02-13 08:31:38,Expected input to torch Embedding layer with pre_trained vectors from gensim,<vector><pytorch><gensim><word2vec><recurrent-neural-network>,,,CC BY-SA 4.0,False,False,True,False,False
20026,54709178,2019-02-15 12:15:36,,"<p>I have a list of ~10 million sentences, where each of them contains up to 70 words.</p>

<p>I'm running gensim word2vec on every word, and then taking the simple average of each sentence. The problem is that I use min_count=1000, so a lot of words are not in the vocab. </p>

<p>To solve that, I intersect the vocab array (that contains about 10000 words) with every sentence, and if there's at least one element left in that intersection, it returns its the simple average, otherwise, it returns a vector of zeros.</p>

<p>The issue is that calculating every average takes a very long time when I run it on the whole dataset, even when splitting into multiple threads, and I would like to get a better solution that could run faster.</p>

<p>I'm running this on an EC2 r4.4xlarge instance.</p>

<p>I already tried switching to doc2vec, which was way faster, but the results were not as good as word2vec's simple average.</p>

<pre><code>word2vec_aug_32x = Word2Vec(sentences=sentences, 
                        min_count=1000, 
                        size=32, 
                        window=2,
                        workers=16, 
                        sg=0)

vocab_arr = np.array(list(word2vec_aug_32x.wv.vocab.keys()))

def get_embedded_average(sentence):
    sentence = np.intersect1d(sentence, vocab_arr)
    if sentence.shape[0] &gt; 0:
        return np.mean(word2vec_aug_32x[sentence], axis=0).tolist()
    else:
        return np.zeros(32).tolist()

pool = multiprocessing.Pool(processes=16)

w2v_averages = np.asarray(pool.map(get_embedded_average, np.asarray(sentences)))
pool.close()
</code></pre>

<p>If you have any suggestions of different algorithms or techniques that have the same purpose of sentence embedding and could solve my problem, I would love to read about it.</p>
",,2019-06-27 05:07:56,How to handle words that are not in word2vec's vocab optimally,<python><numpy><optimization><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20040,54644228,2019-02-12 06:43:41,,"<p>How do you use the Gensim predict output word function?</p>

<pre><code>model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)

model.predict_output_word(['Hi', 'how', 'you'], topn=10)

AttributeError: 'Word2VecKeyedVectors' object has no attribute 'predict_output_word'
</code></pre>

<p>I tried Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True), which was deprecated as well.</p>
",,2019-02-12 07:51:24,Gensim Predict Output Word Function Syntax,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20043,54730414,2019-02-17 05:28:10,,"<p>I have an important Module in my Graduation Project, It is about to learn a model that helps me to give it an input/query which expected to be an ""Islamic"" topic and the model's outputs are some Verses from the ""Holy Quran"" that related to my topic.</p>

<h2>An Example (in Arabic)</h2>

<p>Query may be something like this:  , or  </p>

<p>The Expected output some Quran verses related to the previous query like this:</p>

<ul>
<li>       </li>
<li>   </li>
<li>   </li>
</ul>

<p>This is a simple example, but note that the topic may be not be mentioned itself in the verse's text, so it may be more difficult than the above example.</p>

<h2>My Data Set</h2>

<p>Till now. I have collected an <strong>Arabic, Islamic</strong> data such as:</p>

<ul>
<li>Sayings of the Prophet Mohammed: about 35000 Arabic Documents, this is a sample from the data file:</li>
</ul>

<p>                                        ""       "".
</p>

<p>                                  ""           "".
</p>

<p>                     .               ""          "".
</p>

<p>                            ""      
        "".</p>

<ul>
<li>I have also the Holy Quran Meanings data set, about 7000 documents, and here is a sample of this documents:</li>
</ul>

<p>                                                         
          </p>

<p>        </p>

<p>                                        </p>

<p>              </p>

<p>       </p>

<p>                                                          </p>

<ul>
<li>I collected also manual annotated topics for each verse from an Islamic expert who classified the whole Quran verses to 11 main sections, each section my have subsections and subsubsections, ... so I have a 2-column data set, the first is the verse itself and the second is it's annotated topic. this is a sample:
<a href=""https://drive.google.com/open?id=1tEYs2QV9AmGQFqSzpvscNtlZfO3fVgCE"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1tEYs2QV9AmGQFqSzpvscNtlZfO3fVgCE</a></li>
</ul>

<p>note that, in the <code>ManualKeyword</code> column the topic which is after <code>-</code> mark is a subsection of the topic which is before <code>-</code></p>

<h2>Doc2Vec Model</h2>

<p>After many tries to achieve my goal, I read about <code>doc2vec</code> model and read it's paper and saw some implementations to it on Arabic tasks. I think my problem will be solved if I train a <code>doc2vec</code> model on the whole Arabic, Islamic data sets which I collected for now and any other data sets related to this field.</p>

<p>My Idea after training my model is to use it to embed each Manual annotated topic <strong>individuality</strong> (For clarification: I will separate each row like this <code> ---      </code> to single phrases</p>

<pre><code> 


    
</code></pre>

<p>and embed/represent each single topic with a vector)
then embed user's query (which is expected to be more than one word) to it's vector using my trained model. Then calculate the <strong>Cosine Similarity</strong> between Query's Vector and each topics' vector. so I can map the topic which gets the highest similarity to it's related verses and retrieve them to the user.</p>

<h2>My Code</h2>

<p><strong>After reading quran meanings and the Sayings of the Prophet data in</strong> <code>hadithsDocumentsList</code>:</p>

<pre><code>#Converting docuemnts list into tagged documents (each document is splited)
tagged_data = [gensim.models.doc2vec.TaggedDocument(words=_d.split(),
                              tags=[str(i)]) for i, _d in enumerate(hadithsDocumentsList)]

cores = multiprocessing.cpu_count()     #Getting number of cores

model = Doc2Vec(dm=1, size=200, window=10, workers=6)    #Initialize the model
model.build_vocab(tagged_data)      #Bulding vocabulary
print(""model.corpus_count"" , model.corpus_count)

#Training the model
model.train(tagged_data, total_examples=model.corpus_count, epochs=10)


#Saving Model
model.save(""HadithAndTafserModel"")
print(""Model Saved"")

model= Doc2Vec.load(""HadithAndTafserModel"")

testData = ""   "".split()
testDataVector = model.infer_vector(testData)
print(""Query Vector: "", testDataVector)

mostSemilarDocs = model.wv.most_similar([testDataVector])
print(""MOST SIMILAR DOCUMENTS: \n"")
print(mostSemilarDocs)
</code></pre>

<p>After training the model and getting a vector for a query from my choice and using <code>most_similar</code> function to get similar documents from my model. the output is definitely not what I expected. and model's accuracy is very bad.</p>

<p>I don't know what are the correct parameters like <code>dm</code>, <code>dbow_words</code>, <code>size</code>, <code>window</code>, <code>alpha</code> that should be passed to the model to achieve the highest accuracy. I little understanding each parameter's functionality and need help to tune each of them. I also want to know if my dataset is enough to build this model or I need to increase it? is there any inaccuracy or mistakes while collecting or passing them to the model?</p>

<p>What are your suggestions or opinions?</p>
",,2019-02-17 22:18:10,Need help while building my doc2vec embedding model for Holy Quran verses retrieval system based on verse's topic,<python><deep-learning><nlp><arabic><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20047,54675013,2019-02-13 16:27:23,,"<p>I have a corpus of text. For a preprocessing data I've vectorized all text using gensim Word2Vec.
I don't understand what I do exactly wrong. For the base I've took this discussion (and good tutorial) <a href=""https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation"">predict next word</a>. Code: <a href=""https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b"" rel=""nofollow noreferrer"">Source code</a>.</p>

<p>As input I have lines of sentences. I want to take each line, then take word[0] of this line -> predict word[1 ]. Then using word[0] and word[1 ] predict word[3], and so on to the end of line.</p>

<p>In this tutorial each time predicts fix length of words.
What I do:</p>

<pre><code>def on_epoch_end(epoch, _):
    print('\nGenerating text after epoch: %d' % epoch)
    for sentence in inpt:
        word_first=sentence.split()[0]
        sample = generate_next(word_first, len(sentence))
        print('%s... -&gt; %s' % (word_first, sample))
</code></pre>

<p>I take first word and use it to generate all next. And as second parameter I give length of sentence (not <code>num_generated=10</code>) as in tutorial. But it doesn't help for me at all. Every time I'm getting output predicted sequence of words with random(in my opinion) length.</p>

<p>What am I doing wrong and how to fix it?</p>
",2019-02-15 08:23:57,2019-02-17 05:05:32,"pre-trained Word2Vec with LSTM, predict next word in sentence",<python><machine-learning><keras><nlp><lstm>,,,CC BY-SA 4.0,False,False,True,False,False
20053,54697748,2019-02-14 19:26:53,,"<p>I need to remove an invalid word from the vocab of a ""gensim.models.keyedvectors.Word2VecKeyedVectors"". </p>

<p>I tried to remove it using <code>del model.vocab[word]</code>, if I print the <code>model.vocab</code> the word disappeared, but when I run <code>model.most_similar</code> using other words the word that I deleted is still appearing as similar. 
So how can I delete a word from <code>model.vocab</code> in a way that affect the <code>model.most_similar</code> to not bring it?</p>
",2019-02-14 19:40:27,2019-04-18 07:55:12,Is there a way to remove a word from a KeyedVectors vocab?,<gensim><word2vec><embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
20056,54713844,2019-02-15 16:57:43,,"<p>I have trained a <code>Word2vec</code> model on the ""brown corpus"". </p>

<p>I want to apply the vectorized words to a new text document, whose sentences I then want to cluster by way <code>Affinity Propagation</code>.</p>

<pre><code>import gensim
import nltk
from nltk.corpus import brown

sentences = brown.sents()
model = gensim.models.Word2Vec(sentences, min_count=1)
model.save('brown_model')

model = gensim.models.Word2Vec.load('brown_model')
</code></pre>

<p>My text document contains a list of requests such as:</p>

<pre><code>""I want to go to the store""

""I want the president to help me""

""Clean up my house""
</code></pre>

<p>My question is:</p>

<p>How can I apply the vectorized <code>Brown corpus</code> to my own text data for subsequent clustering purposes?</p>
",2019-02-15 21:43:51,2019-02-15 21:48:52,Vectorizing new text data,<python><vectorization><text-mining><word-embedding><natural-language-processing>,,,CC BY-SA 4.0,True,False,True,False,False
20058,54717449,2019-02-15 21:43:21,,"<p>I am using spaCy as part of a topic modelling solution and I have a situation where I need to map a derived word vector to the ""closest"" or ""most similar"" word in a vocabulary of word vectors.</p>

<p>I see gensim has a function (WordEmbeddingsKeyedVectors.similar_by_vector) to calculate this, but I was wondering if spaCy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?</p>
",2020-10-14 11:47:10,2020-10-14 11:47:10,Mapping word vector to the most similar/closest word using spaCy,<nlp><spacy><word2vec><word-embedding>,,,CC BY-SA 4.0,False,True,True,False,False
20064,54736839,2019-02-17 19:29:56,,"<p>I've trained <code>Doc2Vec</code> model I'm trying to get predictions.</p>

<p>I use</p>

<pre><code>test_data = word_tokenize(""   .."".lower())
model = Doc2Vec.load(model_path)
v1 = model.infer_vector(test_data)
sims = model.docvecs.most_similar([v1])
print(sims)
</code></pre>

<p>returns</p>

<pre><code>[('624319', 0.7534812092781067), ('566511', 0.7333904504776001), ('517382', 0.7264763116836548), ('523368', 0.7254455089569092), ('494248', 0.7212602496147156), ('382920', 0.7092794179916382), ('530910', 0.7086726427078247), ('513421', 0.6893941760063171), ('196931', 0.6776881814002991), ('196947', 0.6705600023269653)]
</code></pre>

<p>Next I've tried to know, what's text of this number</p>

<pre><code>model.docvecs['624319']
</code></pre>

<p>But it returns me only the vector representation</p>

<pre><code>array([ 0.36298314, -0.8048847 , -1.4890883 , -0.3737898 , -0.00292279,
   -0.6606688 , -0.12611026, -0.14547637,  0.78830665,  0.6172428 ,
   -0.04928801,  0.36754376, -0.54034036,  0.04631123,  0.24066721,
    0.22503968,  0.02870891,  0.28329515,  0.05591608,  0.00457001],
  dtype=float32)
</code></pre>

<p>So, is any way to get text of this label from the model?
Loading train dataset takes a lot of time, so I try to find out another way.</p>
",,2019-02-17 22:16:00,Doc2Vec: get text of the label,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20074,54754669,2019-02-18 20:11:40,,"<p>I try to train model to get sentence similarity (In my case names of some organization)</p>

<p>I use to train model</p>

<pre><code>names_tok = [TaggedDocument(words=word_tokenize(name.lower()), tags=[str(i)])
                        for (i, name) in enumerate(names)]

# train model
max_epochs = 50
vec_size = 50
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm=1)
</code></pre>

<p>And I get results with </p>

<pre><code>name = word_tokenize(name.lower())
infer_v = model.infer_vector(name)
results = model.docvecs.most_similar([infer_v]))
</code></pre>

<p>And it returns strange results for all tests. I've already tried to use example from train data and I haven't got it with similarity. For example <code>   ..</code> I get next results</p>

<pre><code>  ..: 0.9336682558059692

    ..: 0.9370058178901672

 ..: 0.9347286224365234

    .: 0.9339677095413208
</code></pre>

<p>And I can't understand Why I get this.
I have 180 000 examples of train data.
How can I improve results of my model?</p>
",2019-02-19 06:31:54,2019-02-20 00:55:19,Doc2Vec: strange results with model.docvecs.most_similar,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20085,54722074,2019-02-16 10:24:22,,"<p>I'm trying to perform sentiment analysis over twitter data using Deep Learning ( RNN ). I know that there are various other deep learning libraries out there like TF, keras , gensim etc., but i wanted to know if it is possible to perform deep learning using the CoreNLP Library. </p>

<p><a href=""https://github.com/charlescc9/deep-learning-sentiment-analysis"" rel=""nofollow noreferrer"">https://github.com/charlescc9/deep-learning-sentiment-analysis</a></p>

<p>This person above tries to compare gensim, tensorflow and core nlp for deep learning. But there's barely any documentation and i can't understand how to run the file (or) the dependecies required . Please help me out here.</p>
",2019-02-18 04:59:21,2019-02-18 05:31:39,"Apart from Keras and Spacy, can I use Stanford Core NLP for Deep Learning?",<deep-learning><nlp><stanford-nlp><lstm>,,,CC BY-SA 4.0,False,True,True,True,False
20113,54823225,2019-02-22 08:49:26,,"<p>I have trained the LDA model to cluster 100 topic, and according to my knowledge, every topic should be outputted with a certain probabiliy, all adding up to 1.</p>

<p>But when I run this code, I am getting only 2 topics.</p>

<p>Please help.</p>

<pre><code>text = ""A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.""

# transform text into the bag-of-words space
bow_vector = dictionary.doc2bow(tokenize(text))
lda_vector = lda_model[bow_vector]
print(""LDA Output: "", lda_vector)
print(""\nTop Keywords from highest prob Topic: "",lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))
print(""\n\nAddition of all the probabilities from LDA output:"",functools.reduce(lambda x,y:x+y,[i[1] for i in lda_vector]))
</code></pre>

<blockquote>
  <p>LDA Output:  [(64, 0.6952628), (69, 0.18223721)]</p>
  
  <p>Top Keywords from highest prob Topic:  0.042*""health"" + 0.032*""medical"" + 0.017*""patients"" + 0.016*""cancer"" + 0.015*""hospital"" + 0.015*""said"" + 0.015*""treatment"" + 0.012*""doctors"" + 0.012*""care"" + 0.012*""drug""</p>
  
  <p>Addition of all the probabilities from LDA output: 0.8775</p>
</blockquote>
",,2019-02-28 00:19:49,Gensim LDA giving output of Topic IDs but probabilities are not adding up to 1,<machine-learning><nlp><gensim><topic-modeling><unsupervised-learning>,,,CC BY-SA 4.0,False,False,True,False,False
20126,54842129,2019-02-23 13:29:22,,"<p>I've been learning NLP text classification via book ""Text Analytics with Python"". It's required several modules to be installed in a virtual environment. I use Anaconda env. I created a blank env with Python 3.7 and installed required pandas, numpy, nltk, gensim, sklearn... then, I have to install Pattern. The first problem is that I can't install Pattern via conda because of a conflict  between Pattern and mkl_random.</p>

<pre><code>(nlp) D:\Python\Text_classification&gt;conda install -c mickc pattern
Solving environment: failed

UnsatisfiableError: The following specifications were found to be in conflict:
  - mkl_random
  - pattern
Use ""conda info &lt;package&gt;"" to see the dependencies for each package.
</code></pre>

<p>It's impossible to remove mkl_random because there're related packages: gensim, numpy, scikit-learn etc. I don't know what to do, I didn't find any suitable conda installations for Pattern that is accepted in my case. Then, I installed Pattern using pip. Installation was successful. Is it okay to have packages from conda and from pip at the same time?</p>

<p>The second problem, I think, is connected with the first one. I downloaded the book's example codes from <a href=""https://github.com/dipanjanS/text-analytics-with-python/tree/master/Old-First-Edition/source_code/Ch04_Text_Classification"" rel=""nofollow noreferrer"">https://github.com/dipanjanS/text-analytics-with-python/tree/master/Old-First-Edition/source_code/Ch04_Text_Classification</a>, added brackets to Python 2.x 'print' functions and run classification.py
The program raised an exception:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 609, in _read
    raise StopIteration
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""classification.py"", line 50, in &lt;module&gt;
    norm_train_corpus = normalize_corpus(train_corpus)
  File ""D:\Python\Text_classification\normalization.py"", line 96, in normalize_corpus
    text = lemmatize_text(text)
  File ""D:\Python\Text_classification\normalization.py"", line 67, in lemmatize_text
    pos_tagged_text = pos_tag_text(text)
  File ""D:\Python\Text_classification\normalization.py"", line 58, in pos_tag_text
    tagged_text = tag(text)
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\en\__init__.py"", line 188, in tag
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\en\__init__.py"", line 169, in parse
    return parser.parse(s, *args, **kwargs)
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 1172, in parse
    s[i] = self.find_tags(s[i], **kwargs)
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\en\__init__.py"", line 114, in find_tags
    return _Parser.find_tags(self, tokens, **kwargs)
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 1113, in find_tags
    lexicon = kwargs.get(""lexicon"", self.lexicon or {}),
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 376, in __len__
    return self._lazy(""__len__"")
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 368, in _lazy
    self.load()
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 625, in load
    dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
  File ""C:\Users\PC\Anaconda3\envs\nlp\lib\site-packages\pattern\text\__init__.py"", line 625, in &lt;genexpr&gt;
    dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
RuntimeError: generator raised StopIteration
</code></pre>

<p>I don't understand what is happening. Is the exception raised because my installation with pip, or the problem is in the wrong or deprecated code in the book... and is it possible to install Pattern in conda with all other necessary packages.</p>

<p>Thank you in advance!</p>
",,2019-12-05 09:00:28,Pattern module issues (NLP learning),<python><design-patterns><pip><nlp><anaconda>,,,CC BY-SA 4.0,True,False,True,False,True
20136,54863236,2019-02-25 09:40:00,,"<p>I am trying to extract Indonesia titles from a wiki titles dump that's in a text file using word2vec-gensim in Python 3. The wiki dump contains titles in other languages also and some symbols. Below is my code:</p>

<pre><code>    if len(sys.argv) != 3: 
    namaFileInput = ""idwiki-latest-pages-articles.xml.bz2""
    namaFileOutput = ""wiki.id.case.text""
    sys.exit(1)
inp, outp = sys.argv[1:3]
space = "" ""
i = 0

output = open(namaFileOutput, 'w')

# lower=False: huruf kecil dan besar dibedakan
wiki = WikiCorpus(namaFileInput, lemmatize=False, dictionary={}, lower=False)
for text in wiki.get_texts():
    if six.PY3:
        output.write(b' '.join(text).encode('utf-8') + '\n')
    else:
        output.write(space.join(text) + ""\n"")
    i = i + 1
    if i % 10000 == 0:
        logger.info(""Saved "" + str(i) + "" articles"")

output.close()
logger.info(""Finished Saved "" + str(i) + "" articles"")
</code></pre>

<p>But I am getting and error:</p>

<pre><code>    TypeError                                 Traceback (most recent call last)
&lt;ipython-input-17-d4c686a9093a&gt; in &lt;module&gt;
     29 for text in wiki.get_texts():
     30     if six.PY3:
---&gt; 31         output.write(b' '.join(text).encode('utf-8') + '\n')
     32     else:
     33         output.write(space.join(text) + ""\n"")

TypeError: sequence item 0: expected a bytes-like object, str found
</code></pre>

<p>I have searched online but could not succeed. Any help will be appreciated.</p>
",,2019-02-25 18:56:44,"TypeError: sequence item 0: expected a bytes-like object, str found",<gensim><word2vec><python-3.7>,,,CC BY-SA 4.0,False,False,True,False,False
20155,54924835,2019-02-28 11:37:28,,"<p>I'm new to Sagemaker and am running some tests to measure the performance of NTM and LDA on AWS compared with LDA mallet and native Gensim LDA model.</p>

<p>I'm wanting to inspect the trained models on Sagemaker and look at stuff like what words have the highest contribution for each topic. And also to get a measure of model coherence.</p>

<p>I have been able to successfully get what words have the highest contribution for each topic for NTM on Sagemaker by downloading the output file untarring it and unzipping to expose 3 files params, symbol.json and meta.json. </p>

<p>However, when I try to do the same process for LDA, the untarred output file cannot be unzipped.</p>

<p>Maybe I'm missing something or should do something different for LDA compared with NTM but I have not been able to find any documentation on this. Also, anyone found a simple way to calculate model coherence?</p>

<p>Any assistance would be greatly appreciated!</p>
",,2020-02-17 10:35:04,Sagemaker LDA topic model - how to access the params of the trained model? Also is there a simple way to capture coherence,<python><lda><amazon-sagemaker>,,,CC BY-SA 4.0,False,False,True,False,False
20166,54888490,2019-02-26 15:05:38,,"<p>how to print to log (file or stout) the loss of each epoch in the training phase, when using gensim word2vec model. </p>

<p>I tried :</p>

<pre><code> logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
 logging.root.setLevel(level=logging.INFO)
</code></pre>

<p>But I didn't saw any loss printing.</p>
",,2019-10-23 04:08:03,gensim word2vec print log loss,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20170,54623849,2019-02-11 04:08:38,,"<p>What is the similarity score in the genism similar_by_word function?</p>

<p>I was reading here about the genism similar_by_word function:
<a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a></p>

<p>The similar_by_word function returns a sequence of (word, similarity). What is the definition by similarity here and how is it calculated?</p>
",,2019-02-11 07:39:27,What is the similarity score in the gensim similar_by_word function?,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20171,54623993,2019-02-11 04:31:22,,"<p>I am following the following gensim tutorial to transform my word2vec model to tensor.
Link to the tutorial: <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/word2vec2tensor.html</a></p>

<p>More specifically, I ran the following command</p>

<pre><code>python -m gensim.scripts.word2vec2tensor -i C:\Users\Emi\Desktop\word2vec\model_name -o C:\Users\Emi\Desktop\word2vec
</code></pre>

<p>However, I get the following error for the above command.</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>When I use <code>model.wv.save_word2vec_format(model_name)</code> to save my model (as mentioned in the following link: <a href=""https://github.com/RaRe-Technologies/gensim/issues/1847"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1847</a>) and then use the above command I get the following error.</p>

<pre><code>ValueError: invalid vector on line 1 (is this really the text format?)
</code></pre>

<p>Just wondering if I have made any mistakes in the syntax of the commads. Please let me know how to resolve this issue.</p>

<p>I am happy to provide more details if needed.</p>
",2019-02-11 04:38:09,2020-06-30 20:57:28,How to use word2vec2tensor in gensim?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20177,54870236,2019-02-25 16:03:52,,"<p>I tried to load gensim in my code. Often it works fine. Today, I get the following exception:</p>

<pre><code>Traceback (most recent call last):
  File ""/project/6008168/tamouze/just.py"", line 2, in &lt;module&gt;
    import gensim
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/__init__.py"", line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/__init__.py"", line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/preprocessing.py"", line 40, in &lt;module&gt;
    from gensim import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/utils.py"", line 44, in &lt;module&gt;
    from smart_open import smart_open
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/__init__.py"", line 1, in &lt;module&gt;
    from .smart_open_lib import *
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 29, in &lt;module&gt;
    import requests
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/__init__.py"", line 97, in &lt;module&gt;
    from . import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/utils.py"", line 26, in &lt;module&gt;
    from ._internal_utils import to_native_string
ImportError: cannot import name to_native_string
</code></pre>

<p>Im using python 2.7.14 and gensim 3.4.0.
How can I solve this problem?</p>
",,2019-02-25 20:03:46,Exception during calling gensim?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20193,54929726,2019-02-28 16:02:09,,"<p>I'm new to Word2Vec and I am trying to cluster words based on their similarity.  To start I am using nltk to separate the sentences and then using the resulting list of sentences as the input into Word2Vec.  However, when I print the vocab, it is just a bunch of letters, numbers and symbols rather than words. To be specific, an example of one of the letters is ""&lt; gensim.models.keyedvectors.Vocab object at 0x00000238145AB438>, 'L':""</p>

<pre><code># imports needed and logging
import gensim
from gensim.models import word2vec
import logging

import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
with open('C:\\Users\\Freddy\\Desktop\\Thesis\\Descriptions.txt','r') as f_open:
    text = f_open.read()
arr = []

sentences = nltk.sent_tokenize(text) # this gives a list of sentences

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)

model = word2vec.Word2Vec(sentences, size = 300)

print(model.wv.vocab)
</code></pre>
",2019-02-28 17:57:20,2019-02-28 17:57:20,Word2Vec vocab results in just letters and symbols,<python><python-3.x><tokenize><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
20209,54839158,2019-02-23 07:09:09,,"<p>After I trained word embeddings, I saved it as npz format.
While I am trying to load it as KeyedVectors format, it makes errors.
How can I load numpy array as gensim.KeyedVectors format?
I really need it because I need to use functions like most_similar() not just vector values.</p>

<p>in model.py with tensorflow,</p>

<pre><code>self.verb_embeddings = tf.Variable(np.load(cfg.pretrained_target)[""embeddings""],
                                               name=""verb_embeddings"",
                                               dtype=tf.float32,
                                               trainable=cfg.tune_emb)
</code></pre>

<p>in saving.py</p>

<pre><code>target_emb = sess.run(model.verb_embeddings)
np.savez_compressed(""trained_target_emb.npz"", embeddings=target_emb)
</code></pre>

<p>in main.py</p>

<pre><code> model = KeyedVectors.load('trained_target_emb.npz')
</code></pre>

<p>I got</p>

<pre><code>_pickle.UnpicklingError: A load persistent id instruction was encountered, but no persistent_load function was specified.
</code></pre>

<p>also tried</p>

<pre><code> model = KeyedVectors.load_word2vec_format('trained_target_emb.npz')
</code></pre>

<p>but got</p>

<pre><code> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xde in position 14: invalid continuation byte
</code></pre>
",,2019-02-25 21:18:57,How to load numpy array to gensim Keyedvector format?,<python><numpy><tensorflow><gensim><embedding>,,,CC BY-SA 4.0,False,False,True,False,False
20236,54878715,2019-02-26 05:02:26,,"<p>I am getting this error when trying to get the coherence score for topic modelling from Gensim package. The error doesn't seem to be syntax related:</p>

<pre><code>    coherence_lda_gens = CoherenceModel(model=lda_gens, 
    texts=data_lemmatized, dictionary=dict_lemm, coherence='c_v')
    coherence = coherence_lda_gens.get_coherence()  #this line fails*emphasized text*
    print('\nCoherence Score: ',coherence)
</code></pre>

<p>and I get this error: </p>

<pre><code> File ""C:\Users\msharifpour\AppData\Local\Continuum\anaconda3\lib\multiprocessing\spawn.py"", line 172, in get_preparation_data
    main_mod_name = getattr(main_module.__spec__, ""name"", None)

AttributeError: module '__main__' has no attribute '__spec__'
</code></pre>

<p>I googled this problem quite a bit, but can't find where the problem comes from and what is the fix?</p>

<p>Thanks</p>
",,2019-02-26 05:02:26,Error in Coherence score in Gensim package,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20243,54950481,2019-03-01 18:39:02,,"<p>I have googled this issue but I cannot find any reliable solution (some sources gives log(V) some log(V/2). But what is the time complexity of the word2vec model with the following parameters:</p>

<p><code>Word2Vec(corpus, size=4000, window=30, min_count=1, workers=50, iter=100, alpha=0.0001)</code></p>

<p>I have a vocabulary that equals to 10000 words (unique words). </p>
",2020-07-25 19:07:00,2020-07-25 20:30:20,Word2Vec time complexity,<python><time-complexity><big-o><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20250,54972481,2019-03-03 18:52:54,,"<p>I am working with gensim WmdSimilarity. I followed <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">this tutorial</a>. But for a pre-trained model, it took 5-6 seconds for each output. I found <a href=""https://github.com/src-d/wmd-relax"" rel=""nofollow noreferrer"">some other implementation</a> of calculating WMD (Word Mover Distance) but I'm not sure about using it with pre-trained W2V </p>
",2019-03-04 07:10:03,2019-03-04 07:10:03,Is there any way to calculate gensim WmdSimilarity faster,<nlp><gensim><word2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
20266,54911712,2019-02-27 17:56:52,,"<p>This may seem like an odd question but I'm new to this so thought I'd ask anyway.</p>

<p>I want to use this Google News model over various different files on my laptop. This means I will be running this line over and over again in different Jupyter notebooks: </p>

<p>model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)</p>

<p>Does this eat 1) Storage (I've noticed my storage filling up exponentially for no reason) 
2) Less memory than it would otherwise if I close the previous notebook before running the next.</p>

<p>My storage has gone down by 50GB in one day and the only thing I have done on this computer is run the Google News model (I didn't do most_similar()). Restarting and closing notebooks hasn't helped and there aren't any big files on the laptop. Any ideas?</p>

<p>Thanks. </p>
",2019-02-28 11:37:21,2019-02-28 11:37:21,Does the Google News Word2Vec model take up storage every time you run it?,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
20290,55028281,2019-03-06 16:50:55,,"<p>I'm using <code>Gensim</code> for loading the german <code>.bin</code> files from <code>Fasttext</code> in order to get vector representations for out-of-vocabulary words and phrases. So far it works fine and I achieve good results overall.<br>
I am familiar with the <code>KeyError :'all ngrams for word &lt;word&gt; absent from model'.</code> Clearly the model doesn't provide a vector representation for every possible ngram combination.<br>
But now I ran into a confusing (at least for me) issue.<br>
I'll just give a quick example:<br>
the model provides a representation for the phrase <code>AuM Wert</code>.<br>
But when I want to get a representation for <code>AuM Wert 50 Mio. Eur</code>, I'll get the <code>KeyError</code> mentioned above. So the model obviously has a representation for the shorter phrase but not for the extended one.<br>
It even returns a representation for <code>AuM Wert 50 Mio.Eur</code> (I just removed the space between 'Mio' and 'Eur')<br>
I mean, the statement in the Error is simply not true, because the first example shows that it knows some of the ngrams. Can someone explain that to me? What don't I understand here? Is my understanding of ngrams wrong?</p>

<p>Heres the code: </p>

<pre><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.de.300.bin')
model.wv['AuM Wert'] #returns a vector
model.wv['AuM Wert 50 Mio.EUR'] #returns a vector
model.wv['AuM Wert 50 Mio. EUR'] #triggers the error
</code></pre>

<p>Thanks in advance,<br>
Amos</p>
",2019-03-10 00:04:23,2019-03-10 06:12:20,"Fasttext representation for short phrase, but not for longer phrase containing the short one",<python><nlp><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
20300,55067412,2019-03-08 16:40:53,,"<p>I'm new to NLP but I'm trying to match a list of sentences to another list of sentences in Python based on their semantic similarity. For example,</p>

<pre><code>list1 = ['what they ate for lunch', 'height in inches', 'subjectid']
list2 = ['food eaten two days ago', 'height in centimeters', 'id']
</code></pre>

<p>Based on previous posts and prior knowledge, it seemed the best way was to create document vectors of each sentence and compute the cosine similarity score between lists. Other posts I've found with regards to Doc2Vec, as well as the tutorial, seem focused on prediction. <a href=""https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings"">This post</a> does a good job doing the calculation by hand, but I thought it was possible for Doc2Vec to do that already. The code I'm using is</p>

<pre><code>import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

def build_model(train_docs, test_docs, comp_docs):
    '''
    Parameters
    -----------
    train_docs: list of lists - combination of known both sentence list
    test_docs: list of lists - one of the sentence lists
    comp_docs: list of lists - combined sentence lists to match the index to the sentence 
    '''
    # Train model
    model = Doc2Vec(dm = 0, dbow_words = 1, window = 2, alpha = 0.2)#, min_alpha = 0.025)
    model.build_vocab(train_docs)
    for epoch in range(10):
        model.train(train_docs, total_examples = model.corpus_count, epochs = epoch)
        #model.alpha -= 0.002
        #model.min_alpha = model.alpha


    scores = []

    for doc in test_docs:
        dd = {}
        # Calculate the cosine similarity and return top 40 matches
        score = model.docvecs.most_similar([model.infer_vector(doc)],topn=40)
        key = "" "".join(doc)
        for i in range(len(score)):
            # Get index and score
            x, y = score[i]
            #print(x)
            # Match sentence from other list
            nkey = ' '.join(comp_docs[x])
            dd[nkey] = y
        scores.append({key: dd})

    return scores
</code></pre>

<p>which works to calculate the similarity scores, but the issue here is that I have to train the model on all the sentences from both lists or one of the lists, then match. Is there a way to use Doc2Vec to just get the vectors, then compute the cosine similarity? To be clear, I'm trying to find the most similar sentences between lists. I'd expect an output like</p>

<pre><code>scores = []
for s1 in list1:
    for s2 in list2:
        scores.append((s1, s2, similarity(s1, s2)))

print(scores)
[('what they ate for lunch', 'food eaten two days ago', 0.23567),
 ('what they ate for lunch', 'height in centimeters', 0.120),
 ('what they ate for lunch', 'id', 0.01023),
 ('height in inches', 'food eaten two days ago', 0.123),
 ('height in inches', 'height in centimeters', 0.8456),
 ('height in inches', 'id', 0.145),
 ('subjectid', 'food eaten two days ago', 0.156),
 ('subjectid', 'height in centimeters', 0.1345),
 ('subjectid', 'id', 0.9567)]
</code></pre>
",2019-03-08 22:05:56,2020-05-13 06:18:02,Cosine Similarity between Lists of Sentences using Doc2Vec,<python-3.x><nlp><data-science><cosine-similarity><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20305,55016629,2019-03-06 06:07:27,,"<p>I downloaded pretrained word vector file (.bin) from facebook (<a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>)
However, when I tried to use this model it happens to make error.</p>

<pre><code>from gensim.models import FastText
fasttext_model = FastText.load_fasttext_format('cc.ko.300.bin', encoding='utf8')

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte
</code></pre>

<p>But weird thing is that it operates well when I use old version bin file (<a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a>)</p>

<p>What is wrong with these files?? And how can I fix it??</p>

<p>And I must use bin file because I need all n-grams to prevent OOV. So, solutions like 'use .vec file' couldn't be any help.</p>

<p>Thank you so much :)</p>
",,2019-08-10 18:12:36,Facebook fasttext bin model UnicodeDecodeError,<python><facebook><utf-8><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
20337,55084361,2019-03-10 04:13:39,,"<p>Are these <code>model.wv.vectors</code> and <code>model.trainables.syn1neg</code>? And will they be similar after enough runs?</p>
",,2019-03-10 06:20:59,How to access W and W' matrices in Gensim Word2Vec in negative sampling setting?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20345,55038641,2019-03-07 07:53:06,,"<h3>Task</h3>

<p>I want to make a class for <code>word2vec</code> model and call it, so the process will be faster and more efficient. </p>

<h3>Script</h3>

<pre><code>from docsim import DocSim
from gensim.models.keyedvectors import KeyedVectors

word2vec_model = 'w2vec_wiki_id_case_doc'

model = KeyedVectors.load(word2vec_model, mmap='r')
ds = DocSim(model)
</code></pre>

<h3>Problem</h3>

<p>With my code, the process takes a long time to read the <code>word2vec</code> model. How can I  solve the problem? </p>
",2019-03-07 08:20:19,2019-03-07 14:51:53,Class of word2vec model (Python),<python><class><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20348,54917218,2019-02-28 01:53:58,,"<p>I have a small python pipeline. One class cleans and lemmatizes the data.
It returns a List of Lists of Strings (i.e., <code>List[List[str]]</code>). I then pass the list on to another class which passes the data to gensim dictionary</p>

<p>The following code, however, throws this exception:</p>

<pre><code>dictionary = corpora.Dictionary(self.bowlist)
AttributeError: 'list' object has no attribute 'bowlist'
</code></pre>

<p>Code:</p>

<pre><code>from typing import List
import re
from gensim import corpora

class ListOfListsToGensimCorpora:
    def __init__(self, bow_list: List[List[str]]):
        self.bowlist = bow_list

    def perform(self):
        dictionary = corpora.Dictionary(self.bowlist)
        print(dictionary)
</code></pre>

<p>I am new to Python but I have checked through debug and other methods, bowlist is a List[List[str]].</p>
",,2019-02-28 02:44:23,python gensim: AttributeError: 'list' object has no attribute,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20360,55075312,2019-03-09 08:10:46,,"<p>this is my code.it reads reviews from an excel file (rev column) and make a list of list.</p>

<p>xp is like this</p>

<pre><code>[""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""]
</code></pre>

<p>but when use list for model, it gives me error""TypeError: 'float' object is not iterable"".i don't know where is my problem.
Thanks.</p>

<pre><code>xp=[]
import gensim 
import logging
import pandas as pd 
file = r'FileNamelast.xlsx'
df = pd.read_excel(file,sheet_name='FileNamex')
pages = [i for i in range(0,1000)]


for page in  pages:

 text =df.loc[page,[""rev""]]
 xp.append(text[0])


model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, 
workers=10)
model.train(xp,total_examples=len(xp),epochs=10)
</code></pre>

<p>this is what i got.TypeError: 'float' object is not iterable</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-aa34c0e432bf&gt; in &lt;module&gt;()
     14 
     15 
---&gt; 16 model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, workers=10)
     17 model.train(xp,total_examples=len(xp),epochs=10)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in __init__(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
    765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
    766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 767             fast_version=FAST_VERSION)
    768 
    769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in __init__(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
    758 
--&gt; 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
    760             self.train(
    761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in build_vocab(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    934         """"""
    935         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
    937         self.corpus_count = corpus_count
    938         self.corpus_total_words = total_words

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in scan_vocab(self, sentences, corpus_file, progress_per, workers, trim_rule)
   1569             sentences = LineSentence(corpus_file)
   1570 
-&gt; 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
   1572 
   1573         logger.info(

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1552                     sentence_no, total_words, len(vocab)
   1553                 )
-&gt; 1554             for word in sentence:
   1555                 vocab[word] += 1
   1556             total_words += len(sentence)

TypeError: 'float' object is not iterable
</code></pre>
",2019-03-09 08:56:24,2019-03-09 22:49:31,train Word2vec model using Gensim,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20362,55018426,2019-03-06 08:17:24,,"<p>Recently, I trained a FastText word embedding from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:</p>

<pre><code>import gensim.models as gs

path = r'\data\word2vec'

w2v = gs.FastText.load(os.path.join(path, 'fasttext_model'))

w2v.wv['']
</code></pre>

<p>It outputs:</p>

<pre><code>array([ 0.00303676,  0.02088235, -0.00815559,  0.00484574, -0.03576371,
       -0.02178247, -0.05090654,  0.03063928, -0.05999983,  0.04547168,
       -0.01778449, -0.02716631, -0.03326027, -0.00078981,  0.0168153 ,
        0.00773436,  0.01966593, -0.00756055,  0.02175765, -0.0050137 ,
        0.00241255, -0.03810823, -0.03386266,  0.01231019, -0.00621936,
       -0.00252419,  0.02280569,  0.00992453,  0.02770403,  0.00233192,
        0.0008545 , -0.01462698,  0.00454278,  0.0381292 , -0.02945416,
       -0.00305543, -0.00690968,  0.00144188,  0.00424266,  0.00391074,
        0.01969502,  0.02517333,  0.00875261,  0.02937791,  0.03234404,
       -0.01116276, -0.00362578,  0.00483239, -0.02257918,  0.00123061,
        0.00324584,  0.00432153,  0.01332884,  0.03186348, -0.04119627,
        0.01329033,  0.01382102, -0.01637722,  0.01464139,  0.02203292,
        0.0312229 ,  0.00636201, -0.00044287, -0.00489291,  0.0210293 ,
       -0.00379244, -0.01577058,  0.02185207,  0.02576622, -0.0054543 ,
       -0.03115215, -0.00337738, -0.01589811, -0.01608399, -0.0141606 ,
        0.0508234 ,  0.00775024,  0.00352813,  0.00573649, -0.02131752,
        0.01166397,  0.00940598,  0.04075769, -0.04704212,  0.0101376 ,
        0.01208556,  0.00402935,  0.0093914 ,  0.00136144,  0.03284211,
        0.01000613, -0.00563702,  0.00847146,  0.03236216, -0.01626745,
        0.04095127,  0.02858841,  0.0248084 ,  0.00455458,  0.01467448],
      dtype=float32)
</code></pre>

<p>Hence, I really want to know why the FastText module trained from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> could do this. Thank you!</p>
",,2019-03-07 15:57:35,Why the FastText word embedding could generate the representation of a word from another language?,<python><gensim><word-embedding><fasttext><natural-language-processing>,,,CC BY-SA 4.0,False,False,True,False,False
20366,55086734,2019-03-10 10:31:00,,"<p>I have a large txt file(150MG) like this</p>

<pre><code>'intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one', 'better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', ...
</code></pre>

<p>I  wanna train word2vec model  model using that file but it gives me RAM problem.i dont know how to feed txt file to word2vec model.this is my code.i know that my code has problem but i don't know where is it.</p>

<pre><code>import gensim 


f = open('your_file1.txt')
for line in f:
    b=line
   model = gensim.models.Word2Vec([b],min_count=1,size=32)

w1 = ""bad""
model.wv.most_similar (positive=w1)
</code></pre>
",2019-03-10 10:37:09,2019-03-10 19:00:33,train Gensim word2vec using large txt file,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20386,55091094,2019-03-10 18:43:37,,"<p>In my project, I use the Python library <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""nofollow noreferrer"">gensim</a> for topic modeling/extraction of text.
I try to load my trained LdaMallet model to classify new unseen texts.</p>

<p>The first part is loading the model.</p>

<pre><code>import os

dirname = os.path.dirname(__file__)
filename = os.path.join(dirname, 'mallet-2.0.8/bin/mallet')

# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
os.environ['MALLET_HOME'] = # path to mallet

ldaMallet = gensim.models.wrappers.LdaMallet.load('lda_malletoutputCommentsAndMethods.model)
ldaModel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldaMallet)
</code></pre>

<p>I am not sure about the last line which converts the ldaMallet to LdaModel. It was the only way to get some result.</p>

<p>Then the second part is preparing the new data and classify it.</p>

<pre><code>from gensim.test.utils import common_dictionary
other_texts = [['new', 'document', 'to', 'classify', 'as', 'array']]
other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]
vector = ldaModel[other_corpus[0]]

# sorts the result by probability and not by topic ID
print(sorted(vector, key=lambda x: x[1], reverse=True))
</code></pre>

<p>Then the result looks something like this:</p>

<pre><code>[(16, 0.143), (17, 0.08), (9, 0.0653),...]
</code></pre>

<p>No matter which text I use in the <code>other_texts</code> array, this result isn't changing, but it should. </p>
",2019-03-10 18:56:35,2019-03-10 18:56:35,Correct way to load LdaMallet model with gensim and classify unseen documents,<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
20401,55002455,2019-03-05 12:04:24,,"<p>May I know which version of the LDA Mallet Wrapper has the random_seed parameter included in the code? I tried version Mallet 2.0.8, but I am getting TypeError unexpected keyword argument. Or should I be downloading the ldamallet.py from github then replacing the ""mallet"" script in the bin folder? Thanks.</p>

<p>TypeError: init() got an unexpected keyword argument 'random_seed'</p>

<p>Update: In order for the random_seed parameter to work, download the ldamallet.py file from Github and paste it into the Python>Gensim>Model directory. No changes are required for the mallet-2.0.8 wrapper. </p>
",2019-03-05 17:06:23,2019-03-05 17:06:23,Setting Random_Seed LDA Mallet Implementation (For replicability of results),<python><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
20417,55169721,2019-03-14 18:30:43,,"<p>is there a way to infer multiple documents at the same time to preserve the random state of the model using Gensim Doc2Vec?</p>

<p>The function infer_vector is defined as</p>

<pre><code>infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)
</code></pre>

<p>where doc_words (list of str)  A document for which the vector representation will be inferred. And I could not find any opther option to infer multiple documents at the same time.</p>
",,2019-03-15 02:15:17,Can I preserve the random state of a doc2vec mode for each document I want to infer by infering all documents at the same time?,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20441,55095368,2019-03-11 04:47:00,,"<p>I am running the below code, but gensim word2vec is throwing a word not in vocabulary error. Can you let me know the solution?</p>

<p>this is my file(file.txt)</p>

<pre><code>'intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one', 'better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', ..
</code></pre>

<p>this is my code </p>

<pre><code> import gensim 
    with open('file.txt', 'r') as myfile:
      data = myfile.read()



    model = gensim.models.Word2Vec(data,min_count=1,size=32)
    w1 = ""good""
    model.wv.most_similar (positive=w1)
</code></pre>

<p>Output:</p>

<pre><code>KeyError: ""word 'good' not in vocabulary""


---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-34-22572d5a8082&gt; in &lt;module&gt;()
      7 model = gensim.models.Word2Vec(data,min_count=1,size=32)
      8 w1 = ""good""
----&gt; 9 model.wv.most_similar (positive=w1)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in most_similar(self, positive, negative, topn, restrict_vocab, indexer)
    529                 mean.append(weight * word)
    530             else:
--&gt; 531                 mean.append(weight * self.word_vec(word, use_norm=True))
    532                 if word in self.vocab:
    533                     all_words.add(self.vocab[word].index)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

KeyError: ""word 'good' not in vocabulary""
</code></pre>

<p></p>
",2019-03-11 04:53:40,2019-03-12 09:15:30,gensim: KeyError: word 'good' not in vocabulary,<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20450,55121095,2019-03-12 12:06:41,,"<p>I'm new to DL and NLP, and recently started using a pre-trained fastText embedding model (cc.en.300.bin) through gensim.</p>

<p>I would like to be able to calculate vectors for out-of-vocabulary words myself, by splitting the word to n-grams and looking up the vector for every n-gram.</p>

<p>I could not find a way to export the n-gram vectors that are part of the model. I realize they are hashed, but perhaps there's a way (not necessarily using gensim) to get them?</p>

<p>Any insight will be appreciated!</p>
",,2019-05-12 20:55:17,fasttext: is there a way export ngrams?,<export><gensim><n-gram><fasttext><oov>,,,CC BY-SA 4.0,False,False,True,False,False
20456,55230344,2019-03-18 21:32:53,,"<p>I'm building a LDA in python using Gensim and I'm struggling to increase the number of words printed per topic from the default of 10.  I'd like 20 topics with 30 words each.  Any advice would be greatly appreciated :)</p>

<pre><code># train the LDA model

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)

# check out the topics

for idx, topic in lda_model.print_topics(-1):
   print('Topic: {} \nWords: {}'.format(idx, topic))
</code></pre>
",2019-04-12 22:23:37,2019-04-12 22:23:37,Add words per topic LDA,<python><windows><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
20468,55137631,2019-03-13 08:47:12,,"<p>Ive tried reimplementing a simple GRU language model using just a GRU and a linear layer (the full code is also at <a href=""https://www.kaggle.com/alvations/gru-language-model-not-training-properly"" rel=""nofollow noreferrer"">https://www.kaggle.com/alvations/gru-language-model-not-training-properly</a>):</p>

<pre><code>class Generator(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):
        super(Generator, self).__init__()

        # Initialize the embedding layer with the 
        # - size of input (i.e. no. of words in input vocab)
        # - no. of hidden nodes in the embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)

        # Initialize the GRU with the 
        # - size of the input (i.e. embedding layer)
        # - size of the hidden layer 
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)

        # Initialize the ""classifier"" layer to map the RNN outputs
        # to the vocabulary. Remember we need to -1 because the 
        # vectorized sentence we left out one token for both x and y:
        # - size of hidden_size of the GRU output.
        # - size of vocabulary
        self.classifier = nn.Linear(hidden_size, vocab_size)

    def forward(self, inputs, use_softmax=False, hidden=None):
        # Look up for the embeddings for the input word indices.
        embedded = self.embedding(inputs)
        # Put the embedded inputs into the GRU.
        output, hidden = self.gru(embedded, hidden)

        # Matrix manipulation magic.
        batch_size, sequence_len, hidden_size = output.shape
        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...
        output = output.contiguous().view(batch_size * sequence_len, hidden_size)
        # Put it through the classifier
        # And reshape it to [batch_size x sequence_len x vocab_size]
        output = self.classifier(output).view(batch_size, sequence_len, -1)

        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)


    def generate(self, max_len, temperature=1.0):
        pass
</code></pre>

<p>And the training routine:</p>

<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Set the hidden_size of the GRU 
embed_size = 100
hidden_size = 100
num_layers = 1

# Setup the data.
batch_size=50
kilgariff_data = KilgariffDataset(tokenized_text)
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

criterion = nn.CrossEntropyLoss(ignore_index=kilgariff_data.vocab.token2id['&lt;pad&gt;'], size_average=True)
model = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers).to(device)

learning_rate = 0.003
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#model = nn.DataParallel(model)

losses = []

def train(num_epochs, dataloader, model, criterion, optimizer):
    plt.ion()
    for _e in range(num_epochs):
        for batch in tqdm(dataloader):
            x = batch['x'].to(device)
            x_len = batch['x_len'].to(device)
            y = batch['y'].to(device)
            # Zero gradient.
            optimizer.zero_grad()
            # Feed forward. 
            output, hidden = model(x, use_softmax=True)
            # Compute loss:
            # Shape of the `output` is [batch_size x sequence_len x vocab_size]
            # Shape of `y` is [batch_size x sequence_len]
            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]

            _, prediction = torch.max(output, dim=2)
            loss = criterion(output.permute(0, 2, 1), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.float().data)

            clear_output(wait=True)
            plt.plot(losses)
            plt.pause(0.05)


train(50, dataloader, model, criterion, optimizer)

#learning_rate = 0.05
#optimizer = optim.SGD(model.parameters(), lr=learning_rate)
#train(4, dataloader, model, criterion, optimizer)
</code></pre>

<p>But when the model is predicting, we see that its only predicting the and comma ,.</p>

<p><strong>Anyone spot something wrong with my code? Or hyperparameters?</strong></p>

<p>The full code:</p>

<pre><code># coding: utf-8

# In[1]:


# IPython candies...
from IPython.display import Image
from IPython.core.display import HTML

from IPython.display import clear_output


# In[2]:


import numpy as np
from tqdm import tqdm

import pandas as pd

from gensim.corpora import Dictionary

import torch
from torch import nn, optim, tensor, autograd
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

device = 'cuda' if torch.cuda.is_available() else 'cpu'


# In[3]:


import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style(""darkgrid"")
sns.set(rc={'figure.figsize':(12, 8)})


torch.manual_seed(42)


# In[4]:


try: # Use the default NLTK tokenizer.
    from nltk import word_tokenize, sent_tokenize 
    # Testing whether it works. 
    # Sometimes it doesn't work on some machines because of setup issues.
    word_tokenize(sent_tokenize(""This is a foobar sentence. Yes it is."")[0])
except: # Use a naive sentence tokenizer and toktok.
    import re
    from nltk.tokenize import ToktokTokenizer
    # See https://stackoverflow.com/a/25736515/610569
    sent_tokenize = lambda x: re.split(r'(?&lt;=[^A-Z].[.?]) +(?=[A-Z])', x)
    # Use the toktok tokenizer that requires no dependencies.
    toktok = ToktokTokenizer()
    word_tokenize = word_tokenize = toktok.tokenize


# In[5]:


import os
import requests
import io #codecs


# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf
if os.path.isfile('language-never-random.txt'):
    with io.open('language-never-random.txt', encoding='utf8') as fin:
        text = fin.read()
else:
    url = ""https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt""
    text = requests.get(url).content.decode('utf8')
    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:
        fout.write(text)


# In[6]:


# Tokenize the text.
tokenized_text = [list(map(str.lower, word_tokenize(sent))) 
                  for sent in sent_tokenize(text)]


# In[7]:


class KilgariffDataset(nn.Module):
    def __init__(self, texts):
        self.texts = texts

        # Initialize the vocab 
        special_tokens = {'&lt;pad&gt;': 0, '&lt;unk&gt;':1, '&lt;s&gt;':2, '&lt;/s&gt;':3}
        self.vocab = Dictionary(texts)
        self.vocab.patch_with_special_tokens(special_tokens)

        # Keep track of the vocab size.
        self.vocab_size = len(self.vocab)

        # Keep track of how many data points.
        self._len = len(texts)

        # Find the longest text in the data.
        self.max_len = max(len(txt) for txt in texts) 

    def __getitem__(self, index):
        vectorized_sent = self.vectorize(self.texts[index])
        x_len = len(vectorized_sent)
        # To pad the sentence:
        # Pad left = 0; Pad right = max_len - len of sent.
        pad_dim = (0, self.max_len - len(vectorized_sent))
        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')
        return {'x':vectorized_sent[:-1], 
                'y':vectorized_sent[1:], 
                'x_len':x_len}

    def __len__(self):
        return self._len

    def vectorize(self, tokens, start_idx=2, end_idx=3):
        """"""
        :param tokens: Tokens that should be vectorized. 
        :type tokens: list(str)
        """"""
        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx 
        # Lets just cast list of indices into torch tensors directly =)

        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]
        return torch.tensor(vectorized_sent)

    def unvectorize(self, indices):
        """"""
        :param indices: Converts the indices back to tokens.
        :type tokens: list(int)
        """"""
        return [self.vocab[i] for i in indices]


# In[8]:


kilgariff_data = KilgariffDataset(tokenized_text)
len(kilgariff_data.vocab)


# In[9]:


batch_size = 10
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

for data_dict in dataloader:
    # Sort indices of data in batch by lengths.
    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()
    data_batch = {name:_tensor[sorted_indices]
                  for name, _tensor in data_dict.items()}
    print(data_batch)
    break


# In[97]:


class Generator(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):
        super(Generator, self).__init__()

        # Initialize the embedding layer with the 
        # - size of input (i.e. no. of words in input vocab)
        # - no. of hidden nodes in the embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)

        # Initialize the GRU with the 
        # - size of the input (i.e. embedding layer)
        # - size of the hidden layer 
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)

        # Initialize the ""classifier"" layer to map the RNN outputs
        # to the vocabulary. Remember we need to -1 because the 
        # vectorized sentence we left out one token for both x and y:
        # - size of hidden_size of the GRU output.
        # - size of vocabulary
        self.classifier = nn.Linear(hidden_size, vocab_size)

    def forward(self, inputs, use_softmax=False, hidden=None):
        # Look up for the embeddings for the input word indices.
        embedded = self.embedding(inputs)
        # Put the embedded inputs into the GRU.
        output, hidden = self.gru(embedded, hidden)

        # Matrix manipulation magic.
        batch_size, sequence_len, hidden_size = output.shape
        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...
        output = output.contiguous().view(batch_size * sequence_len, hidden_size)
        # Put it through the classifier
        # And reshape it to [batch_size x sequence_len x vocab_size]
        output = self.classifier(output).view(batch_size, sequence_len, -1)

        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)


    def generate(self, max_len, temperature=1.0):
        pass


# In[98]:


# Set the hidden_size of the GRU 
embed_size = 12
hidden_size = 10
num_layers = 4

_encoder = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers)


# In[99]:


# Take a batch.
_batch = next(iter(dataloader))
_inputs, _lengths = _batch['x'], _batch['x_len']
_targets = _batch['y']
max(_lengths)


# In[100]:


_output, _hidden = _encoder(_inputs)
print('Output sizes:\t', _output.shape)
print('Input sizes:\t', batch_size, kilgariff_data.max_len -1, len(kilgariff_data.vocab))
print('Target sizes:\t', _targets.shape)


# In[101]:


_, predicted_indices = torch.max(_output, dim=2)
print(predicted_indices.shape)
predicted_indices


# In[103]:


device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Set the hidden_size of the GRU 
embed_size = 100
hidden_size = 100
num_layers = 1

# Setup the data.
batch_size=50
kilgariff_data = KilgariffDataset(tokenized_text)
dataloader = DataLoader(dataset=kilgariff_data, batch_size=batch_size, shuffle=True)

criterion = nn.CrossEntropyLoss(ignore_index=kilgariff_data.vocab.token2id['&lt;pad&gt;'], size_average=True)
model = Generator(len(kilgariff_data.vocab), embed_size, hidden_size, num_layers).to(device)

learning_rate = 0.003
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#model = nn.DataParallel(model)

losses = []

def train(num_epochs, dataloader, model, criterion, optimizer):
    plt.ion()
    for _e in range(num_epochs):
        for batch in tqdm(dataloader):
            x = batch['x'].to(device)
            x_len = batch['x_len'].to(device)
            y = batch['y'].to(device)
            # Zero gradient.
            optimizer.zero_grad()
            # Feed forward. 
            output, hidden = model(x, use_softmax=True)
            # Compute loss:
            # Shape of the `output` is [batch_size x sequence_len x vocab_size]
            # Shape of `y` is [batch_size x sequence_len]
            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]

            _, prediction = torch.max(output, dim=2)
            loss = criterion(output.permute(0, 2, 1), y)
            loss.backward()
            optimizer.step()
            losses.append(loss.float().data)

            clear_output(wait=True)
            plt.plot(losses)
            plt.pause(0.05)


train(50, dataloader, model, criterion, optimizer)

#learning_rate = 0.05
#optimizer = optim.SGD(model.parameters(), lr=learning_rate)
#train(4, dataloader, model, criterion, optimizer)


# In[ ]:


list(kilgariff_data.vocab.items())


# In[105]:


start_token = '&lt;s&gt;'
hidden_state = None
max_len = 20
temperature=0.8

i = 0

while start_token not in ['&lt;/s&gt;', '&lt;pad&gt;'] and i &lt; max_len:
    i += 1
    start_state = torch.tensor(kilgariff_data.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)
    model.embedding(start_state)
    output, hidden_state = model.gru(model.embedding(start_state), hidden_state)

    batch_size, sequence_len, hidden_size = output.shape
    output = output.contiguous().view(batch_size * sequence_len, hidden_size)

    output = model.classifier(output).view(batch_size, sequence_len, -1)
    _, prediction = torch.max(F.softmax(output, dim=2), dim=2)

    start_token = kilgariff_data.vocab[int(prediction.squeeze(0).squeeze(0))]

    print(start_token, end=' ')
</code></pre>
",,2019-03-15 07:57:12,GRU Language Model not Training Properly,<python><nlp><pytorch><recurrent-neural-network><gated-recurrent-unit>,,,CC BY-SA 4.0,True,False,True,False,False
20486,55103288,2019-03-11 13:47:51,,"<p>How I can obtain specific doc vector values? By tag, like this:</p>

<pre><code>modelValues = model.docvecs['myDocTag']
</code></pre>

<p>or it is possible only by index, like this:</p>

<pre><code>modelValues = model.docvecs[12]
</code></pre>

<p>(in last case, I must know matching <code>tag</code><code>index</code>...)</p>
",,2019-03-11 15:44:16,What is correct way to get doc vectors values?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20487,55248396,2019-03-19 19:08:35,,"<p>Thanks for stopping by!  I had a quick question about appending stop words. I have a select few words that show up in my data set and I was hopping I could add them to gensims stop word list.  I've seen a lot of examples using nltk and I was hoping there would be a way to do the same in gensim.  I'll post my code below:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result</code></pre>
</div>
</div>
</p>
",,2019-03-19 21:15:52,Add stop words in Gensim,<python><windows><nlp><gensim><stop-words>,,,CC BY-SA 4.0,True,False,True,False,False
20496,55238328,2019-03-19 10:07:17,,"<p>I've been trying to implement an embedding layer using gensim's word2vec. I have loaded my data using pandas, my data is text type, when it comes to the word2vec part:</p>

<pre><code>embedding_weights = train_word2vec(y_train, vocab['w2idx'], 
num_features=embedding_dim, min_word_count=min_word_count, context=context)
input_shape = (sequence_length,)
model_input = Input(shape=input_shape)
layer = Embedding(len(vocab['idx2w']), embedding_dim,
input_length=sequence_length, name=""embedding"")(model_input)
layer = Dropout(dropout_prob[0])(layer)
</code></pre>

<p>I keep getting this error:</p>

<pre><code>   File ""&lt;ipython-input-9-423d0e432e5b&gt;"", line 3, in &lt;module&gt;
   min_word_count=min_word_count, context=context)

   File ""C:\Users\ACER\Pod_Dsgn_Chatbot\Wor2vec.py"", line 43, in 
   train_word2vec
   for key, word in vocabulary_inv.items()}

   File ""C:\Users\ACER\Pod_Dsgn_Chatbot\Wor2vec.py"", line 43, in &lt;dictcomp&gt;
   for key, word in vocabulary_inv.items()}

   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
   packages\gensim\utils.py"", line 1398, in new_func1
    return func(*args, **kwargs)

   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
    packages\gensim\models\word2vec.py"", line 821, in __getitem__
     return self.wv.__getitem__(words)
   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
   packages\gensim\models\keyedvectors.py"", line 171, in __getitem__
   return vstack([self.get_vector(entity) for entity in entities])

   TypeError: 'int' object is not iterable
</code></pre>

<p>i have no idea how to fix it, and there isn't much information on the internet, also the error  is related to the first line of the shown code. Finally note that i cleaned my data, and i removed every integer, thank you </p>

<p>Edit: And this is the function where the problem happens, Ps: i didn't develop this one </p>

<pre><code>def train_word2vec(sentence_matrix, vocabulary_inv,
               num_features=300, min_word_count=1, context=10):

model_dir = 'models'
model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, 
 min_word_count, context)
model_name = join(model_dir, model_name)
if exists(model_name):
    embedding_model = word2vec.Word2Vec.load(model_name)
    print('Load existing Word2Vec model \'%s\'' % split(model_name)[-1])
else:

    num_workers = 2   
    downsampling = 1e-3   


    print('Training Word2Vec model...')
    sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]
    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,
    size=num_features, min_count=min_word_count, window=context, 
    sample=downsampling)
    embedding_model.init_sims(replace=True)
    if not exists(model_dir):
        os.mkdir(model_dir)
    print('Saving Word2Vec model \'%s\'' % split(model_name)[-1])
    embedding_model.save(model_name)
    pyplot.scatter( embedding_model)
    embedding_weights = {key: embedding_model[word] if word in 
     embedding_model 
    else
                          np.random.uniform(-0.25, 0.25, 
      embedding_model.vector_size)
                     for key, word in vocabulary_inv.items()}
   return embedding_weights
</code></pre>
",2019-03-21 20:19:11,2019-03-21 20:19:11,Gensim/word2vec error: embedding layer error,<python><machine-learning><keras><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20498,55202347,2019-03-16 23:00:59,,"<p><a href=""https://i.stack.imgur.com/3i55D.png"" rel=""nofollow noreferrer"">enter image description here</a>    </p>

<pre><code>PS C:\WINDOWS\system32&gt; python
Python 3.7.0a2 (v3.7.0a2:f7ac4fe, Oct 17 2017, 17:06:29) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import os
&gt;&gt;&gt; import sys
&gt;&gt;&gt; import numpy
PS C:\WINDOWS\system32&gt; python
Python 3.7.0a2 (v3.7.0a2:f7ac4fe, Oct 17 2017, 17:06:29) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import nltk
PS C:\WINDOWS\system32&gt;
</code></pre>

<p>I can not import the mentioned modules however it is not giving any error as well. Even in small program if I import any of these 3 the program just ends without doing anything.</p>

<p>I have only one installation of Python. No other verison.</p>

<p>Update:</p>

<pre><code>PS C:\WINDOWS\system32&gt; pip freeze
boto==2.49.0
boto3==1.9.115
botocore==1.12.115
bz2file==0.98
certifi==2019.3.9
chardet==3.0.4
docutils==0.14
gensim==3.7.1
idna==2.8
jmespath==0.9.4
nltk==3.4
numpy==1.16.2
python-dateutil==2.8.0
requests==2.21.0
s3transfer==0.2.0
scipy==1.2.1
singledispatch==3.4.0.3
six==1.12.0
smart-open==1.8.0
urllib3==1.24.1
PS C:\WINDOWS\system32&gt; python
Python 3.7.0a2 (v3.7.0a2:f7ac4fe, Oct 17 2017, 17:06:29) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; print(numpy.__version__)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'numpy' is not defined
&gt;&gt;&gt; exit()
PS C:\WINDOWS\system32&gt;
</code></pre>
",2019-03-16 23:07:33,2019-03-16 23:51:17,"NLTK, NUMPY and SCIPY - cannot be imported",<python><python-3.x><numpy><nltk>,,,CC BY-SA 4.0,True,False,True,False,False
20509,55288724,2019-03-21 20:24:44,,"<p>I'm getting an error while trying to access gensims mallet in jupyter notebooks.  I have the specified file 'mallet' in the same folder as my notebook, but cant seem to access it.  I tried routing to it from the C drive but I still get the same error.  Please help :)</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import os
from gensim.models.wrappers import LdaMallet

#os.environ.update({'MALLET_HOME':r'C:/Users/new_mallet/mallet-2.0.8/'})

mallet_path = 'mallet' # update this path

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=20, id2word=dictionary)

result = (ldamallet.show_topics(num_topics=3, num_words=10,formatted=False))
for each in result:
    print (each)</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.stack.imgur.com/xcjPF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xcjPF.png"" alt=""Mallet Error CalledProcessError""></a></p>

<p><a href=""https://i.stack.imgur.com/DZJFr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZJFr.png"" alt=""enter image description here""></a></p>
",2019-04-01 16:12:19,2020-04-02 12:06:08,Gensim mallet CalledProcessError: returned non-zero exit status,<python><windows><jupyter-notebook><gensim><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
20525,55309197,2019-03-22 23:51:46,,"<p>I have a dataset of 6000 observations; a sample of it is the following:</p>

<pre><code>job_id      job_title                                           job_sector
30018141    Secondary Teaching Assistant                        Education
30006499    Legal Sales Assistant / Executive                   Sales
28661197    Private Client Practitioner                         Legal
28585608    Senior hydropower mechanical project manager        Engineering
28583146    Warehouse Stock Checker - Temp / Immediate Start    Transport &amp; Logistics
28542478    Security Architect Contract                         IT &amp; Telecoms
</code></pre>

<p>The goal is to predict the job sector of each row based on the job title.</p>

<p>Firstly, I apply some preprocessing on the <code>job_title</code> column:</p>

<pre><code>def preprocess(document):
    lemmatizer = WordNetLemmatizer()
    stemmer_1 = PorterStemmer()
    stemmer_2 = LancasterStemmer()
    stemmer_3 = SnowballStemmer(language='english')

    # Remove all the special characters
    document = re.sub(r'\W', ' ', document)

    # remove all single characters
    document = re.sub(r'\b[a-zA-Z]\b', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r' +', ' ', document, flags=re.I)

    # Converting to lowercase
    document = document.lower()

    # Tokenisation
    document = document.split()

    # Stemming
    document = [stemmer_3.stem(word) for word in document]

    document = ' '.join(document)

    return document

df_first = pd.read_csv('../data.csv', keep_default_na=True)

for index, row in df_first.iterrows():

    df_first.loc[index, 'job_title'] = preprocess(row['job_title'])
</code></pre>

<p>Then I do the following with <code>Gensim</code> and <code>Doc2Vec</code>:</p>

<pre><code>X = df_first.loc[:, 'job_title'].values
y = df_first.loc[:, 'job_sector'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)

tagged_train = TaggedDocument(words=X_train.tolist(), tags=y_train.tolist())
tagged_train = list(tagged_train)

tagged_test = TaggedDocument(words=X_test.tolist(), tags=y_test.tolist())
tagged_test = list(tagged_test)

model = Doc2Vec(vector_size=5, min_count=2, epochs=30)

training_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_train.tolist(), y_train.tolist())]

model.build_vocab(training_set)

model.train(training_set, total_examples=model.corpus_count, epochs=model.epochs)   

test_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_test.tolist(), y_test.tolist())]

predictors_train = []
for sentence in X_train.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.01)

    predictors_train.append(predictor.tolist())

predictors_test = []
for sentence in X_test.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.025)

    predictors_test.append(predictor.tolist())

sv_classifier = SVC(kernel='linear', class_weight='balanced', decision_function_shape='ovr', random_state=0)
sv_classifier.fit(predictors_train, y_train)

score = sv_classifier.score(predictors_test, y_test)
print('accuracy: {}%'.format(round(score*100, 1)))
</code></pre>

<p>However, the result which I am getting is 22% accuracy.</p>

<p>This makes me a lot suspicious especially because by using the <code>TfidfVectorizer</code> instead of the <code>Doc2Vec</code> (both with the same classifier) then I am getting 88% accuracy (!).</p>

<p>Therefore, I guess that I must be doing something wrong in how I apply the <code>Doc2Vec</code> of <code>Gensim</code>.</p>

<p>What is it and how can I fix it?</p>

<p>Or it it simply that my dataset is relatively small while more advanced methods such as word embeddings etc require way more data?</p>
",2019-03-25 09:32:48,2019-09-10 14:38:20,Doc2Vec & classification - very poor results,<python><classification><gensim><text-classification><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20533,55278701,2019-03-21 10:47:33,,"<p>I am topic modelling Harvard Library book title and subjects.</p>

<p>I use Gensim Mallet Wrapper to model with Mallet's LDA.
When I try to get Coherence and Perplexity values to see how good the model is, perplexity fails to calculate with below exception.
I do not get the same error if I use Gensim's built-in LDA model instead of Mallet. 
My corpus holds 7M+ documents of length up to 50 words averaging 20. So documents are short.</p>

<p>Below is the related part of my code:</p>

<pre><code># TOPIC MODELING

from gensim.models import CoherenceModel
num_topics = 50

# Build Gensim's LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics,
                                       random_state=100,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  
# a measure of how good the model is. lower the better.
</code></pre>

<blockquote>
  <p>Perplexity:  -47.91929228302663</p>
</blockquote>

<pre><code># Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, 
texts=data_words_trigrams, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<blockquote>
  <p>Coherence Score:  0.28852857563541856</p>
</blockquote>

<p>LDA gave scores without problem. Now I model the same bag of words with MALLET</p>

<pre><code># Building LDA Mallet Model
mallet_path = '~/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, 
corpus=corpus, num_topics=num_topics, id2word=id2word)

# Convert mallet to gensim type
mallet_model = 
gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)

# Compute Coherence Score
coherence_model_ldamallet = CoherenceModel(model=mallet_model, 
texts=data_words_trigrams, dictionary=id2word, coherence='c_v')
coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print('\nCoherence Score: ', coherence_ldamallet)
</code></pre>

<blockquote>
  <p>Coherence Score:  0.5994123896865993</p>
</blockquote>

<p>Then I ask for the Perplexity values and get below warnings and NaN value.</p>

<pre><code># Compute Perplexity
print('\nPerplexity: ', mallet_model.log_perplexity(corpus))
</code></pre>

<blockquote>
  <p>/app/app-py3/lib/python3.5/site-packages/gensim/models/ldamodel.py:1108:
  RuntimeWarning: invalid value encountered in multiply   score +=
  np.sum((self.eta - _lambda) * Elogbeta)</p>
  
  <p>Perplexity:  nan</p>
  
  <p>/app/app-py3/lib/python3.5/site-packages/gensim/models/ldamodel.py:1109:
  RuntimeWarning: invalid value encountered in subtract   score +=
  np.sum(gammaln(_lambda) - gammaln(self.eta))</p>
</blockquote>

<p>I realize this is a very Gensim specific question and requires deeper knowledge of this function: 
     gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)</p>

<p>Hence I would appreciate any comment on warnings and the Gensim domain.</p>
",2019-03-22 15:15:54,2019-07-02 10:16:19,Gensim Topic Modeling with Mallet Perplexity,<python><gensim><topic-modeling><mallet><perplexity>,,,CC BY-SA 4.0,False,False,True,False,False
20535,55291657,2019-03-22 01:38:09,,"<p>I have trained a LDA model on a corpus using Gensim. Now that I have the topic distribution for each document, how can I compare how similar two documents are in topics? I would like to have a summary measure. For example, the following are the topic distributions of two documents. There are totally 75 topics. For brevity, I show only the first 10 topics with largest probabilities (so the topics are not in order). (40, 0.5523168) means that topic #40 has a probability of 0.5523168 for DOC #1. Should I calculate the Euclidean or Cosine distance between the two vectors? And using this summary measure, can I say that, for example, DOC 1 is more similar to DOC2 than to DOC3, or DOC1 and DOC 2 are more similar to each other than DOC 3 and DOC 4 topically? Thank you!</p>

<pre><code>DOC #1:
[(40, 0.5523168), (60, 0.12225048), (43, 0.07556598), (41, 0.065885976), 
(22, 0.05838573), (24, 0.044774733), (74, 0.019839266), (65, 0.019544959), 
(51, 0.015470431), (36, 0.013449047)]


DOC #2:
[(73, 0.58864516), (41, 0.16827711), (51, 0.09783472), (63, 0.06510383), 
(24, 0.04722658), (32, 0.014467965), (44, 0.012267662), (47, 0.0031533625), 
(18, 0.0022214972), (0, 1.2154361e-05)]
</code></pre>
",2019-03-22 13:56:35,2019-04-14 17:15:23,How to compare the topical similarity between two documents in Python Gensim from their topic distributions?,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
20547,55331731,2019-03-25 05:32:41,,"<p>I tried to load .bin embedding file using gensim but i got  errors. I tried all the methods provided by gensim but couldn't rectify the error</p>

<p><strong>Method 1</strong></p>

<pre><code>import gensim.models.keyedvectors as word2vec

model=word2vec.KeyedVectors.load_word2vec_format('Health_2.5reviews.s200.w10.n5.v10.cbow.bin', binary=True, unicode_errors=ignore')
</code></pre>

<p><strong>Method 2</strong></p>

<pre><code>from gensim.models import KeyedVectors

filename='Health_2.5reviews.s200.w10.n5.v10.cbow.bin'

model=KeyedVectors.load_word2vec_format(filename,binary=True,unicode_errors=ignore')
</code></pre>

<p><strong>Method 1 and 2 gave the error</strong></p>

<blockquote>
  <p>""UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbc in position
  0: invalid start byte""</p>
</blockquote>

<p><strong>Method 3</strong></p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec

filename='Health_2.5reviews.s200.w10.n5.v10.cbow.bin'

model=Word2Vec.load(filename)
</code></pre>

<p><strong>Method 3 gave the error</strong></p>

<blockquote>
  <p>UnpicklingError: invalid load key, '\xbc'.</p>
</blockquote>
",2019-03-25 15:04:06,2019-03-25 15:04:06,Error when loading .bin embedding file using gensim package,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20557,55335354,2019-03-25 10:07:38,,"<p>I've been using doc2vec in the most basic way so far with limited success. I'm able to find similar documents however often I get a lot of false positives.   My primary goal is to build a classification algorithm for user requirements. This is to help with user requirement analysis and search.</p>

<p>I know this is not really a large enough dataset so there are a few questions I'd like help with:</p>

<ol>
<li>How can a train on one set of documents and build vectors on another?</li>
<li>How do I go about tuning the model, specifically selecting the right number of dimensions for the vector space</li>
<li>How can I create a Hierarchical Clustering for the word vectors, should a do this with one model or should I create separate word and document classification models?</li>
<li>I don't have ground truth, this is unsupervised learning when tuning how do I measure the quality of the result?</li>
<li>And finally, are there any recommended online resource that might cover some of the above.</li>
</ol>

<p>I've been calling train once with 100 vectors on 2000 documents, each with about 100 words, each document has 22 columns which are tagged by both cell and row.</p>

<pre><code>def tag_dataframe(df, selected_cols):
    tagged_cells = []
    headers = list(df.columns.values)
    for index, row in df.iterrows():
        row_tag = 'row_' + str(index)
        for col_name in headers:
            if col_name in selected_cols:
                col_tag = 'col_' + col_name
                cell_tag = 'cell_' + str(index) + '_' + col_name
                cell_val = str(row[col_name])
                if cell_val == 'nan':
                    continue
                cleaned_text = clean_str(cell_val)
                if len(cleaned_text) == 0:
                    continue
                tagged_cells.append(
                    gensim.models.doc2vec.TaggedDocument(
                        cleaned_text,
                        [row_tag, cell_tag]))
    print('tagged rows')
    return tagged_cells

def load_or_build_vocab(model_path, tagged_cells):
    if os.path.exists(model_path):
        print('Loading vocab')
        d2vm = gensim.models.Doc2Vec.load(model_path)
    else:
        print('building vocab')
        d2vm = gensim.models.Doc2Vec(
            vector_size=100,
            min_count=0,
            alpha=0.025,
            min_alpha=0.001)
        d2vm.build_vocab(tagged_cells)
        print('    built')
        d2vm.save(model_path)
    return d2vm

def load_or_train_model(model_path, d2vm, tagged_cells):
    if os.path.exists(model_path):
        print('Loading Model')
        d2vm = gensim.models.Doc2Vec.load(model_path)
    else:
        print('Training Model')
        d2vm.train(
            tagged_cells,
            total_examples=len(tagged_cells),
            epochs=100)
        print('    trained')
        d2vm.save(model_path)
    return d2vm
</code></pre>

<p>What I hope to achieve is a set of document vectors which will help with finding similar user requirements from a free text and a Hierarchical Clustering to build navigation of the existing requirements.</p>
",2019-03-25 10:54:21,2019-03-25 15:57:29,Doc2vec beyond beginner guidance,<python><dataframe><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20565,55262684,2019-03-20 14:06:52,,"<p>When working with a sparse matrix, it abruptly kills the kernel and exit code 139.
This happened when working with Gensim, which uses the sparse matrix format.</p>

<p>The failure happens when multiplying the matrix with another matrix, or even when using matrix.sum().</p>

<p>the matrix was created using scipy:</p>

<pre><code> matrix = scipy.sparse.csc_matrix((data, indices, indptr), shape=(num_terms, num_docs), dtype=dtype)
</code></pre>
",,2019-03-21 08:03:20,sparse matrix causes Segmentation fault exit code 139,<scipy><segmentation-fault><sparse-matrix><gensim><exit-code>,,,CC BY-SA 4.0,False,False,True,False,False
20567,55263867,2019-03-20 14:58:16,,"<p>I am training my <code>ldamodel</code> using <code>gensim</code>, and predicting using a test corpus like this <code>ldamodel[doc_term_matrix_test]</code>, it works just fine but I don't understand how the prediction is actually done using the trained model (what <code>ldamodel[doc_term_matrix_test]</code> is doing).</p>

<p>Here is the code :</p>

<pre><code>dictionary2 = corpora.Dictionary(test)
dictionary = corpora.Dictionary(train)
dictionary.merge_with(dictionary2)
doc_term_matrix2 = [dictionary.doc2bow(doc) for doc in test]
doc_term_matrix = [dictionary.doc2bow(doc) for doc in train]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = 
dictionary,random_state=100, iterations=50, passes=1)
topics = sorted(ldamodel[doc_term_matrix2],
                key=lambda 
                x:x[1],
                reverse=True)
</code></pre>
",2019-07-02 21:43:57,2019-07-02 21:43:57,How does LDA (Latent Dirichlet Allocation) inference from `gensim` work for a new data?,<python><gensim><lda><topic-modeling><inference>,,,CC BY-SA 4.0,False,False,True,False,False
20568,55264230,2019-03-20 15:15:27,,"<p>A huge problem with training with <code>LdaMulticore</code>. It takes 2.5h to get only 25 topics. Whilst only one core is active, and I have 16 of them on Amazon EC2.
How can I optimize this? </p>

<p>Something is bottlenecking this process... When I take a look at processes only one core is active, but after some time all cores get active for a couple of seconds, then again one core.</p>

<pre><code>numberTopics = 25   #Number of topics
model_gensim = LdaMulticore(num_topics=numberTopics,
                        id2word=dictionary,
                        iterations=10,
                        passes=1,
                        chunksize=50,
                        eta='auto',
                        workers=12)


perp_gensim = []
times_gensim = []
i=0
max_it = 5
min_prep = np.inf
start = time()
for _ in tqdm_notebook(range(100)):
    model_gensim.update(corpus)
    tmp = np.exp(-1 * model_gensim.log_perplexity(corpus))
    perp_gensim.append(tmp)
    times_gensim.append(time() - start)
    if(tmp&lt;min_prep):
        min_prep = tmp;
        i = 0
    else:
        i = i + 1;
        if (i==max_it):
            break                
model_gensim.save('results/model_genism/model_genism.model')
with open('results/model_genism/perp_gensim.pickle', 'wb') as f:
    pickle.dump(perp_gensim, f)
with open('results/model_genism/time_gensim.pickle', 'wb') as f:
    pickle.dump(times_gensim, f)

for i, topic in enumerate(model_gensim.get_topics().argsort(axis=1)[:, -10:][:, ::-1], 1):
    print('Topic {}: {}'.format(i, ' '.join([vocabulary[id] for id in topic])))
</code></pre>
",,2019-03-20 15:15:27,LDA genism is using only one core out of 16,<machine-learning><amazon-ec2><nlp><lda>,,,CC BY-SA 4.0,False,False,True,False,False
20580,55298609,2019-03-22 11:26:20,,"<p>I am experiencing a problem when using a pre-trained fasttext.bin model (retreived from  <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>). Checking most_similar for in-vocabulary-words returns sensible responses. However, when checking most_similar for an out-of-vocabulary word that only differs one character returns gibberish. </p>

<p>My question: Has this something to do with the model, or am I using it in the wrong way?</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.en.300.bin')
model.most_similar(""universitet"")
[('Universitet', 0.8522759675979614),
 ('hgskolan', 0.677900493144989),
 ('Hgskola', 0.6725144386291504),
 ('hgskola', 0.6724666357040405),
 ('Hgskolan', 0.6600401997566223),
 ('Universitetet', 0.6519213318824768),
 ('Hgskolen', 0.647462010383606),
 ('Universiteti', 0.6399329900741577),
 ('forskning', 0.617483377456665),
 ('sprk', 0.6172543168067932)]

model.most_similar(""universitett"")
[('ESTATERETAILCONSUMERPHONESCARSBIKESAPPSINTERNETTABLETSCOMPUTERSSOCIETYPOLITICSLAWCRIMEENVIRONMENTSCIENCEARTSCELEBRITIESSPORTSSPECIALSFIRST',
  0.47905537486076355),
 ('Wikipedia-Page-Suzannah-B-Troy-6-yrs-after-Misogynist-Cyber-Vandalism-Censorship-via-Deletion-on-a-page-about-Censorship-Wikipedia-Agrees-to-retur',
  0.47733378410339355),
 ('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAkKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.474983274936676),
 ('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAUKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.47364047169685364),
 ('crescendosexibloguerobateyabsorbersexiindesignabledinerolatifundiosexibrezarcularsutesexirapoplinbrezarcorrentosoVd.lazadareflejoreglafeministabrezarchuzasexiouttiqueblogueroin',
  0.47090965509414673),
 ('QQFZAAEACwAAAAAGQASAAAIjgAJCBQIoGDBgQgTKiwooGHDgwshDgTgsOLDhAAGaAQwUYBBhx85EtS4cWLGjR5JSjxZkgDFkwwLohTJUqTLlANiwvQ4seVNjwwfBoVokKjFo0Jlksz506NFiklZtoQKFSjIoktLVv1YsahSn1WP0vzq02VYoAjJMsVYVKHZrDbdupW6Vq5cunHtRjQoMCAAIfkECRQABAAsCQADAAQABAAACAsABQgkILCgwYEBAQAh',
  0.46747487783432007),
 ('VikerraadioOtseEsilehtJrelkuulamineSaatekavaPodcastidRaadioteaterRaadio',
  0.4659830331802368),
 ('deblogueroreflejoantecedentesexitlacuachebateysuteindesignableabsorbersexilatifundiosexibrezarsutemultitnicosexiplinrapobrezarcorrentosoVd.lazadafisiochillidomabrezarsico-chuzaoutcolodrablogueroin',
  0.46159273386001587),
 ('2OtseEsilehtJrelkuulamineSaatedPodcastidKlassikaraadioOtseEsilehtJrelkuulamineSaatekavaPodcastidRaadio',
  0.4609595537185669),
 ('leilighetEiendomstypeSelveierleilighetPlass', 0.4550461769104004)]
</code></pre>
",,2019-03-22 11:44:46,Pretrained Fasttext model returns gibberish for out-of-vocabulary words,<python><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
20612,55302607,2019-03-22 15:08:47,,"<p>I am using a pre-trained  doc2vec model, when I try to find out most similar document to that of my sample document. It gives me  unsupported operand type(s) error.</p>

<pre><code>from gensim.models import Doc2Vec

filename = ""doc2vec.bin""
doc1 =[""This is a sample document.""]

model = Doc2Vec.load(filename)

inferred_vector = model.infer_vector(doc1)

sims = model.docvecs.most_similar(positive=[inferred_vector],topn=1)

print(sims)
</code></pre>

<p>This gives me following error</p>

<pre><code> File ""D:\doc2vectest.py"", line 10, in &lt;module&gt;
    sims = model.docvecs.most_similar(positive=[inferred_vector],topn=1)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1667, in most_similar
    self.init_sims()
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1630, in init_sims
    self.vectors_docs_norm = _l2_norm(self.vectors_docs, replace=replace)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 2346, in _l2_norm
    dist = sqrt((m ** 2).sum(-1))[..., newaxis]
TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'
</code></pre>
",,2019-03-25 03:05:16,Gensim doc2vec most similar gives unsupported operand type(s) error,<machine-learning><nlp><gensim><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20622,55341145,2019-03-25 15:23:22,,"<p>A <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer""><code>gensim.models.Word2Vec</code></a> class has method <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word"" rel=""nofollow noreferrer""><code>predict_output_word()</code></a>. Now I use prelearned model but it was saved in class <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>gensim.models.KeyedVectors</code></a>. Have a the class analogue method? Or how can I get instance of <code>gensim.models.Word2Vec</code> from gensim.models.KeyedVectors`?</p>

<p>I know about <code>most_similar()</code> but it something another.</p>
",,2020-03-27 20:44:56,How to predict output word for KeyedVectors word2vec?,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20629,55424127,2019-03-29 19:10:45,,"<p>I have a data-set that contains search queries terms and the ids of the users that typed/searched for those queries.
I want to get a user-embedding (user2vec) sort of thing to learn to  the type of thing the users search for.</p>

<p>Previously, I merged all the queries for each user id and tokenized it and tagged it using <code>gensim.doc2vec</code> Tagged function.</p>

<p>when, I tried using <code>gensim.doc2vec</code> to learn the embedding i kept getting memory error each time I run it.</p>

<p>I want to do it this time with Keras but don't know how input my data and how to do it.</p>

<p>I want to output to show a matrix <code>m</code> by <code>n</code>
where <code>m</code>, is the number of users 
<code>n</code>, is the dimension of the embedding(30)</p>
",2019-03-29 19:58:37,2019-07-11 14:09:26,How to input the data set in for doc2vec in Keras and train,<keras><out-of-memory><gensim><word-embedding><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20631,55373509,2019-03-27 09:15:34,,"<p>I'm trying to implement <code>word2vec</code> in python to score the trained Skip-gram model on a pair of words. but I can't figure out the error:</p>

<blockquote>
  <p>only integers, slices (<code>:</code>), ellipsis (<code>...</code>), numpy.newaxis (<code>None</code>) and integer or boolean arrays are valid indices.</p>
</blockquote>

<p>Here is the code I tried :</p>

<pre><code>model = Word2Vec.load(r""C:\Users\Lenovo\model\word2vecforlaw.model"")
z=gensim.models.word2vec.score_sg_pair(model, ""patent"", ""law"")
print(z)
</code></pre>
",2019-03-27 10:05:44,2019-03-27 21:01:50,using word2vec.score.sg pair() raises Python error - only integers.....integer or boolean arrays are valid indices,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20632,55376622,2019-03-27 11:56:23,,"<p>I am trying to train a doc2vec model on a corpus of six novels and I need to build the corpus of Tagged Documents. 
Each novel is a txt file, already preprocessed and read into python using the <code>read()</code> method, so that it appears as a ""<code>long string</code>"". If I try to tag each novel using TaggedDocument form gensim, each novel gets only one tag, and the corpus of tagged documents has only six elements (which is not enough to train the doc2vec model). </p>

<p>I have been suggested to split each novel into sentences, then assign each sentence one tag for the ID of the sentence, and then one tag for the ID of the book it belongs to. I am, however, in trouble since I do not know how to structure the code.</p>

<p>This was the first code, i.e. the one using each novel in the format of a ""<code>long string</code>"":</p>

<pre><code>    `documents=[emma_text, persuasion_text, prideandprejudice_text,   
     janeeyre_text, shirley_text, professor_text] 
     corpus=[]`

    `for docid, document in enumerate(documents):
         corpus.append(TaggedDocument(document.split(), tags=
         [""{0:0&gt;4}"".format  
         (docid)]))`    

     `d2v_model = Doc2Vec(vector_size=100, 
                window=15,
                hs=0,
                sample=0.000001,
                min_count=100,
                workers=-1,
                epochs=500,
                dm=0, 
                dbow_words=1) 

    d2v_model.build_vocab(corpus)`

    `d2v_model.train(corpus, total_examples=d2v_model.corpus_count,    
     epochs=d2v_model.epochs)`
</code></pre>

<p>This, however, means that my corpus of tagged documents has only six elements and that my model has not enough elements on which to train. If for instance I try to apply the <code>.most_similar</code> method to a target book, I get completely wrong results</p>

<p>To sum up, I need help to assign each sentence of each book (I have already split the books into sentences) one tag for the ID of the sentence and one tag for the ID of the book it belongs to, using TaggedDocument to build the corpus on which I will train my model.</p>

<p>Thanks for the attention!</p>
",2019-03-27 14:18:02,2019-03-27 14:18:02,Doc2vec on a corpus of novels: how do I assign to each sentence of a novel one tag for the ID of the sentence and one tag for the ID of the book?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20635,55384167,2019-03-27 18:24:15,,"<p>I am getting repeated lines in my summarizer output. I am using genism in python for summarizing text documents. How to remove duplicate lines from the output of the summarizer. The output is coming with repeated content. How can I only keep unique lines in the output from the summarizer .The input file is as follows</p>

<pre><code>From: Jos
To: Halley, Ibizo /FR
Cc: pqr Secretariat; Bjrnsson Ulrika
Subject: [EXTERNAL] pqr Response to Letter of Intent for a Variation WS procedure:SE/H/xxxx/WS/
Date: vendredi 1 juin 2018 13:16:48
Attachments: image001.jpg

A07_SE_xxx yy R&amp;D.PDF

Dear Ibizo,

Thank you for your letter of intent.

The pqr agrees, on the basis of the documentation provided, that the above mentioned work-
sharing application as specified in the enclosed letter of intent is acceptable for submission under
Article 20 of the Commission Regulation (EC) No 1234/2008 of 24 November 2008.

The reference authority for the worksharing procedure will be Sweden and the assigned work sharing
procedure number will be:

A07: SE/H/xxxx/WS/



Please be advised that this confirmation is not to be considered as validation of your application. The
validity of the worksharing application will be checked by the reference authority after submission.

Please liaise with the assigned reference authority for the further proceedings.


Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department
Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Jos 
Sent: 30 April 2018 11:17
To: Ibizo.Halley@xxx.com
Cc: pqr Secretariat
Subject: RE: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Ibizo,
Thank you for your zzil.
The letter of intent will be discussed in the May 2018 pqr meeting and you will receive feedback
within two weeks following the meeting.



Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department

mailto:eretta.ab@zz.europa.eu
mailto:Ibizo.Halley@xxx.com
mailto:H-pqrSecretariat@zz.europa.eu
mailto:Ulrika.Bjornsson@mpa.se
mailto:certificate@zz.europa.eu

pqr/162/2010/Rev.2, August 2014 








26 April 2018 

pqr Secretariat 
Panthers Medicines Agency 
30 Bluegoon Place, ABC Wharf 
ABC E14 5EU  
United Kingdom 



Subject: Letter of intent for the submission of a worksharing procedure to the pqr according 


to Article 20 of Commission Regulation (EC) No 1234/2008 



Worksharing Applicant details: 


Name  : xxx-yy R&amp;D 


   Address : 1, lane Pierre Brossolette  
91385 Chilly-Maz 
Sw



Contact person details  
(i.e. name, address, e-mail 
address, phone number, fax 
number) 


: Ibizo Halley 
1, lane Pierre Brossolette  
91385 Chilly-Maz
Sw 
zzil: Ibizo.halley@xxx.com 
Tel : + 33 1 60 49 51 61 





Application details: 

This letter of intent for the submission of a Type II following a worksharing procedure according to 
Article 20 of Commission Regulation (EC) No 1234/2008, concerns the following medicinal products 
authorised via MRP and national procedures: 


Products authorized via MRP: 

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/001 











mailto:Ibizo.halley@xxx.com





Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


SE/H/0112/002 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/002 



Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL UNO       10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 


ALFUZOSIN WINTHROP 
UNO 10 MG 


Alfuzosin 
hydrochloride 


DE/H/2130/001 


ALFUZOSIN ZENTIVA 10 
MG 


Alfuzosin 
hydrochloride 


DE/H/2131/001/MR 


UROXATRAL Alfuzosin 
hydrochloride 


DE/H/2129/001 


Alfuzosin Zentiva    10 mg 
Retardtabletten 


Alfuzosin 
hydrochloride 


DE/H/2131/001 


XATRAL OD 10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 




Products authorised via national procedure:  

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10600 


Denmark 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


NL 14785 France 


ALFUZOSIN 
WINTHROP 2.5 MG 


Alfuzosin 
hydrochloride 


32177.00.00 Germany 


UROXATRAL Alfuzosin 
hydrochloride 


18111.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10602 


Greece 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PA 540/162/1 Ireland 


XATRAL Alfuzosin 
hydrochloride 


027314018 Italy 


MITTOVAL Alfuzosin 
hydrochloride 


026670024 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10163 


Italy 


XATRAL Alfuzosin 
hydrochloride 


RVG 13689 Netherlands 


DALFAZ Alfuzosin 
hydrochloride 


R/6812 Poland 


BENESTAN 2.5 MG Alfuzosin 
hydrochloride 


60031 Spain 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PL 04425/0655 United Kingdom 







ALFUZOSIN 
HYDROCHLORIDE 


2.5MG 


Alfuzosin 
hydrochloride 


PL 17780/0220 United Kingdom 






Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


NAT-H-4908-01 Belgium 


XATRAL Alfuzosin 
hydrochloride 


17139 



Cyprus 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


NL 19090 France 


ALFUZOSIN 
WINTHROP 5 MG 


Alfuzosin 
hydrochloride 


34637.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10812 


Greece 


ALFETIM SR 5 MG Alfuzosin 
hydrochloride 


OGYI-T-4374/01 Hungary 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#8994 


Italy 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


583/98/12/4785 Luxembourg 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


MA082/05001 Malta 


DALFAZ SR Alfuzosin 
hydrochloride 


8127 Poland 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


1026/2008 Romania 


XATRAL 5-SR Alfuzosin 
hydrochloride 


77/0275/96-S  Slovakia 


BENESTAN 
RETARD 5 MG 


Alfuzosin 
hydrochloride 


60767 Spain 








Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


NAT-H-4908-04 Belgium 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


19244  Cyprus 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


345201 Estonia 


XATRAL CR 10 MG Alfuzosin 
hydrochloride 


13973 Finland 


ALFUZOSINE 
ZENTIVA LP 10 MG 


Alfuzosin 
hydrochloride 


NL 24407 France 


XATRAL LP 10 MG Alfuzosin 
hydrochloride 


NL 24386 France 


XATRAL OD Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9520 


Greece 







ALFETIM UNO     
10 MG 


Alfuzosin 
hydrochloride 


OGYI-T-8022/01 Hungary 


XATRAL 10 MG Alfuzosin 
hydrochloride 


PA 540/162/3 Ireland 


MITTOVAL Alfuzosin 
hydrochloride 


026670048-051 Italy 


XATRAL 10 MG Alfuzosin 
hydrochloride 


027314044-057 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9579 


Italy 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


99-0702 Latvia 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


LT-2000/7118/10 Lithuania 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


0005/01/09/0045 Luxembourg 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


MA082/05002 Malta 


XATRAL XR 10 MG Alfuzosin 
hydrochloride 


RVG 23923 Netherlands 


DALFAZ UNO Alfuzosin 
hydrochloride 


8378 Poland 


BENESTAN OD    
10 MG 


Alfuzosin 
hydrochloride 


99/H/0006/01 Portugal 


ALFUZOSINA 
ZENTIVA, 10 MG 


Alfuzosin 
hydrochloride 


99/H/0007/001 Portugal 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


7893/2006 Romania 


UNIBENESTAN    
10 MG 


Alfuzosin 
hydrochloride 



63605 


Spain 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


PL 04425/0657 United Kingdom 


BESAVAR XL Alfuzosin 
hydrochloride 


PL 17780/0221 United Kingdom 








The following variation is intended to be part of the work-sharing procedure: 





Number as in the 
classification guideline: 


Title of variation as in the classification 
guideline 


Type of variation: 



C.I.4 



Changes in the Summary of Product 
Characteristics, Labelling or package 
Leaflet due new quality, preclinical, 
clinical or pharmacovigilance data 



Type II 








Justification for worksharing : xxx submitted for alfuzosin hydrochloride separate national and MRP variations for implementation of CCDS V13 including 
among other topics the addition of a contraindication to strong 
CYP3A4 inhibitors in the sections 4.3 and 4.5. 

The MAH received on 04 April 2018 a letter from pqr 
(zz/pqr/195547/2018) requesting to re-submit the variation 
for this contraindication as a work-sharing application including 







all MRP and nationally authorised products to harmonise the 
assessment of the contraindication in section 4.3 and 4.5 of the 
SmPC across the EU (provided in Annex I). 





Justification for grouping :  Not applicable 






Intended submission date : 30 June 2018 





Preferred Reference Authority 



: The Para Medical Products Agency, as RMS of the MRP 


procedure SE/H/0112/001-003 








Explanation that all MAs 
concerned belong to the 
same holder 


: I hereby confirm that all the marketing authorisations, listed in application details (refer above), concerned by the worksharing 
procedure belong to the same marketing authorisation holder, as 
they are part of the same mother company xxx, as per the 
Commission communication 98/C 229/03. 








Yours sincerely, 




Ibizo HALLEY 
xxx-yy R&amp;D, Europe Region 
Global Logistics Affairs Europe  






Please send this letter electronically to the pqr Secretariat (H-pqrSecretariat@zz.europa.eu) 
or RMS as relevant. 











mailto:H-pqrSecretariat@zz.europa.eu

























ANNEX 1 













30 Bluegoon Place  ABC Wharf  ABC E14 5EU  United Kingdom 






Telephone +44 (0)20 3660 6000 Facsimile +44 (0)20 3660 5520 

















Dr.ssa Maty Lecc
xxx S.p.A 


Viale L. Bodio 
20158 AUGB   
Italy 
E-mail: DRA@xxx.com 










4 April 2018 


zz/pqr/195547/2018 





Subject: Request for submission of variation worksharing procedure for Xatral (alfuzosin) 


and related names  





Dear Dr Maty Lecchi, 



During the March meeting, the pqr was informed that separate national and MRP variations have 


been submitted across EU Member States to request the inclusion of the below contraindication for 


Xatral (alfuzosin) and related names: 



Section 4.3 


Concomitant intake of strong inhibitors of CYP3A4 (see paragraph 4.5). 





The parallel submissions in several Member States have led to a disharmonised assessment of the 


contraindication. In the interest of public health across the Panthers Union, the pqr requests xxx 


to re-submit the variation as a worksharing application including all MRP, DCP and nationally 


authorised products to harmonise the assessment of the contraindication in section 4.3 of the SmPC 


across the EU. 


Please note that a separate letter on an independent issue to this has been sent to Esther de Bles, 


xxx-yy Netherlands B.V.. However, there are general concerns by the pqr on the lack of use 


of variation worksharing by xxx-yy in these cases.  



Kind Regards, 







Laura Oliveira Santamaria 


Chair of pqr 




mailto:DRA@xxx.com



        Worksharing Applicant details:

        Name 

        xxx-yy R&amp;D, Europe Region

        Global Logistics Affairs Europe






Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 3660 5525 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Ibizo.Halley@xxx.com [mailto:Ibizo.Halley@xxx.com] 
Sent: 27 April 2018 17:40
To: pqr Secretariat
Subject: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Sirs, Madams,

We are pleased to send you a request for the submission of a Type II variation following a worksharing
procedure according to Article 20 of Commission Regulation (EC) No 1234/2008 for Alfuzosin
hydrochloride containing products.
The variation concerns the addition of a contraindication with strong CYP 3A4 inhibitors in section 4.3
and 4.5.
The worksharing procedure has been requested to xxx by the chair of pqr, Mme Oliveira
Santamaria, the letter is attached as Annex of the letter of intent attached.

Thank you in advance for your agreement.

Kind regards,

Ibizo Halley
GEM/EP and OTC switch
EU Regional Logistics Product manager
Global Logistics Affairs
xxx R&amp;D
Phone: +33 1 60 49 51 61



logoGRA 1



________________________________________________________________________

This e-mail has been scanned for all known viruses by Panthers Medicines Agency.
</code></pre>
",,2019-03-27 19:09:42,Gensim summarization returning repeated lines as summary of text documents,<python><nlp><gensim><summarization><summarize>,,,CC BY-SA 4.0,False,False,True,False,False
20648,55343781,2019-03-25 17:49:12,,"<p>I have a text file with million of rows which I wanted to convert into word vectors and later on I can compare these vectors with a search keyword and see which all texts are closer to the search keyword.</p>

<p>My Dilemma is all the training files that I have seen for the Word2vec are in the form of paragraphs so that each word has some contextual meaning within that file. Now my file here is independent and contains different keywords in each row.</p>

<p>My question is whether is it possible to create word embedding using this text file or not, if not then what's the best approach for searching a matching search keyword in this million of texts</p>

<p>**My File Structure: **</p>

<pre><code>Walmart
Home Depot
Home Depot
Sears
Walmart
Sams Club
GreenMile
Walgreen
</code></pre>

<p><strong>Expected</strong></p>

<pre><code>search Text : 'WAL'
</code></pre>

<p><strong>Result from My File:</strong></p>

<pre><code>WALGREEN
WALMART
WALMART
</code></pre>
",2019-03-26 07:47:49,2019-03-26 09:15:17,Convert list of words in Text file to Word Vectors,<python><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20651,55304262,2019-03-22 16:44:49,,"<p>I would like to tokenize a list of strings according to my self-defined dictionary.</p>

<p>The list of string looks like this:</p>

<pre><code>lst = ['vitamin c juice', 'organic supplement'] 
</code></pre>

<p>The self-defined dictionary: </p>

<pre><code>dct = {0: 'organic', 1: 'juice', 2: 'supplement', 3: 'vitamin c'}
</code></pre>

<p>My expected result:</p>

<p>vitamin c juice --> <code>[(3,1), (1,1)]</code>
organic supplement --> <code>[(0,1), (2,1)]</code></p>

<p>My current code:</p>

<pre><code>import gensim
import gensim.corpora as corpora
from gensim.utils import tokenize
dct = corpora.Dictionary([list(x) for x in tup_list]) 
corpus = [dct.doc2bow(text) for text in [s for s in lst]]
</code></pre>

<p>The error message I got is <code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</code> However, I do not want to simply tokenize ""vitamin c"" as <code>vitamin</code> and <code>c</code>. Instead, I want to tokenize based on my existing <code>dct</code> words. That is to say, it should be <code>vitamin c</code>.</p>
",,2019-03-25 21:32:33,tokenize string based on self-defined dictionary,<python><nlp><nltk><tokenize><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
20656,55428777,2019-03-30 06:23:48,,"<p>I have some pre-trained word2vec model and I'd like to evaluate them using the same corpus. Is there a way I could get the raw training loss given a model dump file and the corpus in memory?</p>
",,2019-03-30 21:27:20,How to get word2vec training loss in Gensim from pretrained models?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20659,55468280,2019-04-02 06:32:39,,"<p>To analyze the text, we transform it into a list P1 of words. Then we apply the Bigram methods and get a list X of couples of words (ai,bi) such that ai and bi occur one after another in P1 quite a lot of times. How to get in Python 3 a list P2 from P1 so that every two items ai and bi if they go one after another in P1 and (ai,bi ) from X would be replaced by one element ai_bi?
My ultimate goal is to prepare the text as a list of words for analysis in Word2Vec.
I have my own code and it works but I think it will be slow on big texts.</p>

<pre><code>import nltk
from nltk.collocations import *
import re
import gensim
bigram_measures = nltk.collocations.BigramAssocMeasures()
sentences=[""Total internal reflection ! is the;phenomenon"",
""Abrasive flow machining :is an ? ( interior surface finishing process)"",
""Technical Data[of Electrical Discharge wire cutting and] Cutting Machine"",
""The greenhouse effect. is the process by which, radiation from a {planet atmosphere warms }the planet surface"",
""Absolute zero!is the lowest limit ;of the thermodynamic temperature scale:"",
""The term greenhouse effect ?is mentioned (a lot)"",
""[An interesting] effect known as total internal reflection."",
""effect on impact energies ,Electrical discharge wire cutting of ADI"",
""{Absolute zero represents} the coldest possible temperature"",
""total internal reflection at an air water interface"",
""What is Electrical Discharge wire cutting Machining and how does it work"",
""Colder than Absolute Zero"",
""A Mathematical Model for  Electrical Discharge Wire Cutting Machine Parameters""]
P1=[]
for f in sentences:
    f1=gensim.utils.simple_preprocess (f.lower())
    P1.extend(f1)
print(""First 100 items from P1"")
print(P1[:100])
#  bigram
finder = BigramCollocationFinder.from_words(P1)
# filter only bigrams that appear 2+ times
finder.apply_freq_filter(2)
# return the all bi-grams with the highest PMI
X=finder.nbest(bigram_measures.pmi, 10000)
print()
print(""Number of bigrams= "",len(X))
print(""10 first bigrams with the highest PMI"")
print(X[:10])
# replace ai and bi which are one after another in P1  and (ai,bi) in X  =&gt;&gt;  with ai_bi
P2=[]
n=len(P1)
i=0
while i&lt;n:
    P2.append(P1[i])
    if i&lt;n-2:
        for c in X:
            if c[0]==P1[i] and c[1]==P1[i+1]:
                P2[len(P2)-1]=c[0]+""_""+c[1]
                i+=1    # skip second item of couple from X  
                break
    i+=1
print()
print( ""first 50 items from P2 - results"")
print(P2[:50])
</code></pre>
",2019-04-08 11:35:08,2019-04-08 11:35:08,Replace two consecutive items in the list with one item,<python><python-3.x><list><nltk><n-gram>,,,CC BY-SA 4.0,True,False,True,False,False
20669,55485908,2019-04-03 01:52:55,,"<p>I'm getting a CalledProcessError ""non-zero exit status 1"" error when I run the Gensim LDAMallet model on my full corpus of ~16 million documents.
Interestingly enough, if I run the exact same code on a testing corpus of ~160,000 documents the code runs perfectly fine. Since it's working fine on my small corpus I'm inclined to think that the code is fine, but I'm not sure what else would/could cause this error...</p>

<p>I've tried editing the mallet.bat file as suggested <a href=""https://stackoverflow.com/questions/55288724/gensim-mallet-calledprocesserror-returned-non-zero-exit-status"">here</a>, but to no avail.
I've also double checked the paths, but that shouldn't be an issue given that it works with a smaller corpus.</p>

<pre><code>id2word = corpora.Dictionary(lists_of_words)
corpus =[id2word.doc2bow(doc) for doc in lists_of_words]
num_topics = 30
os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'})
mallet_path = r'C:/mallet-2.0.8/bin/mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
</code></pre>

<p>Here's the full traceback and error:</p>

<pre><code>  File ""&lt;ipython-input-57-f0e794e174a6&gt;"", line 8, in &lt;module&gt;
    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 132, in __init__
    self.train(corpus)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 273, in train
    self.convert_input(corpus, infer=False)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 262, in convert_input
    check_output(args=cmd, shell=True)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py"", line 1918, in check_output
    raise error

CalledProcessError: Command 'C:/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\user\AppData\Local\Temp\2\e1ba4a_corpus.txt --output C:\Users\user\AppData\Local\Temp\2\e1ba4a_corpus.mallet' returned non-zero exit status 1.
</code></pre>
",,2019-04-12 23:14:50,Python Gensim LDAMallet CalledProcessError with large corpus (runs fine with small corpus),<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
20670,55487124,2019-04-03 04:32:52,,"<p>I am following the tutorial here:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>But when I get to this part:</p>

<pre><code>pre = Doc2Vec(min_count=0)
pre.scan_vocab(documents)
</code></pre>

<p>I get the following error on scan_vocab:</p>

<pre><code>    AttributeError: 'Doc2Vec' object has no attribute 'scan_vocab'
</code></pre>

<p>Does anyone know how to fix this? Thanks.</p>
",,2019-04-03 16:31:38,Gensim Attribute Error when trying to use pre_scan on a doc2vec object,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20681,55393311,2019-03-28 08:44:39,,"<p>Gensim uses text streaming to minimize memory requirements. This is at the cost of performance due to endless disk IO. Is there a trick to on the fly copy the complete file from disk (one disk IO) to a temporary in-memory file?
I like to keep the code as is (no recoding into a list structures), but this is not a great way of debugging functionality</p>

<p>Expected result: much faster code</p>

<h3>Some more background on the question</h3>

<p>The original code is at <a href=""https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"" rel=""nofollow noreferrer"">https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb</a>. The example code is taken from the phrase modelling section</p>

<p>I'm calculating the unigrams. All reviews are at  </p>

<pre><code>review_txt_filepath = os.path.join(intermediate_directory,'review_text_all.txt'),
</code></pre>

<p>all unigrams should go to</p>

<pre><code>unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt') 
</code></pre>

<p>The crucial routines are</p>

<pre><code>def punct_space(token):
    return token.is_punct or token.is_space

def line_review(filename):
    # generator function to read in reviews from the file
    with codecs.open(filename, encoding='utf_8') as f:
        for review in f:
            yield review.replace('\\n', '\n')

def lemmatized_sentence_corpus(filename):
    # generator function to use spaCy to parse reviews, lemmatize the text, and yield sentences

    for parsed_review in nlp.pipe(line_review(filename),
                              batch_size=10000, n_threads=4):
        for sent in parsed_review.sents:
            yield u' '.join([token.lemma_ for token in sent
                             if not punct_space(token)])
</code></pre>

<p>The unigrams are calculated as</p>

<pre><code>with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:
    for sentence in lemmatized_sentence_corpus(review_txt_filepath):
        f.write(sentence + '\n')
</code></pre>

<p>Doing this for 5000 lines requires some patience, 1h30m ;-)</p>

<p>I'm not that familiar with iterables, but do I understand it correctly that I first have to read the 
actual file (on disc) into a variable ""list_of_data"" and process that</p>

<pre><code>with (review_txt_filepath, 'r', encoding='utf_8') as f:
    list_of_data = f.read()

with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:
    for sentence in lemmatized_sentence_corpus(list_of_data):
        f.write(sentence + '\n')
</code></pre>

<p>So the strategy is</p>

<pre><code>1. read all data into a list in memory
2. process the data
3. write the results to disc
4. delete the list from memory by setting list_with_data = ()
</code></pre>

<p>A problem with this is obviously that line_review is doing the file reading               </p>
",2019-04-01 19:38:14,2019-04-01 19:38:14,Text streaming in Gensim,<inputstream><gensim>,,,CC BY-SA 4.0,False,True,True,False,False
20705,55476594,2019-04-02 13:54:58,,"<p>I am trying to implement Latent Dirichlet Allocation (LDA) using python with Gensim, also I am referring the LDA code from a website but I am still not very clear with LDA python code. Could someone who knows LDA  explain to me in lucid manner according to the code as given below. I am also uploading the LDA formula here which is an image from wikipedia. In this case, LDA is being used to analyze a collection of text documents.</p>

<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=4, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=10,
                                           passes=10,
                                           alpha='symmetric',
                                           iterations=100,
                                           per_word_topics=True)
</code></pre>

<p><img src=""https://i.stack.imgur.com/yfThK.png"" alt=""LDA formula from wikipedia""></p>
",2019-04-02 14:46:00,2019-04-12 23:29:32,Not very clear with python code of LDA algorithm,<python-3.x><lda>,,,CC BY-SA 4.0,False,False,True,False,False
20709,55545188,2019-04-06 01:53:21,,"<p>The classic example of determining similarity as distance Word Mover's Distance as for example here <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a>,
word2vec model on GoogleNews-vectors-negative300.bin, D1=""Obama speaks to the media in Illinois"",D2=""The president greets the press in Chicago"",D3=""Oranges are my favorite fruit"". When calculated wmd distances: distance (D1,D2)=3.3741, distance (D1,D3)=4.3802. So we understand that (D1,D2) more similar than (D1,D3). What is the threshold value for vmd distance to decide that the two sentences actually contain almost the same information? Maybe in the case of sentences D1 and D2, the value of 3.3741 is too large and in reality these sentences are different? Such decisions need to be made, for example, when there is a question, a sample of the correct answer and a student's answer.
Addition after  the answer by gojomo:
Let's postpone identification and automatic understanding of logic for later. Let's consider the case when in two sentences there is an enumeration of objects, or properties and actions of one object in a positive way, and we need to evaluate how similar the content of these two sentences is.</p>
",2019-04-08 04:31:12,2019-04-08 04:31:12,Decision that texts or sentences are equivalent in content,<word2vec><similarity><wmd>,,,CC BY-SA 4.0,False,False,True,False,False
20711,55490182,2019-04-03 08:13:51,,"<p>/Users/Barry/anaconda/lib/python2.7/site-packages/gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars
  convergence = np.fabs((bound - old_bound) / old_bound)</p>

<pre><code>#dynamic topic model
def run_dtm(num_topics=18):
    docs, years, titles = preprocessing(datasetType=2)

    #resort document by years
    Z = zip(years, docs)
    Z = sorted(Z, reverse=False)
    years_new, docs_new = zip(*Z)

    #generate time slice
    time_slice = Counter(years_new).values()

    for year in Counter(years_new):
        print year,' --- ',Counter(years_new)[year]

    print '********* data set loaded ********'
    dictionary = corpora.Dictionary(docs_new)
    corpus = [dictionary.doc2bow(text) for text in docs_new]

    print '********* train lda seq model ********'
    ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=dictionary, time_slice=time_slice, num_topics=num_topics)

    print '********* lda seq model done ********'
    ldaseq.print_topics(time=1)
</code></pre>

<p>Hey guys, I'm using the dynamic topic models in gensim package for topic analysis, following this tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb</a>, however I always got the same unexpected error. Can anyone give me some guidance? I'm really puzzled even thought I have tried some different dataset for generating corpus and dictionary.
The error is like this:</p>

<p>/Users/Barry/anaconda/lib/python2.7/site-packages/gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars
  convergence = np.fabs((bound - old_bound) / old_bound)</p>
",,2019-04-12 17:43:51,gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20734,55478104,2019-04-02 15:12:48,,"<p>I downloaded the full wikipedia archive 14.9gb and I am running thise line of code:</p>

<pre><code>wiki = WikiCorpus(""enwiki-latest-pages-articles.xml.bz2"")
</code></pre>

<p>My code doesn't seem to be getting past here and it has been running for an hour now, I understand that the target file is massive, but I was wondering how I could tell it is working, or what is the expected time for it to complete?</p>
",,2019-04-02 17:20:24,How to tell if WikiCorpus from gensim is working?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20740,55622251,2019-04-10 22:40:43,,"<p>Can CoreNLP determine whether a common noun (as opposed to a proper noun or proper name) refers to a person out-of-the-box? Or if I need to train a model for this task, how do I go about that? </p>

<p>First, I am <em>not</em> looking for coreference resolution, but rather a building block for it. Coreference by definition depends on the context, whereas I am trying to evaluate whether a word <em>in isolation</em> is a subset of ""person"" or ""human"". For example:</p>

<pre><code>is_human('effort') # False
is_human('dog') # False
is_human('engineer') # True
</code></pre>

<p>My naive attempt to use Gensim's and spaCy's pre-trained word vectors failed to rank ""engineer"" above the other two words.</p>

<pre><code>import gensim.downloader as api
word_vectors = api.load(""glove-wiki-gigaword-100"") 
for word in ('effort', 'dog', 'engineer'):
    print(word, word_vectors.similarity(word, 'person'))

# effort 0.42303842
# dog 0.46886832
# engineer 0.32456854
</code></pre>

<p>I found the following lists from <a href=""https://nlp.stanford.edu/software/dcoref.html"" rel=""nofollow noreferrer"">CoreNLP</a> promising. </p>

<pre><code>dcoref.demonym                   // The path for a file that includes a list of demonyms 
dcoref.animate                   // The list of animate/inanimate mentions (Ji and Lin, 2009)
dcoref.inanimate 
dcoref.male                      // The list of male/neutral/female mentions (Bergsma and Lin, 2006) 
dcoref.neutral                   // Neutral means a mention that is usually referred by 'it'
dcoref.female 
dcoref.plural                    // The list of plural/singular mentions (Bergsma and Lin, 2006)
dcoref.singular
</code></pre>

<p>Would these work for my task? And if so, how would I access them from the <a href=""https://pypi.org/project/corenlp-python/"" rel=""nofollow noreferrer"">Python wrapper</a>? Thank you.</p>
",,2019-04-11 19:44:51,CoreNLP: Can it tell whether a noun refers to a person?,<nlp><stanford-nlp><pycorenlp>,,,CC BY-SA 4.0,False,True,True,True,False
20752,55492888,2019-04-03 10:25:59,,"<p>I am working on a document similarity problem. For each document, I retrieve the vectors for each of its words (from a pre-trained word embedding model) and average them to get the document vector. I end up having a dictionary (say, my_dict) that maps each document in my collection to its vector. </p>

<p>I want to feed this dictionary to gensim and for each document, get other documents in 'my_dict' that are closer to it. How could I do that?</p>
",,2019-04-12 14:26:42,How to get similar words from a custom input dictionary of word to vectors in gensim,<python><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
20771,55660598,2019-04-12 23:25:57,,"<p>I have trained a doc2vec model on the Wikipedia corpus using gensim and I wish to retrieve vectors from different documents. </p>

<p>I was wondering what text processing the WikiCorpus function did when I used it to train my model e.g. removed punctuation, made all the text lower case, removed stop words etc. </p>

<p>This is important as I wish to perform the same text processing on the documents I am inferring vectors from for greater consistency/accuracy with my model. </p>
",,2019-04-13 21:03:07,What text processing does WikiCorpus perform in gensim?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20781,55592142,2019-04-09 11:46:29,,"<p>I don't understand how word vectors are involved at all in the training process with gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">doc2vec</a> in DBOW mode (<code>dm=0</code>). I know that it's disabled by default with <code>dbow_words=0</code>. But what happens when we set <code>dbow_words</code> to 1?</p>

<p>In my understanding of DBOW, the context words are predicted directly from the paragraph vectors. So the only parameters of the model are the <code>N</code> <code>p</code>-dimensional paragraph vectors plus the parameters of the classifier.</p>

<p>But multiple sources hint that it is possible in DBOW mode to co-train word and doc vectors. For instance:</p>

<ul>
<li>section 5 of <a href=""https://www.aclweb.org/anthology/W16-1609"" rel=""nofollow noreferrer"">An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</a></li>
<li>this SO answer: <a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors/30337118#30337118"">How to use Gensim doc2vec with pre-trained word vectors?</a></li>
</ul>

<p>So, how is this done? <strong>Any clarification would be much appreciated!</strong></p>

<p>Note: for DM, the paragraph vectors are averaged/concatenated with the word vectors to predict the target words. In that case, it's clear that words vectors are trained simultaneously with document vectors. And there are <code>N*p + M*q + classifier</code> parameters (where <code>M</code> is vocab size and <code>q</code> word vector space dim).</p>
",2019-04-10 12:47:23,2019-04-10 12:47:23,How are word vectors co-trained with paragraph vectors in doc2vec DBOW?,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20784,55629368,2019-04-11 09:40:00,,"<p>I am using a word embeddings model (FastText via Gensim library) to expand the terms of a search. 
So, basically if the user write ""operating system"" my goal is to expand that term with very similar terms like ""os"", ""windows"", ""ubuntu"", ""software"" and so on.</p>

<p>The model works very well but now the time has come to improve the model with ""external information"", with ""external information"" i mean OOV (out-of-vocabulary) terms OR terms that do not have good context.</p>

<p>Following the example i wrote above when the user writes <strong>operating system</strong> i would like to expand the query with the ""general"" terms:</p>

<p>Terms built in the FastText model:</p>

<ul>
<li>windows </li>
<li>ubuntu</li>
<li>software</li>
</ul>

<p><strong>AND</strong> </p>

<p>terms that represent (organizations/companies) like ""Microsoft"", ""Apple"" so the complete query will be:</p>

<ul>
<li><strong>term</strong>: operating system</li>
<li><strong>query</strong>: operating system, os, software, windows, ios, Microsoft, Apple</li>
</ul>

<p>My problem is that i DO NOT have companies inside the corpus OR, if present, i do not have to much context to ""link"" Microsoft to ""operating system"".</p>

<p>For example if i extract a piece inside the corpus i can read ""... i have started working at Microsoft in November 2000 with my friend John ..."" so, as you can see, i cannot contextualize ""Microsoft"" word because i do not have good context, indeed.</p>

<p>A small recap:</p>

<ol>
<li>I have a corpus where the companies (terms) have poor context</li>
<li>I have a big database with companies and the description of what they do.</li>
</ol>

<p>What i need to do:</p>

<p>I would like to include the companies in my FastText model and set ""manually"" their words context/cloud of related terms.</p>

<p>Ideas?</p>
",,2019-04-11 13:56:53,How to add OOV terms in a word embeddings model,<python><machine-learning><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20789,55612157,2019-04-10 12:05:32,,"<p>I am using spacy 2.1.3</p>

<p>I want to add custom entities to a model. I created the model with word2vec from Gensim using:</p>

<p>python -m spacy init-model en C:\myproject\gcmodel -v gcword2vec.txt</p>

<p>Then I wanted to run the training on my custom entities data, followed the example given in the documentation:</p>

<pre><code>def main(model=None,output_dir=None, n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")

    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe(""ner"")

    prepareTrainingData()   # Our extension to create training data

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # reset and initialize the weights randomly  but only if we're
        # training a new model
        if model is None:
            nlp.begin_training()

        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    texts,  # batch of texts
                    annotations,  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)
......
.....
}
</code></pre>

<p>When I run it, it breaks at the nlp.update() call with error:</p>

<pre><code>    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)
  File ""nn_parser.pyx"", line 391, in spacy.syntax.nn_parser.Parser.update
  File ""nn_parser.pyx"", line 235, in spacy.syntax.nn_parser.Parser.require_model
ValueError: [E109] Model for component 'ner' not initialized. Did you forget to load a model, or forget to call begin_training()?
</code></pre>

<p>The model was passed in the command line and has been loaded up front. What am I doing wrong?</p>

<p>Thanks!</p>
",,2019-04-10 12:05:32,Spacy model update for NER from existing model failure,<model><spacy><ner>,,,CC BY-SA 4.0,False,True,True,False,False
20808,55665180,2019-04-13 11:57:31,,"<p>In gensim I have a trained doc2vec model, if I have a document and either a single word or two-three words, what would be the best way to calculate the similarity of the words to the document? </p>

<p>Do I just do the standard cosine similarity between them as if they were 2 documents? Or is there a better approach for comparing small strings to documents?</p>

<p>On first thought I could get the cosine similarity from each word in the 1-3 word string and every word in the document taking the averages, but I dont know how effective this would be.</p>
",2019-04-13 12:54:14,2019-04-13 21:48:53,How do I calculate the similarity of a word or couple of words compared to a document using a doc2vec model?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20835,55649311,2019-04-12 10:10:07,,"<p>I have a function:</p>

<pre><code>def remove_stopwords(text):
     return [[word for word in simple_preprocess(str(doc), min_len = 2) if word not in stop_words] for doc in texts] 
</code></pre>

<p>My input is a list with a tokenized sentence:</p>

<pre><code>input = ['This', 'is', 'an', 'example', 'of', 'my', 'input']
</code></pre>

<p>Assume that <code>stop_words</code> contains the words: 'this', 'is', 'an', 'of' and 'my', then the output I would like to get is:</p>

<pre><code>desired_output = ['example', 'input']
</code></pre>

<p>However, the actual output that I'm getting now is:</p>

<pre><code>actual_output = [[], [], [], ['example'], [], [], ['input']]
</code></pre>

<p>How can I adjust my code, to get this output?</p>
",,2019-04-12 10:32:24,"How to only return actual tokens, rather than empty variables when tokenizing?",<python><apply><tokenize><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20838,55612440,2019-04-10 12:21:27,,"<p>I am trying to add pretrained vectors to a training model using fasttext and getting the below error. Code is written in python with fasttext 0.8.3.</p>

<p>I thought with fasttext you could add pre trained vectors to a supervised training model?</p>

<p>TypeError: supervised() got an unexpected keyword argument 'pretrainedVectors'</p>

<pre><code>pretrainedVectors = 'vectorFile.vec'
classifier = ft.supervised(model_data, model_name, pretrainedVectors=pretrainedVectors, label_prefix=label_prefix, lr=lr, epoch=epoch, minn=minn, maxn=maxn, dim=dim, bucket=bucket)
</code></pre>
",2019-04-10 12:30:34,2019-04-10 12:30:34,fasttext error TypeError: supervised() got an unexpected keyword argument 'pretrainedVectors',<python><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
20857,55705634,2019-04-16 10:07:28,,"<p>i am training multiple word2vec models on the same corpus. (i am doing this to study the variation in learned word vectors)</p>

<p>i am using this tutorial as reference: <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a></p>

<p>it is suggested that by default gensim.models.word2vec will iterate over the corpus at least twice. once for initialization and then again for training (iterating the number of epochs specified) </p>

<p>since i am always using the same corpus, i want to save time by initializing only once, and providing the same initialization as input to all successive models.</p>

<p>how can this be done?</p>

<p>this is my current setting:</p>

<pre class=""lang-py prettyprint-override""><code>subdirectory = 'corpus_directory'
for i in range(10):
    sentences = MySentences(subdirectory) # a memory-friendly iterator
    model = gensim.models.Word2Vec(sentences, min_count=20, size=100, workers=4)
    model.train(sentences, total_examples=model.corpus_count, epochs=1)
    word_vectors = model.wv
    fname = 'WV{}.kv'
    word_vectors.save(fname.format(i))
</code></pre>

<p>where MySentences is defined similarly to the tutorial:
(i made a slight change, so the order of corpus sentences would be shuffled with each initialization)</p>

<pre class=""lang-py prettyprint-override""><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.file_list = [fname for fname in os.listdir(dirname) if fname.endswith('.txt')]
        random.shuffle(self.file_list)

    def __iter__(self):
        for article in self.file_list:
            for line in open(os.path.join(self.dirname, article)):
                yield line.split()
</code></pre>
",,2019-04-16 21:05:01,how to speed up gensim word2vec initialization with pre proccessed corpus?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20870,55520565,2019-04-04 16:07:48,,"<p><strong>Background :</strong></p>

<p>Given a corpus I want to train it with an implementation of word2wec (Gensim). </p>

<p>Want to understand if the final similarity between 2 tokens is dependent on the frequency of A and B in the corpus <em>(all contexts preserved)</em>, or agnostic of it.</p>

<p><em>Example</em>: 
(May not be ideal, but using it to elaborate the problem statement) </p>

<p>Suppose word 'A' is being used in 3 different contexts within the corpus : </p>

<pre><code>Context 1 : 1000 times
Context 2 : 50000 times
Context 3 : 50000 times
</code></pre>

<p>'B' is being used in 2 different contexts :</p>

<pre><code>Context 1 : 300 times 
Context 5 : 1000 time
</code></pre>

<p><strong>Question :</strong> </p>

<p>If I change the frequency of 'A' in my corpus (ensuring no context is lost, i.e. 'A' is still being used at least once in all the contexts as in the original corpus), is the similarity between A snd B going to be the same ?</p>

<p>New distribution of 'A' across contexts</p>

<pre><code> Context 1 : 5 times
 Context 2 : 10 times
 Context 3 : 5000 times
</code></pre>

<p>Any leads appreciated</p>
",2019-04-04 16:14:09,2019-04-04 17:21:20,Semantic similarity between words A and B : Dependency on frequency of A and B in corpus?,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
20887,55692700,2019-04-15 15:36:47,,"<p>Is it possible to apply a sentence-level LDA model using Gensim as proposed in Bao and Datta(2014)?  The paper <a href=""https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2014.1930"" rel=""nofollow noreferrer"">is here</a>.  </p>

<p>The distinct feature is that it makes the ""one topic per sentence assumption"" (p.1376). This is different from other sentence-level methods, which typically allow each sentence to include multiple topics. ""The most straightforward method is to treat each sentence as a document and apply the LDA model on the collection of sentences rather than documents."" (P.1376). But, I think it is more reasonable to assume that one sentence deals with one topic. </p>

<p>Thank you!</p>
",2019-04-15 19:36:21,2019-04-15 19:36:21,How to apply a sentence-level LDA model using Gensim?,<python><nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
20889,55764137,2019-04-19 15:25:32,,"<p>I have a word list like<code>['like','Python']</code>and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? </p>

<p><strong>What I tried</strong></p>

<p>I iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.</p>

<pre><code>def readWordEmbeddingVector(Wrd):
    f = open('glove.twitter.27B/glove.twitter.27B.200d.txt','r')
    words = []
    a = f.readline()
    while a!= '':
        vector = a.split()
        if vector[0] in Wrd:
            words.append(vector)
            Wrd.remove(vector[0])
        a = f.readline()
    f.close()
    words_vector = pd.DataFrame(words).set_index(0).astype('float')
    return words_vector
</code></pre>

<p>I also tried below, but it loaded the whole file instead of vectors I need</p>

<pre class=""lang-py prettyprint-override""><code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('word2vec.twitter.27B.200d.txt')
</code></pre>

<p><strong>What I want</strong></p>

<p>Method like <code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</code> but I can set a word list to load.</p>
",,2019-04-21 01:31:11,Load a part of Glove vectors with gensim,<python><gensim><word-embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
20904,55693318,2019-04-15 16:13:35,,"<p>I am training a GloVe model with my own corpus and I have troubles to save it/load it in an <code>utf-8</code> format.</p>

<p>Here what I tried: </p>

<pre><code>from glove import Corpus, Glove

#data
lines = [['woman', 'umbrella', 'silhouetted'], ['person', 'black', 'umbrella']]

#GloVe training
corpus = Corpus() 
corpus.fit(lines, window=4)
glove = Glove(no_components=4, learning_rate=0.1)
glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)
glove.add_dictionary(corpus.dictionary)
glove.save('glove.model.txt')
</code></pre>

<p>The saved file <code>glove.model.txt</code> is unreadable and I can't succeed to save it with a <code>utf-8</code> encoding.</p>

<p>When I try to read it, for exemple by converting it in a Word2Vec format:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file=""glove.model.txt"", 
word2vec_output_file=""gensim_glove_vectors.txt"")    

model = KeyedVectors.load_word2vec_format(""gensim_glove_vectors.txt"", binary=False)
</code></pre>

<p>I have the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>Any idea on how I could use my own GloVe model ?  </p>
",,2020-07-03 18:52:23,Encoding problem while training my own Glove model,<python><encoding><nlp><word-embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
20905,55693826,2019-04-15 16:45:59,,"<p>I pretrained a word embedding using wang2vec (<a href=""https://github.com/wlin12/wang2vec"" rel=""nofollow noreferrer"">https://github.com/wlin12/wang2vec</a>), and i loaded it in python through gensim. When i tried to get the vector of some words not in vocabulary, i obviously get:</p>

<pre class=""lang-py prettyprint-override""><code>KeyError: ""word 'kjklk' not in vocabulary""
</code></pre>

<p>So, i thought about adding an item to the vocabulary to map oov (out of vocabulary) words, let's say <code>&lt;OOV&gt;</code>. Since the vocabulary is in <code>Dict</code> format, i would simply add the item <code>{""&lt;OOV&gt;"":0}</code>. </p>

<p>But, i searched an item of the vocabulary, with</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.KeyedVectors.load_word2vec_format(w2v_ext, binary=False, unicode_errors='ignore')
dict(list(model.vocab.items())[5:6])
</code></pre>

<p>The output was something like</p>

<pre class=""lang-py prettyprint-override""><code>{'word': &lt;gensim.models.keyedvectors.Vocab at 0x7fc5aa6007b8&gt;}
</code></pre>

<p>So, is there a way to add the <code>&lt;OOV&gt;</code> token to the vocabulary of a pretrained word embedding loaded through gensim, and avoid the KeyError? I looked at gensim doc and i found this: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab</a>
but it seems not work with the update parameter.</p>
",2019-04-15 18:25:40,2019-04-16 04:57:35,Manage KeyError with gensim and pretrained word2vec model,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20906,55694532,2019-04-15 17:34:08,,"<p>I'm trying to show learning progress in my LdaModel, but every sample I found on the web throws exceptions:</p>

<pre><code>l =  gensim.models.callbacks.CoherenceMetric(corpus=common_corpus, logger='shell')
lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])
</code></pre>

<p>Throws:</p>

<pre><code>  File ""&lt;ipython-input-165-6ad0e2e8516c&gt;"", line 2, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 371, in __init__
    self.update(corpus, chunks_as_numpy=use_numpy)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 750, in update
    current_metrics = callback.on_epoch_end(pass_)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 288, in on_epoch_end
    value = metric.get_value(topics=topics, model=self.model, other_model=self.previous)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 105, in get_value
    coherence=self.coherence, topn=self.topn

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py"", line 190, in __init__
    self.window_size = SLIDING_WINDOW_SIZES[self.coherence]

KeyError: None
</code></pre>

<p>This code (found <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>class EpochLogger(CallbackAny2Vec):
    '''Callback to log information about training'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_begin(self, model):
        print(""Epoch #{} start"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""Epoch #{} end"".format(self.epoch))
        self.epoch += 1

l = EpochLogger()
lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])
</code></pre>

<p>Throws:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-167-e89e2bf41977&gt;"", line 1, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 371, in __init__
    self.update(corpus, chunks_as_numpy=use_numpy)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 688, in update
    callback.set_model(self)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 264, in set_model
    if any(metric.logger == ""visdom"" for metric in self.metrics):

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 264, in &lt;genexpr&gt;
    if any(metric.logger == ""visdom"" for metric in self.metrics):

AttributeError: 'EpochLogger' object has no attribute 'logger'
</code></pre>

<p>Currently I'm mostly interested in monitoring learning progress (to eyeball ETA).</p>

<p>What would be the proper way of setting a callback?</p>
",,2019-08-26 20:46:28,How to log epochs in Gensim's LdaModel,<python><python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20907,55710967,2019-04-16 14:48:12,,"<p>Lets say I am trying to compute the average distance between a word and a document using distances() or compute cosine similarity between two documents using n_similarity(). However, lets say these new documents contain words that the original model did not. How does gensim deal with that?</p>

<p>I have been reading through the documentation and cannot find what gensim does with unfound words.</p>

<p>I would prefer gensim to not count those in towards the average. So, in the case of distances(), it should simply not return anything or something I can easily delete later before I compute the mean using numpy. In the case of n_similarity, gensim of course has to do it by itself....</p>

<p>I am asking because the documents and words that my program will have to classify will in some instances contain unknown words, names, brands etc that I do not want to be taken into consideration during classification. So, I want to know if I'll have to preprocess every document that I am trying to classify. </p>
",,2020-03-19 17:54:08,Dealing with new words in gensim not found in model,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20941,55751027,2019-04-18 17:25:33,,"<p>I have a word2vec file in the standard format, but it is huge with 2M items. I also have a vocabulary file where each row is a word, the file has about ~800K rows. Now I want to load the embeddings from the word2vec file, and I want only embeddings for words in the vocabulary file. Is there an efficient implementation in gensim?</p>
",,2019-04-19 03:36:17,How to load a word2vec txt file with vocabulary constraint,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20956,55774197,2019-04-20 13:54:20,,"<p>I'm facing a Gensim training problem using Word2Vec. 
model.wv.vocab is not getting any further word from the trained corpus 
the only words in are from the ones from initialization instruction ! </p>

<p>In fact, after many times trying on my own code, even the official site's example didn't work !  </p>

<p>I tried saving model at many spots in my code 
I even tried saving and reloading the corpus alongside train instruction</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

path = get_tmpfile(""word2vec.model"")

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

model.train([[""hello"", ""world""]], total_examples=1, epochs=1)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

</code></pre>

<p>first print statement gives 12 which is right </p>

<p>second 12 when it's supposed to give 14 (len(vocab + 'hello' + 'world'))</p>
",,2019-04-21 01:27:02,Gensim's Word2Vec not training provided documents,<python-3.x><gensim><google-colaboratory>,,,CC BY-SA 4.0,False,False,True,False,False
20964,55754594,2019-04-18 22:47:31,,"<p>I would like to use Gensim and Scikit in the same pipeline. </p>

<p><strong>[Update]</strong>
Corpus is created from a list of lemmatized tokens <code>doc.tokens</code></p>

<pre><code>bowlist = []
for doc in linked_doc_list:
    bowlist.append(doc.tokens)

dictionary = corpora.Dictionary(bowlist)
corpus = [dictionary.doc2bow(line) for line in bowlist]
</code></pre>

<p>This involves transforming the Gensim corpus to a numpy array like so:</p>

<pre><code> numpy_matrix = gensim.matutils.corpus2dense(package.corpus, num_terms=len(package.dict.token2id))
</code></pre>

<p>This seems to work. The sklearn lda runs:</p>

<pre><code>model = LatentDirichletAllocation(n_components=components,
                                          max_iter=maxiter,
                                          learning_method=learningmethod,
                                          learning_offset=learningoffset,
                                          random_state=randomstate,
                                          verbose=verbose).fit(numpy_matrix)
</code></pre>

<p>But now, to read the results, I need to read the actual terms from the gensim dict (otherwise I am stuck with meaningless feature numbers).</p>

<p>However, the results from the following code are clearly meaningless. </p>

<pre><code> def filterAndReportResultsLDA(self, model, gensimdict, n_top_words=10):
     for topic_idx, topic in enumerate(model.components_):
         print(""Topic %d:"" % (topic_idx))
         words = []
         for i in topic.argsort()[:-n_top_words - 1:-1]:
            words.append(gensimdict[i])
         print(words)
</code></pre>

<p>Example result is:</p>

<pre><code>['reporting.', '7:23', 'users?', 'breaking', '5am', 'bell', 'c7n', 'content?', 'functions', 'vi']
</code></pre>

<p>Can anyone tell me what I am doing wrong?</p>
",2019-04-19 05:29:32,2019-04-19 05:29:32,Python: using Gensim and Scikit in the same pipeline,<python><scikit-learn><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
20966,55755315,2019-04-19 00:52:55,,"<p>There was a ""cannot index a corpus with zero features (you must specify either <code>num_features</code> or a non-empty corpus in the constructor)"" error</p>

<p>I think it should have gone wrong here, but I don't understand why it went wrong.</p>

<pre><code>dictionary = corpora.Dictionary(corpus)
doc_vectors = [dictionary.doc2bow(text) for text in corpus]
tfidf = models.TfidfModel(doc_vectors)
tfidf_vectors = tfidf[doc_vectors]
</code></pre>

<hr>

<pre><code>df = fetch_news()
stop_words = set(line.strip() for line inn open('desktop/pythontest/stopwords.txt', encoding = 'utf-8'))
for i in range(len(df)):
    newslist = []
    titlelist = []
    corpus = []
    for subject in df.article[i]:
      if subject.isspace():
        continue
      word_list = pseg.cut(subject)
      for word, flag in word_list:
        if not word in stop_words and flag == 'n':
            newslist.append(word)
    for subject in df.title[i]:
      if subject.isspace():
        continue
      word_list = pseg.cut(subject)
      for word, flag in word_list:
        if not word in stop_words and flag == 'n':
            titlelist.append(word)
corpus.append(newslist)
print (len(corpus))
print(""-----"")
dictionary = corpora.Dictionary(corpus)
doc_vectors = [dictionary.doc2bow(text) for text in corpus]
print (doc_vectors)     

tfidf = models.TfidfModel(doc_vectors)
tfidf_vectors = tfidf[doc_vectors]
for doc in tfidf_vectors:
    print (doc)
print (""++++++"")
print (len(tfidf_vectors))
print (tfidf_vectors[0])

query = dictionary.doc2bow(titlelist)

print (query)

index = similarities.MatrixSimilarity(tfidf_vectors)
sims = index[query]

sims = sorted(enmerate(sims), key = lambda item: -item[1])
print(sims)
print(""------------"")
</code></pre>

<p>The output is:</p>

<pre><code>&gt;1
&gt;-----
&gt;[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, &gt;1), (10, 1), (11, 2), (12, 1), (13, 3), (14, 1), (15, 1), (16, 3), (17, 1), (18, 1)]]
&gt;{0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1}
&gt;++++++
&gt;0
&gt;[]
&gt;[(4, 1), (7, 1), (13, 1), (16, 1)]
&gt;Traceback (most recent call last):
&gt;  File ""desktop/pythontest/GraduationDesignCopy1.py"", line 207, in &lt;module&gt;
&gt;    index = similarities.MatrixSimilarity(tfidf_vectors)
&gt;  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/similarities/docsim.py"", line 790, in __init__
&gt;    ""cannot index a corpus with zero features (you must specify either `num_features` ""

&gt;ValueError: cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)
</code></pre>
",2019-04-19 02:01:38,2019-04-19 02:01:38,How to solve cannot index a corpus with zero features error,<tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
20967,55755962,2019-04-19 02:46:54,,"<p>I am trying to build a Fake news classifier and I am quite new in this field. I have a column ""title_1_en"" which has the title for fake news and another column called ""title_2_en"". There are 3 target labels; ""agreed"", ""disagreed"", and ""unrelated"" if the title of the news in column ""title_2_en"" agrees, disagrees or is unrelated to that in the first column. </p>

<p>I have tried calculating basic cosine similarity between the two titles after converting the words of the sentences into vectors. This has resulted in the the cosine similarity score but this needs a lot of improvement as synonyms and semantic relationship has not been considered at all. </p>

<pre><code>def L2(vector):
    norm_value = np.linalg.norm(vector)
    return norm_value

def Cosine(fr1, fr2):
    cos = np.dot(fr1, fr2)/(L2(fr1)*L2(fr2))
    return cos
</code></pre>
",,2019-04-19 11:29:36,How to train a model that will result in the similarity score between two news titles?,<nlp><classification><gensim><cosine-similarity><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
20971,55756841,2019-04-19 05:02:37,,"<p>Am struggling with training wikipedia dump on doc2vec model, not experienced in setting up a server as a local machine is out of question due to the ram it requires to do the training. I couldnt find a pre trained model except outdated copies for python 2.</p>
",,2019-04-21 01:12:19,Where to find a pretrained doc2vec model on Wikipedia or large article dataset like Google news?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
20976,55738632,2019-04-18 03:14:30,,"<p>For example, I've already transformed all the words and numbers into one-hot coding. Then </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec
</code></pre>

<p>and I want to use </p>

<pre class=""lang-py prettyprint-override""><code>word2vec.Word2Vec(sentences=one_hot_vectors)
</code></pre>

<p>However, it popped up an error: </p>

<pre class=""lang-py prettyprint-override""><code>ufunc 'add' did not contain a loop with signature matching types dtype('U32')dtype('U32') dtype('U32')
</code></pre>

<p>I think it is because I cannot directly input one-hot coding to <code>word2vec.Word2Vec</code>. I wonder that does python have any other modules to satisfy my needs. I just want to input one-hot coding vectors not the raw sentences directly into word2vec model. Thank you so much. </p>
",2019-04-18 03:50:48,2019-04-18 15:30:06,Can I input one-hot coding vectors not the raw sentences directly into PYTHON module word2vec.Word2Vec?,<python><gensim><word2vec><one-hot-encoding>,,,CC BY-SA 4.0,False,False,True,False,False
20982,55789477,2019-04-22 05:19:09,,"<p>I have used Gensim LDAMallet for topic modelling but in what way we can predict sample paragraph and get their topic model using pretrained model.</p>

<pre><code># Build the bigram and trigram models
bigram = gensim.models.Phrases(t_preprocess(dataset.data), min_count=5, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram) 

def make_bigrams(texts):
   return [bigram_mod[doc] for doc in texts]

data_words_bigrams = make_bigrams(t_preprocess(dataset.data))

# Create Dictionary
id2word = corpora.Dictionary(data_words_bigrams)

# Create Corpus
texts = data_words_bigrams

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

mallet_path='/home/riteshjain/anaconda3/mallet/mallet2.0.8/bin/mallet' 
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=12, id2word=id2word, random_seed = 0)

coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')

a = ""When Honda builds a hybrid, you've got to be sure its a marvel. And an Accord Hybrid is when technology surpasses the known and takes a leap of faith into tomorrow. This is the next generation Accord, the ninth generation to be precise.""
</code></pre>

<p>How to use this text (a) to get its topic from the pretrained model. Please help.</p>
",,2019-04-23 23:48:08,How to predict test data on Gensim Topic modelling,<python><jupyter-notebook><gensim><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
20986,55812580,2019-04-23 13:38:57,,"<p>I am running Gensim with Python 3.6 on Windows 10. I have tried installing Visual Studio 2019 and MinGW (through TDM-GCC). I have uninstalled and reinstalled Gensim after both installations. I also did that after uninstalling and reinstalling Cython. </p>

<p>Regardless, it's not able to run the C extension, so I am stuck with the slower Numpy code.</p>

<p>I am not sure where the trouble lies, and I have run out of ideas on how to proceed. What can I do to make progress?</p>
",,2019-05-06 12:04:23,Making Gensim FAST_VERSION work on Windows 10 (Python 3.6),<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
20987,55813659,2019-04-23 14:37:45,,"<p>I hava a pandas dataframe that has one column with conversational data. I preprocessed it in the following way:</p>

<pre><code>def preprocessing(text):
     return [word for word in simple_preprocess(str(text), min_len = 2, deacc = True) if word not in stop_words]

dataset['preprocessed'] = dataset.apply(lambda row: preprocessing(row['msgText']), axis = 1)
</code></pre>

<p>To make it one-dimensional I used (both):</p>

<pre><code>processed_docs = data['preprocessed']
</code></pre>

<p>as well as:</p>

<pre><code>processed_docs = data['preprocessed'].tolist()
</code></pre>

<p>Which now looks as follows:</p>

<pre><code>&gt;&gt;&gt; processed_docs[:2]
0    ['klinkt', 'alsof', 'zwaar', 'dingen', 'spelen...
1    ['waar', 'liefst', 'meedenk', 'betekenen', 'pe...
</code></pre>

<p>For both cases, I used: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(processed_docs)     
</code></pre>

<p>However, in both cases I got the error:</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>How can I modify my data, so that I don't get this TypeError?</p>

<hr>

<hr>

<p>Given that similar questions have been asked before, I've considered:</p>

<p><a href=""https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si#33230682"">Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string</a></p>

<p>Based on the first answer, I tried the solution of:</p>

<pre><code>dictionary = gensim.corpora.Dictionary([processed_docs.split()])
</code></pre>

<p>And got the error(/s):</p>

<pre><code>AttributeError: 'Series'('List') object has no attribute 'split'
</code></pre>

<p>And in the second answer someone says that the input needs to be tokens, which already holds for me. </p>

<p>Furthermore, based on (<a href=""https://stackoverflow.com/questions/44352552/typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-single-str?noredirect=1&amp;lq=1"">TypeError: doc2bow expects an array of unicode tokens on input, not a single string when using gensim.corpora.Dictionary()</a>), I used the <code>.tolist()</code> approach as I described above, which does not work either.</p>
",,2019-04-24 15:42:21,How to input a series/list consisting of different tokens in a Gensim Dictionary?,<python><dictionary><nlp><typeerror><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21010,55815556,2019-04-23 16:23:02,,"<p>LDA shows 10 number of words in a topic by default. I want to increase these numbers by 15. I have tried ""topn"" and ""num_words"" keywords but both are giving me an error. how can I change this default behaviour?</p>

<pre><code>model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=8,topn=15,chunksize=10000, passes=30,iterations=300)
</code></pre>

<p>Error is </p>

<pre><code>    model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=8,topn=15,chunksize=10000, passes=30,iterations=300)
TypeError: __init__() got an unexpected keyword argument 'topn'
</code></pre>
",2019-04-25 18:33:33,2019-04-25 18:33:33,How to change the default number of words in LdaMulticore?,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
21015,55902017,2019-04-29 11:04:33,,"<p>I would like to replace a random word from a sentence by the most similar word from word2vec, for example a word from the sentence <code>question = 'Can I specify which GPU to use?'</code>. </p>

<p>I used this recursive method because with the split function, some words (like <code>to</code>) are not in word2vecmodel: </p>

<pre><code>import gensim.models.keyedvectors as word2vec
import random as rd

model = word2vec.KeyedVectors.load_word2vec_format('/Users/nbeau/Desktop/Word2vec/model/GoogleNews-vectors-negative300.bin', binary=True)

def similar_word(sentence, size):
    pos_to_replace = rd.randint(0, size-1)
    try:
        similarity = model.most_similar(positive = [sentence[pos_to_replace]])
        similarity = similarity[0][0]
    except KeyError:
        similarity, pos_to_replace = test(sentence, size)
        return similarity, pos_to_replace
    return similarity, pos_to_replace

question = question.split()
size = len(question)
similarity, pos_to_replace = similar_word(question, size)
sentence[pos_to_replace] = similarity
</code></pre>

<p>I would like to know if there is a better method to avoid the words which are not in the word2vec model.</p>
",2019-04-29 13:48:52,2019-04-30 18:17:41,Replace random word by similarity with word2vec,<python-3.x><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21019,55816686,2019-04-23 17:38:56,,"<p>Thanks for stopping by.  I was trying to get some help with this graph that is showing up blank.  I'm following this tutorial #17 <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a> to build a graph of coherence scores for different numbers of topics using LDAMallet.  Here is my code:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>os.environ['MALLET_HOME'] = 'C:\\mallet\\mallet-2.0.8'

mallet_path = 'C:\\mallet\\mallet-2.0.8\\bin\\mallet'
dictionary = gensim.corpora.Dictionary(processed_docs[:])
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]



def compute_coherence_values(dictionary, bow_corpus, documents, limit, start=2, step=3):
    """"""
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Premium Billing data 
    corpus : Gensim bow_corpus
    texts : document
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """"""
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=num_topics, id2word=dictionary)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=documents, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values
    
# Can take a long time to run.
model_list, coherence_values = compute_coherence_values(dictionary=dictionary, bow_corpus=bow_corpus,
                                                        documents=documents, start=2, limit=40, step=6)
                                                        
# Show graph
limit=40; start=2; step=6;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel(""Num Topics"")
plt.ylabel(""Coherence score"")
plt.legend((""coherence_values""), loc='best')
plt.show()</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.stack.imgur.com/qvdik.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qvdik.png"" alt=""it&#39;s drawing a blank and so am I""></a></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># Print the coherence scores
for m, cv in zip(x, coherence_values):
    print(""Num Topics ="", m, "" has Coherence Value of"", round(cv, 4))</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.stack.imgur.com/JftpN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JftpN.png"" alt=""nan like the bread""></a></p>

<p>The data:</p>

<p><a href=""https://i.stack.imgur.com/4HRy6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4HRy6.png"" alt=""dictionary""></a></p>

<p><a href=""https://i.stack.imgur.com/tOCVl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tOCVl.png"" alt=""bow_corpus""></a></p>

<p><a href=""https://i.stack.imgur.com/C7emw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C7emw.png"" alt=""print stuff""></a></p>

<p>What I wish it looked like:</p>

<p><a href=""https://i.stack.imgur.com/f7qCe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f7qCe.png"" alt=""the dream""></a></p>

<p>Pls help </p>
",,2019-08-09 23:05:16,Coherence graph blank - Coherence Value of nan,<python><graph><nan><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21023,55884548,2019-04-27 20:18:21,,"<p>I am training a <code>doc2vec gensim model</code> with txt file 'full_texts.txt' that contains ~1600 documents. Once I have trained the model, I wish to use similarity methods over words and sentences. </p>

<p>However, since this is my first time using gensim , I am unable to get a solution. If I want to look for similarity by words I try as mentioned below but I get an <strong>error</strong> that the <code>word doesnt exist in the vocabulary</code> and on the other question is how do I check similarity for entire documents? I have read a lot of questions around it, like this <a href=""https://stackoverflow.com/questions/45420466/gensim-keyerror-word-not-in-vocabulary"">one</a> and looked up <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">documentation</a> but still not sure what I am doing wrong.</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedLineDocument
from gensim.models.doc2vec import TaggedDocument

tagdocs = TaggedLineDocument('full_texts.txt')
d2v_mod = Doc2Vec(min_count=3,vector_size = 200, workers = 2, window = 5, epochs = 30,dm=0,dbow_words=1,seed=42)
d2v_mod.build_vocab(tagdocs)
d2v_mod.train(tagdocs,total_examples=d2v_mod.corpus_count,epochs=20)

d2v_mod.wv.similar_by_word('overdraft',topn=10)
KeyError: ""word 'overdraft' not in vocabulary""
</code></pre>
",,2020-08-15 12:37:22,gensim Doc2Vec word not in vocabulary,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21025,55923298,2019-04-30 14:37:51,,"<p>I'm a newbie to <code>gensim</code> and trying to understand the <code>Word2Vec</code> model it generates.</p>

<p>Here is a simple example:- </p>

<pre><code>sentences = [['first', 'sentence', 'for', 'word2vec']]
model = Word2Vec(sentences, min_count=1)
print(model)
print(model['first'])
</code></pre>

<p>Output:- </p>

<pre><code>Word2Vec(vocab=4, size=100, alpha=0.025)

[-3.2170122e-03 -2.9626938e-03 -4.0412871e-03 -5.9279817e-04
  2.5436375e-03  4.5433347e-03 -3.3862963e-03 -4.2654946e-03
  3.8285875e-03  4.3016393e-03  2.3948429e-03  8.1989179e-05
  3.6110645e-03  1.8498371e-03 -2.4455690e-04  4.1978257e-03
  2.9471173e-04  4.9666679e-03 -2.0676558e-03 -1.2046038e-03
 -4.3298928e-03  2.7839688e-03 -2.9434622e-03  4.0511941e-03
 -1.3770841e-03 -8.9504482e-04 -3.1494466e-03 -4.6084630e-03
 -3.3623597e-03  1.6870942e-04 -7.1172835e-04 -4.1482532e-03
  3.7355758e-03  2.3343530e-03 -6.3678029e-04 -1.9861995e-03
 -2.3025211e-03  1.5102652e-03 -2.8942723e-03 -3.0406206e-03
 -7.7123288e-04 -2.1534185e-03  4.0353332e-03 -2.0982060e-03
 -5.1215116e-04 -4.9524521e-03  3.9109741e-03  3.6507500e-03
  5.0717179e-04 -1.2909769e-03  1.7484331e-03  1.8906737e-03
 -2.5824555e-03 -3.3213641e-03  1.3024095e-03  4.8507750e-03
  3.5359471e-03  4.5252368e-03  2.1690773e-03  3.8934432e-03
  4.8941034e-03 -4.3265051e-03  1.2478753e-03  4.8012529e-03
  3.6689214e-04 -3.5324714e-03 -8.2519173e-04  4.6989080e-03
 -4.3403171e-03 -3.2295308e-03 -4.3292320e-03  1.4541810e-03
  2.6360361e-03  4.7351457e-03 -1.1666205e-03  4.0232311e-03
  2.3259546e-03 -4.5906431e-03 -2.3466926e-03 -1.4690498e-03
  4.9304329e-03  3.4869314e-04  1.7118681e-03 -3.9177295e-03
 -1.9519962e-03  4.0137409e-03  1.6459676e-03 -2.6613632e-03
 -3.4537977e-03  1.0973522e-03  1.9739978e-03  4.3450715e-03
  2.8814776e-03 -4.9455655e-03 -1.4207339e-03 -2.8513866e-03
 -3.7962969e-03 -2.7314643e-03 -6.0791872e-04 -5.9866998e-04]
</code></pre>

<p>The size of the model is defaulted to 100, what does each item in the size array represent?</p>

<p>For example:- first element is  <code>-3.2170122e-03</code></p>
",2019-05-01 19:36:36,2019-05-01 19:36:36,Understanding gensim model inference output,<nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21032,55938758,2019-05-01 15:33:27,,"<p>I try create a model which determine the most similar sentence for another sentence using word2vec.</p>

<p>The idea is to determine the most similar for a sentence, I created an average vector for the words composed this sentence.</p>

<p>Then, I should to predict the most similar sentence using embedding words.
My question is: How can I determine the best similar target sentence after created an average vector of source sentence?</p>

<p>Here the code : </p>

<pre><code>import gensim
from gensim import utils
import numpy as np
import sys
from sklearn.datasets import fetch_20newsgroups
from nltk import word_tokenize
from nltk import download
from nltk.corpus import stopwords
import matplotlib.pyplot as plt

model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)


download('punkt') #tokenizer, run once
download('stopwords') #stopwords dictionary, run once
stop_words = stopwords.words('english')

def preprocess(text):
    text = text.lower()
    doc = word_tokenize(text)
    doc = [word for word in doc if word not in stop_words]
    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only
    return doc
############  doc content  -&gt; num label     -&gt; string label
#note to self: texts[XXXX] -&gt; y[XXXX] = ZZZ -&gt; ng20.target_names[ZZZ]

# Fetch ng20 dataset
ng20 = fetch_20newsgroups(subset='all',
                          remove=('headers', 'footers', 'quotes'))
# text and ground truth labels
texts, y = ng20.data, ng20.target

corpus = [preprocess(text) for text in texts]

def filter_docs(corpus, texts, labels, condition_on_doc):
    """"""
    Filter corpus, texts and labels given the function condition_on_doc which takes
    a doc.
    The document doc is kept if condition_on_doc(doc) is true.
    """"""
    number_of_docs = len(corpus)
    print(number_of_docs)

    if texts is not None:
        texts = [text for (text, doc) in zip(texts, corpus)
                 if condition_on_doc(doc)]

    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]
    corpus = [doc for doc in corpus if condition_on_doc(doc)]

    print(""{} docs removed"".format(number_of_docs - len(corpus)))

    return (corpus, texts, labels)

corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: (len(doc) != 0))

def document_vector(word2vec_model, doc):
    # remove out-of-vocabulary words
    #print(""doc:"")
    #print(doc)
    doc = [word for word in doc if word in word2vec_model.vocab]
    return np.mean(word2vec_model[doc], axis=0)

def has_vector_representation(word2vec_model, doc):
    """"""check if at least one word of the document is in the
    word2vec dictionary""""""
    return not all(word not in word2vec_model.vocab for word in doc)

corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: has_vector_representation(model, doc))

x =[]
for doc in corpus: #look up each doc in model

    x.append(document_vector(model, doc))


X = np.array(x) #list to array

model.most_similar(positive=X, topn=1)
</code></pre>
",2019-05-01 15:42:08,2019-05-01 19:45:09,determine most similar phrase using word2vec,<python-3.x><nlp><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
21034,55886735,2019-04-28 03:19:12,,"<p>I am creating a seq2seq model with tensorflow(not Keras) and the input /output are sentences. Something like a chatbot or translator.</p>

<p>But when I run </p>

<pre class=""lang-py prettyprint-override""><code>for epoch in range(total_epoch):
    _, loss = sess.run([optimizer, cost],
                       feed_dict={enc_input: input_batch,
                                  dec_input: output_batch,
                                  targets: target_batch})
</code></pre>

<p>I get </p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p><code>input_batch</code>/<code>output_batch</code> are arrays of <code>gensim.word2vec.wv.vectors</code> of sentences.
I tried other things as input as well but I still get the same error.
For target it is an array of array (each internal array is a list of number mapped to the word for the sentence).</p>

<p>The target_batch that gets the error, looks like this:
<code>
[[297, 242, 430, 451, 507, 507, 505, 506, 506, 506, 506, 506], [297, 242, 430, 451, 507, 507, 505, 506, 506, 506, 506, 506], ...]</code></p>

<p>and for <code>input_batch</code>/<code>output_batch</code> I have tried everything.</p>

<p>I use <code>gensim</code> <code>word2vec</code> and for <code>inputbatch.append(input_data)</code> <code>input_data</code> is made for each sentence using <code>gensim</code> <code>word2vec</code> as following:</p>

<pre class=""lang-py prettyprint-override""><code>model=Word2Vec(input_sentence.split(), size=5, window=10, min_count=1, workers=4, sg=1)
</code></pre>

<p>I have done everything, from saving it to bin and retrieving it to using <code>model.wv.vectors</code>. I get that error for all 3 of <code>enc_input</code>, <code>dec_input</code> and  <code>targets</code></p>

<p><code>enc_input = tf.placeholder(tf.float32, [None, None, n_input])</code></p>

<p>Thanks.</p>
",2019-04-28 10:12:47,2019-04-28 10:12:47,Getting ValueError: setting an array element with a sequence when passing gensim.word2vec to feed_dict,<tensorflow><word2vec><seq2seq>,,,CC BY-SA 4.0,False,False,True,False,False
21040,55924378,2019-04-30 15:36:33,,"<p>I am trying to train a doc2vec model using training data, then finding the similarity of every document in the <strong>test data</strong> for a specific document in the <strong>test data</strong> using the trained doc2vec model. However, I am unable to determine how to do this.</p>

<p>I currently using <code>model.docvecs.most_similar(...)</code>. However, this function only finds the similarity of every document in the <strong>training data</strong> for a specific document in the <strong>test data</strong>. </p>

<p>I have tried manually comparing the inferred vector of a specific document in the test data with the inferred vectors of every other document in the test data using <code>model.docvecs.n_similarity(inferred_vector.tolist(), testvectors[i].tolist())</code> but this returns <code>KeyError: ""tag '-0.3502606451511383' not seen in training corpus/invalid""</code> as there are vectors not in the dictionary.</p>
",,2019-04-30 19:40:10,Doc2Vec - Finding document similarity in test data,<python><machine-learning><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21043,55872853,2019-04-26 18:02:10,,"<p>I'm using Gensim with <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">Fasttext Word vectors</a> for return similar words.</p>

<p>This is my code:</p>

<pre><code>import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('cc.it.300.vec')

words = model.most_similar(positive=['sole'],topn=10)

print(words)
</code></pre>

<p>This will return:</p>

<pre><code>[('sole.', 0.6860659122467041), ('sole.Ma', 0.6750558614730835), ('sole.Il', 0.6727924942970276), ('sole.E', 0.6680260896682739), ('sole.A', 0.6419174075126648), ('sole.', 0.6401025652885437), ('splende', 0.6336565613746643), ('sole.La', 0.6049465537071228), ('sole.I', 0.5922051668167114), ('sole.Un', 0.5904430150985718)]
</code></pre>

<p>The problem is that ""sole"" (""sun"", in english) return a series of words with a dot in it (like sole., sole.Ma, ecc...). Where is the problem? Why most_similar return this meaningless word?</p>

<p><strong>EDIT</strong></p>

<p>I tried with <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">english word vector</a> and the word ""sun"" return this:</p>

<pre><code>[('sunlight', 0.6970556974411011), ('sunshine', 0.6911839246749878), ('sun.', 0.6835992336273193), ('sun-', 0.6780728101730347), ('suns', 0.6730450391769409), ('moon', 0.6499731540679932), ('solar', 0.6437565088272095), ('rays', 0.6423950791358948), ('shade', 0.6366724371910095), ('sunrays', 0.6306195259094238)]
</code></pre>

<p>Is it impossible to reproduce results like relatedwords.org?</p>
",2019-04-26 18:26:57,2019-04-26 21:23:49,Gensim most_similar() with Fasttext word vectors return useless/meaningless words,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
21051,55874253,2019-04-26 19:59:51,,"<p>I am trying to get word2vec to work in python3, however as my dataset is too large to easily fit in memory I am loading it via an iterator (from zip files). However when I run it I get the error </p>

<pre><code>Traceback (most recent call last):
  File ""WordModel.py"", line 85, in &lt;module&gt;
    main()
  File ""WordModel.py"", line 15, in main
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 783, in __init__
    fast_version=FAST_VERSION)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 759, in __init__
    self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 936, in build_vocab
    sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1591, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1576, in _scan_vocab
    total_words += len(sentence)
TypeError: object of type 'generator' has no len()
</code></pre>

<p>Here is the code:</p>

<pre><code>import zipfile
import os
from ast import literal_eval

from lxml import etree
import io
import gensim

from multiprocessing import cpu_count


def main():
    data = TrainingData(""/media/thijser/Data/DataSets/uit2"")
    print(len(data))
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
    word2vec.save('word2vec.save')




class TrainingData:

    size=-1

    def __init__(self, dirname):
        self.data_location = dirname

    def __len__(self):
        if self.size&lt;0: 

            for zipfile in self.get_zips_in_folder(self.data_location): 
                for text_file in self.get_files_names_from_zip(zipfile):
                    self.size=self.size+1
        return self.size            

    def __iter__(self): #might not fit in memory otherwise
        yield self.get_data()

    def get_data(self):


        for zipfile in self.get_zips_in_folder(self.data_location): 
            for text_file in self.get_files_names_from_zip(zipfile):
                yield self.preproccess_text(text_file)


    def stripXMLtags(self,text):

        tree=etree.parse(text)
        notags=etree.tostring(tree, encoding='utf8', method='text')
        return notags.decode(""utf-8"") 

    def remove_newline(self,text):
        text.replace(""\\n"","" "")
        return text

    def preproccess_text(self,text):
        text=self.stripXMLtags(text)
        text=self.remove_newline(text)

        return text




    def get_files_names_from_zip(self,zip_location):
        files=[]
        archive = zipfile.ZipFile(zip_location, 'r')

        for info in archive.infolist():
            files.append(archive.open(info.filename))

        return files

    def get_zips_in_folder(self,location):
       zip_files = []
       for root, dirs, files in os.walk(location):
            for name in files:
                if name.endswith(("".zip"")): 
                    filepath=root+""/""+name
                    zip_files.append(filepath)

       return zip_files

main()


for d in data:
    for dd in d :
        print(type(dd))
</code></pre>

<p>Does show me that dd is of the type string and contains the correct preprocessed strings (with length somewhere between 50 and 5000 words each). </p>
",2019-04-27 02:04:23,2019-04-27 02:10:47,python gensim word2vec gives typeerror TypeError: object of type 'generator' has no len() on custom dataclass,<python><machine-learning><nlp><gensim><training-data>,,,CC BY-SA 4.0,False,False,True,False,False
21056,55892073,2019-04-28 16:04:49,,"<p>I want to get bigrams for symbols (letters of words). For example, for words ""done"" and ""dog"" I want to be able to find bigram ""do"". </p>

<p>I tried to do it using gensim.Phrases, but it doesn`t work for me. </p>

<p>Here is my code:</p>

<pre><code>from gensim.models import Phrases

documents = [""God"", ""Good"",""happy"",""hangry"",""pypi""]
documents_proc = [list(doc) for doc in documents]

bigram = Phrases(documents_proc, min_count=1)
trigram = Phrases(bigram[documents_proc], min_count=1)

for sent in documents_proc:
    print(sent, bigram[sent])
    bigrams_ = [b for b in bigram[sent] if b.count('_') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count('_') == 2]
    print(bigrams_)
    print(trigrams_)
    print()
</code></pre>

<p>I expected the output of
<code>['Go', 'od', 'ha', 'py']</code>, but there are nothing in the output. 
What am I doing wrong? </p>

<p>Thank you.</p>
",,2019-04-28 17:29:45,Gensim phrases don`t find some bigrams,<python><gensim><phrase>,,,CC BY-SA 4.0,False,False,True,False,False
21063,55962586,2019-05-03 02:32:37,,"<p>Hi I am running a NLP pipeline where I am trying to fit a model but I am getting an error. Please check these LoC and suggest what am I doing wrong.</p>

<p>I have tried resampling the scraping and run it with 255 characters and it seems to produce results for it. But when I am trying to run it with 15000 characters(actual length of scraping) I get this error : </p>

<blockquote>
  <p>The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>

<pre><code>
    from nltk.tokenize import word_tokenize
    alltext = pd.concat([training_set.scrape_text, test_set.scrape_text]) 
    alltext.shape 
    rows = [] 
    for row in alltext.values: 
        rows.append(word_tokenize(row)) 
        model = gensim.models.FastText(rows, size=100, window=4, min_count=2, iter=10)
    pipeline = Pipeline([
      ('feats', FeatureUnion([ (""word2vec vectorizer"",TfidfEmbeddingVectorizer(model)), 
      ('vec', TfidfVectorizer(preprocessor=preprocessor, tokenizer=word_tokenize,analyzer='word', lowercase=True, strip_accents='unicode', stop_words='english', ngram_range=(1,2)))])),
      ('voting', VotingClassifier(estimators=[(""xgb"",XGBClassifier(nthread= 4, learning_rate=0.08, n_estimators=1000, max_depth=10))], voting='soft' ))])


#Loggin time for model fitting
start_time = time.time()
pipeline.fit(training_set.scrape_text.values, y)

print(""Time taken for modelling with current pipeline --- %s seconds ---"" % (time.time() - start_time))
</code></pre>
",2019-05-06 00:12:35,2019-05-06 00:12:35,The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() NLP pipeline,<python><machine-learning><nlp><artificial-intelligence><pipeline>,,,CC BY-SA 4.0,True,False,True,False,False
21070,55939511,2019-05-01 16:37:26,,"<p>I trained a gensim's Doc2Vec model with default word2vec training (dm=1). I can get the word vectors from the global model in model.wv.vectors.
But the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">documentation</a> says that the same word (""leaves"" in the example) won't have the same vector depending of the document context where it appear.</p>

<p>So I'm a bit confused : in the model.wv.vectors, will the word ""leaves"" by example, have the same vector for all the documents used to train the model (that may be contradictory with what I understand from the documentation) ? If not, how to get the word vectors from a particular document ?</p>
",,2019-05-01 19:17:18,Word vectors from a whole doc2vec model vs. word vectors from a particular document,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21074,55978013,2019-05-03 23:01:20,,"<p>I'm using the library node2vec, which is based on gensim word2vec model to encode nodes in an embedding space, but when i want to fit the word2vec object I get this warning:</p>

<blockquote>
  <p>C:\Users\lenovo\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py:743:
  UserWarning: C extension not loaded, training will be slow. Install a
  C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>Can any one help me to fix this issue please ?</p>
",2019-05-04 04:08:30,2019-10-25 07:18:00,"How to fix 'C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.'",<python-3.x><jupyter-notebook><anaconda><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21089,55936182,2019-05-01 12:19:34,,"<p>I have installed the gensim module for MAC by passing the following command in my Terminal:</p>

<pre><code>pip3 install gensim
</code></pre>

<p>I already have many other modules such as pandas and numpy that have been installed but I am able to import the same to my Jupyter Notebook without any issues. </p>

<p>This is how I am importing gensim:</p>

<pre><code>from gensim.models import Word2Vec
from gensim.models import KeyedVectors
</code></pre>

<p>So I checked the path where the 2 modules have been installed through the terminal, these being the same for a module I am able to import such as pandas as well as for gensim. </p>

<pre><code>pip3 show pandas
pip3 show gensim
</code></pre>

<p>In both the cases I get the same output: </p>

<blockquote>
  <p>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages    </p>
</blockquote>

<p>Can anyone tell me what could be the issue in this case?</p>
",2019-05-01 12:28:34,2019-05-01 12:28:34,Why am I unable to import the gensim module that has definitely been installed?,<python><python-3.x><machine-learning><jupyter-notebook><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21115,56036435,2019-05-08 08:22:53,,"<p>I have used td-idf vectorisation to extract features in a text classification problem and now I want to search for specific extracted features. Is there a way to develop a search functionality using python or any other tool for those extracted features? I have used facebook fasttext for out of vocabulary support and td-idf for feature extraction. then feature union is used in the pipeline. Now I want to search for these features(specific to one vector or in whole vector. so is there an option to do so? Here is the code I have used for feature extraction. Thanks</p>

<pre><code>
from nltk.tokenize import word_tokenize

alltext = pd.concat([training_set.scrape_text, test_set.scrape_text])
alltext.shape

rows = []
for row in alltext.values:
    rows.append(word_tokenize(row))

model = gensim.models.FastText(rows, size=100, window=4, min_count=2, iter=10)
#NLP pipeline
import re

from nltk.stem import SnowballStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict


class TfidfEmbeddingVectorizer(object):

    def __preprocess(self, row):
        #print(""TfidfEmbeddingVectorizer  __preprocess begin"")
        cleanr = re.compile('&lt;.*?&gt;')
        cleantext = re.sub(cleanr, ' ', str(row))


        cleaned = cleantext.replace(""\n"","" "").strip()

        alpha_sent = """"

        for word in cleaned.split():
            alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
            alpha_sent += self.stemmer.stem(alpha_word)
            #alpha_sent += lemmatizer.lemmatize(alpha_word)
            alpha_sent += "" ""
        alpha_sent = alpha_sent.strip()
        return alpha_sent



    def __tokenize(self, row):
        return word_tokenize(row)

    def __init__(self, model):

        self.model = model
        self.model2weight = None
        self.dim = model.vector_size
        self.stemmer = SnowballStemmer(""english"")


    def fit(self, X, y):
        print(""TfidfEmbeddingVectorizer  fit begin"")
        tfidf = TfidfVectorizer(analyzer=lambda x: x)

        preprocessed_X = []
        for row in X:
            preprocessed_X.append(self.__tokenize(self.__preprocess(row)))

        preprocessed_X = np.array(preprocessed_X)


        tfidf.fit(preprocessed_X)

        max_idf = max(tfidf.idf_)
        self.model2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])
        print(""TfidfEmbeddingVectorizer  fit end"")

        return self

    def transform(self, X):


        preprocessed_X = []
        for row in X:
            preprocessed_X.append(self.__tokenize(self.__preprocess(row)))

        preprocessed_X = np.array(preprocessed_X)

        return np.array([
                np.mean([self.model[w] * self.model2weight[w]
                         for w in words if w in self.model], axis=0)
                for words in preprocessed_X
            ])


from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline, FeatureUnion

from xgboost import XGBClassifier

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier


pipeline = Pipeline([
    ('feats', FeatureUnion([

        ('vec', TfidfVectorizer(preprocessor=preprocessor, 
                                tokenizer=word_tokenize,
                                analyzer='word', 
                                lowercase=True,
                                strip_accents='unicode',
                                stop_words='english',
                                ngram_range=(1,2)
                               ))

    ])),

    ('voting', VotingClassifier(estimators=[(""rf1"", RandomForestClassifier(n_jobs=1)),
                                        (""gb"", GradientBoostingClassifier()),
                                         (""et"", ExtraTreesClassifier(n_jobs=1)),
                                         (""xgb"",XGBClassifier(nthread= 4, learning_rate=0.08, n_estimators=1000, max_depth=10))
                                       ], 
                                voting='soft',

])
</code></pre>
",2019-05-08 17:24:04,2019-05-09 15:41:27,Is there a way to search for extracted features using python?,<python><machine-learning><nlp>,,,CC BY-SA 4.0,True,False,True,False,True
21121,56021542,2019-05-07 11:24:55,,"<p>I am actually working with <code>doc2vec</code> from gensim library and I want to get all similarities with probabilites not only the top 10 similarities provided by <code>model.docvecs.most_similar()</code></p>

<p>Once my model is trained </p>

<pre><code>In [1]: print(model)
Out [1]: Doc2vec(...)
</code></pre>

<p>If I use <code>model.docvecs.most_similar()</code> I get only the Top 10 similar docs </p>

<pre><code>In [2]: model.docvecs.most_similar('1')
Out [2]: [('2007', 0.9171321988105774),
 ('606', 0.5638039708137512),
 ('2578', 0.530228853225708),
 ('4506', 0.5193327069282532),
 ('2550', 0.5178008675575256),
 ('4620', 0.5098666548728943),
 ('1296', 0.5071642994880676),
 ('3943', 0.5070815086364746),
 ('438', 0.5057751536369324),
 ('1922', 0.5048809051513672)]
</code></pre>

<p>And I am looking to get all probilities not only the top 10 for some analysis.</p>

<p>Thanks for your help :)</p>
",2019-05-07 11:48:01,2019-05-07 22:52:14,Get all similar documents with doc2vec,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21126,56059927,2019-05-09 12:54:42,,"<p>I'm trying to do topic modeling with Gensim and Mallet (<a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">link</a>). 
When I locate the mallet_path and then try to assign it to gensim, I get the error
    subprocess.CalledProcessError : returned non-zero exit status 1</p>

<p>And I get prompted to update Java (which I have done).</p>

<p>Any hints on how to solve it? </p>

<pre><code>mallet_path = '/Users/username/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
Traceback (most recent call last):
  File ""&lt;pyshell#85&gt;"", line 1, in &lt;module&gt;
    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py"", line 132, in __init__
    self.train(corpus)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py"", line 273, in train
    self.convert_input(corpus, infer=False)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py"", line 262, in convert_input
    check_output(args=cmd, shell=True)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/utils.py"", line 1918, in check_output
    raise error
subprocess.CalledProcessError: Command '/Users/username/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input /var/folders/76/hdlh6w8d3nbb4m424wx3010w0000gn/T/adc98e_corpus.txt --output /var/folders/76/hdlh6w8d3nbb4m424wx3010w0000gn/T/adc98e_corpus.mallet' returned non-zero exit status 1.

</code></pre>
",,2019-08-07 03:03:35,subprocess.CalledProcessError when trying to run Mallet with Gensim,<python-3.x><subprocess><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21128,55951158,2019-05-02 11:10:59,,"<p>To my understanding, batch (vanilla) gradient descent makes one parameter update for all training data. Stochastic gradient descent (SGD) allows you to update parameter for each training sample, helping the model to converge faster, at the cost of high fluctuation in function loss. </p>

<p><a href=""https://i.stack.imgur.com/0LGA5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0LGA5.png"" alt=""enter image description here""></a></p>

<p>Batch (vanilla) gradient descent sets <code>batch_size=corpus_size</code>.</p>

<p>SGD sets <code>batch_size=1</code>.</p>

<p>And mini-batch gradient descent sets <code>batch_size=k</code>, in which <code>k</code> is usually 32, 64, 128...</p>

<p>How does gensim apply SGD or mini-batch gradient descent? It seems that <code>batch_words</code> is the equivalent of <code>batch_size</code>, but I want to be sure. </p>

<p>Is setting <code>batch_words=1</code> in gensim model equivalent to applying SGD?</p>
",,2019-05-02 17:30:23,gensim Word2Vec - how to apply stochastic gradient descent?,<nlp><gensim><word2vec><gradient-descent><stochastic>,,,CC BY-SA 4.0,False,False,True,False,False
21143,56063312,2019-05-09 15:54:42,,"<p>I have trained an LDA model using the <code>gensim</code> framework and saved it in a file, however, the model was saved with extra (separate) files, having the same prefix (the name of the original file for the model):</p>

<pre><code>lda.save('lda_model.gensim')
</code></pre>

<p>Result (four files):</p>

<pre><code>lda_model.gensim
lda_model.gensim.expElogbeta.npy
lda_model.gensim.id2word
lda_model.gensim.state
</code></pre>

<p>My question is whether loading the model file automatically loads all the extra (separate) files too, or whether I have to load each file separately, i.e.: will this line of code load everything:</p>

<pre><code>lda = LdaModel.load('lda_model.gensim')
</code></pre>

<p>And if it doesn't, how do I load the model along with all of its extras?</p>
",2019-05-10 09:01:02,2019-05-10 09:01:02,Load gensim LdaModel that has seperate files,<python><machine-learning><gensim><topic-modeling>,2019-05-10 09:15:22,,CC BY-SA 4.0,False,False,True,False,False
21147,56023167,2019-05-07 12:57:37,,"<p>Here is word2vec  model=gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True) and it contains words in uppercase. How I can produce new model from this one with all words from it and these words are lowercased? All words would have the same vectors as in source model.</p>
",,2019-05-08 00:48:03,Produce new word2vec model from existing one,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21160,56076298,2019-05-10 10:59:45,,"<p>I have trained Doc2Vec paragraph embeddings on text documents using the <code>Doc2Vec</code> module in Python's <code>gensim</code> package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see <a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">this link</a> for details):</p>

<pre><code>def tag_docs(docs, col):
    tagged = docs.apply(lambda r: TaggedDocument(words=simple_preprocess(r[col]), tags=[r.label]), axis=1)
    return tagged
</code></pre>

<p>However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:</p>

<pre><code>print(model.docvecs.count)
</code></pre>

<p>My question is as follows: I trained the model of <code>n</code> classes of documents, yielding <code>n</code> document vectors in <code>model.docvecs</code>. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?</p>
",2019-05-13 09:16:55,2019-05-13 09:16:55,Mapping doc2vec paragraph representation to its class tag post-training,<python><gensim><word2vec><text-classification><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21161,56076714,2019-05-10 11:24:57,,"<p>I have a model trained with Word2Vec. It works well.
I would like to plot <strong>only a list of words</strong> which I have entered in a list.
I have written the function below (and reused some code found) and get the following error message when a vector is added to <strong>arr</strong>: 
<em>'ValueError: all the input arrays must have same number of dimensions'</em></p>

<pre><code>def display_wordlist(model, wordlist):
    vector_dim = model.vector_size
    arr = np.empty((0,vector_dim), dtype='f') #dimension trained by the model
    word_labels = [word]

    # get words from word list and append vector to 'arr'
    for wrd in wordlist:
        word_array = model[wrd]
        arr = np.append(arr,np.array(word_array), axis=0) #This goes wrong

    # Use tsne to reduce to 2 dimensions
    tsne = TSNE(perplexity=65,n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)

    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    # display plot
    plt.figure(figsize=(16, 8)) 
    plt.plot(x_coords, y_coords, 'ro')

    for label, x, y in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)
    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)
    plt.show()

</code></pre>
",,2019-05-13 15:39:16,Gensim: Plot list of words from a Word2Vec model,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21168,56027806,2019-05-07 17:34:08,,"<p>Thanks for stopping by.  I have a directional question - I've built a Latent Dirichlet Allocation using Gensims Mallet wrapper.  I trained the model once on OldDataSet.csv and measured coherence.  I have been using it to pass NewDataSet.csv through for topic allocation.  I need some guidance on how I might be able to predict how accurately my pre-trained model is allocating NewDataSet.csv.  That coherence score only checks the accuracy of the pre-trained model not the allocated data set.  I'd like a way to  track the occurrence of historical topics and detect the emergence of new topics without re-training the model.  Like say these are the topics in OldDataSet.csv:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Foxtrot</li>
</ol>

<p>It will assign NewDataSet.csv 1. whiskey 2. Tango or 3. Foxtrot but a more accurate allocation might be:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Alpha</li>
</ol>

<p>If I keep running the same model I might miss this new topic.  If there exists a numeric score that would measure how closely the topics adhere to NewDataSet.csv that would be a huge time saver.  Thanks Stack you always save me :)</p>
",,2019-05-17 19:45:20,LDA detect new emerging topics,<python><windows><machine-learning><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
21171,56045317,2019-05-08 16:36:29,,"<p>I have a LSI model stored and the model is getting stored as model.pkl and model.pkl.projection.</p>

<p>However, when I try to load the model the loading is failing because its trying to look for projection file with .npy</p>

<pre><code>loading LsiModel object from /var/app/data/lsi_model.pkl.projection
loading u from /var/app/data/lsi_model.pkl.projection.u.npy with mmap=r
failed to load projection from /var/app/data/lsi_model.pkl.projection: 
[Errno 2] No such file or directory: 
'/var/app/data/lsi_model.pkl.projection.u.npy'
</code></pre>

<p>Any idea why this would be happening?</p>
",,2020-04-03 11:18:36,LSI Model fails to load the model,<scikit-learn><gensim><latent-semantic-indexing>,,,CC BY-SA 4.0,False,False,True,False,True
21178,56097390,2019-05-12 08:22:33,,"<p>I am trying to implement word to vector with Supervised learning to predict sentiment of any document or News article. word2vec represents each words corresponding to real numbers and create vector based on occurance of that word in the document.</p>

<p>My data Preprocessing is as follows: </p>

<pre><code>text_clf = Pipeline([('vect', CountVectorizer(analyzer=to_lemmas)),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])
tuned_parameters = {
    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],
    'tfidf__use_idf': (True, False),
    'tfidf__norm': ('l1', 'l2'),
    'clf__alpha': [1, 1e-1, 1e-2]
}

score = 'f1_macro'

clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring=score)
clf.fit(x_train, y_train)
</code></pre>

<p>Here I'm applying countvectorizor which gives word count, it is passed to tf-idf which gives word frequency and then as classifier Multinomial naive Bays is Applied. </p>

<p>Grid searchCV is used to tune best out model for classification </p>

<p>then clf.fit is used to train the model and clf.predict is used to predict sentiment of sentence passed. </p>

<p>My this model gives me classification report like this:</p>

<pre><code> irrelevant     0.9739    0.7517    0.8485       149
    negative     0.6308    0.5430    0.5836       151
     neutral     0.7082    0.8687    0.7803       419
    positive     0.5797    0.3670    0.4494       109
</code></pre>

<p>I want to apply Word to vector embedding here, Please suggest how i can implement word to vector embedding with Naive and SVM.</p>

<p><a href=""https://github.com/akanshajainn/Sentiment-Analysis-Twitter-word2vec-keras"" rel=""nofollow noreferrer"">https://github.com/akanshajainn/Sentiment-Analysis-Twitter-word2vec-keras</a></p>

<p><a href=""https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-word2vec-and-keras-convolutional-networks/"" rel=""nofollow noreferrer"">https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-word2vec-and-keras-convolutional-networks/</a></p>
",,2019-05-12 08:22:33,How to apply Word to vector algorithm with Naive or Svm intead of tf-idf and count vectorizor?,<keras><scikit-learn><neural-network><word2vec><supervised-learning>,,,CC BY-SA 4.0,False,False,True,False,True
21196,56033651,2019-05-08 04:40:58,,"<p>I am currently working with python where I train a Word2Vec model using sentences that I provide. Then, I save and load the model to get the word embedding of each and every word in the sentences that were used to train the model. However, I get the following error.</p>

<blockquote>
  <p>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""</p>
</blockquote>

<p>whereas, one of the sentences provided during training is as follows.</p>

<pre><code>sportsteam n1985_chicago_bears teamplaysincity city chicago
</code></pre>

<p>Hence I would like to know why some words are missing from the vocabulary, despite being trained on those words from that sentence corpus. </p>

<p><strong>Training the word2vec model on own corpus</strong></p>

<pre><code>import nltk
import numpy as np
from termcolor import colored
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.decomposition import PCA


#PREPARING DATA

fname = '../data/sentences.txt'

with open(fname) as f:
    content = f.readlines()

# remove whitespace characters like `\n` at the end of each line
content = [x.strip() for x in content]


#TOKENIZING SENTENCES

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

#TRAINING THE WORD2VEC MODEL

model = Word2Vec(sentences)

words = list(model.wv.vocab)
model.wv.save_word2vec_format('model.bin')
</code></pre>

<p><strong>Sample sentences from sentences.txt</strong></p>

<pre><code>sportsteam hawks teamplaysincity city atlanta
stadiumoreventvenue honda_center stadiumlocatedincity city anaheim
sportsteam ducks teamplaysincity city anaheim
sportsteam n1985_chicago_bears teamplaysincity city chicago
stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta
stadiumoreventvenue united_center stadiumlocatedincity city chicago
...
</code></pre>

<p>There are 1860 such lines in the <code>sentences.txt</code> file, each containing exactly 5 words and no stop words.</p>

<p>After saving the model, I tried to load it from a different python file within the same directory as the saved <code>model.bin</code> as shown below.</p>

<p><strong>Loading the saved model.bin</strong></p>

<pre><code>import nltk
import numpy as np
from gensim import models

w = models.KeyedVectors.load_word2vec_format('model.bin', binary=True)
print(w['n1985_chicago_bears'])
</code></pre>

<p>However, I end up with the following error</p>

<pre><code>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""
</code></pre>

<p>Is there a way to get the word embedding for each and every word in the trained sentence corpus using the same method?</p>

<p>Any suggestions in this regard will be much appreciated.</p>
",,2019-05-08 05:11:44,Words missing from trained word2vec model vocabulary,<python><tensorflow><nltk><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
21220,56105853,2019-05-13 04:44:50,,"<p>I have textual data that I want to discover topics it has, I used trained doc2vec on large corpus such as Wikipedia, but there is inconsistency in the results. Is there a better approach to discover topics.</p>
",,2019-05-15 06:22:20,How to detect topics in arbitrary text file or data? not knowing number of topics beforehand,<python><nltk><gensim><word2vec><lda>,,,CC BY-SA 4.0,True,False,True,False,False
21221,56106821,2019-05-13 06:29:31,,"<p>I'm currently learning gensim doc2model in Python3.6 to see similarity between sentences.
I created a model but it returns <code>KeyError: ""word 'WORD' not in vocabulary""</code> when I input a word which obviously exists in the training dataset, to find a similar word/sentence.
Does it automatically skip some words not very important to define sentences? or is that simply a bug or something?
Very appreciated if I could have any way out to cover all the appearing words in the dataset. thanks.</p>
",,2019-05-13 17:16:54,gensim doc2vec Model doesn't learn some words,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21237,56213690,2019-05-20 02:34:12,,"<p>I am new in natural language processing and I found <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">this</a> interesting tutorial which describes how to do the topic modeling.</p>

<p>Available <a href=""https://www.kaggle.com/therohk/million-headlines/data"" rel=""nofollow noreferrer"">data</a> for this tutorial </p>

<p>Source code: <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">here</a></p>

<p>The above code can provide topic modeling using LDA and generates the k number of topic. My question is how can I find which document belongs to which topic (cluster)? Like the example shown in figure <a href=""https://imgur.com/hA0dqUL"" rel=""nofollow noreferrer"">here</a>. I wondering something like:</p>

<blockquote>
  <p>publish_date:20030219 with text (aba ...) belongs to topic 1 cluster
  or ..</p>
</blockquote>

<p>I already read the post such as:
<a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi/20991190"">[1]</a> or <a href=""https://stackoverflow.com/questions/51448833/topicmodel-how-to-query-documents-by-topic-model-topic"">[2]</a> but still, I couldn't get my answer.</p>

<p>I also tried Matlab text analytic toolbox but I couldn't figure that out yet. </p>

<p>It would be great if you can provide me any help.</p>
",2019-05-22 18:27:53,2019-05-22 18:27:53,How to find which document is belong to which cluster?,<python><gensim><lda><topic-modeling><natural-language-processing>,,,CC BY-SA 4.0,False,False,True,False,False
21245,56128701,2019-05-14 10:47:33,,"<p>I have a ndarray with words and their corresponding vector (with the size of 100 per word).
For example:</p>

<pre><code>Computer 0.11 0.41 ... 0.56
Ball     0.31 0.87 ... 0.32
</code></pre>

<p>And so on.</p>

<p>I want to create a word2vec model from it:</p>

<pre><code>model = load_from_ndarray(arr)
</code></pre>

<p>How can it be done? I saw </p>

<blockquote>
  <p>KeyedVectors</p>
</blockquote>

<p>but it only takes file and not array</p>
",,2019-07-28 01:22:04,Python gensim create word2vec model from vectors (in ndarray),<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21248,56130065,2019-05-14 12:06:29,,"<p>Im working on a sentence similarity algorithm with the following use case: given a new sentence, I want to retrieve its n most similar sentences from a given set. I am using Gensim v.3.7.1, and I have trained both word2vec and doc2vec models. The results of the latter outperform word2vecs, but Im having trouble performing efficient queries with my Doc2Vec model. This model uses the distributed bag of words implementation (dm = 0).</p>

<p>I used to infer similarity using the built in method <code>model.most_similar()</code>, but this was not possible once I started training with more data that the one I want to query against. That's to say, <strong>I want to find the most similar sentence among a subset of my training dataset</strong>. My quick fix to this was comparing the vector of the new sentence with every vector on my set using cosine similarity, but obviously this does not scale as I have to compute loads of embeddings and make a lot of comparisons.</p>

<p>I successfully use <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.WmdSimilarity"" rel=""nofollow noreferrer"">word-mover distance</a> for both of word2vec and doc2vec, but I get better results for doc2vec when using cosine similarity. How can I efficiently query a new document against my set using the PV-DBOW Doc2Vec model and a method from <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#how-it-works"" rel=""nofollow noreferrer"">class Similarity</a>?</p>

<p>I'm looking for a similar approach to what I do with WMD, but for doc2vec cosine similarity:</p>

<pre class=""lang-py prettyprint-override""><code># set_to_query contains ~10% of the training data + some future updates
set_to_query_tokenized = [sentence.split() for sentence in set_to_query]
w2v_model = gensim.models.Word2Vec.load(""my_w2v_model"")
w2v_to_query = gensim.similarities.WmdSimilarity(
               corpus = set_to_query_tokenized,
               w2v_model = w2v_model,
               num_best=10
              )
new_query = ""I want to find the most similar sentence to this one"".split()
most_similar = w2v_to_query[new_query]
</code></pre>
",2019-05-15 10:33:15,2019-05-20 18:34:31,How to perform efficient queries with Gensim doc2vec?,<python><gensim><similarity><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
21249,56082233,2019-05-10 17:23:33,,"<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from random import shuffle
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

tagged_data = []
clas = ['type1', 'type2', 'type3']
for cla in clas:
  with open(f'../data/jieba/{cla}train.txt', 'r', encoding='UTF-8')as f:
    i = 0
    lines = f.readlines()
    for line in lines:
      tagged_data.append(TaggedDocument(words=line.split(' ')[:-1], tags=[cla + str(i)]))
      i += 1

num_doc = len(tagged_data)
shuffle(tagged_data)

model = Doc2Vec(dm=1, vector_size=128, window=5, alpha=0.01, min_alpha=0.0001, max_vocab_size=100000, sample=1e-5, workers=4, epochs=3, hs=1, dm_mean=1)
model.build_vocab(tagged_data)
model.train(documents=tagged_data, epochs=model.epochs, total_examples=num_doc)
model.save(""d2v.model"")
</code></pre>

<p>The above is my code and the output is like</p>

<pre><code>2019-05-11 01:11:48,177 : INFO : EPOCH 1 - PROGRESS: at 3.64% examples, 307751 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:49,195 : INFO : EPOCH 1 - PROGRESS: at 7.63% examples, 316010 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:50,196 : INFO : EPOCH 1 - PROGRESS: at 11.44% examples, 316465 words/s, in_qsize 8, out_qsize 0
</code></pre>

<p>How to get the value of loss function in each step so I can visualize it?</p>
",,2020-05-19 16:21:26,"Gensim doc2vec, how to get the value of loss function in each step",<nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21258,56177411,2019-05-16 22:35:37,,"<p>Assume you have a (wikipedia) pre-trained word2vec model, and train it on an additional corpus (very small, 1000 scentences).</p>

<p>Can you imagine a way to limit a vector-search to the ""re-trained"" corpus only?</p>

<p>For example</p>

<pre><code>model.wv.similar_by_vector() 
</code></pre>

<p>will simply find the closest word for a given vector, no matter if it is part of the Wikipedia corpus, or the re-trained vocabulary.</p>

<p>On the other hand, for 'word' search the concept exists:</p>

<pre><code>most_similar_to_given('house',['garden','boat'])
</code></pre>

<p>I have tried to train based on the small corpus from scratch, and it somewhat works as expected. But of course could be much more powerful if the assigned vectors come from a pre-trained set.</p>
",,2019-05-20 22:00:37,word2vec limit similar_by_vector() result to re-trained corpus,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21278,56148576,2019-05-15 11:44:23,,"<p>When I try to run:</p>

<pre><code>def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod1[doc] for doc in texts]

# Remove Stop Words
data_words_nostops1 = remove_stopwords(data_words1)

# Form Bigrams
data_words_bigrams1 = make_bigrams(data_words_nostops1)    
# Create Dictionary
    id2word1 = corpora.Dictionary(data_words_bigrams1)

# Create Corpus
texts1 = data_words_bigrams1

# Term Document Frequency
corpus1 = [id2word1.doc2bow(text) for text in texts1]

mallet_path = 'T:Python/Mallet/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus1, num_topics=15, id2word=id2word1)
</code></pre>

<p>I get the following error:</p>

<pre><code>CalledProcessError: Command 'T:/Python/Mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.txt --output C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>What can I do in my code specifically to make it work? </p>

<p>Furthermore, the question on this error has been asked a few times before. However, each answer seems so specific to a particular case, that I don't see what I can change on my code now so that it will work. Can someone elaborate on the meaning of this problem?</p>
",,2020-04-13 05:10:23,CalledProcessError: Returned non-zero exit status 1,<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21280,56166089,2019-05-16 10:10:08,,"<p>I need to fine-tune my word2vec model. I have two datasets, <code>data1</code> and <code>data2</code>.</p>
<p>What I did so far is:</p>
<pre><code>model = gensim.models.Word2Vec(
        data1,
        size=size_v,
        window=size_w,
        min_count=min_c,
        workers=work)
model.train(data1, total_examples=len(data1), epochs=epochs)

model.train(data2, total_examples=len(data2), epochs=epochs)
</code></pre>
<p>Is this correct? Do I need to store learned weights somewhere?</p>
<p>I checked <a href=""https://stackoverflow.com/questions/46244286/fine-tuning-pre-trained-word2vec-google-news/55751018#55751018"">this answer</a> and <a href=""https://www.kaggle.com/kfujikawa/word2vec-fine-tuning"" rel=""nofollow noreferrer"">this one</a> but I couldnt understand how its done.</p>
<p>Can someone explain to me the steps to follow?</p>
",2020-08-03 12:13:04,2020-08-03 12:13:04,Wor2vec fine-tuning,<python><machine-learning><data-science><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21282,56167224,2019-05-16 11:12:27,,"<p>I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.</p>
<p>I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:</p>
<h3>Dataset</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;sentence&gt;
  &lt;text&gt;word1 word2 word3&lt;/text&gt;
  &lt;annotations&gt;
    &lt;annotation anchor=word1 lemma=lemma1&gt;tag1&lt;/annotation&gt;
    &lt;annotation anchor=word2 lemma=lemma2&gt;tag2&lt;/annotation&gt;
    &lt;annotation anchor=word3 lemma=lemma3&gt;tag3&lt;/annotation&gt;
  &lt;annotations&gt;
&lt;/sentence&gt;
</code></pre>
<p>Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:</p>
<h3>Embedding</h3>
<pre><code>lemma1_tag1 dim 1 dim 2 dim 3 ... dim n
lemma2_tag2 dim 1 dim 2 dim 3 ... dim n
lemma3_tag3 dim 1 dim 2 dim 3 ... dim n
</code></pre>
<p>I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each <code>anchor</code> with the respective <code>lemma1_tag1</code> (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:</p>
<h3>Corpus Example</h3>
<pre><code>let just list most_recent_01730444a headline_06344461n
</code></pre>
<p>Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:</p>
<h3>Vocabulary Example</h3>
<pre><code>lemma1_tag1
lemma2_tag2
</code></pre>
<p>So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:</p>
<h3>Terminal emulation</h3>
<pre><code>./word2vec -train data/test.txt -output data/embeddings.vec -size 300 -window 7 -sample 1e-3 -hs 1 -negative 0 -iter 10 -min-count 1 -read-vocab data/dictionary.txt -cbow 1
</code></pre>
<h3>Output</h3>
<pre><code>Starting training using file data/test.txt
Vocab size: 80
Words in train file: 20811
</code></pre>
<p>The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format <code>lemma1_tag1</code> because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!</p>
",2020-06-20 09:12:55,2019-05-16 19:36:33,Use Word2Vec to build a sense embedding,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21287,56150678,2019-05-15 13:33:38,,"<p>I'm looking for a solution to use something like <code>most_similar()</code> from <code>Gensim</code> but using <code>Spacy</code>.
I want to find the most similar sentence in a list of sentences using NLP.</p>

<p>I tried to use <code>similarity()</code> from <code>Spacy</code> (e.g. <a href=""https://spacy.io/api/doc#similarity"" rel=""nofollow noreferrer"">https://spacy.io/api/doc#similarity</a>) one by one in loop, but it takes a very long time.</p>

<p>To go deeper :</p>

<p>I would like to put all these sentences in a graph (like <a href=""https://cdn-images-1.medium.com/max/1600/1*vvtIsW1AblmgLkq1peKfOg.png"" rel=""nofollow noreferrer"">this</a>) to find sentence clusters.</p>

<p>Any idea ?</p>
",,2019-05-15 13:33:38,Use Spacy to find most similar sentences in doc,<gensim><similarity><spacy><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,True,True,False,False
21295,56236404,2019-05-21 10:33:38,,"<p>According to the docs and wikipedia:</p>

<p>mmap allows processes to share same chunk of ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file))
</code></pre>

<p>This model loaded like this takes ~2.2 GB ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file), mmap='r')
</code></pre>

<p>This model loaded like this takes ~1.2 GB ram</p>

<p>Why am I observing such drastic decrease in ram consumption?</p>

<p>Loading multiple models simultaneously, works as expected and models share the ~1 GM memory.  </p>
",,2019-05-21 18:54:24,Why mmap flag reduces memory consumption for single Word2Vec instance,<ram><gensim><mmap><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21307,56170250,2019-05-16 13:55:25,,"<p>I'm on Windows OS10, using python 2.7.15 | Anaconda. Whenever I run </p>

<pre><code>mymodel=gensim.models.Word2Vec.load (pretrain)
mymodel.min_count = mincount
sentences =gensim.models.word2vec.LineSentence('ontology_corpus.lst')
mymodel.build_vocab(sentences, update=True) # ERROR HERE ****
</code></pre>

<p>I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""runWord2Vec.py"", line 23, in &lt;module&gt;
    mymodel.build_vocab(sentences, update=True)
  File ""C:xxxx\lib\site-packages\gensim\models\ba
se_any2vec.py"", line 936, in build_vocab
    sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, tri
m_rule=trim_rule)
  File ""C:xxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1591, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_r
ule)
  File ""C:xxxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1560, in _scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""C:xxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1442, in __iter__
    line = utils.to_unicode(line).split()
  File ""C:xxxx\lib\site-packages\gensim\utils.py""
, line 359, in any2unicode
    return unicode(text, encoding, errors=errors)
  File ""C:xxxxx\lib\encodings\utf_8.py"", line 16,
in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe6 in position 124: invalid
 continuation byte
</code></pre>

<p>Now this traces back to this LineSentence Class</p>

<pre><code>class LineSentence(object):

def __init__(self, source, max_sentence_length=MAX_WORDS_IN_BATCH, limit=None):

    self.source = source
    self.max_sentence_length = max_sentence_length
    self.limit = limit

def __iter__(self):
    """"""Iterate through the lines in the source.""""""
    try:
        # Assume it is a file-like object and try treating it as such
        # Things that don't have seek will trigger an exception
        self.source.seek(0)
        for line in itertools.islice(self.source, self.limit):
            line = utils.to_unicode(line).split()
            i = 0
            while i &lt; len(line):
                yield line[i: i + self.max_sentence_length]
                i += self.max_sentence_length
    except AttributeError:
        # If it didn't work like a file, use it as a string filename
        with utils.smart_open(self.source) as fin:
            for line in itertools.islice(fin, self.limit):
                line = utils.to_unicode(line).split() # ERROR HERE *************
                i = 0
                while i &lt; len(line):
                    yield line[i: i + self.max_sentence_length]
                    i += self.max_sentence_length
</code></pre>

<p>In the last return that can be seen from the error, I can just change the error parameter to be error='ignore' or change this line:</p>

<pre><code> utils.to_unicode(line).split()
</code></pre>

<p>to this:</p>

<pre><code> line.split()
</code></pre>

<p>ontology_corpus.lst file sample: </p>

<pre><code>&lt;http://purl.obolibrary.org/obo/GO_0090141&gt; EquivalentTo &lt;http://purl.obolibrary.org/obo/GO_0065007&gt; and  &lt;http://purl.obolibrary.org/obo/RO_0002213&gt; some &lt;http://purl.obolibrary.org/obo/GO_0000266&gt; 
&lt;http://purl.obolibrary.org/obo/GO_0090141&gt; SubClassOf &lt;http://purl.obolibrary.org/obo/GO_0065007&gt;
</code></pre>

<p>The problem is that it's working but I'm afraid that the results will be flawed due to the encoding error ignored! Is there a solution to this or would my approach will be just fine?</p>
",2019-05-16 15:28:48,2019-05-16 18:18:23,Gensim sentences from ontology corpus Unicode error,<python><unicode><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21356,56243568,2019-05-21 17:28:54,,"<p>My goal is to import gensim in Python 3 on Windows.</p>

<p>I am using Python 3.7.2 (checked by running <code>python -V</code> in Windows command prompt). I installed gensim by running <code>pip install gensim</code>. I checked the installation by running <code>pip freeze</code>, and saw the line <code>gensim==3.7.3</code>.</p>

<p>Then, I ran the command <code>py</code> to enter the interactive python mode (still in Windows command prompt). I ran the line <code>import gensim</code> and got the following output:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>I also tried <code>from gensim import test</code> and got the following output:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Any suggestions? How do I install gensim on Windows with Python 3? How do I test gensim?</p>
",,2020-09-21 16:47:50,ModuleNotFoundError: No module named 'gensim',<python><python-3.x><pip><python-import><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21367,56292749,2019-05-24 12:30:44,,"<p>I want to tokenize text with <code>gensim.utils.tokenize()</code>. And I want to add some phrases that would be recognized as single tokens, for example: <code>'New York', 'Long Island'</code>. </p>

<p>Is it possible with gensim? If not, what other libraries is it possible to use?</p>
",2019-05-27 08:42:23,2019-05-27 08:42:23,How to specify additional tokens for tokenizator?,<python><nlp><token><tokenize><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21369,56367510,2019-05-29 19:45:41,,"<p>I'm doing arabic dialect text classification and I've used Word2Vec to train the model, I got this so far:</p>

<pre><code> def read_input(input_file):

    with open (input_file, 'rb') as f:
        for i, line in enumerate (f): 
            yield gensim.utils.simple_preprocess (line)

documents = list (read_input (data_file))
logging.info (""Done reading data file"")

model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)
model.train(documents,total_examples=len(documents),epochs=10)
</code></pre>

<p>What do I do now to predict a new text if it's of any of the 5 dialects I have?<br>
Also, I looked around and found this code: </p>

<pre><code># load the pre-trained word-embedding vectors 
embeddings_index = {}
for i, line in enumerate(open('w2vmodel.vec',encoding='utf-8')):
    values = line.split()
    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')

# create a tokenizer 
token = text.Tokenizer()
token.fit_on_texts(trainDF['text'])
word_index = token.word_index

# convert text to sequence of tokens and pad them to ensure equal length vectors 
train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)
valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)

# create token-embedding mapping
embedding_matrix = numpy.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>But it gives me this error when I run it and load my trained word2vec model:</p>

<pre><code>ValueError: could not convert string to float: '\x00\x00\x00callbacksq\x04)X\x04\x00\x00\x00loadq\x05cgensim.utils'
</code></pre>

<h2>Note:</h2>

<p>Actually, there's another code that I didn't post here, I wanted to use word2vec with neural networks, I have the code for neural network, but I don't know how to make the features I got from word2vec to be as an input to the neural net and with labels as output. Is it possible to connect word2vec to a deep neural net and how?</p>
",2019-05-31 23:20:34,2019-05-31 23:20:34,How to predict with Word2Vec?,<python><gensim><word2vec><text-classification>,,,CC BY-SA 4.0,False,False,True,False,False
21370,56369258,2019-05-29 22:17:35,,"<p>Gensim Word2Vec that I've trained lacks vectors for some words. That is, although I have a word ""yuval"" as an input, the model lacks a vector for it. What is the cause?</p>
",,2019-05-30 07:03:25,Gensim Word2Vec lacks vectors for input words,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21402,56251839,2019-05-22 07:51:27,,"<blockquote>
  <p>edit</p>
</blockquote>

<p>The train corpus is a Spark dataframe I built before this step. I load it from parquet format and created a ""Feed"" class that give to Gensim lib the iterator on the train corpus :</p>

<pre><code>class Feed():
    def __init__(self, train_data):
        self.train_data = train_data

    def __iter__(self):
        for row in self.train_data.rdd.toLocalIterator():
            yield \
                gensim.models.doc2vec.TaggedDocument(\
                words=[kw.lower() for kw in row[""keywords""]] + list(row[""tokens_filtered""]),\
                tags=[row[""id""]])


sdf = spark.read.parquet(save_dirname)
train_corpus = Feed(sdf)
</code></pre>

<blockquote>
  <p>end edit</p>
</blockquote>

<p>I wish to train a Gensim Doc2Vec model on ~9 millions news text documents. Here is my model definition :</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(
        workers=8,
        vector_size=300,
        min_count=50,
        epochs=10)
</code></pre>

<p>The first step is getting the vocabulary :</p>

<pre><code>model.build_vocab(train_corpus)
</code></pre>

<p>It ends up in 90 minutes. Here is the logging info at the end of this process :</p>

<pre><code>INFO:gensim.models.doc2vec:collected 4202859 word types and 8950263 unique tags from a corpus of 8950339 examples and 1565845381 words
INFO:gensim.models.word2vec:Loading a fresh vocabulary
INFO:gensim.models.word2vec:min_count=50 retains 325027 unique words (7% of original 4202859, drops 3877832)
INFO:gensim.models.word2vec:min_count=50 leaves 1546772183 word corpus (98% of original 1565845381, drops 19073198)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 4202859 items
INFO:gensim.models.word2vec:sample=0.001 downsamples 9 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 1536820314 word corpus (99.4% of prior 1546772183)
INFO:gensim.models.base_any2vec:estimated required memory for 325027 words and 300 dimensions: 13472946500 bytes
</code></pre>

<p>Then I train the model with an iterator class on the train corpus :</p>

<pre><code>model.train(train_corpus, total_examples=nb_rows, epochs=model.epochs)
</code></pre>

<p>The last training logs are :</p>

<pre><code>INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 99.99% examples, 201921 words/s, in_qsize 16, out_qsize 0
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads
</code></pre>

<p>But it never finish the remaining threads.
It's not the first time I encounter this problem, even with much smaller train corpus. Usually, I relaunch the entire process (vocabulary setting and model training) and it goes on.</p>

<p>By now, to save time, I wish to NOT calculate again the vocabulary, getting in place the previously succesfully calculated one, and only try to train again the model. Is there a way to save the vocab part only of the model, then load it to train the model directly on train corpus ?</p>
",2019-05-23 07:14:11,2019-05-23 07:14:11,Is there a way to save and load the vocabulary of a Gensim Doc2Vec model,<python><pyspark><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21409,56339518,2019-05-28 09:52:26,,"<p>I'm using gensim to perform cosine similarity on a bunch of documents getting the Segmentation fault: 11. Could you please help me to resolve this issue?</p>

<p><strong>Error Trace:</strong></p>

<pre><code>2019-05-28 15:11:22,779 : INFO : creating sparse index
2019-05-28 15:11:22,779 : INFO : creating sparse matrix from corpus
2019-05-28 15:11:22,780 : INFO : PROGRESS: at document #0/546
2019-05-28 15:11:22,790 : INFO : created &lt;546x430 sparse matrix of type '&lt;class 'numpy.float32'&gt;'
        with 2191 stored elements in Compressed Sparse Row format&gt;
2019-05-28 15:11:22,791 : INFO : creating sparse shard #0
2019-05-28 15:11:22,791 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,791 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0, separately None
2019-05-28 15:11:22,794 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
Segmentation fault: 11
</code></pre>

<p><strong>Code</strong></p>

<pre><code>    def cosine_similarity(self,documents, query_docs=None, task='pairwise_similarity', metric_threshold=0.85, num_best=20):
        self.log('computing cosine similarity started')
        # Compute cosine similarity between the query_docs and the documents.
        dictionary = Dictionary(documents)
        corpus = [dictionary.doc2bow(doc) for doc in documents]
        # index_tmpfile = get_tmpfile(""index"")
        index = Similarity(output_prefix=None,corpus=corpus, num_best=num_best, num_features=len(dictionary))
        similarities = []
        if task == 'pairwise_similarity':
            self.log('computing pairwise_similarity')
            for sim in index:
                similarities.append(sim)
        elif task == 'batch_query':
            self.log('computing similarity using batch query')

            query_docs = [self.tfidf[self.dictionary.doc2bow(doc)] for doc in query_docs]
            for sim in index[query_docs]:
                similarities.append(sim)
        # filter results based on metric threshold
        filtered_results = []
        for ind_sim in similarities:
            filtered_results.append([item[0] for item in ind_sim if item[1] &gt;= metric_threshold])
        self.log('computing cosine similarity completed')
        return filtered_results
</code></pre>
",,2019-06-13 08:38:24,Getting Segmentation fault: 11 while running gensim's Cosine Similarity function on a bunch of documents,<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21411,56408849,2019-06-01 17:16:13,,"<p>I have used gensim LDA Topic Modeling to get associated topics from a corpus. Now I want to get the top 20 documents representing each topic: documents that have the highest probability in a topic. And I want to save them in a CSV file with this format: 4 columns for Topic ID, Topic words, probability of each word in the topic, top 20 documents for each topic.</p>

<p>I have tried get_document_topics which I think it is the best approach for this task:</p>

<p>all_topics = lda_model.get_document_topics(corpus, minimum_probability=0.0, per_word_topics=False)</p>

<p>But I am not sure how to get top 20 documents that best represent the topic and add them to the CSV file.</p>

<pre class=""lang-py prettyprint-override""><code>    data_words_nostops = remove_stopwords(processed_docs)
    # Create Dictionary
    id2word = corpora.Dictionary(data_words_nostops)
    # Create Corpus
    texts = data_words_nostops
    # Term Document Frequency
    corpus = [id2word.doc2bow(text) for text in texts]
    # Build LDA model
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                               id2word=id2word,
                                               num_topics=20,
                                               random_state=100,
                                               update_every=1,
                                               chunksize=100,
                                               passes=10,
                                               alpha='auto',
                                               per_word_topics=True)

    pprint(lda_model.print_topics())
    #save csv
    fn = ""topic_terms5.csv""
    if (os.path.isfile(fn)):
        m = ""a""
    else:
        m = ""w""

    num_topics=20
    # save topic, term, prob data in the file
    with open(fn, m, encoding=""utf8"", newline='') as csvfile:
        fieldnames = [""topic_id"", ""term"", ""prob""]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if (m == ""w""):
            writer.writeheader()

        for topic_id in range(num_topics):
            term_probs = lda_model.show_topic(topic_id, topn=6)
            for term, prob in term_probs:
                row = {}
                row['topic_id'] = topic_id
                row['prob'] = prob
                row['term'] = term
                writer.writerow(row)

</code></pre>

<p>Expected result: CSV file with this format: 4 columns for Topic ID, Topic words, probability of each word, top 20 documents for each topic.</p>
",,2020-03-26 14:18:28,"After applying gensim LDA topic modeling, how to get documents with highest probability for each topic and save them in a csv file?",<python><csv><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
21422,56361138,2019-05-29 12:51:44,,"<p>I am trying to create a corpus out of Wiki DumpFile.</p>

<p>I've downloaded the Wiki enwiki-latest-pages-articles.xml.bz2 file, but when I run the code(script) it gives me some errors. </p>

<p>I am relatively new to this, but I do not understand how the python code and the wiki file should be placed (same folders, which folder, etc.). </p>

<p>I've run this command: <code>make_wiki_corpus enwiki-latest-pages-articles.xml.bz2 wiki_en.txt</code></p>

<ul>
<li><code>make_wiki_corpus</code> - being my python script</li>
<li><code>enwiki-latest-pages-articles.xml.bz2</code> - is the wikipedia database</li>
<li><code>wiki_en.txt</code> - the textfile I want to write into.</li>
</ul>

<pre><code>import sys
from gensim.corpora import WikiCorpus

def make_corpus(in_f, out_f):

    """"""Convert Wikipedia xml dump file to text corpus""""""

    output = open(out_f, 'w')
    wiki = WikiCorpus(in_f)

    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('Processed ' + str(i) + ' articles')
    output.close()
    print('Processing complete!')


if __name__ == '__main__':

    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
</code></pre>

<p>I ran the command, containing this code, being in the same file with the enwiki-latest-pages-articles.xml.bz2  file, but at the command prompt I get some error messages like:</p>

<pre><code>line 636 in \__init__  
line 92 in __init__  
FileNotFound Eroor : [Errorno21] No such file or directory ""enwiki-latest-pages-articles.xml.bz2""
</code></pre>
",2019-05-29 15:11:23,2019-05-29 15:11:23,Making a Corpus out of Wiki DumpFile using Python in NLTK,<python><nltk><wikipedia><corpus>,,,CC BY-SA 4.0,True,False,True,False,False
21435,56444845,2019-06-04 13:23:49,,"<p>Lately I am doing a research with purpose of unsupervised clustering of a huge texts database. Firstly I tried bag-of-words and then several clustering algorithms which gave me a good result, but now I am trying to step into doc2vec representation and it seems to not be working for me, I cannot load prepared model and work with it, instead training my own doesnt prove any result.</p>

<p>I tried to train my model on 10k texts</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=2, epochs=100,workers=8)
</code></pre>

<p>(around 20-50 words each) but the similarity score which is proposed by gensim like</p>

<pre><code>sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>is working much worse than the same for Bag-of-words with my model.
By much worse i mean that identical or almost identical text have similarity score compatible to text which dont have any connection i can think about. So i decided to use model from <a href=""https://stackoverflow.com/questions/51132848/is-there-pre-trained-doc2vec-model"">Is there pre-trained doc2vec model?</a> to use some pretrained model which might have more connections between words. Sorry for somewhat long preambula but the question is how do i plug it in? Can someone provide some ideas how do i, using the loaded gensim model from <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> convert my own dataset of text into vectors of same length? My data is preprocesssed (stemmed, no punctuation, lowercase, no nlst.corpus stopwords)and i can deliver it from list or dataframe or file if needed, the code question is how to pass my own data to pretrained model? Any help would be appreciated.</p>

<p>UPD: outputs that make me feel bad</p>

<blockquote>
  <p>Train Document (6134): use medium paper examination medium habit one
  week must chart daily use medium radio television newspaper magazine
  film video etc wake radio alarm listen traffic report commuting get
  news watch sport soap opera watch tv use internet work home read book
  see movie use data collect journal basis analysis examining
  information using us gratification model discussed textbook us
  gratification article provided perhaps carrying small notebook day
  inputting material evening help stay organized smartphone use note app
  track medium need turn diary trust tell tell immediately paper whether
  actually kept one begin medium diary soon possible order give ample
  time complete journal write paper completed diary need write page
  paper use medium functional analysis theory say something best
  understood understanding used us gratification model provides
  framework individual use medium basis analysis especially category
  discussed posted dominick article apply concept medium usage expected
  le medium use cognitive social utility affiliation withdrawal must
  draw conclusion use analyzing habit within framework idea discussed
  text article concept must clearly included articulated paper common
  mistake student make assignment tell medium habit fail analyze habit
  within context us gratification model must include idea paper</p>
  
  <p>Similar Document (6130, 0.6926988363265991): use medium paper examination medium habit one week must chart daily use medium radio
  television newspaper magazine film video etc wake radio alarm listen
  traffic report commuting get news watch sport soap opera watch tv use
  internet work home read book see movie use data collect journal basis
  analysis examining information using us gratification model discussed
  textbook us gratification article provided perhaps carrying small
  notebook day inputting material evening help stay organized smartphone
  use note app track medium need turn diary trust tell tell immediately
  paper whether actually kept one begin medium diary soon possible order
  give ample time complete journal write paper completed diary need
  write page paper use medium functional analysis theory say something
  best understood understanding used us gratification model provides
  framework individual use medium basis analysis especially category
  discussed posted dominick article apply concept medium usage expected
  le medium use cognitive social utility affiliation withdrawal must
  draw conclusion use analyzing habit within framework idea discussed
  text article concept must clearly included articulated paper common
  mistake student make assignment tell medium habit fail analyze habit
  within context us gratification model must include idea paper</p>
</blockquote>

<p>This looks perfectly ok, but looking on other outputs</p>

<blockquote>
  <p>Train Document (1185): photography garry winogrand would like paper
  life work garry winogrand famous street photographer also influenced
  street photography aim towards thoughtful imaginative treatment detail
  referencescite research material academic essay university level</p>
  
  <p>Similar Document (3449, 0.6901006698608398): tang dynasty write page
  essay tang dynasty essay discus buddhism tang dynasty name artifact
  tang dynasty discus them history put heading paragraph information
  tang dynasty discussed essay</p>
</blockquote>

<p>Shows us that the score of similarity between two exactly same texts which are the most similar in the system and two like super distinct is almost the same, which makes it problematic to do anything with the data.
To get most similar documents i use</p>

<pre><code> sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>
",2019-06-05 08:44:56,2019-06-05 08:44:56,is there a way to use pretrained doc2vec model to evaluate some document dataset,<python><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21438,56446696,2019-06-04 15:12:08,,"<p>I have been trying to develop a code to read nucleotide in fasta format  as strings(each input as one word) and then use already known binding site sequences(11 bp long) to search amongst the nucleotide sequences through word2vec model</p>

<p>The fasta file looks like and all values are read in sequences as string</p>

<p>`sequences:</p>

<p>ATCGTGACGTGACGTGACGT</p>

<p>CGTAGCTAGAGCTAGCGGATCGA </p>

<p>and the binding sites are stored as a column in dataframe as df['binding']</p>

<p>ATGACTCAGCA</p>

<p>GTGACTAAGCA</p>

<p>ATGACTCAGCA</p>

<p>ATGACTCAGCA</p>

<p>...</p>

<p>Here is my code in python:</p>

<pre><code>import gensim 
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = gensim.models.Word2Vec(sequences, size=2, min_count=len(sequences), sg = 1)
model.train(sequences,total_examples=len(sequences),epochs=10)
w1 = df['binding']
model.wv.most_similar(positive=w1)
</code></pre>

<p>I was hoping to get a relation between each binding sites but it throws error as <code>KeyError: ""word 'ATGACTCAGCA' not in vocabulary""</code> here ATGACTCAGCA is the first value in <code>df['binding']</code></p>

<p>If I change the <code>w1 = df['binding'] to w1='A'</code>, I get the results as </p>

<pre><code>[('T', 0.9952122569084167),
 ('G', 0.9772425889968872),
 ('C', 0.9460670351982117)]
</code></pre>

<p>What should change to get relation between two binding sites and not two/more base pairs?</p>
",,2019-06-04 16:58:49,code error in word2vec program for DNA sequence,<python-3.x><list><word2vec><fasta>,,,CC BY-SA 4.0,False,False,True,False,False
21442,56363181,2019-05-29 14:41:44,,"<p>I am training a ldamallet model in python and saving it. I am also saving training dictionary that I can use to create corpus for unseen documents later. If I perform every action (i.e. train a model, save trained model, load saved model, infer unseen corpus) within same console, everything works fine. However, I want to use the trained model in different console / computer. </p>

<p>I passed prefix while training to look at the temp files created by the model. Following files are created when the model is trained:</p>

<blockquote>
  <p>'corpus.mallet'</p>
  
  <p>'corpus.txt'</p>
  
  <p>'doctopics'txt'</p>
  
  <p>inferencer.mallet'</p>
  
  <p>'state.mallet.gz'</p>
  
  <p>'topickeys.txt'</p>
</blockquote>

<p>Now when I load the saved model in a different console and infer unseen corpus created using the saved dictionary, I can see no other temp files being created and produces following error:</p>

<pre><code>FileNotFounderror: No such file or directory : 'my_directory\\doctopics.txt.infer'
</code></pre>

<p>For some odd reason, if I load the saved model in same console (console it was trained on) and infer unseen corpus like above, 'corpus.txt' is updated and two new temp files are created:</p>

<blockquote>
  <p>'corpus.mallet.infer'</p>
  
  <p>'doctopics.txt.infer'</p>
</blockquote>

<p>Any idea why I might be having this issue?</p>

<p>I have tried using LdaModel instead of LdaMallet and LdaModel works fine irrespective of whether I perform whole task in same console or different console.</p>

<p>Below is the snippet of the code I am using.</p>

<pre class=""lang-py prettyprint-override""><code>    def find_optimum_model(self):
        lemmatized_words = self.lemmatization()
        id2word = corpora.Dictionary(lemmatized_words)
        all_corpus = [id2word.doc2bow(text) for text in lemmatized_words]

        #For two lines below update with your path to new_mallet
        os.environ['MALLET_HOME'] = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet-2.0.8'
        mallet_path = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet-2.0.8\\bin\\mallet.bat'
        prefix_path = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet_temp\\'

    def compute_coherence_values(dictionary, all_corpus, texts, limit, start=2, step=4):
        coherence_values = []
        model_list = []
        num_topics_list = []


        for num_topics in range(start, limit, step):
            model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=all_corpus, num_topics=num_topics, id2word=dictionary,
                                                random_seed=42)
            #model = gensim.models.ldamodel.LdaModel(corpus=all_corpus,num_topics=num_topics,id2word=dictionary,eval_every=1,
            #                                        alpha='auto',random_state=42)
            model_list.append(model)
            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
            coherence_values.append(coherencemodel.get_coherence())
            num_topics_list.append(num_topics)

        return model_list, coherence_values, num_topics_list

    model_list, coherence_values, num_topics_list = compute_coherence_values(dictionary=id2word,all_corpus=all_corpus,
                                                                                texts=lemmatized_words,start=5,limit=40, step=6)
    model_values_df = pd.DataFrame({'model_list':model_list,'coherence_values':coherence_values,'num_topics':num_topics_list})

    optimal_num_topics = model_values_df.loc[model_values_df['coherence_values'].idxmax()]['num_topics']

    optimal_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=all_corpus, num_topics=optimal_num_topics, id2word=id2word,
                                                        prefix=prefix_path, random_seed=42)

    #joblib.dump(id2word,'id2word_dictionary_mallet.pkl')
    #joblib.dump(optimal_model,'optimal_ldamallet_model.pkl')
    id2word.save('id2word_dictionary.gensim')
    optimal_model.save('optimal_lda_model.gensim')

    def generate_dominant_topic(self):
        lemmatized_words = self.lemmatization()
        id2word = corpora.Dictionary.load('id2word_dictionary.gensim')
        #id2word = joblib.load('id2word_dictionary_mallet.pkl')
        new_corpus = [id2word.doc2bow(text) for text in lemmatized_words]
        optimal_model = gensim.models.wrappers.LdaMallet.load('optimal_lda_model.gensim')
        #optimal_model = joblib.load('optimal_ldamallet_model.pkl')


        def format_topics_sentences(ldamodel, new_corpus):
            sent_topics_df = pd.DataFrame()
            for i, row in enumerate(ldamodel[new_corpus]):
                row = sorted(row, key=lambda x: (x[1]), reverse=True)
                for j, (topic_num, prop_topic) in enumerate(row):
                    if j == 0:
                        wp = ldamodel.show_topic(topic_num)
                        topic_keywords = "", "".join([word for word, prop in wp])
                        sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]),
                                                               ignore_index=True)
                    else:
                        break
            sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
            return (sent_topics_df)
</code></pre>

<p>My expectation is use <code>find_optimum_model</code> function with the training data and save the optimum model and dictionary. Once saved, use <code>generate_dominant_topic</code> function to load saved model and dictionary, create corpus for unseen text and run the model to get desired topic modeling output.</p>
",2019-05-30 13:07:25,2020-08-26 06:51:50,Saved Gensim LdaMallet model not working in different console,<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21446,56309232,2019-05-25 22:30:18,,"<p>I am training a Phrases model to identify bigrams on large corpus using Gensim. When I reload the model I get the following error:</p>

<blockquote>
  <p>TypeError: 'float' object is not subscriptable</p>
</blockquote>

<p>My code to save the model (and also the training) is like this one:</p>

<pre><code>from gensim.models.phrases import Phrases, Phraser
def train_bigram(corpus):
    sentence_stream = [doc.split("" "") for doc in corpus]
    bigram_model = Phrases(sentence_stream, min_count=180, threshold=3.5)
    return bigram_model

print(""train bigram on cleaned text"")
phrases = train_bigram(corpus_cleaned)
print(""Build faster model for Gensim"")
bigram = Phraser(phrases)  # construct faster model (this is only an wrapper)
# Store the bigram model
bigram.save(path_to_save + ""bigram""
</code></pre>

<p>When I have to reload the model what cain I do? At the moment I am using the following:
<code>
bigram_reloaded = Phraser.load(path_to_save + 'bigram')
</code></p>

<p>but in this case I get the error shown before. Any idea or tips on how to solve?</p>
",,2019-05-25 22:30:18,TypeError: 'float' object is not subscriptable when reloading a Gensim model,<python><nlp><save><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21450,56341586,2019-05-28 11:48:09,,"<p>I am trying to perform Topic modelling on Databricks using the Gesim wrapper for Mallet. </p>

<p>I already have running code for the same on my Local system.</p>

<p><strong>Here is some sample code that already works on my local System:</strong></p>

<pre><code>import os

os.environ['MALLET_HOME'] = 'C:/Users/Soumadiptya.c/Desktop/mallet-2.0.8'

mallet_path = 'C:/Users/Soumadiptya.c/Desktop/mallet-2.0.8/bin/mallet'

ldamallet_model = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word, random_seed = 123)
</code></pre>

<p><strong>Here is what I am trying to execute on my Databricks instance:</strong></p>

<pre><code>os.environ['MALLET_HOME'] = '/dbfs/FileStore/tables/mallet-2.0.8'

mallet_path_new = '/dbfs/FileStore/tables/mallet-2.0.8/bin/mallet'

new_model = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path_new, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>

<p>But this exits with the following error:</p>

<pre><code>CalledProcessError: Command '/dbfs/FileStore/tables/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input /tmp/e091ce_corpus.txt --output /tmp/e091ce_corpus.mallet' returned non-zero exit status 126
</code></pre>

<p>Please note that I have already imported the necessary mallet files to the mentioned directories and the paths themselves exist. I'm assuming the problem is something with setting up Env variables inside databricks but unable to figure out. Any help would be much appreciated.</p>
",2019-05-28 11:51:38,2019-05-28 11:51:38,Unable to perform Topic Modelling in Databricks with gensim mallet,<python><gensim><databricks><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21453,56408959,2019-06-01 17:31:23,,"<p>I already trained a word2vec model with gensim library. For example, my model contains vectors for 2 words: ""new"" and ""york"". However, I also want to train a vector for the word ""new york"", so I transform ""new york"" into ""new_york"" and train a new vector model. Finally, I want to combine 3 vectors: vector of the word ""new"", ""york"" and ""new_york"" into one vector representation for the word ""new york"".</p>

<p>How can I save the new vector value to the model?</p>

<p>I try to assign the new vector to the model but gensim did not allow to assign the new value for vector model.</p>
",,2019-06-02 01:06:49,Create a new vector model in gensim,<python><vector><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21454,56412272,2019-06-02 04:51:47,,"<p>I use gensim Doc2Vec package to train doc2vec embeddings. I would expect that two models trained with the identical parameters and data would have very close values of the doc2vec vectors. However, in my experience it is only true with doc2vec trained in the PV-DBOW without training word embedding (dbow_words = 0).
For PV-DM and for PV-DBOW with dbow_words = 1, i.e. every case the word embedding are trained along with doc2vec, the doc2vec embedding vectors for identically trained models are fairly different. </p>

<p>Here is my code</p>

<pre class=""lang-py prettyprint-override""><code>    from sklearn.datasets import fetch_20newsgroups
    from gensim import models
    import scipy.spatial.distance as distance
    import numpy as np
    from nltk.corpus import stopwords
    from string import punctuation
    def clean_text(texts,  min_length = 2):
        clean = []
        #don't remove apostrophes
        translator = str.maketrans(punctuation.replace('\'',' '), ' '*len(punctuation))
        for text in texts:
            text = text.translate(translator)
            tokens = text.split()
            # remove not alphabetic tokens
            tokens = [word.lower() for word in tokens if word.isalpha()]
            # filter out stop words
            stop_words = stopwords.words('english')
            tokens = [w for w in tokens if not w in stop_words]
            # filter out short tokens
            tokens = [word for word in tokens if len(word) &gt;= min_length]
            tokens = ' '.join(tokens)
            clean.append(tokens)
        return clean
    def tag_text(all_text, tag_type =''):
        tagged_text = []
        for i, text in enumerate(all_text):
            tag = tag_type + '_' + str(i)
            tagged_text.append(models.doc2vec.TaggedDocument(text.split(), [tag]))
        return tagged_text

    def train_docvec(dm, dbow_words, min_count, epochs, training_data):
        model = models.Doc2Vec(dm=dm, dbow_words = dbow_words, min_count = min_count)
        model.build_vocab(tagged_data)
        model.train(training_data, total_examples=len(training_data), epochs=epochs)    
        return model

    def compare_vectors(vector1, vector2):
        cos_distances = []
        for i in range(len(vector1)):
            d = distance.cosine(vector1[i], vector2[i])
            cos_distances.append(d)
        print (np.median(cos_distances))
        print (np.std(cos_distances))    

    dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))
    n_samples = len(dataset.data)
    data = clean_text(dataset.data)
    tagged_data = tag_text(data)
    data_labels = dataset.target
    data_label_names = dataset.target_names

    model_dbow1 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow2 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow3 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dbow4 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dm1 = train_docvec(1, 0, 4, 30, tagged_data)
    model_dm2 = train_docvec(1, 0, 4, 30, tagged_data)

    compare_vectors(model_dbow1.docvecs, model_dbow2.docvecs)
    &gt; 0.07795828580856323
    &gt; 0.02610614028793008

    compare_vectors(model_dbow1.docvecs, model_dbow3.docvecs)
    &gt; 0.6476179957389832
    &gt; 0.14797587172616306

    compare_vectors(model_dbow3.docvecs, model_dbow4.docvecs)
    &gt; 0.19878000020980835
    &gt; 0.06362519480831186

    compare_vectors(model_dm1.docvecs, model_dm2.docvecs)
    &gt; 0.13536489009857178
    &gt; 0.045365127475424386

    compare_vectors(model_dbow1.docvecs, model_dm1.docvecs)
    &gt; 0.6358324736356735
    &gt; 0.15150255674571805
</code></pre>

<blockquote>
  <p>UPDATE</p>
</blockquote>

<p>I tried, as suggested by gojomo, to compare the differences between the vectors, and, unfortunately, those are even worse:</p>

<pre class=""lang-py prettyprint-override""><code>def compare_vector_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( vector1[i+1] - vector1[i])
    for i in range(len(vector2)-1):
        diff2[i].append(vector2[i+1] - vector2[i])
    cos_distances = []
    for i in range(len(diff1)):
        d = distance.cosine(diff1[i], diff2[i])
        cos_distances.append(d)
    print (np.median(cos_distances))
    print (np.std(cos_distances))    

compare_vector_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt; 0.1134452223777771
&gt; 0.02676398444178949

compare_vector_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt; 0.8464127033948898
&gt; 0.11423789350773429

compare_vector_differences(model_dbow4.docvecs, model_dbow3.docvecs)

&gt; 0.27400463819503784
&gt; 0.05984108730423529
</code></pre>

<blockquote>
  <p>SECOND UPDATE</p>
</blockquote>

<p>This time, after I finally understood gojomo, the things look fine.</p>

<pre class=""lang-py prettyprint-override""><code>def compare_distance_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( distance.cosine(vector1[i+1], vector1[i]))
    for i in range(len(vector2)-1):
        diff2.append( distance.cosine(vector2[i+1], vector2[i]))
    diff_distances = []
    for i in range(len(diff1)):
        diff_distances.append(abs(diff1[i] - diff2[i]))
    print (np.median(diff_distances))
    print (np.std(diff_distances))    

compare_distance_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt;0.017469733953475952
&gt;0.01659284710785352

compare_distance_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt;0.0786697268486023
&gt;0.06092163158218411

compare_distance_differences(model_dbow3.docvecs, model_dbow4.docvecs)
&gt;0.02321992814540863
&gt;0.023095123172320778
</code></pre>
",2019-06-03 17:20:17,2019-06-03 17:20:17,Discrepancies in gensim doc2vec embedding vectors,<gensim><word-embedding><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,True
21467,56431471,2019-06-03 16:44:56,,"<p>I'm training word2vec model where each word belongs to a specific class. </p>

<p>I want my embeddings to learn differences of words within each class, but don't want them to learn the differences between classes. </p>

<p>This can be achieved by negative sampling from only the words of same class as the target word. </p>

<p>In gensim <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">word2vec</a>, we can specify the number of words to negative sample using <code>negative</code> parameter, but it doesn't mention any options to modify/filter the sampling function. </p>

<p>Is there any method to achieve this?</p>

<p><strong>Update:</strong></p>

<p>Consider the classes to be like languages. So I have words from different languages. In training data, each sentence/document contains mostly words from same language, but sometimes from other languages. </p>

<p>Now I want embeddings where words with similar meanings are together irrespective of the language.</p>

<p>But because words from different languages do not occur together as frequently as words from same language, the embeddings basically groups words from same language together. </p>

<p>Because of this, I wanted to try negative sampling target words with words from same language so that it learns to distinguish the words within same language. </p>
",2019-06-04 07:44:59,2019-06-04 07:44:59,Specify condition for negative sampling in gensim word2vec,<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21473,56418817,2019-06-02 20:43:08,,"<p>I am following this blog trying to train doc2vec on wikipedia corpus using gensim. <a href=""https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html</a>.</p>

<p>I notice that the output is extremely case-sensitive. For example,</p>

<pre><code>string1=model.infer_vector(""machine Learning"".split())
string2=model.infer_vector(""computer Science"".split())
spatial.distance.cosine(string1, string2)
output is 0.25
</code></pre>

<p>If I change the case for the input,</p>

<pre><code>string1=model.infer_vector(""machine learning"".split())
string2=model.infer_vector(""computer science"".split())
spatial.distance.cosine(string1, string2)
output is 1.0535
</code></pre>

<p>I think that I should lower case everything before train the model. However, as shown in the link above, I first read the input directly:</p>

<pre><code>class TaggedWikiDocument(object):
    def __init__(self, wiki):
        self.wiki = wiki
        self.wiki.metadata = True
    def __iter__(self):
        for content, (page_id, title) in self.wiki.get_texts():
            yield TaggedDocument([c for c in content], [title])
wiki = WikiCorpus(""enwiki-latest-pages-articles-sample2.xml.bz2"")
documents = TaggedWikiDocument(wiki)
</code></pre>

<p>Any suggestion on how I can lower all the cases in the wiki document?</p>

<p>By the way, I used a very small file size for now just to test out whether there are errors in running the code, hence it can be the reason that infer_vector is very sensitive to case. Maybe this problem is minimum if I use the actual wiki dataset ?</p>
",2019-06-12 00:33:39,2019-06-12 00:33:39,How to train wiki with Doc2Vec Wiki with case insensitive features,<python-3.x><mediawiki><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21474,56418980,2019-06-02 21:11:50,,"<p>Suppose, I have a Seq2Seq model. I want to have the Embedding layer in this model.</p>

<p>Based on my research I can do it in three ways:</p>

<ol>
<li><p>train a word embedding separately on my data set or download a pre-trained word embedding, then use the weights of those embedding as the weight of the words in my data set. So here I do not need to have an embedding layer at all, I just load the weights of the already trained words into the words in my data set.</p></li>
<li><p>I create an embedding layer and set the trainable true, so not only I have an embedding, but also, that embedding will be trained based on my task</p></li>
<li><p>I create an Embedding layer, load already trained weights, and set trainable False. in this case, the weights will not get updated.</p></li>
</ol>

<p>(please correct me if Im wrong).</p>

<p>I have used the <code>first approach</code>. I want to know what will be the interpretation of the output of this code:</p>

<pre><code>model_wv = Word2Vec.load(""word2vec_50d_7w"")
embeddings = np.zeros((len(model_wv.wv.vocab), emb_dim))
for i in range(len(model_wv.wv.vocab)):
    # print(i)
    embedding_vector = model_wv.wv[model_wv.wv.index2word[i]]
    if embedding_vector is not None:
        embeddings[i] = embedding_vector

print(embeddings[[1,2,3],[3,4,1]])
</code></pre>

<p>this is the output:</p>

<pre><code>[-0.01566689 -1.36469996  0.59684211]
</code></pre>

<p>consider this <code>[1,2,3],[3,4,1]</code> as two sequence with <code>length=3</code>.</p>

<p>I was thinking we use word embedding in lstm to transform each word of the sequence into an embedding. I expected to see <code>two vectors</code> and <code>three items</code> in each vector.</p>

<p>The embedding is the word2vec in gensim,</p>

<p>Appreciate it if someone shed light on it where I am getting lost?</p>

<p>Thanks~</p>
",2019-06-02 21:29:54,2019-06-02 23:11:55,word embedding of a lstm sequence,<tensorflow><keras><lstm><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21484,56346717,2019-05-28 16:40:18,,"<p>I would like to see similarity between lists using <code>TFIDFVectorizer</code> and <code>CountVectorizer</code>.</p>

<p>I have lists like below:</p>

<pre class=""lang-py prettyprint-override""><code>list1 = [['i','love','machine','learning','its','awesome'],
         ['i', 'love', 'coding', 'in', 'python'],
         ['i', 'love', 'building', 'chatbots']]
list2 = ['i', 'love', 'chatbots']
</code></pre>

<p>I would like to see similarity between <code>list1[0]</code> and <code>list2</code>, <code>list1[1]</code> and <code>list2</code> , <code>list1[2]</code> and <code>list2</code>. </p>

<p>Expecting output should be like <code>[0.99 , 0.67, 0.54]</code></p>
",2020-06-19 14:16:10,2020-06-19 14:16:10,"Calculate text similarity between lists using CountVectorizer, TFIDFVectorizer",<python><scikit-learn><gensim><countvectorizer><tfidfvectorizer>,,,CC BY-SA 4.0,False,False,True,False,True
21487,56316146,2019-05-26 18:08:18,,"<p>I am trying to gauge the impact of part of speech information with Word2Vec embeddings but am not obtaining expected results.</p>

<p>I expected POS included word2vec embeddings to perform better in a machine translation task but it is actually performing worse.</p>

<p>I am creating two sets of embedding off of the same corpus using Gensim, one is normal Word2Vec, the other, I am changing tokens to ""[WORD]__[POS]"".</p>

<p>I am gauging differences in performance by using the embeddings in a Seq2Seq machine translation task. I am evaluating the two approaches with BLEU</p>

<p>This is how I am training the word2vec + POS embeddings with SpaCy:</p>

<pre><code>sentences = []
    for sent in doc.sents:
        tokens = []
        for t in sent:
            tokens += [""{}__{}"".format(t.text, t.pos_)]
        sentences += tokens
    pos_train += [sentences]
</code></pre>

<p>This is my benchmark machine translation model with Keras + Tensorflow:</p>

<pre><code>encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(LATENT_DIM, return_state=True)
_, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
</code></pre>

<p>With BLEU, the Word2Vec+POS approach consistently scores the same as Word2Vec or 0.01-0.02 points below the normal Word2Vec embeddings.</p>

<p>Does anyone know why this might be happening? Is there a gap in my reasoning or expectations?</p>
",,2019-05-29 02:03:26,Word2Vec with POS not producing expected results?,<keras><nlp><word2vec><word-embedding><seq2seq>,,,CC BY-SA 4.0,False,True,True,False,False
21489,56316903,2019-05-26 19:53:31,,"<p>I am interested in calculating similarity between vectors, however this similarity has to be a number between 0 and 1. There are many questions concerning tf-idf and cosine similarity, all indicating that the value lies between 0 and 1. From <a href=""https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>In the case of information retrieval, the cosine similarity of two
  documents will range from 0 to 1, since the term frequencies (using
  tfidf weights) cannot be negative. The angle between two term
  frequency vectors cannot be greater than 90.</p>
</blockquote>

<p>The peculiarity is that I wish to calculate the similarity between two vectors from two different word2vec models. These models have been aligned, though, so they should in fact represent their words in the same vector space. I can calculate the similarity between a word in <code>model_a</code> and a word in <code>model_b</code> like so</p>

<pre class=""lang-py prettyprint-override""><code>import gensim as gs
from sklearn.metrics.pairwise import cosine_similarity

model_a = gs.models.KeyedVectors.load_word2vec_format(model_a_path, binary=False)
model_b = gs.models.KeyedVectors.load_word2vec_format(model_b_path, binary=False)

vector_a = model_a[word_a].reshape(1, -1)
vector_b = model_b[word_b].reshape(1, -1)

sim = cosine_similarity(vector_a, vector_b).item(0)
</code></pre>

<p>But <code>sim</code> is then a similarity metric in the [-1,1] range. Is there a scientifically sound way to map this to the [0,1] range? Intuitively I would think that something like</p>

<pre><code>norm_sim = (sim + 1) / 2
</code></pre>

<p>is okay, but I'm not sure whether that is good practice with respect to the actual meaning of cosine similarity. If not, are other similarity metrics advised? </p>

<p>The reason why I am trying to get the values to be between 0 and 1 is because the data will be transferred to a colleague who will use it as a feature for her machine learning system, which expects all values to be between 0 and 1. Her intuition was to take the absolute value, but that seems to me to be a worse alternative because then you map opposites to be identical. Considering the actual meaning of cosine similarity, though, I might be wrong. So if taking the absolute value is the good approach, we can do that as well.</p>
",2019-05-28 09:32:48,2019-08-22 08:51:51,Cosine similarity between 0 and 1,<python><scikit-learn><gensim><similarity><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,True
21493,56508631,2019-06-08 17:25:40,,"<p>I am trying to create a W2V model and then generate train and test data to be used for my model.My question is how can I generate test data after I am done with creating a W2V model with my train data.</p>
",,2019-06-09 18:46:13,"Creating train,test data for Word2Vec model",<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21495,56421404,2019-06-03 05:05:14,,"<p>In understanding what <code>isolation forest</code> really does, I did a sample project as follows using 8 features as follows.</p>

<pre><code>from sklearn.ensemble import IsolationForest    
#features
df_selected = df[[""feature1"", ""feature2"", ""feature3"", ""feature4"", ""feature5"", ""feature6"", ""feature7"", ""feature8""]]
X = np.array(df_selected)

#isolation forest
clf = IsolationForest(max_samples='auto', random_state=42, behaviour=""new"", contamination=.01)
clf.fit(X)
y_pred_train = clf.predict(X)

print(np.where(y_pred_train == -1)[0])
</code></pre>

<p>Now, I want to identify what are the <em>outlier documents</em> using <code>isolation forest</code>. For that I trained a <code>doc2vec</code> model using <code>gensim</code>. Now for each of my document in the dataset I have a <code>300-dimensional vector</code>.</p>

<p>My question is can I straight away use the document vectors in <code>isolation forest</code> as <code>X</code> in the above code to detect outliers? Or do I need to reduce the dimensionality of the vectors before applying them to <code>isolation forest</code>?</p>

<p>I am happy to provide more details if needed.</p>
",2019-06-03 06:11:28,2019-06-03 06:11:28,How to use document vectors in isolationforest in sklearn,<python><scikit-learn><gensim><outliers><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
21501,56468865,2019-06-05 22:29:49,,"<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
",,2019-06-05 22:29:49,Sentence iterator to pass to Gensim language model,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21516,56438104,2019-06-04 05:59:47,,"<p>I've recently started experimenting with pre-trained word embeddings to enhance the performance of my LSTM model on a NLP task. In this case, I looked into Google's Word2Vec. Based on online tutorials, I first downloaded Word2Vec with <code>wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</code> and used python's <code>gensim</code> package to query the embeddings, using the following code.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors

if __name__ == ""__main__"":
    model = KeyedVectors.load_word2vec_format(""./data/word2vec/GoogleNews-vectors-negative300.bin"", binary=True)
    print(model[""bosnia""])
</code></pre>

<p>However, after noticing that many common words weren't found in the model, I started to wonder if something was awry. I tried searching for <code>bosnia</code> in the embedding repo, as shown above, but it wasn't found. So, I went on the <a href=""https://projector.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow embedding projector</a>, loaded the Word2Vec model, and searched for <code>bosnia</code> - it was there.</p>

<p>So, my question is: why is this happening? Was the version of Word2Vec I downloaded not complete? Or is gensim unable to load all words into memory and therefore omitting some?</p>
",,2019-06-04 17:01:09,Word2Vec word not found with Gensim but shows up on TensorFlow embedding projector?,<python-3.x><deep-learning><nlp><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21526,56323377,2019-05-27 09:34:45,,"<p>I'm trying to find out the <em>similarity between 2 documents</em>. I'm using <strong>Doc2vec Gensim</strong> to train around <strong>10k documents</strong>. There are around <strong>10 string type of tags</strong>. Each tag consists of a unique word and contains some sort of documents. Model is trained using <strong>distributed memory method</strong>.</p>

<pre><code>Doc2Vec(alpha=0.025, min_alpha=0.0001, min_count=2, window=10, dm=1, dm_mean=1, epochs=50, seed=25, vector_size=100, workers=1)
</code></pre>

<p>I've tried both <strong>dm</strong> and <strong>dbow</strong> as well. <strong>dm</strong> gives better <em>result(similarity score)</em> as compared to <strong>dbow</strong>. I understood the concepts of <strong>dm vs dbow</strong>. But don't know which method is good for similarity measures between two documents. </p>

<p>First question: <strong>Which method is the best to perform well on similarities?</strong></p>

<p><code>model.wv.n_similarity(&lt;words_1&gt;, &lt;words_2&gt;)</code> gives similarity score using <strong>word vectors</strong>.</p>

<p><code>model.docvecs.similarity_unseen_docs(model, doc1, doc2)</code> gives similarity score using <strong>doc vectors</strong> where doc1 and doc2 are not tags/ or indexes of doctags. <em>Each doc1 and doc2 contains 10-20 words kind of sentences.</em></p>

<p>Both <strong>wv.n_similarity</strong> and <strong>docvecs.similarity_unseen_docs</strong> provide different similarity scores on same types of documents. </p>

<p><strong>docvecs.similarity_unseen_docs</strong> gives little bit good results as compared to <strong>wv.n_similarity</strong> but <strong>wv.n_similarity</strong> sometimes also gives good results.</p>

<p>Question: <strong>What is the difference between docvecs.similarity_unseen_docs and wv.n_similarity? Can I use docvecs.similarity_unseen_docs to find the similarity score between unseen data (It might be a silly question)?</strong> </p>

<p>Why I asked because <strong>docvecs.similarity_unseen_docs</strong> provides similarity score on tags, not on actual words belonging to their tags. I'm not sure, please correct me here, if I'm wrong.</p>

<p><strong>How can I convert cosine similarity score to probability?</strong></p>

<p>Thanks.</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(alpha=0.025, min_alpha=0.0001, min_count=2, window=10, dm=1, dm_mean=1, epochs=50, seed=25, vector_size=100, workers=4)
# Training of the model
tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(&lt;list_of_list_of_tokens&gt;)]
model.build_vocab(tagged_data)
model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)

# Finding similarity score
model.wv.n_similarity(&lt;doc_words1&gt;, &lt;doc_words2&gt;)
model.random.seed(25)
model.docvecs.similarity_unseen_docs(model, &lt;doc_words1&gt;, &lt;doc_words2&gt;)
</code></pre>
",2019-05-29 08:04:10,2019-05-29 08:04:10,Which method dm or dbow works well for document similarity using Doc2Vec?,<python-3.x><gensim><similarity><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21532,56524948,2019-06-10 10:48:52,,"<p>First time using <code>word2vec</code> and the file I am working with is in <code>XML</code> format. I want to iterate through the patents to find each Title then apply <code>word2vec</code> to see if there are similar words(to indicate similar titles). </p>

<p>So far I have parsed the XML file using Element tree to retrieve each title, then I have applied <code>sent_tokenizer</code> followed by tweet <code>tokenizer</code> to return a list of sentences where each word has been tokenized (not sure if this was the best method). I then put the tokenized sentenses into my <code>word2vec</code> model and tested with one word to see if it returned a vector. This seems to only work for a word in the first sentence. I'm not sure it is recognising all the sentences? </p>

<pre class=""lang-py prettyprint-override""><code>    import numpy as np
    import pandas as pd
    import gensim
    import nltk
    import xml.etree.ElementTree as ET
    from gensim.models.word2vec import Word2Vec
    from nltk.tokenize import word_tokenize
    from nltk.tokenize import sent_tokenize
    from nltk.corpus import stopwords
    from nltk.tokenize import TweetTokenizer, sent_tokenize

    tree = ET.parse('6785.xml')
    root = tree.getroot()

    for child in root.iter(""Title""):
        Patent_Title = child.text
        sentence = Patent_Title
        stopWords = set(stopwords.words('english'))
        tokens = nltk.sent_tokenize(sentence)
        print(tokens)

        tokenizer_words = TweetTokenizer()
        tokens_sentences = [tokenizer_words.tokenize(t) for t in tokens]
        #print(tokens_sentences)

        model = gensim.models.Word2Vec(tokens_sentences, min_count=1,size=32)
        words = list(model.wv.vocab)
        print(words)
        print(model['Solar'])
</code></pre>

<p>I would expect it to identify the word 'solar' in a sentence and print out the vector then I could look for similar words. I am receiving the <code>error</code>:</p>

<p>word 'Solar' not in vocabulary""</p>
",2019-06-10 11:49:29,2019-06-11 19:23:38,word not in vocabulary,<python-3.x><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
21541,56443667,2019-06-04 12:13:17,,"<p>ImportError: cannot import name 'mean_absolute_difference'</p>

<p>Tried uninstalling and installing again.</p>

<pre><code>import gensim
</code></pre>

<hr>

<pre><code>ImportError                          Traceback (most recent call last)
&lt;ipython-input-28-e70e92d32c6e&gt; in &lt;module&gt;()
----&gt; 1 import gensim
2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/hdpmodel.py in 
&lt;module&gt;()
 61 
 62 from gensim import interfaces, utils, matutils
 ---&gt; 63 from gensim.matutils import dirichlet_expectation, 
 mean_absolute_difference
 64 from gensim.models import basemodel, ldamodel
 65 

 ImportError: cannot import name 'mean_absolute_difference'
</code></pre>
",,2020-05-04 15:40:01,Error while importing gensim package in colab,<python><importerror><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21542,56326458,2019-05-27 12:49:24,,"<p>I am trying to implement a piece of code using GuidedLDA but I do not understand format my text data needs to be in for it to work. The tutorial code uploads a dataset from the module itself whereas I have extracted my text data from my pandas dataframe, created the corpus and dictionary using gensim and am still unable to make it functional.</p>

<p>Could you help explain how I can structure my text data into a sparse array dataset used in LDA models (document term matrix I think?) so I can utilised the GuidedLDA module?</p>

<p>I originally followed this tutorial to run LDA analysis on text data:
<a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></p>

<p>After wanting to improve it, I wanted to use seed topics to tailor it. So I found this tutorial:
<a href=""https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164"" rel=""nofollow noreferrer"">https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164</a></p>

<p>But the GuidedLDA has been giving me so many errors because I can't match the structure of the dataset used in it.</p>

<p>I have tried:
 - changing my corpus into an array to get the shape</p>

<ul>
<li><p>Made X = corpus</p></li>
<li><p>Structure my corpus as a BoW model</p></li>
<li><p>Used DictVectorized</p></li>
</ul>

<p>This is tutorial code found here <a href=""https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164"" rel=""nofollow noreferrer"">https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164</a>:</p>

<pre><code>X = guidedlda.datasets.load_data(guidedlda.datasets.NYT)
vocab = guidedlda.datasets.load_vocab(guidedlda.datasets.NYT)
word2id = dict((v, idx) for idx, v in enumerate(vocab))
print(X.shape)
print(X.sum())

model = guidedlda.GuidedLDA(n_topics=5, n_iter=100, random_state=7, 
refresh=20)
model.fit(X)

topic_word = model.topic_word_
n_top_words = 8
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:- 
</code></pre>

<p>(n_top_words+1):-1]
        print('Topic {}: {}'.format(i, ' '.join(topic_words)))</p>

<p>My code:</p>

<pre><code># Create Dictionary 
id2word = corpora.Dictionary(processed_sums_bigrams)

# Create Corpus (list of documents)
texts = processed_sums_bigrams

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

X = corpus
vocab = id2word
print(X.shape)
print(X.sum())
</code></pre>

<p>When X = corpus or id2word:</p>

<p>AttributeError: 'list' object has no attribute 'shape'</p>

<p>When X = np.array(corpus[0]): </p>

<p>X.shape >>> (36,7)</p>

<p>This shape is too small. 
The tutorial code shape is (8447, 3012) and sum is 1221626. </p>

<p>Then I receive this error: </p>

<p>--> 47     print('Topic {}: {}'.format(str(i), ' '.join(topic_words)))
TypeError: sequence item 0: expected str instance, numpy.int32 found</p>

<p>I have also encountered other errors in my attempts to turn my corpus of text data into some kind of sparse array that resembles the document term matrix used in the tutorial code including:</p>

<p>ValueError: not enough values to unpack (expected 2, got 1)</p>

<p>TypeError: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'</p>
",,2019-05-27 12:49:24,How do I create a document term matrix from my pandas nested lists?,<python-3.x><pandas><scikit-learn><nlp><lda>,,,CC BY-SA 4.0,False,False,True,False,True
21552,56478384,2019-06-06 13:14:00,,"<p>Doc2vec while creating the vocabulary has possibility to put minimum occurence of the word in documents to be included in vocabulary as parameter <code>min_count</code>.</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=3, epochs=100,workers=8)
</code></pre>

<p>How is it possible to exclude words which appear far too often, with some parameter?</p>

<p>I know that one way is to do this in preprocessing step by manually deleting those words, and counting each, but would be nice to know if there is maybe some built in method to do so, as it gives more space for testing.
Many thanks for the answer.</p>
",2019-06-06 17:23:24,2019-06-06 17:34:37,How to put maximum vocabulary frequency in doc2vec,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21555,56483468,2019-06-06 18:51:00,,"<p>I am trying to use gensim for doc2vec and word2vec.</p>

<p>Since PV-DM approach can generate word2vec and doc2vec at the same time,
I thought PV-DM is the right model to use.</p>

<p>So, I created a model using <code>gensim</code> by specifying <code>dm=1</code> for PV-DM</p>

<p>My questions are followings:</p>

<ol>
<li><p>Is it true that word2vec model gets trained along with doc2vec when I call <code>train</code> on Doc2vec object??</p></li>
<li><p>it seems like property <code>wv</code> contains word2vec and available even before training. Is this static version of word2vec?</p></li>
<li><p>I also created DBOW model and noticed that it also contains <code>wv</code>. Is this also the same static version of word2vec that I mentioned in the previous question?</p></li>
</ol>
",2019-06-06 19:47:10,2019-06-06 23:41:25,Where is word2vec mapping coming from for DBOW doc2vec in gensim implementation?,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21565,56605373,2019-06-14 21:32:01,,"<p>The question is two-fold:
1.  How to select the ideal value for <code>size</code>?
2.  How to get the vocabulary size dynamically (per row as I intend) to set that ideal size? </p>

<p>My data looks like the following (example)just one row and one column:</p>

<p>Row 1</p>

<pre><code>{kfhahf}    
Lfhslnf;
.
.
. 
</code></pre>

<p>Row 2</p>

<pre><code>(stdgff  ksshu, hsihf)
asgasf;
.
.
. 
</code></pre>

<p>Etc.</p>

<p>Based on this post: <a href=""https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"">Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class</a> The <code>size</code> parameter should be less than (or equal to?) the vocabulary size. So, I am trying to dynamically assign the size as following:</p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# I do Word2Vec for each row
For item in dataset:
    Tokenized = word_tokenize(item)
    model = Word2Vec([Tokenized], min_count=1)
</code></pre>

<p>I get the vocabulary size here. So I create a second model:</p>

<pre><code>model1 = Word2Vec([Tokenized], min_count=1, size=len(model.wv.vocab))
</code></pre>

<p>This sets the <code>size</code> value to the current vocab value of the current row, as I intended. But is it the right way to do? What is the right size for a small vocabulary text?</p>
",,2019-06-15 15:33:06,"How to dynamically assign the right ""size"" for Word2Vec?",<python><python-3.x><nltk><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
21572,56551612,2019-06-11 21:01:09,,"<p>I am asking this question as a lazy researcher who just wants to try out random crazy ideas quickly, without spending a ton of time reinventing wheels. I completely understand these aren't the intended use cases.</p>

<p>To test a number of hypothesis, I would love to</p>

<ul>
<li>generate the (target, context, +1) tuples differently, instead of the default sliding window.</li>
<li>generate the negative samples (target, random_context, -1) tuples based on some rules, instead of from random NCE draws.</li>
</ul>

<p>For example, I can get the parse tree of a sentence and use parent-child relationship to generate tuples, which is a non-linear window(somebody already tried it in NLP research community, hand-coded ofc...). I can also get an antonyms dictionary to lookup and to generate more negative samples in addition to the random ones (not sure, may help with faster convergence).</p>

<p>Are there some private member functions (something that starts with <code>_XX</code>)I can override to achieve these?</p>
",,2019-06-12 05:53:39,"Gensim: Manual generation of training tuples of (target, context, label)",<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21573,56456051,2019-06-05 07:36:06,,"<p>I get the following deprecation warning when saving/loading a gensim word embedding:</p>

<pre><code>model.save(""mymodel.model"")

/home/.../lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: 
UserWarning: This function is deprecated, use smart_open.open instead. 
See the migration notes for details:
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function</a></p>

<pre><code>  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
</code></pre>

<p>I don't understand what to do following the notes on the page.
So, <strong>how should I save and open my models instead?</strong></p>

<p>I use python 3.7 , gensim 3.7.3. and smart_open 1.8.4. I think I did not get the warning when using gensim 3.7.1. and python 3.5. smart_open should have been 1.8.4. </p>
",,2019-06-06 00:16:02,gensim save load model deprecation warning,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21585,56394227,2019-05-31 11:25:49,,"<p>i'm using word features such as additional input for my nmt model. I used gensim to create pretrained embedding for each features and concated them with my word embedding vectors. So the question is can i use named entity tags as word feature and concate named entity tag one-hot vector to word embedding vector ? Main idea come from this <a href=""https://www.aclweb.org/anthology/U16-1001"" rel=""nofollow noreferrer"">paper</a></p>

<p>Example:</p>

<p>word embedding vector: [1,2,3,4]</p>

<p>pos tag: [5,6,7]</p>

<p>named entity tag: [0,1,0]</p>

<p>=> final embedding vector: [1,2,3,4,5,6,7,0,1,0]</p>
",,2019-05-31 11:25:49,Can i use one-hot vector for named entity tags to concate with word embedding to improve neural machine translation?,<nlp><machine-translation>,,,CC BY-SA 4.0,False,False,True,False,False
21594,56553753,2019-06-12 02:07:03,,"<p>I am using Doc2Vec to analysis some paragraph and wish to get deterministic vector representation of the train data. Based on the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">official documentation</a>, it seems that I need to set the parameters ""seed"" and ""workers"", as well as the PYTHONHASHSEED environment variable in Python 3. Therefore, I wrote the script as follows.</p>

<pre class=""lang-py prettyprint-override""><code>import os
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec


def main():
    # Check whether the environment variable has been set successfully
    print(os.environ.get('PYTHONHASHSEED'))

    docs = [TaggedDocument(['Apple', 'round', 'apple', 'red', 'Apple', 'juicy', 'apple', 'sweet'], ['A']),
            TaggedDocument(['I', 'have', 'a', 'little', 'frog', 'His', 'name', 'is', 'Tiny', 'Tim'], ['B']),
            TaggedDocument(['On', 'top', 'of', 'spaghetti', 'all', 'covered', 'with', 'cheese'], ['C'])]

    # Loop 3 times to check whether consistent results are produced within each run
    for i in range(3):
        model = Doc2Vec(min_count=1, seed=12345, workers=1)
        model.build_vocab(docs)
        model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
        print(model.docvecs['B'])


if __name__ == '__main__':
    os.environ['PYTHONHASHSEED'] = '12345'
    main()
</code></pre>

<p>The problem is that within each run it does produce deterministic results, but when I run the whole script again it gives different results. Is there any problem with my environment variable setting, or am I missing out something else?</p>

<p>I am on Python 3.6.5.</p>
",2019-06-12 02:12:08,2019-06-12 16:47:48,How to get deterministic train results in Doc2Vec?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21598,56605777,2019-06-14 22:24:15,,"<p>I would like to compare two documents semantically and generate a similarity score. The following docs are from wikipedia and when compare them, I expect to see a higher score for world_1 and world_2 as they have similar context.</p>

<p>Would training a Doc2vec model on ""world_1"" and testing other two docs with that model be a good approach? </p>

<p>thermo = ""Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following:Zeroth law of thermodynamics:If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.This statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration.""</p>

<p>world_1 = ""World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as the war to end all wars,[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide. On 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.""</p>

<p>world_2 = ""World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countriesincluding all the great powerseventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China.""</p>
",,2019-06-15 15:18:24,How to generate a similarity score for two documents,<gensim><word2vec><similarity><cosine-similarity><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21613,56591149,2019-06-14 03:52:24,,"<p>I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:</p>

<pre><code>model_w2v = models.Word2Vec(sg = 1, size=300)
model_w2v.build_vocab(all_tokens)
model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30)
</code></pre>

<p>Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training </p>

<pre><code>model_w2v_new = models.Word2Vec(sg = 1, size=300)
model_w2v_new.build_vocab(all_tokens)
</code></pre>

<p>and set the embeddings of the new Word2Vec equal to the first one</p>

<pre><code>model_w2v_new.wv.vectors = model_w2v.wv.vectors
</code></pre>

<p>Most of the functions work as expected, e.g.</p>

<pre><code>model_w2v.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
model_w2v_new.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
</code></pre>

<p>and</p>

<pre><code>model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
</code></pre>

<p>and</p>

<pre><code>entities_list = list(model_w2v.wv.vocab.keys()).remove('religion')

model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
</code></pre>

<p>However, most_similar doesn't work:</p>

<pre><code>model_w2v.wv.most_similar(positive=['religion'], topn=3)
[('religions', 0.4796232581138611),
 ('judaism', 0.4426296651363373),
 ('theists', 0.43141329288482666)]

model_w2v_new.wv.most_similar(positive=['religion'], topn=3)
&gt;[('roderick', 0.22643062472343445),
&gt; ('nci', 0.21744996309280396),
&gt; ('soviet', 0.20012077689170837)]
</code></pre>

<p>What am I missing? </p>

<p>Disclaimer. I posted this question on <a href=""https://datascience.stackexchange.com/questions/53601/copying-embeddings-for-gensim-word2vec"">datascience.stackexchange</a> but got no response, hoping to have a better luck here. </p>
",,2019-06-14 16:06:36,Copying embeddings for gensim word2vec,<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,True
21630,56625109,2019-06-17 05:20:10,,"<p>I have trained Gensim's WordToVec on a text corpus,converted it to DocToVec and then used cosine similarity to find the similarity between documents. I need to suggest similar documents. Now suppose among the top 5 suggestions for a particular document, we manually find that 3 of them are not similar.Can this feedback be incorporated in retraining the model?</p>
",,2019-06-17 18:04:39,Incorporating feedback to retrain WordToVec for finding document similarity,<machine-learning><deep-learning><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
21634,56630600,2019-06-17 11:42:07,,"<p>While defining corpus and dictionary for building the LDA model by defining topics how can we different topics keywords</p>
<p>It is working while giving an explicitly topic number, but I want that to be iterated.</p>
<h1>Preparing dictionary and corpus</h1>
<pre><code>from gensim import corpora

dictionary = corpora.Dictionary(doc_clean)

corpus = [dictionary.doc2bow(doc) for doc in doc_clean]
</code></pre>
<h1>Building LDA Model</h1>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus , id2word=dictionary , num_topics=10 , random_state=100, update_every=1 , chunksize=100 , passes=10 , alpha='auto' , per_word_topics=True)
</code></pre>
<h1>Printing the Keywords in topics</h1>
<pre><code>topics = print(lda_model.print_topic(6))

doc_lda = lda_model[corpus]
</code></pre>
<p>I want to know how can we iterate the topics instead of giving manually each time topic number</p>
",2020-06-20 09:12:55,2019-06-17 12:25:21,Topic modelling using LDA,<python><deep-learning><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
21635,56593904,2019-06-14 07:59:36,,"<p>I am using word2vec. When I am using the doesnt_match function it is showing a warning. Can anyone help:</p>

<p>venv/lib/python3.6/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a ""sequence"" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)</p>

<p>CODE:</p>

<pre><code>if len(words) &gt; 1:
    print(type(words))
    test = model.wv.doesnt_match(words)
    return test
else:
    return words
</code></pre>
",,2019-06-14 08:53:50,Word2Vec doesnt_match function throws Numpy warning,<python><numpy><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21646,56653627,2019-06-18 16:41:26,,"<p>I am trying to use LDA Mallet to assign my tweets to topics, and it works perfectly well when I feed it with up to 500,000 tweets, but it seems to stop working when I use my whole data set, which is about 2,500,000 tweets. Do you have any solutions for that?</p>

<p>I am monitoring my CPU and RAM usage when I run my codes as one way to make sure the code is actually running (I use Jupyter notebook). I use the code below to assign my tweets to topics. </p>

<pre class=""lang-py prettyprint-override""><code>import os
from gensim.models.wrappers import LdaMallet

os.environ.update({'MALLET_HOME':r'C:/new_mallet/mallet-2.0.8/'})
mallet_path = 'C:/new_mallet/mallet-2.0.8/bin/mallet'

ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)

</code></pre>

<p>The code seems to work when my data set contains fewer than 500,000 tweets: it spits out the results, and I can see python and/or java use my RAM and CPU. However, when I feed the code my entire data set, Java and Python temporarily show some CPU and RAM usage in the first few seconds, but after that the CPU usage drops to below 1 percent and the RAM usage starts to shrink gradually. I tried running the code several times, but after waiting on the code for 6-7 hours, I saw no increase in the CPU usage and the RAM usage dropped after a while. Also, the code did not produce any results. I finally had to stop the code. 
Has this happen to you? Do you have any solutions for it?
Thank you!</p>
",,2019-06-24 20:31:07,Mallet stops working for large data sets?,<python><nlp><text-mining><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
21651,56563339,2019-06-12 13:24:12,,"<p>I tried to use <code>gensim.downloader</code> to download <code>word2vec-google-news-300</code>, but my network isn't very reliable, so I downloaded <code>word2vec-google-news-300.gz</code> and <code>__init__.py</code> from github and put them into <code>~/gensim-data/word2vec-google-news-300/</code>. </p>

<p>But when I use <code>api.load(""word2vec-google-news-300"")</code> to load this model, I received error like this:</p>

<blockquote>
  <p>AttributeError: module 'word2vec-google-news-300' has no attribute 'load_data'</p>
</blockquote>

<p>My code:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim.downloader as api
model = api.load(""word2vec-google-news-300"")
</code></pre>
",2019-06-12 13:36:59,2020-03-16 15:50:49,module 'word2vec-google-news-300' has no attribute 'load_data',<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21653,56577964,2019-06-13 09:58:31,,"<p><strong>Problem description</strong></p>

<p>Unable to run gensims Distributed LSI due to this <code>failed to initialize distributed LSI (Failed to locate the nameserver)</code></p>

<p><strong>Steps/code/corpus to reproduce</strong></p>

<pre><code>from gensim.corpora import Dictionary
from gensim.models import TfidfModel, LsiModel
from gensim.similarities import Similarity
from gensim.test.utils import get_tmpfile
import sys
import time, traceback
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

documents=['simple document','trying to reproduce lsi error','example document']
documents=[doc.split(' ') for doc in documents]    
dictionary = Dictionary(documents)
tfidf = TfidfModel(dictionary=dictionary)
corpus = [dictionary.doc2bow(doc) for doc in documents]
model = LsiModel(corpus, id2word=dictionary,num_topics=200,distributed=True)
</code></pre>

<p><strong>Log Trace:</strong></p>

<pre><code>
2019-06-13 15:15:40,268 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2019-06-13 15:15:40,269 : INFO : built Dictionary(11 unique tokens: ['document', 'simple', 'error', 'lsi', 'reproduce']...) from 5 documents (total 17 corpus positions)
2019-06-13 15:15:40,292 : INFO : looking for dispatcher at PYRONAME:gensim.lsi_dispatcher
2019-06-13 15:15:42,414 : ERROR : failed to initialize distributed LSI (Failed to locate the nameserver)
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in connect_and_handshake(conn)
    514                                                nodelay=config.SOCK_NODELAY,
--&gt; 515                                                sslContext=sslContext)
    516                 conn = socketutil.SocketConnection(sock, uri.object)

~/anaconda3/lib/python3.6/site-packages/Pyro4/socketutil.py in createSocket(bind, connect, reuseaddr, keepalive, timeout, noinherit, ipv6, nodelay, sslContext)
    306         try:
--&gt; 307             sock.connect(connect)
    308         except socket.error:

ConnectionRefusedError: [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

CommunicationError                        Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _locateNS(host, port, broadcast, hmac_key)
   2004     try:
-&gt; 2005         proxy._pyroBind()
   2006         log.debug(""located NS"")

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _pyroBind(self)
    407         """"""
--&gt; 408         return self.__pyroCreateConnection(True)
    409 

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __pyroCreateConnection(self, replaceUri, connected_socket)
    595             else:
--&gt; 596                 connect_and_handshake(conn)
    597             if config.METADATA:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in connect_and_handshake(conn)
    548                         ce.__cause__ = x
--&gt; 549                     raise ce
    550             else:

CommunicationError: cannot connect to ('localhost', 9090): [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

NamingError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/gensim/models/lsimodel.py in __init__(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)
    431                 logger.info(""looking for dispatcher at %s"", str(dispatcher._pyroUri))
--&gt; 432                 dispatcher.initialize(
    433                     id2word=self.id2word, num_topics=num_topics, chunksize=chunksize, decay=decay,

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __getattr__(self, name)
    274             if not self._pyroMethods and not self._pyroAttrs:
--&gt; 275                 self._pyroGetMetadata()
    276         if name in self._pyroAttrs:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _pyroGetMetadata(self, objectId, known_metadata)
    614             try:
--&gt; 615                 self.__pyroCreateConnection()
    616             except errors.PyroError:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __pyroCreateConnection(self, replaceUri, connected_socket)
    587             else:
--&gt; 588                 uri = _resolve(self._pyroUri, self._pyroHmacKey)
    589             # socket connection (normal or Unix domain socket)

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _resolve(uri, hmac_key)
   1909     if uri.protocol == ""PYRONAME"":
-&gt; 1910         with _locateNS(uri.host, uri.port, hmac_key=hmac_key) as nameserver:
   1911             return nameserver.lookup(uri.object)

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _locateNS(host, port, broadcast, hmac_key)
   2011             e.__cause__ = x
-&gt; 2012         raise e
   2013 

NamingError: Failed to locate the nameserver

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-d3777d1d3a97&gt; in &lt;module&gt;()
     24 tfidf = TfidfModel(dictionary=dictionary)
     25 corpus = [dictionary.doc2bow(doc) for doc in documents]
---&gt; 26 model = LsiModel(corpus, id2word=dictionary,num_topics=200,distributed=True)
     27 corpus = [model[tfidf[doc]] for doc in corpus]
     28 index_tmpfile = get_tmpfile(""index"")

~/anaconda3/lib/python3.6/site-packages/gensim/models/lsimodel.py in __init__(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)
    440                 # distributed version was specifically requested, so this is an error state
    441                 logger.error(""failed to initialize distributed LSI (%s)"", err)
--&gt; 442                 raise RuntimeError(""failed to initialize distributed LSI (%s)"" % err)
    443 
    444         if corpus is not None:

RuntimeError: failed to initialize distributed LSI (Failed to locate the nameserver)
</code></pre>

<p><strong>Versions</strong></p>

<pre><code>Python 3.6.4
NumPy 1.15.4
SciPy 1.1.0
gensim 3.7.1
FAST_VERSION 1
</code></pre>
",2019-06-13 10:11:39,2019-06-13 10:11:39,Unable to run gensims Distributed LSI,<python><python-3.6><gensim><latent-semantic-indexing><pyro4>,,,CC BY-SA 4.0,False,False,True,False,False
21654,56579523,2019-06-13 11:33:16,,"<p>I want to retrain pre-trained word embeddings in Python using Gensim. The pre-trained embeddings I want to use is Google's Word2Vec in the file GoogleNews-vectors-negative300.bin.</p>

<p>Following Gensim's word2vec tutorial, ""its not possible to resume training with models generated by the C tool, load_word2vec_format(). You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there."" 
Therefore I can't use the KeyedVectors and for training a model the tutorial suggests to use:</p>

<pre><code>    model = gensim.models.Word2Vec.load('/tmp/mymodel')
    model.train(more_sentences)
</code></pre>

<p>(<a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a>)</p>

<p>However, when I try this:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model = Word2Vec.load('data/GoogleNews-vectors-negative300.bin')
</code></pre>

<p>I get an error message:</p>

<pre><code>    1330         # Because of loading from S3 load can't be used (missing readline in smart_open)
    1331         if sys.version_info &gt; (3, 0):
    -&gt; 1332             return _pickle.load(f, encoding='latin1')
    1333         else:
    1334             return _pickle.loads(f.read())

    UnpicklingError: invalid load key, '3'.
</code></pre>

<p>I didn't find a way to convert the binary google new file into a text file properly, and even if so I'm not sure whether that would solve my problem.</p>

<p>Does anyone have a solution to this problem or knows about a different way to retrain pre-trained word embeddings?</p>
",,2019-06-13 17:06:51,Retraining pre-trained word embeddings in Python using Gensim,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21656,56582711,2019-06-13 14:25:26,,"<p>I've already built my Doc2Vec model, using around 20.000 files. I'm looking for a way to find the string representation of a given vector/ID, which might be similar to Word2Vec's index2entity. I'm able to get the vector itself, using model['n'], but now I'm wondering whether there's a way to get some sort of string representation of it as well.</p>
",,2019-06-13 16:48:03,Python3 - Doc2Vec: Get document by vector/ID,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21660,56599306,2019-06-14 13:33:49,,"<p>So I want to use word-embeddings in order to get some handy dandy cosine similarity values. After creating the model and checking for similarity of the word ""not"" (which is in the data I give the model) it tells me that the word is not in the vocabulary.</p>

<p>Why can't it find the similarity for the word 'not'?</p>

<p>the description data looks as follows:<br>
[['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent', 'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in', 'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a', 'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the', 'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our', 'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look', 'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various', 'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw', 'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions', 'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion', 'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]</p>

<p>Note that I've already tried to give the data as separate sentences instead of separate words.</p>

<pre><code>def word_vec_sim_sum(row):
    description = row.product_description.split()
    description_embedding = gensim.models.Word2Vec([description], size=150,
        window=10,
        min_count=2,
        workers=10,
        iter=10)       
    print(description_embedding.wv.most_similar(positive=""not""))
</code></pre>
",2019-06-14 13:38:37,2019-06-14 13:39:52,"word not in vocabulary after training gensim word2vec model, why?",<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21667,56686830,2019-06-20 13:16:11,,"<p>I am trying to get the embeddings for a list of 1043 nodes with word2vec. When I try to build the vocabulary I find that word2vec takes the list of lists with the nodes and treats them as single digits, eg that ""143"" becomes ""1"",""4"",""3"".</p>

<p>I already tried to have all the numbers as single entries and see wether it is an formatting problem and went with a buil_vocab_from_freq instead of build_vocab, but this also just produces errors (object of type 'int' has no len()).</p>

<p>My code is the following:</p>

<pre><code>from gensim.models import Word2Vec

def generateEmbeddings(all_walks,dimension,min_count):
    model = Word2Vec(min_count = min_count, size = dimension)
    mylist = list(range(1,1043))
    corpus = {}
    j=1
    for i in mylist:
      corpus[str(i)] = j
      j=j+1
    #mylist = [str(i) for i in mylist]
    print(corpus)
    model.build_vocab_from_freq(corpus)
    model.train(mylist, total_examples=model.corpus_count, epochs = 30)
    #if it reaches this point it throws the error ""14 not found in vocabulary""
    print(model.wv.most_similar(positive=['14']))
    return model

print(generateEmbeddings(all_walks,128,2))
</code></pre>

<p>I want to get the embedding for eg. the number ""14"" and not ""1"" as it is by now. Thanks for your help!</p>

<p>//Edit</p>

<p>I managed to fix this, if anybody else is having this specific problem:
you have to format the list as mentioned as [[""1"",""102"",""43""],[""54"",""43""]] etc.
You cant change the old list at runtime (or at least it didnt work the way I did it), so you could create a new list at runtime with</p>

<pre><code>new_list = []
    for i in all_walks:
      temp_list = []
      for j in i:
        temp_list.append(str(j))
      new_list.append(temp_list)
</code></pre>
",2019-06-20 18:00:38,2019-06-21 04:22:47,How to add numbers with more than one digit to the word2vec-vocabulary,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21685,56667348,2019-06-19 12:11:57,,"<p>Basically I have installed Gensim 3.7.3 from Python 3.7.1  , but while importing it in Pycharm i got an error: 
""ImportError: DLL load failed: %1 is not a valid Win32 application.""</p>

<p>I want to use Word2Vec model of Gensim but due to this error, I am stuck. I can't Change Python Version too. </p>

<p>Need Help! how I get Gensim Imported in this version of Python using Pycharm </p>

<p>import gensim
from gensim.models import Word2Vec</p>
",,2019-07-08 12:30:10,"Gensim Import Error ""ImportError: DLL load failed: %1 is not a valid Win32 application.""",<python><winapi><dll><pycharm><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21688,56638258,2019-06-17 20:08:47,,"<p>From Stanford's CS244N course, I know Gensim provides a fantastic method to play around the embedding data: most_similar</p>

<p>I was trying to find some equivalent in Keras Embedding layer but I couldn't. It isn't possible out of the box from Keras? Or was it any wrapper on top of it?</p>
",2020-01-16 22:32:43,2020-01-16 22:32:43,How to find similar words in Keras Word Embedding layer,<keras><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21701,56639993,2019-06-17 23:26:44,,"<p>According to <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors"" rel=""nofollow noreferrer"">Gensim's page on WordEmbeddingKeyedVectors</a>, you can add a new key-value pair of new word vectors incrementally. However, after initializing WordEmbeddingKeyedVectors with pre-trained vectors and its tags, and adding new unseen model-inferred word vectors to it, the <code>most_similar</code> method could no longer be used. </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors

test = WordEmbeddingsKeyedVectors(vector_size=3)

test.add(entities=[""1"", ""2""], weights=[np.random.randint(5, size=3), 
                                  np.random.randint(5, size=3)])

test.most_similar(""2"") #THIS WORKS

test.add(entities=['3'], weights=[np.random.randint(5, size=3)])

test.most_similar(""3"") #THIS FAILS
</code></pre>

<p>I expect the output to be a list of vector tags most similar to the input tag, but the output is:</p>

<blockquote>
  <p>IndexError: index 2 is out of bounds for axis 0 with size 2</p>
</blockquote>
",2019-06-17 23:35:48,2019-06-18 22:25:15,How to add new word vectors to gensim.models.keyedvectors and calculate most_similar,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21708,56707416,2019-06-21 16:39:13,,"<p>The code below takes forever to execute. Probably due to the large size of the dictionary. Is there a way to make it faster, by e.g. cropping the visualized data?</p>

<pre><code>vis = pyLDAvis.gensim.prepare(lda, corpus, id2word)
</code></pre>
",,2019-08-19 07:35:49,pyLDAvis prepare() is slow,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
21733,56712365,2019-06-22 03:33:15,,"<p>I am new to LDA and when I am calculating the coherence score for my LDA model using gensim CoherenceModel, it takes extremely long time to run. However, the training part is relatively fast, and in a reasonable time. I wonder if this is because of my data size(about 250000 long text) and are there any ways to speed up this process? Thanks</p>

<p>This is my code, which is exactly the same as the tutorial</p>

<pre><code>from gensim.models import CoherenceModel
coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=LDA_, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>
",2019-06-22 13:17:12,2019-06-29 17:23:46,Why it is so slow when calculating the coherence score for LDA using gensim,<nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
21740,56641954,2019-06-18 04:53:03,,"<p>I am trying to convert tokens of sentences into integers. But it is giving me floats. </p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
model = Word2Vec([tokens], min_count=1, size=1)
</code></pre>

<p>when I do,</p>

<pre><code>print(model[""hello""])
</code></pre>

<p>it gives me,</p>

<pre><code>[-0.3843384]
</code></pre>

<p>I want this to be a positive integer. </p>
",,2019-06-18 08:12:52,Converting string tokens into integers,<python><python-3.x><nltk><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
21750,56699846,2019-06-21 08:43:37,,"<p>i am trying to run proses_wiki2 but an error occured:</p>

<blockquote>
  <p>ValueError: not enough values to unpack (expected 2, got 0)</p>
</blockquote>

<p>anyone can help?</p>

<p>i use spyder python 3.6, on windows 10.</p>

<pre><code>import logging
import os.path
import sys
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, 
module='gensim')
from gensim.corpora import WikiCorpus

if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""Running %s"", ' '.join(sys.argv))

    if len(sys.argv) &lt; 3:
        print(globals()['__doc__'] % locals())
    inp, outp = sys.argv[1:3]

    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})
    with open(outp, 'w', encoding=""utf-8"") as output:
        for i, text in enumerate(wiki.get_texts()):
            # Note: if you're using Python 2, use:
            # output.write("" "".join(unicode(text)) + ""\n"")
            output.write("" "".join(text) + ""\n"")
            if i &gt; 0 and i % 10000 == 0:
                logger.info(""Saved %s articles"", i)
            n = i

    logger.info(""Finished saving %s articles"", n)
</code></pre>

<p>This is the message that it shows when I run the program:
Traceback (most recent call last):</p>

<blockquote>
  <p>File ""E:/pra tesis/Koding LSTM/proses_wiki2.py"", line 18, in 
      inp, outp = sys.argv[1:3]</p>
  
  <p>ValueError: not enough values to unpack (expected 2, got 0)</p>
</blockquote>
",,2019-06-21 08:43:37,"How to fix 'ValueError: not enough values to unpack (expected 2, got 0)' in python 3?",<python><python-3.x><tensorflow><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21759,56715394,2019-06-22 12:17:33,,"<p>I am trying to use the English Wikipedia dump (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) as my pre-trained word2vec model using <code>Gensim</code>.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model_path = 'enwiki-latest-pages-articles.xml.bz2'
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)
</code></pre>

<p>when I do this, I get</p>

<pre><code>   342     with utils.smart_open(fname) as fin:
    343         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 344         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    345         if limit:
    346             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;mediawiki'
</code></pre>

<p>Do I have to re-download or something?</p>
",,2019-06-24 03:50:04,How do I use the wikipedia dump as a Gensim model?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21775,56791240,2019-06-27 12:39:57,,"<p>I have a structured dataset with columns 'text' and 'topic'. Someone has already conducted a word embedding/topic modeling so each row in 'text' is assigned a topic number (1-200). I would like to create a new data frame with the topic number and the top 5-10 key words that represent that topic.</p>

<p>I've done this before, but I usually start from scratch and run an LDA model. Then use the objects created by the LDA to find keywords per topic. That said, I'm starting from a mid-point that my supervisor gave me, and it's throwing me off.</p>

<p>The data structure looks like below:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'text': ['foo bar baz', 'blah bling', 'foo'], 
               'topic': [1, 2, 1]})
</code></pre>

<p>So would the plan be to create a bag of words, groupby 'topic,' and count the words? Or is there a keywords function and group by a column option that I don't know about in gensim or nltk?</p>
",,2019-06-27 15:40:06,Extract key words by topic,<python><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
21794,56793969,2019-06-27 15:05:26,,"<p>I have different LDA models (on the same text, but all with different #topics) stored in one list. Now, I want to save this list with all the models in it to my disk. However, I am not sure how this works. Should I treat is as a list or as a LDA model?</p>

<p>On the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.save"" rel=""nofollow noreferrer"">gensim website</a> I found the following code:</p>

<pre><code>from gensim.test.utils import datapath
&gt;&gt;&gt;
&gt;&gt;&gt; # Save model to disk.
&gt;&gt;&gt; temp_file = datapath(""model"")
&gt;&gt;&gt; lda.save(temp_file)
</code></pre>

<p>However, this works for separate LDA models, not for lists with multiple models. What is the best way to save my list of models?</p>
",,2019-07-05 18:02:20,How to save a list of Gensim LDA models?,<python><list><save><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21808,56779011,2019-06-26 18:26:37,,"<p>I use LDAvis library to visualize my LDA topics. It works fine before, but it gets me this error when I download the saved model files from Sagemaker to the local computer. I don't know why does this happen? Does that relate to Sagemaker?</p>

<p>If I run from the local, and saved the model from local, and then run LDAviz library, it works fine.</p>

<hr>

<p>KeyError                                  Traceback (most recent call last)
 in ()</p>

<p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\pyLDAvis\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)
    116     See <code>pyLDAvis.prepare</code> for **kwargs.
    117     """"""
--> 118     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)
    119     return vis_prepare(**opts)</p>

<p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\pyLDAvis\gensim.py in _extract_data(topic_model, corpus, dictionary, doc_topic_dists)
     46           gamma = topic_model.inference(corpus)
     47       else:
---> 48           gamma, _ = topic_model.inference(corpus)
     49       doc_topic_dists = gamma / gamma.sum(axis=1)[:, None]
     50    else:</p>

<p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py in inference(self, chunk, collect_sstats)
    665             # phinorm is the normalizer.
    666             # TODO treat zeros explicitly, instead of adding epsilon?
--> 667             eps = DTYPE_TO_EPS[self.dtype]
    668             phinorm = np.dot(expElogthetad, expElogbetad) + eps
    669 </p>

<p>KeyError: dtype('float32')</p>
",,2019-12-23 18:05:23,How to fix 'KeyError: dtype('float32')' in LDAviz,<python><lda><amazon-sagemaker>,,,CC BY-SA 4.0,False,False,True,False,False
21812,56796761,2019-06-27 18:22:26,,"<p>Thanks for stopping by!  I have a question about the dynamic topic model path: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&gt;&gt;&gt; from gensim.test.utils import common_corpus, common_dictionary
&gt;&gt;&gt; from gensim.models.wrappers import DtmModel
&gt;&gt;&gt;
&gt;&gt;&gt; path_to_dtm_binary = ""/path/to/dtm/binary""
&gt;&gt;&gt; model = DtmModel(
...     path_to_dtm_binary, corpus=common_corpus, id2word=common_dictionary,
...     time_slices=[1] * len(common_corpus)</code></pre>
</div>
</div>
</p>

<p>what is the path the dynamic topic model binary?  Is that something I need to install or download?  Where can I install or download that?</p>

<p>Thanks!</p>
",,2019-06-29 17:39:10,Dynamic Topic Model Path,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
21818,56897173,2019-07-05 05:48:32,,"<p>I have a list of 1000 documents, where the first 500 belongs to documents in <code>movies</code> (i.e. list index from <code>0</code> to <code>499</code>) and the remaining 500 belings to documents in <code>tv series</code> (i.e. list index from <code>500</code> to <code>999</code>).</p>

<p>For movies the <code>document tag</code> starts with <code>movie_</code> (e.g., <code>movie_fast_and_furious</code>) and for tv series the document tag starts with <code>tv_series_</code> (e.g., <code>tv_series_the_office</code>)</p>

<p>I use these movies and tv series dataset to build a <code>doc2vec</code> model as follows.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>Now for each <code>movie</code>, I want to get its nearest 5 <code>tv series</code> along with their cosine similarity.</p>

<p>I know, the function gensim provides <code>model.docvecs.most_similar</code>. However, the results of this include movies as well (which is not my intension). Is it possible to do this in gensim (I assume that the document vectors are creating in the order of the <code>documents list</code> that we provide).</p>

<p>I am happy to provide more details if needed.</p>
",2019-07-05 10:52:25,2019-07-05 17:46:46,How to identify doc2vec instances seperately in gensim in python,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21820,56757166,2019-06-25 15:11:35,,"<blockquote>
  <p>I receive an error when trying to upload a pre-trained word2vec file
  (compiled with fasttext) using Gensim. File has '.vec' extension and
  can be found here:
  <a href=""http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip"" rel=""nofollow noreferrer"">http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip</a></p>
  
  <p>What I've tried so far: Option 1: KeyedVectors from gensim.models
  Option 2: FastText wrapper</p>
</blockquote>

<pre><code>#Option 1
    from gensim.models import KeyedVectors
    model = KeyedVectors.load_word2vec_format('Word_embeddings/corola.300.20.vec', binary=True)
######

#Option 2
    from gensim.models.wrappers import FastText
    model = FastText.load_word2vec_format('Word_embeddings/corola.300.20.vec')
</code></pre>

<blockquote>
  <p>Error option 1: UnicodeDecodeError: 'utf-8' codec can't decode byte
  0x9b in position 0: invalid start byte</p>
  
  <p>Deprecation Error option 2: DeprecationWarning: Deprecated. Use
  gensim.models.KeyedVectors.load_word2vec_format instead.</p>
  
  <p>I need the correct method to successfully upload the word2vec file,
  using gensim.</p>
  
  <p>Thank you.</p>
</blockquote>
",,2019-06-26 09:55:10,Word2Vec: Error received at uploading a pre-trained word2vec file using Gensim,<python><nlp><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
21841,56883959,2019-07-04 08:45:50,,"<p>I am having a data-set consisting of faculty id and the feedback of students regarding the respective faculty. There are multiple comments for each faculty and therefore the comments regarding each faculty are present in the form of a list. I want to apply gensim summarization on the ""comments"" column of the data-set to generate the summary of faculty performance according to the student feedback.</p>

<p>Just for a trial I tried to summarize the feedbacks corresponding to the first faculty id. There are 8 distinct comments (sentences) in that particular feedback, still gensim throws an error ValueError: input must have more than one sentence. </p>

<pre><code>df_test.head()
    csf_id  comments
0   9   [' good subject knowledge.', ' he has good kn...
1   10  [' good knowledge of subject. ', ' good subjec...
2   11  [' good at clearing the concepts interactive w...
3   12  [' clears concepts very nicely interactive wit...
4   13  [' good teaching ability.', ' subject knowledg...
from gensim.summarization import summarize
text = df_test[""comments""][0]
print(""Text"")
print(text)
print(""Summary"")
print(summarize(text))
</code></pre>

<blockquote>
  <p>ValueError: input must have more than one sentence  </p>
</blockquote>

<p>what changes shold i make so that the summarizer reads all the sentenses and summarizes them.</p>
",2019-07-04 08:46:32,2019-07-05 08:58:07,Columnwise Summarize multiple sentences present in a list using the gensim summarizer,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21847,56833365,2019-07-01 09:44:01,,"<p>There are quite some posts about this specific issue, but I was unable to solve this problem.
I have been experimenting with LDA on the 20newgroup corpus with both the Sklearn and Gensim implementation. It is described in the literature that perplexity usually decreases with a higher amount of topics, but I am having different results.</p>

<p>I have already experimented with different parameters, but in general the perplexity increases for the test set, and decreases for the train set, when increasing the amount of topics. This could indicate that the model is overfitting on the training set. But similar patterns occur when using other text data sets. Also, research specifically using this data set have experienced a decrease in perplexity. (e.g. <a href=""https://www.researchgate.net/figure/a-and-b-Perplexity-results-on-Reuters-21578-and-20-newsgroups-for-DELSA-PLSI-and_fig3_221146965"" rel=""nofollow noreferrer"">ng20 perplexity</a>)</p>

<p>I have experimented with SkLearn, Gensim and the Gensim Mallet wrapper, and all packages do show different perplexity values (which can be expected since LDA is randomly initalized + different inference algorithms), but the common pattern is that the perplexity does increase for every package, which contradicts many papers from the literature.</p>

<pre class=""lang-py prettyprint-override""><code># imports for code sample
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.decomposition import LatentDirichletAllocation
</code></pre>

<p>small sample code</p>

<pre class=""lang-py prettyprint-override""><code># retrieve the data
newsgroups_all = datasets.fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle = True)
print(""Extracting tf features for LDA..."")
tf_vectorizer_train = CountVectorizer(max_df=0.95, min_df=2,stop_words='english')
X = tf_vectorizer_train.fit_transform(newsgroups_all.data)
X_train, X_test = train_test_split(X,  test_size=0.2, random_state=42)
</code></pre>

<pre class=""lang-py prettyprint-override""><code>k = N
lda = LatentDirichletAllocation(n_components = k, doc_topic_prior = 1/k, topic_word_prior = 0.1)
lda.fit(X_train)
perp_test = lda.perplexity(X_test)
perp_train = lda.perplexity(X_train)
</code></pre>

<p>I expect all perplexities to decrease, but I am getting the following output:</p>

<p>k = 5,
train perplexity: 5531.15, 
test perplexity: 7740.95</p>

<p>k = 10, 
train perplexity: 5202.80, 
test perplexity: 8805.57</p>

<p>k = 15,
train perplexity: 5095.42, 
test perplexity: 10193.42</p>

<p>Edit: After running 5 fold cross validation (from 10-150, step size: 10), and averaging the perplexity per fold, the following plot is created. It seems that the perplexity for the training set only decreases between 1-15 topics, and then slightly increases when going to higher topic numbers. The perplexity of the test set constantly increases, almost lineary. Can there be a difference in perplexity calculations between sklearn/gensim implementation and research publishing a decrease of perplexity.</p>

<p><a href=""https://i.stack.imgur.com/Ej6WZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ej6WZ.png"" alt=""Train and test perplexity""></a></p>
",2019-07-02 10:22:21,2019-07-02 10:22:21,Perplexity increases with number of topics,<python><scikit-learn><lda><topic-modeling><perplexity>,,,CC BY-SA 4.0,False,False,True,False,True
21867,56835032,2019-07-01 11:39:01,,"<p>I would like to find bigrams in a large corpus in text format. As the corpus cannot be loaded at once in memory and its lines are very big, I load it by chunks, each 1 kb</p>

<pre><code>def read_in_chunks(filename, chunk_size=1024):
    """"""Lazy function (generator) to read a file piece by piece.
    Default chunk size: 1k.""""""
    while True:
        data = filename.read(chunk_size)
        if not data:
            break
        yield data

</code></pre>

<p>Then I want to go piece by piece through the corpus and find bigrams and I use the gensim Phrases() and Phraser() functions, but while training, my model constantly loses state. Thus, I tried to save and reload the model after each megabyte that I read and then free the memory, but it still loses state. My code is here:</p>

<pre><code>with open(""./final/corpus.txt"", ""r"", encoding='utf8') as r:
    max_vocab_size=20000000
    phrases = Phrases(max_vocab_size=max_vocab_size)
    i=1
    j=1024
    sentences = """"
    for piece in read_in_chunks(r):   

        if i&lt;=j:
            sentences = sentences + piece

        else:
            phrases.add_vocab(sentences)
            phrases = Phrases(sentences)
            phrases = phrases.save('./final/phrases.txt')
            phrases = Phraser.load('./final/phrases.txt')

            sentences = """"
            j+=1024
        i+=1
print(""Done"")

</code></pre>

<p>Any suggestion?
Thank you.</p>
",,2019-07-01 17:25:24,Detect bigram collection on a very large text dataset,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21882,56906376,2019-07-05 16:13:30,,"<p>I have some problems with the code, which he can't process data lists and int.
but at first he was able to produce csv data on line 27 ... but then it can't ...
what's wrong with  hasil=total/kata</p>

<pre><code>import gensim
#import pandas as  pd
import re
import csv
import numpy as np
def processing(kata):
    words=re.sub(r'([^\s\w]|_)', '', kata)
    words= re.sub(r'[0-9]+', '', words)
    return words
def data():
    path = './model_terbaru/idwiki_word2vec_100.model'
    model = gensim.models.word2vec.Word2Vec.load(path)
    fp = open('data/data_train.csv', 'r')
    line = fp.readline()
    while line:
        processed = processing(line)
        print (processed)
        kata=len(processed.split())
        print(kata)
        total=[100]
        print(total)
        for word in processed.split():
            try:
                vector=model[""""+word+""""]
                print(vector)
                total=total+vector
                print(word)
            except:
                pass
        hasil=total/kata
        print(hasil)
        mylist= hasil 
        ok=open('data/vector_train.csv','a')
        a=csv.writer(ok,lineterminator='\n')
        a.writerows([mylist])
        ok.close()
        line = fp.readline()
    fp.close()

data()
</code></pre>

<p>hasil=total/kata
TypeError: unsupported operand type(s) for /: 'list' and 'int'</p>

<p>please help me.</p>
",,2019-07-05 17:30:37,unsupported operand type(s) for /: 'list' and 'int' in pyhton,<python><list><int><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21886,56853512,2019-07-02 13:23:33,,"<p>I am still a beginner with neural networks and NLP. 
In this code I'm training cleaned text (some tweets) with skip-gram.
But I do not know if I do it correctly.
Can anyone inform me about the correctness of this skip-gram text training? 
Any help is appreciated.</p>

<p><strong>This my code :</strong></p>

<pre><code>from nltk import word_tokenize

from gensim.models.phrases import Phrases, Phraser

sent = [row.split() for row in X['clean_text']]

phrases = Phrases(sent, max_vocab_size = 50, progress_per=10000)

bigram = Phraser(phrases)

sentences = bigram[sent]

from gensim.models import Word2Vec

w2v_model = Word2Vec(window=5,
                     size = 300,
                     sg=1)

w2v_model.build_vocab(sentences)


w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=25)


del sentences #to reduce memory usage


def get_mat(model, corpus, size):

    vecs = np.zeros((len(corpus), size))

    n = 0

    for i in corpus.index:
        vecs[i] = np.zeros(size).reshape((1, size))
        for word in str(corpus.iloc[i,0]).split():
            try:
                vecs[i] += model[word]
                #n += 1
            except KeyError:
                continue

    return vecs

X_sg = get_vectors(w2v_model, X, 300)

del X

X_sg=pd.DataFrame(X_sg)
X_sg.head()
from sklearn import preprocessing
scale = preprocessing.normalize
X_sg=scale(X_sg)

for i in range(len(X_sg)):
    X_sg[i]+=1 #I did this because some weights where negative! So could not 
               #apply LSTM on them later
</code></pre>
",,2019-07-02 20:25:38,Is this text training with skip-gram correct?,<python-3.x><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
21888,56837259,2019-07-01 14:15:05,,"<p>I cannot install gensim==3.5.0 in my elastic beanstalk environment (python 3.4). I get an error that gensim needs python >= 3.5 to run.</p>

<p>This was not a problem until a mid-day deployment today, that made only project code changes, nothing related to elastic beanstalk, requirements or settings.</p>

<p>At the same time, I'm succesfully running the same version in another identical environment. That means the same pip, same python version, same required dependencies.</p>

<p>I tried lowering the gensim requirement to gensim==0.13.4 which officially supports python 3.4, but I get the same error.</p>

<p>EDIT: I managed to make things work by installing gensim==0.10.0 and then redeploying with gensim=3.5.0. I still don't know the cause of the issue and the solution is not really a solution, so I'm still interested in insights.</p>
",2019-07-03 11:34:07,2019-07-03 11:34:07,Can't install gensim with python 3.4,<amazon-web-services><pip><amazon-elastic-beanstalk><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21899,56681210,2019-06-20 07:38:02,,"<h1>Intro</h1>
<p>Currently I am trying to use dask in concert with gensim to do NLP document computation and I'm running into an issue when converting my corpus into a &quot;<a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">TaggedDocument</a>&quot;.</p>
<p>Because I've tried so many different ways to wrangle this problem I'll list my attempts.</p>
<p>Each attempt at dealing with this problem is met with slightly different woes.</p>
<h1>First some initial givens.</h1>
<h2>The Data</h2>
<pre><code>df.info()
&lt;class 'dask.dataframe.core.DataFrame'&gt;
Columns: 5 entries, claim_no to litigation
dtypes: object(2), int64(3)
</code></pre>
<pre><code>  claim_no   claim_txt I                                    CL ICC lit
0 8697278-17 battery comprising interior battery active ele... 106 2 0
</code></pre>
<h2>Desired Output</h2>
<pre><code>&gt;&gt;tagged_document[0]
&gt;&gt;TaggedDocument(words=['battery', 'comprising', 'interior', 'battery', 'active', 'elements', 'battery', 'cell', 'casing', 'said', 'cell', 'casing', 'comprising', 'first', 'casing', 'element', 'first', 'contact', 'surface', 'second', 'casing', 'element', 'second', 'contact', 'surface', 'wherein', 'assembled', 'position', 'first', 'second', 'contact', 'surfaces', 'contact', 'first', 'second', 'casing', 'elements', 'encase', 'active', 'materials', 'battery', 'cell', 'interior', 'space', 'wherein', 'least', 'one', 'gas', 'tight', 'seal', 'layer', 'arranged', 'first', 'second', 'contact', 'surfaces', 'seal', 'interior', 'space', 'characterized', 'one', 'first', 'second', 'contact', 'surfaces', 'comprises', 'electrically', 'insulating', 'void', 'volume', 'layer', 'first', 'second', 'contact', 'surfaces', 'comprises', 'formable', 'material', 'layer', 'fills', 'voids', 'surface', 'void', 'volume', 'layer', 'hermetically', 'assembled', 'position', 'form', 'seal', 'layer'], tags=['8697278-17'])
&gt;&gt;len(tagged_document) == len(df['claim_txt'])
</code></pre>
<h1>Error Number 1 No Generators Allowed</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>TypeError: Could not serialize object of type generator.</p>
<p>I found no way of getting around this while still using a generator. A fix for this would be great! As this works perfectly fine for regular pandas.</p>
<h1>Error Number 2 Only the first element of each partition</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>This one is a bit dumb as the function won't iterate (I know) but gives the desired format, but only returns the first row in each partition.</p>
<h1>Error Number 3 function call hangs with 100% cpu</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    tagged_list = []
    for i, line in enumerate(df[corp]):
        tagged = gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))
        tagged_list.append(tagged)
    return tagged_list
</code></pre>
<p>Near as I can tell when refactoring the return outside the loop this function hangs builds memory in the dask client and my CPU utilization goes to 100% but no tasks are being computed. Keep in mind I'm calling the function the same way.</p>
<h1>Pandas Solution</h1>
<pre><code>def tag_corp(corp,tag):
    return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(corp), ([tag]))

tagged_document = [tag_corp(x,y) for x,y in list(zip(df_smple['claim_txt'],df_smple['claim_no']))]
</code></pre>
<p>List comp I haven't time tested this solution</p>
<h1>Other Pandas Solution</h1>
<pre><code>tagged_document = list(read_corpus_tag_sub(df))
</code></pre>
<p>This solution will chug along pretty much for hours. However I don't have enough memory to juggle this thing when it's done.</p>
<h1>Conclusion(?)</h1>
<p>I feel Super lost right now. Here is a list of threads I've looked at. I admit to being really new to dask I've just spent so much time and I feel like I'm on a fools errand.</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/50862165/creating-a-dask-bag-from-a-generator"">Dask Bag from generator</a></li>
<li><a href=""https://medium.com/mindorks/speeding-up-text-pre-processing-using-dask-45cc3ede1366"" rel=""nofollow noreferrer"">Processing Text With Dask</a></li>
<li><a href=""https://gdcoder.com/speed-up-pandas-apply-function-using-dask-or-swifter-tutorial/"" rel=""nofollow noreferrer"">Speed up Pandas apply using Dask</a></li>
<li><a href=""https://stackoverflow.com/questions/45545110/how-do-you-parallelize-apply-on-pandas-dataframes-making-use-of-all-cores-on-o"">How do you parallelize apply() on Pandas Dataframes making use of all cores on one machine?</a></li>
<li><a href=""https://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply"">python dask DataFrame, support for (trivially parallelizable) row apply?</a></li>
<li><a href=""https://stackoverflow.com/questions/39215617/what-is-map-partitions-doing"">What is map_partitions doing?</a></li>
<li><a href=""https://stackoverflow.com/questions/47125665/simple-dask-map-partitions-example"">simple dask map_partitions example</a></li>
<li><a href=""https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions"" rel=""nofollow noreferrer"">The Docs</a></li>
</ol>
",2020-06-20 09:12:55,2019-06-21 18:31:48,Convert a column in a dask dataframe to a TaggedDocument for Doc2Vec,<python><dask><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21903,56889408,2019-07-04 13:49:34,,"<p>I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:</p>

<pre class=""lang-py prettyprint-override""><code>text=['schottky','diode','semiconductors',...]
</code></pre>

<p>So then I tried to use the Word2Vec:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model=Word2Vec(text, size=100, window=5, min_count=5, workers=4)
words=list(model.wv.vocab)

</code></pre>

<p>but the output was like this:</p>

<pre class=""lang-py prettyprint-override""><code>print(words)
['c','h','t','k','d',...]
</code></pre>

<p>I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.</p>

<p>My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.</p>

<p>Does someone know what's the problem? Thanks for your help!</p>

<p>Appendix:</p>

<p>Importing the book</p>

<p>content_text=[]
number_of_inputs=len(os.listdir(path))</p>

<pre><code>    file_to_open=path
open_file=open(file_to_open,'rb')
read_pdf=PyPDF2.PdfFileReader(open_file)
number_of_pages=read_pdf.getNumPages()
page_content=""""
for page_number in range(number_of_pages):
    page = read_pdf.getPage(page_number)
    page_content += page.extractText()
content_text.append(page_content)
</code></pre>
",,2019-07-05 17:53:21,Gensim Word2Vec Vocabulary: Unclear output,<python><python-3.x><text-mining><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21905,56908407,2019-07-05 19:33:03,,"<p>I have a target NumPy array with shape (300,) and a set of candidate arrays also of shape (300,). These arrays are Word2Vec representations of words; I'm trying to find the candidate word that is most similar to the target word using their vector representations. What's the best way to find the candidate word that is most similar to the target word?</p>

<p>One way to do this is to sum up the absolute values of the element-wise differences between the target word and the candidate words, then select the candidate word with the lowest overall absolute difference. For example:</p>

<pre><code>candidate_1_difference = np.subtract(target_vector, candidate_vector)
candidate_1_abs_difference = np.absolute(candidate_1_difference)
candidate_1_total_difference = np.sum(candidate_1_abs_difference)
</code></pre>

<p>Yet, this seems clunky and potentially wrong. What's a better way to do this?</p>

<p>Edit to include example vectors:</p>

<pre><code>import numpy as np
import gensim

path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'


def func1(path):
    #Limited to 50K words to reduce load time
    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, limit=50000)
    context =  ['computer','mouse','keyboard']
    candidates = ['office','house','winter']
    vectors_to_sum = []
    for word in context:
        vectors_to_sum.append(model.wv[word])
    target_vector = np.sum(vectors_to_sum)

    candidate_vector = candidates[0]
    candidate_1_difference = np.subtract(target_vector, candidate_vector)
    candidate_1_abs_difference = np.absolute(candidate_1_difference)
    candidate_1_total_difference = np.sum(candidate_1_abs_difference)
    return candidate_1_total_difference
</code></pre>
",2019-07-05 19:53:49,2019-07-05 19:57:52,Comparing NumPy Arrays for Similarity,<python><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21906,56909294,2019-07-05 21:14:14,,"<p><strong>Intro</strong></p>

<p>Currently I am using Gensim in combination with pandas and numpy to run document NLP computation.  I'd like to build a LDA seqential model to track how our topics change over time but am running into errors with the corpus format.</p>

<p>I am trying to figure out how to set time slices for dynamic topic models.  I am using <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">LdaSeqModel</a> which requires an integer time slice. </p>

<p><strong>The Data</strong></p>

<p>It's a csv:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = pd.read_csv('CGA Jan17 - Mar19 Time Slice.csv', encoding = ""ISO-8859-1"");
documents = data[['TextForTopics']]
documents['index'] = documents.index</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>	       Month	Year	Begin Date	TextForTopics	                                      time_slice
0	march	2017	3/23/2017	request: the caller is requesting an appointme...	1</code></pre>
</div>
</div>
</p>

<p>This is then converted into an array of tuples called the bow_corpus:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[[(12, 2), (25, 1), (30, 1)], [(33, 1), (136, 1), (159, 1), (161, 1)], [(165, 1), (247, 2)], (326, 1), (354, 1), (755, 1), (821, 1)]]</code></pre>
</div>
</div>
</p>

<p><strong>Desired Output</strong></p>

<p>It should print one topic allocation for each time slice. If I entered 3 topics and two time slices I should get three topics printed twice showing how the topics evolved over time.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[(0,
  '0.165*""enrol"" + 0.108*""medicar"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""inform"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""effect"" + 0.060*""medicaid""')]
[(0,
  '0.165*""enrol"" + 0.108*""cats"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""puppies"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""elephants"" + 0.060*""medicaid""')]</code></pre>
</div>
</div>
</p>

<p><strong>What I've tried</strong></p>

<p>This is the function - the bow corpus is an array of tuples</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)</code></pre>
</div>
</div>
</p>

<p>I've tried every version of integer inputs for those time_slices and they all produce errors.  The premise was that the time_slice would represent the number of indicies/rows/documents in each time slice.  For example my data has 1.8 million rows if I wanted two time slices I would order my data by time and enter an integer cutoff like time_slice = [489234, 1310766].  All inputs produce this error:</p>

<p><strong>The Error</strong>
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-5-e58059a7fb6f&gt; in &lt;module&gt;
----&gt; 1 ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in __init__(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    186 
    187             # fit DTM
--&gt; 188             self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    189 
    190     def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    275             # seq model and find the evidence lower bound. This is the E - Step
    276             bound, gammas = \
--&gt; 277                 self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    278             self.gammas = gammas
    279 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    351             bound, gammas = self.inferDTMseq(
    352                 corpus, topic_suffstats, gammas, lhoods, lda,
--&gt; 353                 ldapost, iter_, bound, lda_inference_max_iter, chunksize
    354             )
    355         elif model == ""DIM"":

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)
    401         time = 0  # current time-slice
    402         doc_num = 0  # doc-index in current time-slice
--&gt; 403         lda = self.make_lda_seq_slice(lda, time)  # create lda_seq slice
    404 
    405         time_slice = np.cumsum(np.array(self.time_slice))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in make_lda_seq_slice(self, lda, time)
    459         """"""
    460         for k in range(self.num_topics):
--&gt; 461             lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]
    462 
    463         lda.alpha = np.copy(self.alphas)

IndexError: index 0 is out of bounds for axis 1 with size 0</code></pre>
</div>
</div>
</p>

<p><strong>Solutions</strong></p>

<p>I tried going back to the documentation and looking at the format of the common_corpus used as an example and the format of my bow_corpus is the same.  I also tried running the code in the documentation to see how it worked but it also produced the same error.  I'm not sure if the problem is my code anymore but I hope it is.</p>

<p>I've also tried messing with the file format by manually dividing my csv into 9 csvs containing my time_slices and creating an iterated corpus out of those, but that didn't work.  I've considered converting each row of my csv into txt files and then creating a corpus out of that like David Beil does, but that sounds pointlessly tedious as I already have an iterated corpus.</p>
",2019-08-01 17:11:48,2019-08-13 10:23:20,How to set time slices - Dynamic Topic Model,<python-3.x><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
21914,56976941,2019-07-10 18:50:07,,"<p>I am working with text data and at the moment I have put my data into a term document matrix and calculated the TF, term frequency and TF-IDF, term frequency inverse document frequency. From here my matrix looks like:</p>

<p>columns = document names</p>

<p>rownames = words</p>

<p>filled with their TF and TF-IDF scores. </p>

<p>I have been using the <code>tm</code> package in <code>R</code> for much of my current analysis but to take it further I have started playing around with the <code>gensim</code> library in Python.</p>

<p>Its not clear to me if I have the word embeddings as in the TF and TF-IDF. I am hopeing to use Word2Vec/Doc2Vec and obtain a matrix similar to what I currently have and then calculate the cosine similarity between document. Is this one of the outputs of the models?</p>

<p>I basically have about 6000 documents I want to calculate the cosine similarity between them and then rank these cosine similarity scores.</p>
",,2019-07-10 20:33:51,Can I obtain Word2Vec and Doc2Vec matrices to calculate a cosine similarity?,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21916,56959798,2019-07-09 20:02:16,,"<p>Is it possible to train a Kmeans ML model using a multidimensional feature matrix?</p>

<p>I'm using sklearn and KmeansClass for clustering, Word2Vec for extracting the bag of words, and TreeTagger for the text pre-processing</p>

<pre><code>from gensim.models import Word2Vec
from sklearn.cluster import KMeans

lemmatized_words = [[""be"", ""information"", ""contract"", ""residential""], [""can"", ""send"", ""package"", ""recovery""]

w2v_model = Word2Vec.load(wiki_path_model)

bag_of_words = [w2v_model.wv(phrase) for phrase in lemmatized_words]

#
#
# bag_of_words = [array([[-0.08796783,  0.08373307,  0.04610106, ...,  0.41964772,
#        -0.1733183 ,  0.09438939],
#       [ 0.11526374,  0.09092105, -0.2086806 , ...,  0.5205145 ,
#        -0.11455593, -0.05190944],
#       [-0.05140354,  0.09938619,  0.07485678, ...,  0.73840886,
#        -0.17298238,  0.09994634],
#       ...,
#       [-0.01144416, -0.17129216, -0.04012141, ...,  0.05281362,
#        -0.23109615,  0.02297313],
#       [-0.08355679,  0.24799444,  0.04348441, ...,  0.27940673,
#        -0.14400786, -0.09187686],
#       [ 0.11022831,  0.11035886,  0.19900796, ...,  0.12891224,
#        -0.09379898,  0.10538024]],dtype=float32)
#       array([[ 1.73330009e-01,  1.26429915e-01, -3.47578406e-01, ...,
#         8.09064806e-02, -3.02738965e-01, -1.61911864e-02],
#       [ 2.47227158e-02, -6.48087710e-02, -1.97364464e-01, ...,
#         1.35158226e-01,  1.72204189e-02, -1.14456110e-01],
#       [ 8.07424933e-02,  2.69261692e-02, -4.22120057e-02, ...,
#         1.01349883e-01, -1.94084793e-01, -2.64464412e-04],
#       ...,
#       [ 1.36009008e-01,  1.50609210e-01, -2.59797573e-01, ...,
#         1.84113771e-01, -6.85161874e-02, -1.04138054e-01],
#       [ 4.83367145e-02,  1.17820159e-01, -2.43335906e-02, ...,
#         1.33836940e-01, -1.55749675e-02, -1.18981823e-01],
#       [-6.68482706e-02,  4.57039356e-01, -2.20365867e-01, ...,
#         2.95841128e-01, -1.55933857e-01,  7.39804050e-03]], dtype=float32)
#       ]
#
#

model = KMeans(algorithm='auto0', max_iter=300, n_clusters=2)

model.fit(bag_of_words)

</code></pre>

<p>I expect that the Kmeans is trained, so I can store the model and use for predictions, but I receive this error message:</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
",2019-07-09 20:21:13,2019-07-10 08:42:50,KMeans clustering multidimensional features,<python><scikit-learn><nlp><k-means><word2vec>,,,CC BY-SA 4.0,False,False,True,False,True
21919,56961877,2019-07-10 00:11:50,,"<p>I am running some code on a Google Platform Compute Engine VM and I get an error when I imported <code>Python boto</code> library.</p>

<p>The first time if I run 'import boto', the error message would be: </p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'urllib2'</p>
</blockquote>

<p>Then I ran it again, a different error message came out:</p>

<blockquote>
  <p>AttributeError: module 'boto' has no attribute 'plugin'</p>
</blockquote>

<p>I tried installing <strong>google-compute-engine</strong> but it didn't work. I also tried different versions of <strong>boto</strong> but failed as well.</p>
",2019-07-10 11:14:47,2019-07-12 21:56:05,Python3: AttributeError: module 'boto' has no attribute 'plugin',<google-cloud-platform><boto><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
21922,56910538,2019-07-06 01:11:46,,"<p>I'm trying to install gensim in a specific conda env on my Python 3 only, Windows 10 machine. I've tried 3 different ways based on suggestions in SO and elsewhere, summarized below. Each time it shows as successfully installed and present in the env, but when I try to import it in jupyter notebook I get the <code>ModuleNotFoundError: No module named 'gensim'</code> error. </p>

<p>Note: I closed and relaunched anaconda and jupyter after each install.</p>

<p>SUMMARY: 
3 attempts with 3 install commands:</p>

<pre><code>COMMAND                              CONDA LIST                              IMPORT IN JUPYTER NOTEBOOK
conda install -c anaconda gensim     gensim 3.4.0 py36hfa6e2cd_0 anaconda    ModuleNotFoundError: No module named 'gensim'
pip install -U gensim                gensim 3.7.3 pypi_0 pypi                ModuleNotFoundError: No module named 'gensim'
conda install -c conda-forge gensim  gensim 3.7.3 py36h6538335_0 conda-forge ModuleNotFoundError: No module named 'gensim'
</code></pre>

<pre><code>(base) C:\Users\kb&gt;conda activate SARC
(SARC) C:\Users\kb&gt;conda install -c anaconda gensim
(SARC) C:\Users\kb&gt;conda list
. . .
gensim                    3.4.0            py36hfa6e2cd_0    anaconda
. . .

. . .
</code></pre>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-e92e291fb8cb&gt; in &lt;module&gt;
      1 import loader
      2 import reader
----&gt; 3 import transformers
      4 import vectorization

~\OneDrive\Documents\ds\courses_books\Applied_Text_Analysis_Python_book_code\atap-master\snippets\ch04\transformers.py in &lt;module&gt;
      3 import os
      4 import nltk
----&gt; 5 import gensim
      6 import unicodedata
      7 

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Details of the install commands and output can be seen <a href=""https://github.com/nicolas-ivanov/debug_seq2seq/issues/23#issuecomment-508880177"" rel=""nofollow noreferrer"">here</a>.</p>
",2019-07-06 03:34:32,2019-07-09 21:59:49,gensim installed in anaconda env but won't import in jupyter notebook,<python><jupyter-notebook><anaconda><conda><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
21943,56874230,2019-07-03 16:09:41,,"<p>I have a binary word2vec file and I am using <code>gensim</code> to load it.</p>

<p>While there is function to get <code>similarity</code> between 2 words in gensim but no function to calculate and return difference vector.</p>

<p>How can I use two vectors and get there difference vector?</p>

<p>And also I am trying to use these difference vectors as feature in document classification.Calculating diff vector between each word and each class.Is this right approach?</p>

<p>For example if classes are <code>sport</code> and <code>politics</code></p>

<pre><code>sport = [0.4,0.456,45,...] #wordvector of class
politics = [0.23,0.56...] #wordvector of class
</code></pre>

<p>And my word is <code>football</code></p>

<pre><code>football = [0.2,0.6,0.45,...] #wordvector of football
</code></pre>

<p>I want to calculate diff vector</p>

<pre><code>(sport - football) = [some vector] # this as a feature for classification
</code></pre>
",2019-07-03 16:23:51,2019-07-05 18:01:15,How to calculate difference vector in word2vec,<python><gensim><word2vec><calculation><document-classification>,,,CC BY-SA 4.0,False,False,True,False,False
21944,56999139,2019-07-12 01:29:56,,"<p>I am trying to build a doc2vec model with more or less 10k sentences, after that I will use the model to find the most similar sentence in the model of some new sentences. </p>

<p>I have trained a gensim doc2vec model using the corpus(10k sentences) I have. This model can to some extend tell me if a new sentence is similar to some of the sentences in the corpus. 
But, there is a problem: it may happen that there are words in new sentences which don't exist in the corpus, which means that they don't have a word embedding. If this happens, the prediction result will not be good. 
As far as I know, the trained doc2vec model does have a matrix of doc vectors as well as a matrix of word vectors. So what I were thinking is to load a set of pre-trained word vectors, which contains a large number of words, and then train the model to get the doc vectors. Does it make sense? Is it possible with gensim? Or is there another way to do it?</p>
",2019-07-12 02:24:39,2019-07-13 03:33:51,Is there a way to load pre-trained word vectors before training the doc2vec model?,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21946,57033566,2019-07-15 05:14:25,,"<p>I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:</p>

<p>In:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

# Set file names for train and test data
corpus = df['sentences'].values.tolist()

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(sentences=corpus)
model_gensim
</code></pre>

<p>Out:</p>

<pre><code>&lt;gensim.models.fasttext.FastText at 0x7f6087cc70f0&gt;
</code></pre>

<p>In:</p>

<pre><code># train the model
model_gensim.train(
    sentences = corpus, 
    epochs = model_gensim.epochs,
    total_examples = model_gensim.corpus_count, 
    total_words = model_gensim.corpus_total_words
)

print(model_gensim)
</code></pre>

<p>Out:</p>

<pre><code>FastText(vocab=107, size=100, alpha=0.025)
</code></pre>

<p>However, when I try to look in a vocabulary words:</p>

<pre><code>print('return' in model_gensim.wv.vocab)
</code></pre>

<p>I get <code>False</code>, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:</p>

<pre><code>model_gensim.most_similar(""return"")

[('R', 0.15871645510196686),
 ('2', 0.08545402437448502),
 ('i', 0.08142799884080887),
 ('b', 0.07969795912504196),
 ('a', 0.05666942521929741),
 ('w', 0.03705815598368645),
 ('c', 0.032348938286304474),
 ('y', 0.0319858118891716),
 ('o', 0.027745068073272705),
 ('p', 0.026891689747571945)]
</code></pre>

<p>What is the correct way of using gensim's fasttext wrapper?</p>
",,2020-08-04 13:29:14,How to train a word embedding representation with gensim fasttext wrapper?,<machine-learning><nlp><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
21954,56816261,2019-06-29 08:50:28,,"<p>I am new to NLP, how to find the similarity between 2 sentences and also how to print scores of each word. And also how to implement the gensim word2Vec model.</p>

<p>Try this code:
here my two sentences :</p>

<pre><code>sentence1=""I am going to India""
sentence2="" I am going to Bharat""
from gensim.models import word2vec
import numpy as np


words1 = sentence1.split(' ')
words2 = sentence2.split(' ')

#The meaning of the sentence can be interpreted as the average of its words
sentence1_meaning = word2vec(words1[0])
count = 1
for w in words1[1:]:
    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words2[0])
count = 1
for w in words2[1:]:
    sentence2_meaning = np.add(sentence2_meaning, word2vec(w))
    count += 1
sentence2_meaning /= count

#Similarity is the cosine between the vectors
similarity = np.dot(sentence1_meaning, sentence2_meaning)/(np.linalg.norm(sentence1_meaning)*np.linalg.norm(sentence2_meaning))
</code></pre>
",,2019-06-29 17:02:13,How to find the score for sentence Similarity using Word2Vec,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
21958,56965668,2019-07-10 07:38:49,,"<p>I am working on a NLP project at the moment. One task is to compare how similar two news articles/titles are. </p>

<p>To do that, i have already trained a doc2vec model using English wikipeida articles on gensim library. Now i want to compare similarities of new text by inferring vectors from the wiki doc2vec model. 
One of the method i have tried is the gensim docvecs's 'similarity_unseen_docs' function. However, the result isn't really intuitive.
May i know are there any other way to get a similarity score with better performance? Or maybe some part of code is wrong?  </p>

<p>Training of the doc2vec model:</p>

<pre class=""lang-py prettyprint-override""><code>models = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=5, min_count=20, epochs =10, sample=0.001, negative=5, workers=cores)
</code></pre>

<p>Comparing similarity:</p>

<pre class=""lang-py prettyprint-override""><code>arg1='China is  the leader in manufacturing electric vehicle'
arg2='The biggest electric car producer is China'

model.docvecs.similarity_unseen_docs(model=wikimodel, doc_words1=tokenize_text(arg1), 
                                         doc_words2=tokenize_text(arg2), alpha=0.025, min_alpha=0.0001, steps=50)
</code></pre>

<p>The output similarity score is only ~0.25 which doesn't seem to be good.</p>

<p>Here's the code for the training: </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.corpora.wikicorpus import WikiCorpus
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.models import KeyedVectors
from pprint import pprint
import multiprocessing

#read the wiki corpus
wiki = WikiCorpus(""trend_analysis/enwiki-latest-pages-articles.xml.bz2"")

#define the class to convert wikicorpus into suitable form to train Doc2vec
class TaggedWikiDocument(object):
    def __init__(self, wiki):
        self.wiki = wiki
        self.wiki.metadata = True
    def __iter__(self):
        for content, (page_id, title) in self.wiki.get_texts():
            yield TaggedDocument([c for c in content], [title])

documents = TaggedWikiDocument(wiki)


cores = multiprocessing.cpu_count()

#initialize a model and choose training method. (we use DM here)
models = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=5, min_count=20, epochs =10, sample=0.001, negative=5, workers=cores)

#building vocab for the model
models.build_vocab(documents)

#train the Doc2vec model
models.train(documents,total_examples=models.corpus_count, epochs=1)
</code></pre>

<p>Please let me know if the training is having any error that caused the inaccuracy for the inferring of unseen docs.</p>
",2019-07-11 02:35:10,2019-07-11 02:35:10,How to get sentence/text similarity of new corpus from a WIKI-doc2vec model?,<python><gensim><cosine-similarity><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
21963,56968915,2019-07-10 10:37:59,,"<p>In Gensims word2vec api, I trained a model where I initialized the model with max_final_vocab = 100000 and saved the model using model.save()
(This gives me one .model file, one .model.trainables.syn1neg.npy and one .model.wv.vectors.npy file).</p>

<p>I do not need to train model any further, so I'm fine with using just</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(""train.fr.model"")
kv = model.wv
del model


</code></pre>

<p>the kv variable shown here. I now want to use only the <em>top</em> N (N=40000 in my case) vocabulary items instead of the entire vocabulary. The only way to even attempt cutting down the vocabulary I could find was</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
emb_matrix = np.load(""train.fr.model.wv.vectors.npy"")
emb_matrix.shape
# (100000, 300)
new_emb_matrix = emb_matrix[:40000]
np.save(""train.fr.model.wv.vectors.npy"", new_emb_matrix)
</code></pre>

<p>If I load this model again though, the vocabulary still has length 100000.</p>

<p>I want to reduce the vocabulary of the model or model.wv while retaining a working model. Retraining is not an option.</p>
",,2019-07-10 12:16:06,"In Gensim Word2vec, how to reduce the vocab size of an existing model?",<gensim><word2vec><vocabulary>,,,CC BY-SA 4.0,False,False,True,False,False
21969,56984758,2019-07-11 08:25:54,,"<p>I have used the gensim Word2Vec model and applied it in my list of documents. Well , the word embedding is getting created. I want to know if Word2Vec is performing well on my list of documents. Is there any metrics to measure that? How will I understand if Word2Vec has really worked well on my document corpus or should I try some different embedding?
Below is the code I have used from gensim.</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(documents , size=150, window=10, min_count=2, sg=1, workers=10)
</code></pre>
",2019-07-11 08:27:42,2019-07-11 21:51:28,How to check the performance of word embedding,<python><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
21977,57020405,2019-07-13 15:17:54,,"<p>I am getting an import error as i try to import gensim, pandas and numpy in django views.py . It works fine in shell.
How can I import them in views.py. I have installed the libraries in virtual environment.</p>
",2019-07-14 11:26:16,2019-07-14 11:26:16,"I am trying to import gensim, pandas and numpy in my django project but getting import error",<django><pandas><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22009,57060692,2019-07-16 15:25:27,,"<p>So I created a Word2Vec model using Gensim using a document. Then I did some calculations on the vector array which I obtained by punching the code <code>model[model.wv.vocab]</code>. 
Now I have a new vector array with me and I want to ""devectorize"" this new vector array into text. How can this be done in python?</p>
",,2019-07-16 15:25:27,"How to ""devectorize"" a vector array (nx100) into text using a pre-trained model in Word2Vec using Gensim?",<python><nlp><artificial-intelligence><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22023,57079642,2019-07-17 15:39:01,,"<p>After reading the tutorial at gensim's <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">docs</a>, I do not understand what is the correct way of generating new embeddings from a trained model. So far I have trained gensim's fast text embeddings like this:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(corpus_file=corpus_file)

# train the model
model_gensim.train(
    corpus_file=corpus_file, epochs=model_gensim.epochs,
    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words
)
</code></pre>

<p>Then, let's say I want to get the embeddings vectors associated with this sentences:</p>

<pre><code>sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
sentence_president = 'The president greets the press in Chicago'.lower().split()
</code></pre>

<p>How can I get them with <code>model_gensim</code> that I trained previously?</p>
",,2019-07-17 18:22:00,"After training word embedding with gensim's fasttext's wrapper, how to embed new sentences?",<machine-learning><nlp><gensim><embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22027,56974450,2019-07-10 15:53:10,,"<p>I'm working on a model consisting in 2 parts, as i discussed in <a href=""https://stackoverflow.com/questions/56951787/triplet-loss-on-text-embeddings-with-keras"">this question</a>: the first should take the elements of a triplet (consisting in an anchor, a positive example and a negative example, same principle adopted in FaceNet) and turn them into vectors (word2vec + lstm), while the second should take those vectors and use them to calculate the triplet loss. I started working on some code, here's what i have now:</p>

<hr>

<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
from nltk.tokenize import WordPunctTokenizer
from collections import Counter
from string import punctuation, ascii_lowercase
import regex as re
from tqdm import tqdm
from gensim.models import Word2Vec
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Bidirectional, concatenate, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.layers.normalization import BatchNormalization
from keras.utils import plot_model

# Constants and initial settings
path = 'Datasets/DBLP-ACM/'
tf.compat.v1.set_random_seed(1)
ALPHA = 0.2
TRIPLETS_DATA_FILE = path + 'triplets/random_triplets.csv'
MAX_SEQUENCE_LENGTH = 300
tokenizer = WordPunctTokenizer()
vocab = Counter()

# Tokenize the text
def text_to_wordlist(text, lower=False):
    # Tokenize
    text = tokenizer.tokenize(text)
    # Optional: lower case
    if lower: text = [t.lower() for t in text]
    # Return a list of words
    vocab.update(text)
    return text

# Process data
def process_triplets(list_sentences, lower=False):
    triplet_elements = []
    for text in tqdm(list_sentences):
        txt = text_to_wordlist(text, lower=lower)
        triplet_elements.append(txt)
    return triplet_elements

# Define the custom loss (Triplet Loss)
def triplet_loss(x):
    anchor, positive, negative = x
    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1) 
    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)
    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)
    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)
    return loss

# Build the embedding model 
def build_embedding_model(): # How can i feed the input to the word2vec part?
    # Inputs
    wv_layer = Embedding(nb_words, WV_DIM, mask_zero=False, weights=[wv_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)
    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
    embedded_sequences = wv_layer(embedding_input)
    # BiGRU (aka bidirectional gru, bidirectional LSTM)
    embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)
    x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)
    x = Dropout(0.2)(x)
    x = BatchNormalization()(x)
    # Output
    preds = Dense(1, activation='sigmoid')(x) # Just one output class (dummy)

    # Build the model
    model = Model(inputs=[embedding_input], outputs=preds)
    model.compile(loss='mse', optimizer = ""adam"")

    return model

# Build the entire model
def build_model():
    # Inputs
    anchor_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='anchor_input')
    positive_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='positive_input')
    negative_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='negative_input')

    embedding_model = build_embedding_model()

    # Outputs
    anchor_embedding = embedding_model(anchor_input)
    positive_embedding = embedding_model(positive_input)
    negative_embedding = embedding_model(negative_input)

    merged_output = concatenate([anchor_embedding, positive_embedding, negative_embedding])
    loss = Lambda(triplet_loss, (1,))(merged_output)
    triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=loss)
    triplet_model.compile(loss = 'mean_absolute_error', optimizer = Adam())

    return triplet_model

triplets = pd.read_csv(TRIPLETS_DATA_FILE, error_bad_lines=False, sep=""|"", quotechar=""\"""", encoding=""latin_1"")
list_sentences_anchor = list((triplets[""anchor""].astype(str)).fillna("""").values)
list_sentences_positive = list((triplets[""positive""].astype(str)).fillna("""").values)
list_sentences_negative = list((triplets[""negative""].astype(str)).fillna("""").values)

# Fill an array for anchors, one for positives and one for negatives
anchors = process_triplets(list_sentences_anchor, lower=True)
positives = process_triplets(list_sentences_positive, lower=True)
negatives = process_triplets(list_sentences_negative, lower=True)

model_anchor = Word2Vec(anchors, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)
model_positive = Word2Vec(positives, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)
model_negative = Word2Vec(negatives, size=100, window=5, min_count=5, workers=16, sg=0, negative=5)

word_vectors_anchor = model_anchor.wv
word_vectors_positive = model_positive.wv
word_vectors_negative = model_negative.wv

# Use the embeddings in Keras
MAX_NB_WORDS = max(len(word_vectors_anchor.vocab), len(word_vectors_positive.vocab), len(word_vectors_negative.vocab))

word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}
sequences = [[word_index.get(t, 0) for t in anchor] for anchor in anchors[:len(anchors)]]
# Pad
anchor_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=""pre"", truncating=""post"")
# Create the embedding matrix
WV_DIM = 200
nb_words = min(MAX_NB_WORDS, len(word_vectors_anchor.vocab))
# Initialize the matrix with random numbers
wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0
for word, i in word_index.items():
    if i &gt;= MAX_NB_WORDS: continue
    try: # Words not found in embedding index will be all-zeros
        embedding_vector = word_vectors_anchor[word]
        wv_matrix[i] = embedding_vector
    except: pass  

# Build and fit the model
triplet_model = build_model()
hist = triplet_model.fit([anchor_data, anchor_data, anchor_data], 0, validation_split=0.1, epochs=50, batch_size=256, shuffle=True)
</code></pre>

<hr>

<p>As you will surely see, there's a lot of confusion. Basically, i split the triplets in 3 different pieces, i apply word2vec on each piece and i use the result in the embedding model (i used the same result 3 times just to test if it works, and it doesn't). </p>

<p>The embedding model should compute a vector to be used in the second model, during the fit process, and in the triplet loss. I'm new to Keras and i'm surely doing something wrong here, since i get this error at the moment:</p>

<pre><code>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
</code></pre>

<hr>

<p>This happens in the first line of the triplet loss function itself, and it's probably related to the input format. So the question is: given this code, how can i modify it to correctly accept 3 inputs, producing 3 vectors, and use those vectors in the triplet_model, during the fit?</p>

<p>I'll update the question if i modify the code or i get different errors. </p>
",,2019-07-10 17:40:02,Correctly submitting 3 inputs to a Keras model based on Triplet Loss,<python><keras><lstm><word2vec><triplet>,,,CC BY-SA 4.0,True,False,True,False,False
22029,57097233,2019-07-18 14:44:39,,"<p>I want to compare the similarity between two strings, I can calculate the wmd distance with a word2vec model or with a doc2vec model in gensim. But I could not understand how does wmd work for a doc2vec model. </p>

<pre><code>def preprocess(doc):    
    return doc.lower().split()

s1 = 'i would like five rooms'
s2 = 'i would like four rooms'
s1 = preprocess(s1)
s2 = preprocess(s2)

model1 = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model, binary = False) 
d1 = model1.wmdistance(s1, s2)
print('wmd distance using a word2vec model:', d1)

model2 = gensim.models.Doc2Vec.load(doc2vec_model)
d2 = model2.wmdistance(s1, s2)
print('wmd distance using a doc2vec model:', d2)

# wmd distance using a word2vec model: 0.502799493163681
# wmd distance using a doc2vec model: 0.008121068463511764
</code></pre>

<p>Does wmd still take the word embeddings for each word from the doc2vec model as it does with a word2vec model? Is there no difference with a word2vec model or a doc2vec model to calculate the wmd distance? In the below example, the wmd distances calculated from these 2 models are very different, why is this? I understand how wmd works generally for two sentences, but I just cannot figure out how it works for a doc2vec model. I would appreciate it if someone can help me understand it.</p>
",,2019-07-18 17:56:12,What is the wmdistance for a word2vec model and for a doc2vec model in gensim?,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22045,57100323,2019-07-18 18:03:42,,"<p>I have this data (the data is generated in R) and I use the <code>reticulate</code> package to port over to Python. The problem is with my Python code.</p>

<p>R code :</p>

<pre><code>text &lt;- c(""Because I could not stop for Death -"",
          ""He kindly stopped for me -"",
          ""The Carriage held but just Ourselves -"",
          ""and Immortality"")

ID &lt;- c(1,2,3,4)    
df &lt;- data.frame(cbind(ID, text))
library(reticulate)

df_py &lt;- r_to_py(df)
repl_python()
</code></pre>

<p>Python code :</p>

<pre><code>import gensim
LabeledSentence1 = gensim.models.doc2vec.TaggedDocument
all_content_data = []
j = 0
for em in r.df_py['text'].values:
  all_content_data.append(LabeledSentence1(em,[j]))
j+=1
print('Number of texts processed: ', j)
</code></pre>

<p>Note: The <code>r.df_py['text']</code> is a ""special"" function which calls R data, it can be changed to <code>df_py['text']</code> if just using Python. </p>

<p>The data is supposed to process the documents but when I print it says <code>Number of texts processed:  1</code> when it should say <code>Number of texts processed:  4</code>. I just don't know where I am going wrong in that function. My data is a data frame and in each row I have a unique ""book"" all the text of that book is in one cell and I want to process that cell.</p>
",2019-07-18 18:06:23,2019-07-18 18:08:53,Number of texted processed = 1 when it should = 4 (function to process documents),<python><r><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22047,57102264,2019-07-18 20:33:38,,"<p>Assume I have a number of pictures. Lets say 10 pictures which are annotated by 50 people each.
So Pic 1 might be beach, vacation, relax, sand, sun I now trained word2vec with a domain specific content. I have the vectors of each word and can represent them. But what I want now, is to create ONE final vector representing each picture. So one vector with represents the 50 annotations (beach, vacation, relax, sand, sun)</p>

<p>Lets assume each vector is represented with 100 dimensions  do I just add the first dimension (the 100 dimensions) of all 50 vectors, than the 2nd dimension of all 50 vectors etc.</p>

<p>I am very thankful for any comments that might help me!</p>

<p>I tried this, but I am not sure if this is the right way to do it. 
I also tried doc2vec but I guess this is problematic as the word order of the annotations is irrelevant  but relevant for doc2vec.???</p>
",,2019-07-22 18:25:27,How to calculate vectors with word2vec,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22068,57138453,2019-07-22 02:17:04,,"<p>I am using the doc2vec model as follows to construct my document vectors.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I have seen that <strong>gensim doc2vec also includes word vectors</strong>. Suppose I have a word vector created for the word <code>deep learning</code>. My question is; is it possible to get the <code>documents</code> nearest to <code>deep learning</code> word vector in gensim in python?</p>

<p>I am happy to provide more details if needed.</p>
",2019-07-22 05:35:59,2019-07-22 18:42:34,How to get the nearest documents for a word in gensim in python,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22070,57157137,2019-07-23 05:20:00,,"<p>I loaded a gensim continuous skipgram model from <code>http://vectors.nlpl.eu/repository/#</code> built on Google News 2013 with a vocabulary size of 2883863. However, I am getting an error message for any two random words I'm trying to get a similarity on.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format(r'C:\Users\Projects\NLPL\model.txt', binary=False)
model.similarity('president','minister')
</code></pre>

<p>While loading the model I also tried the binary file instead of <code>txt</code>file but that didn't work either.</p>

<pre><code>model = KeyedVectors.load_word2vec_format(r'C:\Users\Projects\NLPL\model.bin', binary=True)
</code></pre>

<p>Also for the similarity score, I tried using unicode characters in the parameters but that didn't work either.</p>

<pre><code>model.similarity(u'president',u'minister')
</code></pre>

<p>I'm pretty sure it is a huge corpus and should have these words and not sure why I'm not getting the results. I also tried some other common words such as <code>weapon</code>, military, car, etc. but a same error message.</p>
",2019-07-23 05:28:21,2019-07-23 05:28:21,"raise KeyError(""word '%s' not in vocabulary"" % word) for gensim model",<python><gensim><word2vec><similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22071,57157390,2019-07-23 05:44:44,,"<p>I have a large corpus (~100 million documents, 59GB) in a CSV. I want to create a TF-IDF vector and do some feature engineering on the data, but it's too large to load into memory all at once (I'm working on Google Colab, GPU with 12GB RAM). I imagine there is a way to process the data in chunks and then combine the TF-IDFs at the end but I'm not sure how to proceed. Here's my code so far:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

chunks = pd.read_csv(""data.csv.bz2"", 
                     chunksize=1000000,
                     nrows=120000000,
                    )

print(type(chunks))  # &lt;class 'pandas.io.parsers.TextFileReader'&gt;
</code></pre>

<p>Then, after removing stopwords and punctuation, lemmatizing (WordNetLemmatizer()), and stemming (SnowballStemmer('english')):</p>

<pre><code>count_vectorizer = CountVectorizer()
chunk1_counts = count_vectorizer.fit_transform(chunk1.comment)

tfidf_transformer = TfidfTransformer()
chunk1_tfidf = tfidf_transformer.fit_transform(chunk1_counts)
</code></pre>

<p>I can read in a few chunks at a time, but to avoid memory errors I'll probably have to save the results to disk and delete the objects from memory before processing the next set of chunks.</p>

<p>At that point, what's the process to combine the multiple TF-IDFs?</p>
",,2019-07-23 05:44:44,NLP Combining multiple TF-IDF matrices,<python><scikit-learn><gensim><tf-idf><tfidfvectorizer>,,,CC BY-SA 4.0,False,False,True,False,True
22080,57090378,2019-07-18 08:38:12,,"<p>Is Google's pretrained word2vec model CBO or skipgram.</p>

<p>We load pretrained model by:</p>

<pre><code>from gensim.models.keyedvectors as word2vec

model= word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz')
</code></pre>

<p>How can we specifically load pretrained CBOW or skipgram model ?</p>
",2019-07-18 08:39:38,2019-07-18 18:04:27,Is Google word2vec pertrained model CBOW or skipgram,<python-3.x><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22081,57090689,2019-07-18 08:56:50,,"<p>I have two lists (say  list A and list B) of texts. I want to find most similar texts in list B for each text in list A.
This I want to do using bag of words and later cosine similarity.</p>

<p>I created a dictionary using gensim</p>

<pre><code>tokensA = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listA]
tokensB = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listB]
# Create dictionary
dictionary = corpora.Dictionary(tokensA)
</code></pre>

<p>Then I obtained the occurences of words for each text in list B,</p>

<pre><code>mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokensB]

[[(9, 1), (25, 1), (611, 1), (627, 1), (1917, 1), (1918, 1)],
[(9, 1), (25, 1), (627, 1), (1918, 1), (1919, 1), (1920, 1)],...
</code></pre>

<p>How can I convert this to vectors so that I can do a cosine_similarity?</p>
",,2019-07-18 08:56:50,How to create one hot vectors to find similarity of texts after gensim doc2bow implementation?,<nlp><gensim><cosine-similarity><one-hot-encoding>,,,CC BY-SA 4.0,False,False,True,False,False
22095,57125117,2019-07-20 13:17:11,,"<p>I have about 9000 documents and I am using Gensim's <code>doc2vec</code> to embed my documents. My code is as follows:</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I would like to get all the documents related to topic ""deep learning"". i.e. the documents that mainly have content related to deep learning. Is it possible to do this in doc2vec model in gensim?</p>

<p>I am happy to provide more details if needed.</p>
",,2019-07-20 18:56:36,How to get document vectors for a given topic in gensim,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22096,57125306,2019-07-20 13:42:09,,"<p>I am trying to apply word embedding on tweets. I was trying to create a vector for each tweet by taking the average of the vectors of the words present in the tweet as follow:</p>

<pre><code>def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model_w2v[word].reshape((1, size))
            count += 1.
        except KeyError: # handling the case where the token is not in vocabulary

            continue
    if count != 0:
        vec /= count
    return vec
</code></pre>

<p>Next, when I try to Prepare word2vec feature set as follow:</p>

<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 200))
#the length of the vector is 200

for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)

wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape
</code></pre>

<p>I get the following error inside the loop:</p>

<blockquote>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-72aee891e885&gt; in &lt;module&gt;
      4 # wordvec_arrays.reshape(1,200)
      5 for i in range(len(tokenized_tweet)):
----&gt; 6     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)
      7 
      8 wordvec_df = pd.DataFrame(wordvec_arrays)

&lt;ipython-input-31-9e6501810162&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v.wv.__getitem__(word).reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

ValueError: cannot reshape array of size 3800 into shape (1,200)
</code></pre>
</blockquote>

<p>I checked all the available posts in stackOverflow but non of them really helped me.</p>

<p>I tried reshaping the array and it still give me the same error. </p>

<p>My model is:</p>

<pre><code>tokenized_tweet = df['tweet'].apply(lambda x: x.split()) # tokenizing

model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(df['tweet']), epochs=20)
</code></pre>

<p>any suggestions please? </p>
",2019-07-20 20:29:34,2019-07-22 17:09:54,"ValueError: cannot reshape array of size 3800 into shape (1,200)",<python><deep-learning><tokenize><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22099,57107945,2019-07-19 07:53:23,,"<p>When trying to load  <code>GoogleNews-vectors-negative300.bin</code> with pytorch <code>Vector</code> <a href=""https://torchtext.readthedocs.io/en/latest/vocab.html#vectors"" rel=""nofollow noreferrer"">struct</a> I am getting </p>

<blockquote>
  <p>ValueError: could not convert string to float: b'\x00\x00\x94:\x00\x00k\xba\x00\x00\x</p>
</blockquote>

<p>I have tried this <a href=""https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings"">post</a> (@robodasha) but without success. My goal is to build a vocabulary with the loaded embedding using <code>build_vocab</code> Any suggestions? </p>
",2019-10-04 16:25:34,2019-10-04 16:26:24,Building dictionary with GoogleNews-vectors-negative300.bin returns ValueError: could not convert string to float,<python><pytorch><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22110,57125757,2019-07-20 14:44:18,,"<p>I need to process a large number of <code>txt</code> files for building a <code>word2vec</code> model.
Now, my txt-files are a bit messy and I need to remove all <code>\n</code> newlines, read all sentences from my loaded string (txt-file) and then tokenize each sentence for using the word2vec model.</p>

<p>The thing is: I cant read the files line-by-line, cause some sentences do not end after one line. Therefore, I use <code>nltk.tokenizer.tokenize()</code>, which splits the file into sentences.</p>

<blockquote>
  <p>I cant figure out, how to convert a list of strings into a list of list, where each sub-list contains the sentences, while passing it thourgh a generator.</p>
</blockquote>

<p>Or do I actually need to save each sentences into a new file (one sentence per line) to pass it through a generator?</p>

<p>Well, my code looks like this:
<code>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')</code></p>

<pre><code># initialize tokenizer for processing sentences

class Raw_Sentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for file in file_loads: ## Note: file_loads includes directory name of files (e.g. 'C:/Users/text-file1.txt')
            with open(file,'r', encoding='utf-8') as t:     
               # print(tokenizer.tokenize(t.read().replace('\n', ' ')))           
                storage = tokenizer.tokenize(t.read().replace('\n', ' '))
# I tried to temporary store the list of sentences to a list for an iteration
                for sentence in storage:
                    print(nltk.word_tokenize(sentence))
                    yield nltk.word_tokenize(sentence)
</code></pre>

<p>So the goal is: 
load file 1: <code>'some messy text here. And another sentence'</code>
 tokenize into sentences <code>['some messy text here','And another sentence']</code>
and then split each sentence into words <code>[['some','messy','text','here'],['And','another','sentence']]</code></p>

<p>load file 2: <code>'some other messy text. sentence1. sentence2.'</code>
etc.</p>

<p>and input sentences into word2vec model:
<code>sentences = Raw_Sentences(directory)</code></p>

<p><code>model = gensim.models.Word2Vec(sentences)</code></p>
",,2019-07-20 17:16:02,Training word2vec model streaming data from file and tokenize to sentence,<python><streaming><nltk><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22112,57198286,2019-07-25 09:11:09,,"<p>I have created word embeddings (Word2vec) using my own dataset. I have used Gensim module to create word embeddings. I want to evaluate my word embeddings.</p>

<p>I have used Wordsim353 dataset to evaluate word embeddings. The following code Shows the result of Evaluation. </p>

<p>Code:</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

print(similarities)
</code></pre>

<p>Result:</p>

<pre><code>((0.09410256722489568, 0.3086953732794174), SpearmanrResult(correlation=0.06101508426787973, pvalue=0.5097769955392246), 66.28895184135978)
</code></pre>

<p>How can I interprete the result?</p>

<p>Please help me to interprete the results.</p>
",2019-07-25 09:37:42,2019-07-25 11:07:01,Interepretation of word2vec evaluation result,<word2vec><evaluation><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22116,57179502,2019-07-24 09:23:30,,"<p>All in all I need to run multiple word2vec over a period of time. For example I will be running word2vec once every month. To reduce computing workload I would like to run word2vec only on the data that was accumulated during the last month. My problem stems from the fact that for further processing I require the embeddings from the models I ran in previous months.</p>

<p>I know, also from reading other posts, that if the individual word2vec models are run on different samples which each are not a representative sample of an overarching corpus, obtaining word embeddings that are comparable is not possible. I have a similar problem, where I am analysing network data, which evolves over time (effectively doing a kind of graph2vec, but analysing node behaviour).</p>

<p>Yet I've been wondering if comparable embeddings can be achieved using PCA as follows:</p>

<ul>
<li>all models create ""node"" embeddings of length x</li>
<li>for each model: 

<ul>
<li>run PCA on the ""node"" embeddings and retain all x principal        components, whereby whitening is enabled</li>
<li>transform the individual ""node"" embeddings to their corresponding PCA coordinates</li>
</ul></li>
<li>since the individual samples used to train the individual models share a high proportion of nodes as existing ones tend to stay and
new ones are likely to be added do the following:

<ul>
<li>append all pca-transformed embeddings into one database</li>
<li>by nodeID calculate mean pca_transformed embedding</li>
</ul></li>
</ul>

<p>This would only work, if the PCA transformation of the embeddings of each model ensures that the resulting embeddings measure the ""same thing"". For example, the first principal component of each PCA should capture the same kind of information, etc. And that's what I'm not sure about.</p>
",2019-07-24 09:30:29,2019-07-24 19:14:06,Are Principal Components of different word2vec models measuring the same thing?,<math><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22133,57200067,2019-07-25 10:41:26,,"<p>I have a code that loads a file, strips each sentence and then removes some stopwords and returns the tokens.</p>

<p>So far so good.. If I include a <code>print()</code> statement or do a simple example, I see that stopwords are removed BUT..
when I run the sentences in my word2vec model, the model still creates a wordvector for stopwords like 'the' .. is there an error in my code??</p>

<pre><code>class Raw_Sentences(object):

    def __init__(self, dirname):
        self.dirname = dirname
    def __iter__(self):
        for file in file_loads: # list with the according file names e.g. 'Users/file1.txt'
                       with open(file,'r', buffering=20000000, encoding='utf-8') as t:     
                for sentence in tokenizer.tokenize(t.read().replace('\n', ' ').lower()):
                    sent = remove_stopwords(sentence)
                    print(sent)
                    yield gensim.utils.simple_preprocess(sent, deacc=True)
</code></pre>

<p>Then I run:</p>

<pre><code>sentences = Raw_Sentences(directory)
num_features = 200  
min_word_count = 2 
num_workers = cpu_count()
context_size = 4  
downsampling = 1e-5  
seed = 2 

model = gensim.models.Word2Vec(sentences,
                               sg=1, #skip-gram
                               seed=seed,
                               workers=num_workers,
                               size=num_features,
                               min_count=min_word_count,
                               window=context_size,
                               sample=downsampling)     

model.most_similar('the')
</code></pre>

<p>and it returns similar words.. But the word 'the' should have been removed...</p>

<p>crying out loud</p>

<p><code>remove_stopwords</code> is a gensim function <code>from gensim.parsing.preprocessing import remove_stopwords</code>  which takes a set of stopwords <code>stoplist = set(stop_words)</code> and removes them <code>def remove_stopwords(s):       ## del 
    s = utils.to_unicode(s)
    return "" "".join(w for w in s.split() if w not in stoplist)</code></p>
",,2019-07-25 17:20:22,Code removes stopwords but Word2vec still creates wordvector for stopword?,<python><nltk><gensim><stop-words>,,,CC BY-SA 4.0,True,False,True,False,False
22136,57148357,2019-07-22 14:38:05,,"<p>I am building the vocabulary table using Doc2vec, but there is an error ""AttributeError: module 'gensim.utils' has no attribute 'smart_open'"". How do I solve this?</p>

<p>This is for a notebook on Databricks platform, running in Python 3. In the past, I've tried on running the code on a local Jupyter Notebook but the same error occurred.</p>

<p>I've also searched <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> but could not find anything related to smart_open.</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(window=5, min_count=1, size=50, sample=1e-5, negative=5, workers=1)

model.build_vocab(sentences.to_array())
</code></pre>

<p>I ran the above lines separately. The first line worked fine. The second says:
 AttributeError: module 'gensim.utils' has no attribute 'smart_open'</p>
",2019-07-22 18:14:14,2020-07-25 14:49:35,AttributeError: module 'gensim.utils' has no attribute 'smart_open',<python><gensim><databricks><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22157,57189149,2019-07-24 18:26:00,,"<p>I try to compare texts in form of 'text-files' concerning their content.<br>
<strong>e.g.</strong>: I got 100 texts about animals and I want to analyze each text about what animals it discusses.<br>
I am looking for an analysis output like: <code>doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...</code></p>

<p>I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.</p>

<p>Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? </p>

<p>Also I've seen a tutorial manipulating the topics via the <code>eta-value</code> in <code>gensim</code> but I don't know how I could use this in my favor. </p>

<p>I am grateful for any advice that can lead me to the right direction. Thanks!</p>
",2019-07-24 20:22:56,2019-07-29 12:34:35,How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?,<python><text-mining><gensim><text-classification><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22176,57244699,2019-07-28 20:25:02,,"<p>I am trying to learn word2vec.</p>

<p>I am using the code below to load the Google pre-trained word2vec model in Python 3. But I am unsure how to turn a list such as :[""I"", ""ate"", ""apple""] to a list of vectors (ie how to get vectors from this model?).</p>

<pre><code>import nltk
import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
",,2019-07-28 22:46:18,How to turn a list of words into a list of vectors using a pre-trained word2vec model(Google)?,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22177,57329913,2019-08-02 15:55:33,,"<p>I am testing with Word2Vec to find words that have the same meaning, so far it is going great as the list of positive words is accurate. However, I would like to know where each positive word was found, as in which document.</p>

<p>I tried to iterate each document and compare each word with the list of positive words, something like this:</p>

<pre><code>for i in documents: # iterating the documents
    for j in i: # iterating the words in the document
        for k in similar_words: # iterating the positive words
            if k[0] in j: # k[0] is the positive word, k[1] is the positive value
                print('found word')
</code></pre>

<p>This works fine. However, with this, the positive words are actually stemmed down, that is why I am using ""in"". So let's say the stemmed down positive word is 'ice', many words contain the phrase 'ice' in them, and maybe more than one are in the document and only one of them is the real positive word.</p>

<p>Is there a way to avoid stemming words when using Word2Vec? Or is there a way to find the document number of each positive word found?</p>

<p><strong>UPDATE</strong></p>

<p>Here is my code for training the model and using 'most_similar()'</p>

<pre><code>def remove_stopwords(texts):
    # Removes stopwords in a text
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]


def sent_to_words(sentences):
    # Tokenize each sentence into a list of words and remove unwanted characters
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))


df = pd.read_excel('my_file.xlsx')
df.columns = map(str.lower, df.columns)

data = df['Comment Section'].values.tolist()

# Remove the new line character and single quotes
data = [re.sub(r'\s+', ' ', str(sent)) for sent in data]
data = [re.sub(""\'"", """", str(sent)) for sent in data]

# Convert our data to a list of words. Now, data_words is a 2D array,
# each index contains a list of words
data_words = list(sent_to_words(data))

# Remove the stop words
data_words_nostops = remove_stopwords(data_words)

model = gensim.models.Word2Vec(
            data_words_nostops,
            alpha=0.1,
            min_alpha=0.001,
            size=250,
            window=1,
            min_count=2,
            workers=10)

model.train(data_words_nostops, total_examples=len(data_words_nostops), epochs=10)

print(model.wv.vocab) # At this step, the words are not stemmed

positive = ['injuries', 'fail', 'dangerous', 'oil']

negative = ['train', 'westward', 'goods', 'calgary', 'car', 'automobile', 'appliance']

similar_words_size = array_length(model.wv.most_similar(positive=positive, negative=negative, topn=0))

for i in model.wv.most_similar(positive=positive, negative=negative, topn=similar_words_size):
    if len(i[0]) &gt; 2:
        risks.append(i)
        print(risks) # At this step, the words are stemmed
</code></pre>
",2019-08-06 17:44:16,2019-08-06 17:44:16,How to find where a positive word is in a set of documents after using Word2Vec?,<python><machine-learning><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22198,57297194,2019-07-31 18:49:40,,"<p>I'm new to NLP and gensim, currently trying to solve some NLP problems with gensim word2vec module. I my current understanding of word2vec, the result vectors/matrix should have all entries between -1 and 1. However, trying a simple one results into a vector which has entries greater than 1. I'm not sure which part is wrong, could anyone give some suggestions, please?</p>

<p>I've used gensim utils.simple_preprocess to generate a list of list of token. The list looks like: </p>

<pre><code>[['buffer', 'overflow', 'in', 'client', 'mysql', 'cc', 'in', 'oracle', 'mysql', 'and', 'mariadb', 'before', 'allows', 'remote', 'database', 'servers', 'to', 'cause', 'denial', 'of', 'service', 'crash', 'and', 'possibly', 'execute', 'arbitrary', 'code', 'via', 'long', 'server', 'version', 'string'], ['the', 'xslt', 'component', 'in', 'apache', 'camel', 'before', 'and', 'before', 'allows', 'remote', 'attackers', 'to', 'read', 'arbitrary', 'files', 'and', 'possibly', 'have', 'other', 'unspecified', 'impact', 'via', 'an', 'xml', 'document', 'containing', 'an', 'external', 'entity', 'declaration', 'in', 'conjunction', 'with', 'an', 'entity', 'reference', 'related', 'to', 'an', 'xml', 'external', 'entity', 'xxe', 'issue']]
</code></pre>

<p>I believe this is the correct input format for gensim word2vec.</p>

<pre><code>word2vec = models.word2vec.Word2Vec(sentences, size=50, window=5, min_count=1, workers=3, sg=1)
vector = word2vec['overflow']
print(vector)
</code></pre>

<p>I expect the output to be a vector containing probabilities (i.e., all between -1 and 1), but it actually turned out to be the following:</p>

<pre><code>[ 0.12800379 -0.7405527  -0.85575     0.25480416 -0.2535793   0.142656
 -0.6361196  -0.13117172  1.1251501   0.5350017   0.05962601 -0.58876884
  0.02858278  0.46106443 -0.22623934  1.6473309   0.5096218  -0.06609935
 -0.70007527  1.0663376  -0.5668168   0.96070313 -1.180383   -0.58649933
 -0.09380565 -0.22683378  0.71361005  0.01779896  0.19778453  0.74370056
 -0.62354785  0.11807996 -0.54997736  0.10106519  0.23364201 -0.11299669
 -0.28960565 -0.54400533  0.10737313  0.3354464  -0.5992898   0.57183135
 -0.67273194  0.6867607   0.2173506   0.15364875  0.7696457  -0.24330224
  0.46414775  0.98163396]
</code></pre>

<p>You can see there are <code>1.6473309</code> and <code>-1.180383</code> in the above vector.</p>
",2019-07-31 18:55:19,2019-07-31 22:12:07,gensim word2vec entry greater than 1,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22201,57298877,2019-07-31 21:10:19,,"<p>i want to do in python 3.7.4:</p>

<p>and getting this Error:</p>

<p>i already tried: </p>

<p>using <code>conda</code> and <code>pip</code></p>

<p>using local windows and windows server </p>

<p>multiple reinstallments of diffenent versions of packages (e.g. <code>numpy</code> and <code>scipy</code>)</p>

<pre><code>from gensim.models import Word2Vec 
</code></pre>

<blockquote>
  <p>Traceback (most recent call last):
    File ""c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py"", line 1, in 
      import gensim
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim__init__.py"", line 5, in 
      from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing__init__.py"", line 4, in 
      from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing\preprocessing.py"", line 42, in 
      from gensim import utils
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\utils.py"", line 40, in 
      import scipy.sparse
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse__init__.py"", line 230, in 
      from .csr import *
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\csr.py"", line 13, in 
      from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,
  ImportError: DLL load failed: The specified module could not be found.
  PS C:\Users\Administrator\Documents\GitHub\contract-criteria-identifier-on-aws> &amp; C:/Users/Administrator/AppData/Local/Programs/Python/Python37/python.exe c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py
  Traceback (most recent call last):
    File ""c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py"", line 1, in 
      import gensim
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim__init__.py"", line 5, in 
      from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing__init__.py"", line 4, in 
      from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing\preprocessing.py"", line 42, in 
      from gensim import utils
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\utils.py"", line 40, in 
      import scipy.sparse
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse__init__.py"", line 230, in 
      from .csr import *
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\csr.py"", line 13, in 
      from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,
  ImportError: DLL load failed: The specified module could not be found.</p>
</blockquote>
",2019-07-31 21:22:23,2019-07-31 22:18:52,How to install gensim and run package in python?,<python><machine-learning><pip><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22204,57264086,2019-07-30 03:56:34,,"<p>I have a set of strings that I am tokenizing. I am sending each string into the <code>word2vec</code> model in <code>gensim</code>. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (<code>list</code> of <code>list</code> in <code>python</code>). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  </p>

<p>I am sending this 3D into a model in <code>Tensorflow</code> library. I am doing the following,</p>

<p><code>model.add(Embedding(max_features, 128, input_length=maxlen))</code></p>

<p>Here max_features is the size of the token vector i.e. 100 and input_length is also the same. </p>

<p>But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model.</p>
",,2019-07-30 19:32:43,How to combine 3D token embeddings into 2D vectors?,<python><tokenize><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22210,57335044,2019-08-03 02:26:38,,"<p>I am trying to understand how to use Tensorflow2 to train word embeddings without the preset labels.</p>

<p>In the Tensorflow2 tutorial (<a href=""https://www.tensorflow.org/beta/tutorials/text/word_embeddings"" rel=""nofollow noreferrer"">https://www.tensorflow.org/beta/tutorials/text/word_embeddings</a>) it shows how to train word embeddings using pre-structured dataset with labels.</p>

<pre><code>imdb = keras.datasets.imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=vocab_size)

embedding_dim=16

model = keras.Sequential([
layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),
layers.GlobalAveragePooling1D(),
layers.Dense(16, activation='relu'),
layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
          loss='binary_crossentropy',
          metrics=['accuracy'])

history = model.fit(
    train_data,
    train_labels,
    epochs=30,
    batch_size=512,
    validation_data=(test_data, test_labels))
</code></pre>

<p>However, I wonder how to train - with Tensorflow2 - the embeddings on the non-labeled text, similar to what can be done with Gensim's Word2Vec?</p>
",,2019-09-01 03:05:09,Word embeddings in Tensorflow2,<nlp><gensim><word2vec><word-embedding><tensorflow2.0>,,,CC BY-SA 4.0,False,False,True,False,False
22213,57316991,2019-08-01 21:07:17,,"<pre><code>model.similar_by_vector(model['king'] - model['man'] + model['woman'], topn=1)[0]
</code></pre>

<p>Results in </p>

<pre><code>('king', 0.8551837205886841)
</code></pre>

<p>Whereas </p>

<pre><code>model.most_similar(positive=['king', 'queen'], negative=['man'], topn=1)[0]
</code></pre>

<p>Gives a different answer (the one you'd expect)</p>

<p><code>('monarch', 0.6350384950637817)</code></p>

<p>But I'd expect both of these to return the same thing. Am I misunderstanding how vector math should be performed on these vectors?</p>
",,2019-08-01 22:02:28,Why do passing 'positive' and 'negative' parameters into gensim's most_similar function not return the same as the vector math results?,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22233,57337791,2019-08-03 11:13:10,,"<p>I am following a tutorial here: <a href=""https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568</a></p>

<p>I am at the part ""Word2vec and Logistic Regression"". I have downloaded the ""GoogleNews-vectors-negative300.bin.gz"" file and I am tyring to apply it to my own text data. However when I get to the following code:</p>

<pre><code>%%time
from gensim.models import Word2Vec

wv = gensim.models.KeyedVectors.load_word2vec_format(""/data/users/USERS/File_path/classifier/GoogleNews_Embedding/GoogleNews-vectors-negative300.bin.gz"", binary=True)
wv.init_sims(replace=True)
</code></pre>

<p>I run into the following error:</p>

<pre><code>/data/users/msmith/env/lib64/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
---------------------------------------------------------------------------
EOFError                                  Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

~/env/lib64/python3.6/site-packages/gensim/models/keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
   1492         return _load_word2vec_format(
   1493             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,
-&gt; 1494             limit=limit, datatype=datatype)
   1495 
   1496     def get_keras_embedding(self, train_embeddings=False):

~/env/lib64/python3.6/site-packages/gensim/models/utils_any2vec.py in _load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    383                 with utils.ignore_deprecation_warning():
    384                     # TODO use frombuffer or something similar
--&gt; 385                     weights = fromstring(fin.read(binary_len), dtype=REAL).astype(datatype)
    386                 add_word(word, weights)
    387         else:

/usr/lib64/python3.6/gzip.py in read(self, size)
    274             import errno
    275             raise OSError(errno.EBADF, ""read() on write-only GzipFile object"")
--&gt; 276         return self._buffer.read(size)
    277 
    278     def read1(self, size=-1):

/usr/lib64/python3.6/_compression.py in readinto(self, b)
     66     def readinto(self, b):
     67         with memoryview(b) as view, view.cast(""B"") as byte_view:
---&gt; 68             data = self.read(len(byte_view))
     69             byte_view[:len(data)] = data
     70         return len(data)

/usr/lib64/python3.6/gzip.py in read(self, size)
    480                 break
    481             if buf == b"""":
--&gt; 482                 raise EOFError(""Compressed file ended before the ""
    483                                ""end-of-stream marker was reached"")
    484 

EOFError: Compressed file ended before the end-of-stream marker was reached
</code></pre>

<p>Any idea whats gone wrong/ how to overcome this issue?</p>

<p>Thanks in advance!</p>
",,2019-08-03 11:13:10,Word2Vec error when loading in GoogleNews data,<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22258,57373626,2019-08-06 10:05:48,,"<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>I am trying to find similar documents for these 400 datasets using gensim doc2vec. The paper ""Distributed Representations of Sentences and Documents"" says that ""The combination of PV-DM and PV-DBOW often work consistently better (7.42% in IMDB) and therefore recommended.""</p>

<p>So I would like to combine the vectors of these two methods and find cosine similarity with all the train documents and select the top 5 with the least cosine distance.</p>

<p>So what's the effective method to combine the vectors of these 2 methods: adding or averaging or any other method ???</p>

<p>After combining these 2 vectors I can normalise each vector and then find the cosine distance.</p>
",,2019-08-06 20:22:26,How to combine vectors generated by PV-DM and PV-DBOW methods of doc2vec?,<python><nlp><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22274,57283636,2019-07-31 05:14:40,,"<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>At present I am removing those 400 documents and using remaining 19600 documents for training the doc2vec. Then I extract the vectors of train and test data. Now for each test data document, I find it's cosine distance with all the 19600 train documents and select the top 5 with least cosine distance. If the similar document marked is present in these top 5 then take it to be accurate. Accuracy% = No. of Accurate records / Total number of Records.</p>

<p>The other way I find similar documents is by using the doc2Vec most similiar method. Then calculate accuracy using the above formula.</p>

<p>The above two accuracy doesn't match. With each epoch one increases other decreases.</p>

<p>I am using the code given here: <a href=""https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"" rel=""nofollow noreferrer"">https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a>. For training the Doc2Vec.</p>

<p>I would like to know how to tune the hyperparameters so that I can get making accuracy by using above-mentioned formula. Should I use cosine distance to find the most similar documents or shall I use the gensim's most similar function?</p>
",,2019-07-31 21:58:09,How to effectively tune the hyper-parameters of Gensim Doc2Vec to achieve maximum accuracy in Document Similarity problem?,<python><nlp><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22276,57391090,2019-08-07 09:24:38,,"<p>Distributed Online LDA is implemented by <a href=""https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda"" rel=""nofollow noreferrer"">Spark MLlib</a> and <a href=""https://radimrehurek.com/gensim/dist_lda.html"" rel=""nofollow noreferrer"">Gensim</a>. I would like to choose one of them to do my project.</p>

<p>At present, I already have a hadoop system running with 7 worker nodes.</p>

<p>Could someone having experience with both give a recommendation and point out pros and cons of them, e.g. the difficulty of cluster setup, speed of the modelling process, etc. Thanks.</p>
",2019-08-07 12:02:36,2019-08-07 12:02:36,Comparison between Distributed Online LDA implemented by PySpark and Gensim,<pyspark><nlp><apache-spark-mllib><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
22294,57412511,2019-08-08 12:23:40,,"<p>I was trying to use Gensim to import GoogelNews-pretrained model on some English words (sampled 15 ones here only stored in a txt file with each per line, and there are no more context as corpus). Then I could use ""model.most_similar()"" to get their similar words/phrases for them. But actually the file loaded from Python-Pickle method couldn't be used for gensim-built-in <code>model.load()</code> and <code>model.most_similar()</code> function directly. </p>

<p>how should I do to cluster the 15 English words (and more in the future), since I couldn't train and save and load a model  from the beginning?</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_WORD2VEC_MODEL = '../GoogleNews-vectors-negative300.bin'

GOOGLE_ENGLISH_WORD_PATH = '../testwords.txt'

GOOGLE_WORD_FEATURE = '../word.google.vector'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, binary=True) 

word_vectors = {}

#load 15 words as a test to word_vectors

with open(GOOGLE_ENGLISH_WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None
try:
    import cPickle
except :
    import _pickle as cPickle

def save_model(clf,modelpath): 
    with open(modelpath, 'wb') as f: 
        cPickle.dump(clf, f) 

def load_model(modelpath): 
    try: 
        with open(modelpath, 'rb') as f: 
            rf = cPickle.load(f) 
            return rf 
    except Exception as e:        
        return None 

for word in word_vectors:
    try:
        v= model[word]
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,GOOGLE_WORD_FEATURE)

words_set = load_model(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)
</code></pre>

<blockquote>
<pre><code>---------------error message--------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-86c15e366696&gt; in &lt;module&gt;
----&gt; 1 words_set.most_similar(""knit"", topn=3)

AttributeError: 'dict' object has no attribute 'most_similar'
---------------error message--------
</code></pre>
</blockquote>
",2019-08-08 14:28:21,2019-08-08 18:06:42,Gensim built-in model.load function and Python Pickle.load file,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22299,57391750,2019-08-07 09:59:35,,"<p>I have 250k text documents (tweets and newspaper articles) represented as vectors obtained with a doc2vec model. Now, I want to use a regressor (multiple linear regression) to predict continuous value outputs - in my case the UK Consumer Confidence Index. 
My code runs, since forever. What am I doing wrong?</p>

<p>I imported my data from Excel and splitted it into x_train and x_dev. The data are composed of preprocessed text and CCI continuous values. </p>

<pre><code># Import doc2vec model
dbow = Doc2Vec.load('dbow_extended.d2v')
dmm = Doc2Vec.load('dmm_extended.d2v')
concat = ConcatenatedDoc2Vec([dbow, dmm]) # model uses vector_size 400

def get_vectors(model, input_docs):
    vectors = [model.infer_vector(doc.words) for doc in input_docs]
    return vectors

# Prepare X_train and y_train
train_text = x_train[""preprocessed_text""].tolist()
train_tagged = [TaggedDocument(words=str(_d).split(), tags=[str(i)]) for i, _d in list(enumerate(train_text))]
X_train = get_vectors(concat, train_tagged)
y_train=x_train['CCI_UK']

# Fit regressor 
from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit(X_train, y_train)

# Predict and evaluate
prediction=reg.predict(X_dev)
print(classification_report(y_true=y_dev,y_pred=prediction),'\n')
</code></pre>

<p>Since the fitting never completed, I wonder whether I am using a wrong input. However, no error message is shown and the code simply runs forever. What am I doing wrong?</p>

<p>Thank you so much for your help!!</p>
",2019-08-07 11:01:20,2019-08-07 12:29:43,How to use Sklearn linear regression with doc2vec input,<scikit-learn><linear-regression><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
22348,57226900,2019-07-26 20:54:44,,"<p>I have a quite long text document describing behaviours of different animals. I want to extract text about a specific animal and haven't figured out how this can be done.</p>

<p>So for example, if the document descibes 15 different animals, I want my alorithm to output all information from the input file that related to lions. Lions described and discussed in several different places of the document - how do I do ""selective extraction"" for text that is only related to lions, does anyone know?</p>

<h1>EDIT - inputs and outputs</h1>

<p>Inputs:
(1) Text file (e.g. ""document.txt"")
(2) Key word(s) (e.g. ""lion"")</p>

<p>Output (example):
""Lions are large felines that are traditionally depicted as the 'king of the jungle.' These big cats once roamed Africa, Asia and Europe. [...] Males are generally larger than females and have a distinctive mane of hair around their heads [...] Asiatic lions eat large animals as well, such as goats, nilgai, chital, sambhar and buffaloes. [...] Females have a gestation period of around four months. She will give birth to her young away from others and hide the cubs for the first six weeks of their lives.""</p>
",2019-07-26 21:04:42,2019-07-28 01:30:55,Selective text extraction in Python based on certain topics or keywords,<python><nlp><nltk><gensim><pythonanywhere>,,,CC BY-SA 4.0,True,False,True,False,False
22356,57457214,2019-08-12 07:35:16,,"<p>I'm using LDA with gensim for topic modeling. My data has 23 documents and I want separate topics/words for each document but gensim is giving topics for entire set of documents together. How to get it for  individual docs?</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using 
#dictionary prepared above.

corpus = [dictionary.doc2bow(doc) for doc in doc_clean]


# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel

# Running and Trainign LDA model on the document term matrix.
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)

result=ldamodel.print_topics(num_topics=3, num_words=3)
</code></pre>

<p>This is the output I'm getting:</p>

<pre><code>[(0, '0.011*""plex"" + 0.010*""game"" + 0.009*""racing""'),
(1, '0.008*""app"" + 0.008*""live"" + 0.007*""share""'),
(2, '0.015*""device"" + 0.009*""file"" + 0.008*""movie""')]
</code></pre>
",,2019-08-12 10:53:42,How can I print document wise topics in Gensim?,<python><nltk><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
22373,57461705,2019-08-12 12:59:41,,"<p>I trained fasttext or Sen2vec, or word2vec model for my news collection in csv file, were each news have one line like that</p>

<pre><code>0 Trump is a liar.....
1 Europa going for brexit.....
2 Russia is no more world power......
</code></pre>

<p>So, I got trained model and now I can happily get vectors for any line in my csv file like that 
(fasttext)</p>

<pre><code>import csv  
import re

train = open('tweets.train3','w')  
test = open('tweets.valid3','w')  
with open(r'C:\Users\123\Desktop\data\osn-9.csv', mode='r', encoding = ""utf- 
 8"" ,errors='ignore') as csv_file:  
csv_reader = csv.DictReader(csv_file, fieldnames=['sen', 'text'])
line = 0
for row in csv_reader:
    # Clean the training data
    # First we lower case the text
    text = row[""text""].lower()
    # remove links
    text = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','',text)
    #Remove usernames
    text = re.sub('@[^\s]+','', text)
    text = ' '.join(re.sub(""[\.\,\!\?\:\*\(\)\;\-\=]"", "" "", text).split())
    # replace hashtags by just words
    text = re.sub(r'#([^\s]+)', r'\1',  text)
    #correct all multiple white spaces to a single white space
    text = re.sub('[\s]+', ' ', text)
    # Additional clean up : removing words less than 3 chars, and remove 
    space at the beginning and teh end
    text = re.sub(r'\W*\b\w{1,3}\b', '', text)
    text = text.strip()
    line = line + 1
    # Split data into train and validation
    if line &gt; 8416:
        print(f'__label__{row[""sen""]} {text}', file=test)
    else:
        print(f'__label__{row[""sen""]} {text}', file=train)
 import fasttext
 hyper_params = {""lr"": 0.1,
""epoch"": 500,
""wordNgrams"": 2,
""dim"": 100,
""loss"":""softmax""}


model = fasttext.train_supervised(input='tweets.train3',**hyper_params)
model.get_sentence_vector('Trump is a liar.....')
array([-0.20266785,  0.3407566 ,  ...,  0.03044436,  0.39055538], 
dtype=float32).
</code></pre>

<p>or like that
(gensim)</p>

<pre><code>In [10]:
model.infer_vector(['Trump', 'is', 'a ', 'liar'])
Out[10]:
array([ 0.24116205,  0.07339828, -0.27019867, -0.19452883,  0.126193  ,
 ........................,
    0.09754166,  0.12638392, -0.09281237, -0.04791372,  0.15747668],
  dtype=float32)
</code></pre>

<p>But how I can get vectors not as arrays for each line in my csv file? Like that</p>

<pre><code>0  Trump is a liar..... -0.20266785,  0.3407566 ,  ...,  0.03044436,  
1  Europa going for brexit..... 0.24116205,  0.07339828,.... -0.27019867
2  Russia is no more world power...... 0.12638392, -0.09281237 
 ...-0.04791372, 
</code></pre>

<p>Or like that</p>

<pre><code>0   -0.20266785,  0.3407566 ,  ...,  0.03044436,  
1   0.24116205,  0.07339828,.... -0.27019867
2   0.12638392, -0.09281237...-0.0479137
</code></pre>
",2019-08-12 17:29:56,2019-08-12 17:52:26,How to get doc2vec or sen2vec trained vectors in readable (csv or txt) format linewise?,<python><arrays><vector><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22374,57528271,2019-08-16 16:41:01,,"<p>I would like to import a pre-trained <code>word2vec</code> dictionary (in binary format) into <code>spacy</code> for vectorizing some text</p>

<p>I am able to import the vectors with <code>gensim</code> through:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim 
model = gensim.models.KeyedVectors.load_word2vec_format('PubMed- 
shuffle-win-2.bin', binary=True)
</code></pre>

<p>Then I initialize a blank spacy nlp object and get the words associated with each index:</p>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.blank('en')
keys = []
for idx in range(len(model.index2word)):
keys.append(model.index2word[idx])`
</code></pre>

<p>Then set the vectors for the nlp object:</p>

<pre class=""lang-py prettyprint-override""><code>nlp.vocab.vectors = spacy.vocab.Vectors(data=model.syn0, keys=keys)
</code></pre>

<p>I am able to get to this stage without any problems. However, I was wondering <strong><em>how to save this nlp object and load it again into spacy</em></strong> to vectorize new text as efficiently as possible</p>
",,2019-09-08 06:17:21,Import word2vec vectors in binary format into spacy,<python><nlp><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
22378,57475889,2019-08-13 10:30:07,,"<p>I found it was a failure that I had used Gensim with GoogleNews pre-trained model to cluster phrases like:</p>

<ul>
<li>knitting</li>
<li>knit loom</li>
<li>loom knitting</li>
<li>weaving loom</li>
<li>rainbow loom</li>
<li>home decoration accessories</li>
<li>loom knit/knitting loom</li>
<li>...</li>
</ul>

<p>I am advised that <a href=""https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim"">GoogleNews model does't have the phrases in it</a>. The phrases I have are a little specific to GoogleNews model while I don't have corpus to train a new model. I have only the phrases. And now I am considering to turn to BERT. But could BERT do that as I expected as above? Thank you.</p>
",,2019-08-13 11:30:49,Could I use BERT to Cluster phrases with pre-trained model,<tensorflow><nlp><pytorch><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22392,57496250,2019-08-14 14:00:39,,"<p>I am pretty new to doc2vec then I made small research and found a couple of things. Here is my story: I am trying to learn using doc2vec 2.4 million documents. At first, I tried only doing so with a small model of 12 documents. I checked the results with infer vector of the first document and found it to be similar indeed to the first document by 0.97-0.99 cosine similarity measure. Which I found good, even though when I tried to enter a new document of completely different words I received a high score of 0.8 measure similarity. However, I had put it aside and tried to go on and build the full model with the 2.4 million documents. In this point, my problems began. The result was complete nonsense, I received in the most_similar function results with a similarity of 0.4-0.5 which were completely different from the new document checked. I tried to tune parameters but no result yet. I tried also to remove randomness both from the small and big model, however, I still got different vectors. Then I had tried to use get_latest_training_loss on each epoch in order to see how the loss changes over each epoch. This is my code: </p>

<pre><code>model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, pretrained_emb="".../glove.840B.300D/glove.840B.300d.txt"", seed=1, workers=1, compute_loss=True)

workers=1, compute_loss=True)
model.build_vocab(documents)

for epoch in range(10):
    for i in range(model_glove.epochs):
        model.train(documents, total_examples = token_count, epochs=1)
        training_loss = model.get_latest_training_loss()
        print(""Training Loss: "" + str(training_loss))

    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha # fix the learning rate, no decay
</code></pre>

<p>I know this code is a bit awkward, but it is used here only to follow the loss. 
The error I receive is: </p>

<pre><code>AttributeError: 'Doc2Vec' object has no attribute 'get_latest_training_loss'
</code></pre>

<p>I tried looking at model. and auto-complete and found that indeed there is no such function, I found something similar name training_loss, but it gives me the same error. </p>

<p>Anyone here can give me an idea? </p>

<p>Thanks in Advance</p>
",,2019-08-14 18:26:04,'Doc2Vec' object has no attribute 'get_latest_training_loss',<python><gensim><doc2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
22395,57443879,2019-08-10 16:33:19,,"<p>I'm training a Doc2Vec model from the french wikipedia.</p>

<p>My code is based on this notebook :
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>It's actually in the training phase, but, I don't know how to vectorize new sentences after that.</p>

<p>Should I just use : model.infer_vector[""Example sentence here""] ?
But in this case, how to make the same processing than the Wikicorpus method does ? (This is not explained here : <a href=""https://radimrehurek.com/gensim/corpora/wikicorpus.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/wikicorpus.html</a>)</p>

<p>Thanks!</p>
",2019-08-10 16:40:01,2019-08-12 03:10:32,New sentence from doc2vec model trained with wikicorpus,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22415,57532018,2019-08-16 23:07:02,,"<p>I am using 24 cores virtual CPU and 100G memory to training Doc2Vec with Gensim, but the usage of CPU always is around 200% whatever to modify the number of cores.</p>
<pre><code>top
</code></pre>
<p><a href=""https://i.stack.imgur.com/1FgE9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1FgE9.png"" alt=""enter image description here"" /></a></p>
<pre><code>htop
</code></pre>
<p><a href=""https://i.stack.imgur.com/6jgCK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6jgCK.png"" alt=""enter image description here"" /></a></p>
<p>The above two pictures showed the percentage of cpu usage, this pointed out that cpu wasn't used efficiently.</p>
<pre><code>cores = multiprocessing.cpu_count()
assert gensim.models.doc2vec.FAST_VERSION &gt; -1, &quot;This will be painfully slow otherwise&quot;

simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode
    # window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
]

for model in simple_models:
    model.build_vocab(all_x_w2v)
    print(&quot;%s vocabulary scanned &amp; state initialized&quot; % model)

models_by_name = OrderedDict((str(model), model) for model in simple_models)
</code></pre>
<p>Edit:</p>
<p>I tried to use parameter corpus_file instead of documents, and resolved above problem. but, I need to adjust the code and convert all_x_w2v to file, and all_x_w2v didn't directly do this.</p>
",2020-06-20 09:12:55,2019-08-17 00:06:53,Not efficiently to use multi-Core CPU for training Doc2vec with gensim,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22432,57551270,2019-08-19 06:11:43,,"<p>When using gensim utils to preprocess text for NLP, the library makes a call to numpy and returns (inter alia) this error message</p>

<pre><code>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the multiarray numpy extension module failed.  Most
likely you are trying to import a failed build of numpy.
Here is how to proceed:
- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.
- If you are simply trying to use the numpy version that you have installed:
  your installation is broken - please reinstall numpy.
- If you have already reinstalled and that did not fix the problem, then:
  1. Check that you are using the Python you expect (you're using /Users/lorajohns/anaconda3/bin/python),
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy versions you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

     Note: this error has many possible causes, so please don't comment on
     an existing issue about this - open a new one instead.

Original error was: dlopen(/Users/$(USER)/anaconda3/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib
  Referenced from: /Users/$(USER)/anaconda3/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so
  Reason: image not found
</code></pre>

<p>I already tried fixing the path and my .bash profile, in case it was a case of competing python installations. I am using venvs, so I also uninstalled any additional virtual env extensions that could potentially cause a conflict (pyenv). I reinstalled all packages into a new <code>conda</code> env, and updated <code>conda</code> and <code>anaconda</code>.</p>

<p>Essentially, I followed all the instructions, and nothing succeeded.</p>
",2019-10-07 16:57:39,2019-10-07 16:57:39,numpy image not found when importing gensim in python 3.7,<numpy><scipy><anaconda><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22438,57426745,2019-08-09 09:01:22,,"<p>What I want exactly is to cluster words and phrases, e.g.
knitting/knit loom/loom knitting/weaving loom/rainbow loom/home decoration accessories/loom knit/knitting loom/...And I don'd have corpus while I have only the words/phrases. Could I use a pre-trained model like the one from GoogleNews/Wikipedia/... to realise it?</p>

<p>I am trying now to use Gensim to load GoogleNews pre-trained model to get phrases similarity. I've been told that The GoogleNews model includes vectors of phrases and words. But I find that I could only get word-similarity while phrase-similarity fails with an error message that the phrase is not in the vocabulary. Please advise me. Thank you.</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_MODEL = '../GoogleNews-vectors-negative300.bin'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_MODEL, binary=True) 


# done well
model.most_similar(""computer"", topn=3) 

# done with error message ""computer_software"" is not in the vocabulory.
model.most_similar(""computer_software"", topn=3) 
</code></pre>
",2019-08-10 14:28:00,2019-08-10 14:28:00,How to Cluster words and phrases with pre-trained model on Gensim,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22444,57539043,2019-08-17 18:47:56,,"<p>I am applying wordNet lemmatizer into my corpus and I need to define the pos tagger for lemmatizer:</p>

<pre><code>stemmer = PorterStemmer()
def lemmitize(document):
    return stemmer.stem(WordNetLemmatizer().lemmatize(document, pos='v'))

def preprocess(document):
output = []
    for token in gensim.utils.simple_preprocess(document):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            print(""lemmitize: "", lemmitize(token))
            output.append(lemmitize(token))
    return output
</code></pre>

<p>Now as you can see I am defining pos for verb (and I know wordNet default pos is a noun), however when I lemmatized my document:</p>

<pre><code>the left door closed at the night  
</code></pre>

<p>I am getting out put as: </p>

<pre><code>output:  ['leav', 'door', 'close', 'night']
</code></pre>

<p>which this is not what i was expecting. In my above sentences, <code>left</code> points to which door (e.g. right or left). If I choose <code>pos ='n'</code> this problem may solve but it will then act as a wornNet default and there will be no effects on words like <code>taken</code>. </p>

<p>I found a similar issue in <a href=""https://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet"">here</a> and I modified the exception list in <code>nltk_data/corpora/wordnet/verb.exc</code> and I changed <code>left leave</code> to <code>left left</code> but still, I am getting the same results as <code>leav</code>.<br>
Now I am wondering if there is any solution to this problem or in the best case, is there any way that I can add a custom dictionary of some words (only limited to my document) that wordNet does not lemmatize them like:</p>

<pre><code>my_dict_list = [left, ...]
</code></pre>
",2019-08-17 21:18:49,2019-09-11 02:18:28,how to modify Wordnet Lemmatizer to lemmitize specific words?,<python><nlp><wordnet><linguistics><lemmatization>,,,CC BY-SA 4.0,True,False,True,False,False
22468,57626276,2019-08-23 12:22:52,,"<p>I am trying to find similarity score between two documents (containing around 15000 records).</p>

<p>I am using two methods in python:
1. TFIDF (Scikit learn) 2. Word2Vec (gensim, google pre-trained vectors)</p>

<p>Example1</p>

<p>Doc1- Click on ""Bills"" tab</p>

<p>Doc2- Click on ""CHAPS"" tab</p>

<p>First method gives 0.9 score.
Second method gives 1 score</p>

<p>Example2</p>

<p>Doc1- See following requirements:</p>

<p>Doc2- See following requirements</p>

<p>First method gives 1 score.
Second method gives 0.98 score</p>

<p>Can anyone tell me:</p>

<p>why in Example1 Word2Vec is giving 1 though they are very different</p>

<p>and in Example2 Word2Vec is giving 0.98 though they are having difference of only "":""</p>
",,2019-08-23 13:33:33,TFIDF vs Word2Vec,<python><machine-learning><data-science><word2vec><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,True
22487,57543403,2019-08-18 09:53:41,,"<p>Problem in reading the JSON dumps created from Wikipedia Dump using gensim</p>

<p>I am trying to follow the instructions given on this link to read wiki dump and create JSON file. </p>

<p><a href=""https://radimrehurek.com/gensim/scripts/segment_wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/segment_wiki.html</a></p>

<p>But the code is failing. </p>

<p>The code I am running is given below</p>

<pre><code>from gensim import utils
import json

 # iterate over the plain text data we just created
with utils.open('D:\\enwiki-latest.json.gz', 'rb') as f:
    for line in f:
      # decode each JSON line into a Python dictionary object
      article = json.loads(line)

      # each article has a ""title"", a mapping of interlinks and a list of ""section_titles"" and
       # ""section_texts"".
    print(""Article title: %s"" % article['title'])
    print(""Interlinks: %s"" + article['interlinks'])
    for section_title, section_text in zip(article['section_titles'], article['section_texts']):
        print(""Section text: %s"" % section_text)

and the stack trace is as follows

AttributeError                            Traceback (most recent call last)
&lt;ipython-input-1-8b1125fd41d0&gt; in &lt;module&gt;()
      3 
      4  # iterate over the plain text data we just created
----&gt; 5 with utils.open('D:\\enwiki-latest.json.gz', 'rb') as f:
      6     for line in f:
      7       # decode each JSON line into a Python dictionary object

AttributeError: module 'gensim.utils' has no attribute 'open'
</code></pre>

<p>Please help me understand whats wrong with my code. </p>

<p>I am running in the code on a Windows 10 machine using Anaconda</p>
",,2019-08-18 09:53:41,Read JSON files created by gensim from Wikipedia dump,<json><gensim><wikipedia>,,,CC BY-SA 4.0,False,False,True,False,False
22489,57630389,2019-08-23 16:57:30,,"<p>I trained the LDA model on my PC and saved it locally by using model.save() command. I can load this model and output topics in PyCharm, but when I try to load the same model in Jupiter Notebook I get an error.</p>

<p>Did anyone encounter the same problem and fix it?
Below is the full error output:</p>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-14-043e6d6083e2&gt; in &lt;module&gt;
      1 # Loading saved model
----&gt; 2 model = models.LdaModel.load('information_extraction/optimal_LDA3.model')
      3 # model_topics = model.show_topics(formatted=True)
      4 # pprint.pprint(model.print_topics(num_words=15))

~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py in load(cls, fname, *args, **kwargs)
   1636         """"""
   1637         kwargs['mmap'] = kwargs.get('mmap', None)
-&gt; 1638         result = super(LdaModel, cls).load(fname, *args, **kwargs)
   1639 
   1640         # check if `random_state` attribute has been set after main pickle load

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    424         compress, subname = SaveLoad._adapt_by_suffix(fname)
    425 
--&gt; 426         obj = unpickle(fname)
    427         obj._load_specials(fname, mmap, compress, subname)
    428         logger.info(""loaded %s"", fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1382         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1383         if sys.version_info &gt; (3, 0):
-&gt; 1384             return _pickle.load(f, encoding='latin1')
   1385         else:
   1386             return _pickle.loads(f.read())

ModuleNotFoundError: No module named 'numpy.random._pickle'
</code></pre>
",,2019-08-23 17:35:44,How to load pre-trained LDA model to Jupiter Notebook?,<pycharm><jupyter-notebook><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22493,57617061,2019-08-22 21:05:49,,"<p>I am building a program that assigns multiple labels/tags to textual descriptions. I am using Scikit-Learn's OneVsRestClassifier+XGBClassifier to classify the vectorized textual descriptions. I am using Gensim's Word2Vec to vectorize the texts. However, when I try to fit the classifier to the vectorized data, I get the following error: </p>

<blockquote>
  <p>IndexError: tuple index out of range</p>
</blockquote>

<p>Below is my code (the error happens on the last line where I try to fit the classifier):</p>

<pre><code>w2vModel = Word2Vec(sentences, size=150, window=10, min_count=2, workers=multiprocessing.cpu_count())
modelCorpus = list(w2vModel.wv.vocab)

descriptions = []
for sentence in sentences:
    wordList = []
    for word in sentence: 
        if (word in modelCorpus):
            wordList.append(w2vModel.wv[word])
    descriptions.append(np.concatenate(wordList))

x = np.array(descriptions)

# Vectorize ticket labels/tags using MultiLabelBinarizer
tagList = relevantDF.Tags # Retrieve list of tags
vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

# Split test data and convert test data to arrays
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
yTrain = csr_matrix(yTrain).toarray()

# Fit OneVsRestClassifier w/ XGBClassifier
clf = OneVsRestClassifier(XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.003))
clf.fit(xTrain, yTrain)
</code></pre>

<p>The shape of x is: (8347,)</p>

<p>The shape of y is: (8347, 24)</p>

<p>The shape of xTrain is: (6677,)</p>

<p>The shape of yTrain is: (6677, 24)</p>
",2019-08-22 21:23:11,2019-08-23 22:07:35,Sklearn classifier can't be trained with Gensim Word2Vec data,<python><machine-learning><scikit-learn><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,True
22500,57652804,2019-08-26 06:31:05,,"<p>i have trained my model with Gensim.now i wanna evaluate my model with simlexx-999 but it gives me error.
my code.</p>

<pre><code>model.wv.evaluate_word_analogies('SimLex-999.txt')
2019-08-25 13:43:22,766 : INFO : Evaluating word analogies for top 300000 words in the model on SimLex-999.txt
</code></pre>

<p>error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-60cb96c45579&gt; in &lt;module&gt;()
----&gt; 1 model.wv.evaluate_word_analogies('SimLex-999.txt')

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_analogies(self, analogies, restrict_vocab, case_insensitive, dummy4unknown)
   1088             else:
   1089                 if not section:
-&gt; 1090                     raise ValueError(""Missing section header before line #%i in %s"" % (line_no, analogies))
   1091                 try:
   1092                     if case_insensitive:

ValueError: Missing section header before line #0 in SimLex-999.txt
</code></pre>

<p>i have tried</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.evaluate_word_pairs(datapath('SimLex-999.txt'))

print(similarities)
</code></pre>

<p>but it gives me keyError.Please help me to solve the problem.</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-29-caeb682cb7ff&gt; in &lt;module&gt;()
      1 from gensim.test.utils import datapath
      2 
----&gt; 3 similarities = model.wv.evaluate_word_pairs(datapath('SimLex-999.txt'),dummy4unknown=True)
      4 
      5 print(similarities)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_pairs(self, pairs, delimiter, restrict_vocab, case_insensitive, dummy4unknown)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in &lt;listcomp&gt;(.0)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

KeyError: 'movie'
</code></pre>
",2019-08-27 04:15:31,2019-12-20 11:20:18,evaluating word2vec model using SimLex-999,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22505,57599259,2019-08-21 21:14:47,,"<p>I am building a multilabel text classification program and I am trying to use OneVsRestClassifier+XGBClassifier to classify the text. Initially I used Sklearn's Tf-Idf Vectorization to vectorize the texts, which worked without error. Now I am using <strong>Gensim's Word2Vec</strong> to vectorize the texts. When I feed the vectorized data into the OneVsRestClassifier+XGBClassifier however, I get the following error on the line where I split the test and training data:</p>

<blockquote>
  <p>TypeError: Singleton array array(,
        dtype=object) cannot be considered a valid collection.</p>
</blockquote>

<p>I have tried converting the vectorized data into a feature array (np.array), but that hasn't seemed to work.
Below is my code:</p>

<pre><code>x = np.array(Word2Vec(textList, size=120, window=6, min_count=5, workers=7, iter=15))

vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

# Split test data and convert test data to arrays
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
</code></pre>

<p>The variables <code>textList</code> and <code>tagList</code> are a list of strings (textual descriptions I am trying to classify).</p>
",2019-08-21 21:36:39,2019-08-22 15:59:03,Sklearn+Gensim: How to use Gensim's Word2Vec embedding for Sklearn text classification,<python><machine-learning><scikit-learn><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
22522,57579009,2019-08-20 17:44:10,,"<p>The following code when run gives the cosine distance between two words.</p>

<p>model.wv.distance('word1','word2')</p>

<p>How do I find the euclidean distance between two words?
I am using gensim for word2vec implementation</p>
",,2019-08-20 18:01:59,How to change the code to find the euclidean distance (not cosine) between words in a word2vec impementation?,<python><gensim><word2vec><similarity><euclidean-distance>,,,CC BY-SA 4.0,False,False,True,False,False
22531,57605476,2019-08-22 08:49:25,,"<p>I am building a program that assigns multiple labels/tags to textual descriptions. I am using Gensim's Doc2Vec to vectorize each of the text descriptions. However, when I print out the length of the Doc2Vec's model's vectors, it returns the number of different tags there are, not the number of descriptions. In other words, it returns vectors representing the tags, not the documents. This inevitable leads to a ValueError when I try splitting the data (using sklearn):</p>

<blockquote>
  <p>ValueError: Found input variables with inconsistent numbers of
  samples: [64, 8370]</p>
</blockquote>

<p>Below is my code:</p>

<pre><code>textList = []

for i in range(0, len(unformattedText)):
    text = unformattedText[i]
    tag = tagList[i]
    textList.append(TaggedDocument(words=text.split("" ""), tags=[tag]))

numCores = multiprocessing.cpu_count() 
model = Doc2Vec(textList, workers=numCores, vector_size=100)

docVectors = []
for j in range(0, len(model.docvecs)):
    docVectors.append(model.docvecs[j])
x = docVectors

vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
</code></pre>

<p>Variable dimensions: <code>x</code> is an array of length 64 and <code>y.shape = (8370, 24)</code></p>
",2019-08-22 11:58:16,2019-08-23 08:33:22,Gensim Doc2Vec: Less vectors generated than expected,<python><machine-learning><scikit-learn><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
22541,57672901,2019-08-27 10:39:27,,"<p>I wanted to successfully run LDAseq model on my very huge corpus. I finally want to extract 100 topics from it.</p>

<p>I am getting an error ""out of memory"" on the step of ldaseq model. This is because I have a huge token and I don't want to truncate it. How to resolve this memory issue?</p>

<ul>
<li>Windows-10-10.0.17763-SP0</li>
<li>Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]</li>
<li>NumPy 1.17.0</li>
<li>SciPy 1.3.0</li>
<li>gensim 3.8.0</li>
<li>FAST_VERSION 0</li>
</ul>

<p>My expected result is the same as shown in the documentation. I need a topic-term and topic-doc matrix finally.</p>
",2019-08-27 11:39:20,2020-01-17 12:01:40,Out of memory issue in gensim topic modeling,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
22561,57728181,2019-08-30 13:59:02,,"<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
",,2019-08-30 15:40:08,I get 'single' characters as learned vocabulary on word2vec genism as an output,<nlp><gensim><word2vec><feature-extraction><text-classification>,,,CC BY-SA 4.0,True,False,True,False,False
22562,57729538,2019-08-30 15:28:23,,"<p>I wanted to fix topics to include some words, e.g. 
Topic 0 - cloud_computing,hybrid_cloud, ...
Topic 1 - smartphone,mobile, ...</p>

<p>So I can across this blog <a href=""http://scignconsulting.com/2019/03/09/guided-lda/"" rel=""nofollow noreferrer"">http://scignconsulting.com/2019/03/09/guided-lda/</a> which attempts to do just that by settings priors for eta.</p>

<p>But what I've found is that for large collections of documents (10s of 1000s) the seed words are getting downranked, with only 5/104 of the final topics actually including any of the original seed words.</p>

<p>I have a hypothesis for why this is happening. I believe that the alphas for the probability of a topic given a document would also need to be set, otherwise, if the probability for a seeded topic is very low, the seedwords may not matter much at all. </p>

<p>Has anyone had experience in this or any pointers to avoid the seed words being ignored.</p>
",,2020-01-03 15:51:47,Guided LDA in GenSim with fixed Eta,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22563,57729961,2019-08-30 15:59:38,,"<p>I have a set of documents that all fit a pre-defined category and have successfully trained a model off of those documents.</p>

<p>The question is, if I have a novel document, how can I calculate how closely this new document lines up with my trained model?</p>

<p>My current solution:</p>

<pre><code>novel_vector = model.infer_vector(novel_doc_words, steps = 20)
similarity_scores = model.docvecs.most_similar([novel_vector])
average = 0
for score in similarity_scores:
  average += score[1]
overall_similarity = average/len(similarity_scores)
</code></pre>

<p>I was unable to find any convenience methods in the documentation</p>
",,2019-08-31 09:25:08,Can gensim Doc2Vec be used to compare a novel document to a trained model?,<python><python-3.x><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22564,57730631,2019-08-30 16:57:26,,"<p>The <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">documentation</a> is a bit unclear how to save the fasttext model to disk - how do you specify a path in the argument, I tried doing so and it failed with an error</p>

<p>Example in documentation</p>

<pre><code>&gt;&gt;&gt; from gensim.test.utils import get_tmpfile
&gt;&gt;&gt;
&gt;&gt;&gt; fname = get_tmpfile(""fasttext.model"")
&gt;&gt;&gt;
&gt;&gt;&gt; model.save(fname)
&gt;&gt;&gt; model = FastText.load(fname)
</code></pre>

<p>Furthermore, how can I save the model in text format like can be done with word2vec models?</p>

<pre><code>'word2vecmodel.wv.save_word2vec_format(""D:\w2vmodel.txt"")'
</code></pre>

<p><strong>EDIT</strong></p>

<p>After trying the suggestion to make a file first I keep kgetting the same error as before when I run this code</p>

<pre><code>savepath = os.path.abspath('D:\fasttextmodel.v3.bin');
from gensim.test.utils import get_tmpfile
fname = get_tmpfile(savepath)
fasttext_model.save(fname)
</code></pre>

<blockquote>
  <p>TypeError: file must have a 'write' attribute</p>
</blockquote>
",2019-08-30 19:33:31,2019-08-30 19:33:31,How to save fasttext model in binary and text formats?,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22565,57674722,2019-08-27 12:29:01,,"<p>I have evaluated my model with SimLex-999 and wordsim353 but i don't know if the result is ok or not?<a href=""https://i.stack.imgur.com/U0fi4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U0fi4.jpg"" alt=""please see the result""></a></p>

<p>wordsim353 result </p>

<pre><code> Pearson correlation coefficient against C:\ProgramData\Anaconda3\lib\site-packages\gensim\test\test_data\wordsim353.tsv: 0.4895
2019-08-27 08:30:06,655 : INFO : Spearman rank-order correlation coefficient against C:\ProgramData\Anaconda3\lib\site-packages\gensim\test\test_data\wordsim353.tsv: 0.4799
2019-08-27 08:30:06,656 : INFO : Pairs with unknown words ratio: 7.1%

((0.4894983099817645, 3.6324947252392034e-21), SpearmanrResult(correlation=0.4798812637344527, pvalue=2.6991867797169835e-20), 7.0821529745042495)
</code></pre>

<p>SimLex-999 result</p>

<pre><code> 2019-08-27 15:43:13,000 : INFO : Pearson correlation coefficient against C:\ProgramData\Anaconda3\lib\site-packages\gensim\test\test_data\simlex999.txt: 0.3138
    2019-08-27 15:43:13,001 : INFO : Spearman rank-order correlation coefficient against C:\ProgramData\Anaconda3\lib\site-packages\gensim\test\test_data\simlex999.txt: 0.2992
    2019-08-27 15:43:13,002 : INFO : Pairs with unknown words ratio: 1.2%
    ((0.31381174440491943, 5.375150591505246e-24), SpearmanrResult(correlation=0.29915866880742126, pvalue=7.433265418805336e-22), 1.2012012012012012)
</code></pre>
",2019-08-27 12:36:55,2019-08-27 16:31:30,evaluate word2vec with SimLex-999 and wordsim353,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22568,57688029,2019-08-28 08:26:15,,"<p>I am trying to process a large corpus but in preprocess_string( ) it returns an error shown below</p>

<pre><code>Traceback (most recent call last): File ""D:/Projects/docs_handler/data_preprocessing.py"", line 60, in &lt;module&gt; for temp in batch(iterator,1000): File ""D:/Projects/docs_handler/data_preprocessing.py"", line 30, in batch for item in iterable: File ""D:/Projects/docs_handler/data_preprocessing.py"", line 23, in iter_tokenized_documents document = preprocess_string(open(os.path.join(root, file)).read().strip(),filters=CUSTOM_FILTERS) File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\encodings\cp1252.py"", line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 16144: character maps to &lt;undefined&gt;
</code></pre>

<hr>

<pre><code>Versions
Windows-10-10.0.17763-SP0
Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]
NumPy 1.17.0
SciPy 1.3.0
gensim 3.8.0
FAST_VERSION 0
</code></pre>

<hr>

<pre><code>def iter_tokenized_documents(input_directory):
    """"""Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.""""""
    for root, dirs, files in os.walk(input_directory):
        for file in filter(lambda file: file.endswith('.txt'), files):
            document = preprocess_string(open(os.path.join(root, file)).read().strip(),filters=CUSTOM_FILTERS)
            if(len(document)):
                yield document
</code></pre>

<p>How to run it without any error?</p>
",2019-08-28 08:27:50,2020-04-23 08:07:00,Having issue with character encoding while processing a text,<python><encoding><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22570,57695150,2019-08-28 14:53:10,,"<p>I want to train part of the corpus first and then based on the embeddings train on the whole corpus. Can I achieve this with gensim skipgram?</p>

<p>I haven't found an API that can pass initial embeddings.</p>

<p>what I want is some thing like</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""],
             [""cat2"", ""say2"", ""meow""], [""dog2"", ""say"", ""woof""]]
model = Word2Vec(sentences[:2], min_count=1)
X = #construct a new one
model = Word2Vec(sentences, min_count=1, initial_embedding=X)
</code></pre>
",,2019-08-28 17:41:32,How can I use a pretrained embedding to gensim skipgram model?,<python><machine-learning><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22578,57525190,2019-08-16 13:04:12,,"<p>I am doing text classification and plan to use word2vec word embeddings.
I have used gensim module for word2vec Training.</p>

<p>I have tried several Options. But I am getting error that word 'xyz' not in vocabulary. I am not able to find my mistake.</p>

<h1>Text processing</h1>

<pre><code>def clean_text(text):

text = text.translate(string.punctuation)

text = text.lower().split()

stops = set(stopwords.words(""english""))
text = [w for w in text if not w in stops]

text = "" "".join(text)
text = re.sub(r""[^\w\s]"", "" "",text)
text = re.sub(r""[^A-Za-z0-9^,!.\/'+-=]"", "" "",text)

text = text.split()
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(w) for w in text]
text = "" "".join(lemmatized_words)


return text

data['text'] = data['text'].map(lambda x: clean_text(x))
</code></pre>

<p>Please help me to solve my issue.</p>

<h1>Definig Corpus</h1>

<pre><code>def build_corpus(data):
""Creates a list of lists containing words from each sentence""
corpus = []
for col in ['text']:
    for sentence in data[col].iteritems():
        word_list = sentence[1].split("" "")
        corpus.append(word_list)
return corpus

corpus = build_corpus(data)
</code></pre>

<h1>Word2vec model</h1>

<pre><code>from gensim.models import word2vec
 model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=20,    workers=12, sg=1)

words = list(model.wv.vocab)

tokenizer = Tokenizer()
X = data.text
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
X = pad_sequences(sequences, maxlen=10000)

embedding_vector_size=100

vocab_size = len(words)
embedding_matrix = np.zeros((vocab_size, embedding_vector_size))
for index, word in enumerate(words):    
 embedding_vector = model.wv[word]
 if embedding_vector is not None:
    embedding_matrix[index] = embedding_vector
</code></pre>

<p>Now I am using my created word embeddings on the downstream classification task.</p>

<h1>classification model</h1>

<pre><code>labels = data['Priority']
</code></pre>

<p>where I have two priorities. I want to classify it.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)
</code></pre>

<p>I am using folllowing network for classification</p>

<pre><code>model3 = Sequential()
model3.add(Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, input_length = max_len, weights=[embedding_matrix]))
model3.add(SpatialDropout1D(0.7))
model3.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))
model3.add(Dense(2, activation='softmax'))
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
print(model3.summary())
</code></pre>

<p>I am getting error here:</p>

<pre><code>'ValueError: ""input_length"" is 10000, but received input has shape (None, 3)'
</code></pre>

<p>Please help me to solve it out.Thank you.</p>
",2019-09-11 11:36:05,2019-09-11 11:36:05,Text Classification with word2vec,<python-3.x><word2vec><text-classification>,,,CC BY-SA 4.0,False,False,True,False,False
22591,57752992,2019-09-02 07:33:40,,"<p>I have created a LDA model using Gensim, for which I first iterated from num_topics in range 3 to 10, and based on pyLDAvis plots, chose n = 3 in final lda model.</p>

<pre><code>import glob
import sys
sys.path.append('/Users/tcssig/Documents/NLP_code_base/Doc_Similarity')
import normalization
from gensim.models.coherencemodel import CoherenceModel
datalist = []

for filename in glob.iglob('/Users/tcssig/Documents/Speech_text_files/*.*'):
    text = open(filename).readlines()
    text = normalization.normalize_corpus(text, only_text_chars=True, tokenize=True)
    datalist.append(text)

datalist = [datalist[i][0] for i in range(len(datalist))]

from gensim import models,corpora
import spacy
dictionary = corpora.Dictionary(datalist)
num_topics = 3
Lda = models.LdaMulticore

#lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, passes=20,chunksize=2000,random_state=3)

doc_term_matrix = [dictionary.doc2bow(doc) for doc in datalist]

dictionary = corpora.Dictionary(datalist)
import numpy as np 
import pandas as pd
import spacy
import re
from tqdm._tqdm_notebook import tqdm_notebook,tnrange,tqdm
from collections import Counter,OrderedDict
from gensim import models,corpora
from gensim.summarization import summarize,keywords
import warnings
import pyLDAvis.gensim
import matplotlib.pyplot as plt
import seaborn as sns

Lda = models.LdaMulticore
coherenceList_umass = []
coherenceList_cv = []
num_topics_list = np.arange(3,10)
for num_topics in tqdm(num_topics_list):
    lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, passes=20,chunksize=4000,random_state=43)
    cm = CoherenceModel(model=lda, corpus=doc_term_matrix, dictionary=dictionary, coherence='u_mass')
    coherenceList_umass.append(cm.get_coherence())
    cm_cv = CoherenceModel(model=lda, corpus=doc_term_matrix, texts=datalist, dictionary=dictionary, coherence='c_v')
    coherenceList_cv.append(cm_cv.get_coherence())
    vis = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary)
    pyLDAvis.save_html(vis,'pyLDAvis_%d.html' %num_topics)


plotData = pd.DataFrame({'Number of topics':num_topics_list,'CoherenceScore':coherenceList_umass})
f,ax = plt.subplots(figsize=(10,6))
sns.set_style(""darkgrid"")
sns.pointplot(x='Number of topics',y= 'CoherenceScore',data=plotData)
plt.axhline(y=-3.9)
plt.title('Topic coherence')
plt.savefig('Topic coherence plot.png')

#################################################################
#################################################################

lda_final= Lda(doc_term_matrix, num_topics=3,id2word = dictionary, passes=20,chunksize=4000,random_state=43)

lda_final.save('lda_final')

dictionary.save('dictionary')

corpora.MmCorpus.serialize('doc_term_matrix.mm', doc_term_matrix)


a = lda_final.show_topics(num_topics=3,formatted=False,num_words=10)
b = lda_final.top_topics(doc_term_matrix,dictionary=dictionary,topn=10)


topic2wordb = {}
topic2csb = {}
topic2worda = {}
topic2csa = {}
num_topics =lda_final.num_topics
cnt =1

for ws in b:
    wset = set(w[1] for w in ws[0])
    topic2wordb[cnt] = wset
    topic2csb[cnt] = ws[1]
    cnt +=1

for ws in a:
    wset = set(w[0]for w in ws[1])
    topic2worda[ws[0]+1] = wset

for i in range(1,num_topics+1):
    for j in range(1,num_topics+1):  
        if topic2worda[i].intersection(topic2wordb[j])==topic2worda[i]:
            topic2csa[i] = topic2csb[j]

print('the final data block')
finalData = pd.DataFrame([],columns=['Topic','words'])
finalData['Topic']=topic2worda.keys()
finalData['Topic'] = finalData['Topic'].apply(lambda x: 'Topic'+str(x))
finalData['words']=topic2worda.values()
finalData['cs'] = topic2csa.values()
finalData.sort_values(by='cs',ascending=False,inplace=True)
finalData.to_csv('CoherenceScore.csv')
print(finalData)
</code></pre>

<p>Now i have the trained model with me, but I want to know how I use the model on the docs used for training and also on new unseen document to assign the topic </p>

<p>I'm using the below code to do this but getting the error as below :</p>

<pre><code>unseen_document = 'How a Pentagon deal became an identity crisis for Google'

text = normalization.normalize_corpus(unseen_document, only_text_chars=True, tokenize=True)

bow_vector = dictionary.doc2bow(text)

corpora.MmCorpus.serialize('x.bow_vector', bow_vector)

corpus = [dictionary.doc2bow(text)]

x = lda_final[corpus]
</code></pre>

<p>Error Message :</p>

<pre><code>    Topic                                              words        cs
2  Topic3  {senator, people, power, home, year, believe, ... -0.175486
1  Topic2  {friend, place, love, play, general, house, ye... -0.318839
0  Topic1  {money, doe, fucking, play, love, people, worl... -1.360688

Traceback (most recent call last):
  File ""LDA_test.py"", line 141, in &lt;module&gt;
    corpus = [dictionary.doc2bow(text)]
  File ""/Users/tcssig/anaconda/lib/python3.5/site-packages/gensim/corpora/dictionary.py"", line 250, in doc2bow
    counter[w if isinstance(w, unicode) else unicode(w, 'utf-8')] += 1
TypeError: coercing to str: need a bytes-like object, list found
</code></pre>
",,2019-09-08 22:51:53,Unable to classify topics using LDA trained model,<python><nlp><lda>,,,CC BY-SA 4.0,False,True,True,False,False
22593,57755481,2019-09-02 10:37:09,,"<p>I cannot update the training of my gensim fasttext model with the command : model.build_vocab</p>

<p>I think the key is ""AttributeError: 'FastText' object has no attribute 'syn1neg'""</p>

<p>Please give me some suggestion. Thanks a lot</p>

<h3>Load the pre-trained model, not pretrained vector to make sure that i can train the model</h3>

<p>print('load fasttext pretrain model ')
pretrained_model=FastText_gensim.load(pretrained_model_file)</p>

<h3>Load the tokens of articles i wanna update and convert the tokens into list of list</h3>

<p>sent=token_df['token'].values.tolist()   </p>

use the "".build_vocab"" of  pretrain model and state ""update = True""

<p>pretrained_model.build_vocab(sent,update=True)</p>

<p>Traceback (most recent call last):
File ""C:/Users/marcus/PycharmProjects/DIVA_CWS/FastText_pretrain.py"", line 313, in 
pretrained_model.build_vocab(sent,update=True)
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 712, in build_vocab
self.finalize_vocab(update=update)  # build tables &amp; arrays
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 953, in finalize_vocab
self.update_weights()
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 1373, in update_weights
self.syn1neg = vstack([self.syn1neg, zeros((gained_vocab, self.layer1_size), dtype=REAL)])
AttributeError: 'FastText' object has no attribute 'syn1neg'</p>
",,2019-09-20 10:52:09,Any bugs on FastText.build_vocab?,<python><gensim><word-embedding><fasttext><incremental-build>,,,CC BY-SA 4.0,False,False,True,False,False
22596,57697374,2019-08-28 17:27:28,,"<p>With Gensim, after I've trained my own model, I can use <code>model.wv.most_similar('cat', topn=5)</code> and get a list of the 20 words that are closest to <code>cat</code> in the vector space. For example:</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('mymodel.model')

In: model.wv.most_similar('cat', topn=5)
Out: ('kitten', .99)
     ('dog', .98)
     ...
</code></pre>

<p>With spaCy, as per the <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">documentation</a>, I can do:</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_md')
tokens = nlp(u'dog cat banana')

for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))
</code></pre>

<p>which gives similarity for tokens in a specified string. But combing through the docs and searching, I can't figure out if there is a gensim-type way of listing all similar words for a preloaded model with either <code>nlp = spacy.load('en_core_web_lg')</code> or <code>nlp = spacy.load('en_vectors_web_lg')</code>. Is there a way to do this?</p>
",,2020-10-03 05:35:16,List most similar words in spaCy in pretrained model,<python><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
22601,57757356,2019-09-02 12:57:55,,"<p>I have a test sentence (which define a skill, such as ""Perform equipment maintenance"") and a set of diplomas (10000 different diplomas) with description of the needed skills (=1 paragraph per diploma). My problem consists in finding the diploma closest to the test sentence in terms of semantic similarity.</p>

<p>I thought about creating a doc2vec model (multi-class, 1 class per diploma) in order to transform each diploma in feature vector, then infer vector for the test sentence and calculate cosine similarity with each feature vector. Yet, I only have one sample for each diploma. Will it still work? 
Or do I have to split the sentences of each diploma text in order to obtain several samples for a diploma ?</p>
",,2019-09-03 20:21:06,Does doc2vec work with multi-class problem with only 1 sample per class?,<nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22624,57760664,2019-09-02 17:11:25,,"<p>I am trying to get unique words for each topic.</p>

<p>I am using gensim and this is the line that help me to generate my model</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary)
</code></pre>

<p>But I have repeated words in two different topics, I would like to have different words per topic</p>
",2019-09-02 17:11:46,2020-04-23 01:57:03,How can I get unique words per each topic LDA?,<python><gensim><word><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22626,57762713,2019-09-02 21:11:33,,"<p>I have two lists:</p>

<pre><code>list_1 = [['flavor', 'flavors', 'fruity_flavor', 'taste'],
          ['scent', 'scents', 'aroma', 'smell', 'odor'],
          ['mental_illness', 'mental_disorders','bipolar_disorder']
          ['romance', 'romances', 'romantic', 'budding_romance']]

list_2 = [['love', 'eating', 'spicy', 'hand', 'pulled', 'noodles'],
          ['also', 'like', 'buy', 'perfumes'],
          ['suffer', 'from', 'clinical', 'depression'],
          ['really', 'love', 'my', 'wife']]
</code></pre>

<p>I would like to compute the cosine similarity between the two lists above in such a way where the cosine similarity between the first sub-list in list1 and all sublists of list 2 are measured against each other. Then the same thing but with the second sub-list in list 1 and all sub-lists in list 2, etc.</p>

<p>The goal is to create a <strong>len(list_2) by len(list_1) matrix</strong>, and each entry in that matrix is a cosine similarity score. Currently I've done this the following way:</p>

<pre><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)
</code></pre>

<p>However, I'd like to implement this with matrix multiplication and no for loops. </p>

<p>My two questions are:</p>

<ol>
<li>Is there a way to do some sort of element-wise matrix multiplication but with <code>gensim's n_similiarity() method</code> to generate the required matrix?</li>
<li>Would it be more efficient and faster using the current method or matrix multiplication?</li>
</ol>

<p>I hope my question was clear enough, please let me know if I can clarify even further.</p>
",2019-09-02 23:02:38,2019-09-02 23:49:10,Perform matrix multiplication with cosine similarity function,<python-3.x><numpy><gensim><word2vec><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22638,57795240,2019-09-04 20:26:48,,"<p>I am doing some <code>topic modelling</code>work. If I understand things correctly, a <code>LDA</code> model is entirely defined by its word/topic distribution (which can be obtained with <code>model.get_topics()</code> in <code>gensim</code>. I am wondering if it is possible to create a <code>gensim</code> model by specifying this distribution, notably in order to use gensim's functions, such <code>CoherenceModel</code>, and topic prediction.  </p>
",,2019-09-04 20:26:48,Create a Gensim model by specifying the word/topics distributions,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
22639,57796091,2019-09-04 21:56:09,,"<p>As a part of the assignment, I am asked to do topic modeling using LDA and visualize the words that come under the top 3 topics as shown in the below screenshot <a href=""https://i.stack.imgur.com/e4Buq.png"" rel=""nofollow noreferrer"">1</a>. However, even after searching a lot I am not able to find any helpful resource that would help me achieve my goal. All resources about text visualization are pointed towards the word cloud, but my goal is not to use word cloud visualizations.
<a href=""https://i.stack.imgur.com/e4Buq.png"" rel=""nofollow noreferrer"">Required LDA topic visulization</a></p>
<p>Any help will be greatly appreciated.</p>
",2020-06-20 09:12:55,2019-09-11 10:08:56,How to visualize results of LDA topic modelling as shown below,<matplotlib><plotly><seaborn><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22641,57798839,2019-09-05 05:17:50,,"<p>I'm working on a project for text similarity using FastText, the basic example I have found to train a model is:</p>

<pre><code>from gensim.models import FastText

model = FastText(tokens, size=100, window=3, min_count=1, iter=10, sorted_vocab=1)
</code></pre>

<p>As I understand it, since I'm specifying the vector and ngram size,  the model is been trained from scratch here and if the dataset is small I would spect great resutls.</p>

<p>The other option I have found is to load the original Wikipedia model which is a huge file: </p>

<pre><code>from gensim.models.wrappers import FastText

model = FastText.load_fasttext_format('wiki.simple')
</code></pre>

<p>My question is, can I load the Wikipedia or any other model, and fine tune it with my dataset? </p>
",,2019-09-10 03:30:29,Is it possible to fine tune FastText models,<python><nlp><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22646,57834425,2019-09-07 14:05:51,,"<p>I am trying to apply the doc2vec <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">tutorial</a> and instead of testing on a random test corpus document, testing on the entire test corpus</p>

<p>I just modified the following line:</p>

<p>code:</p>

<pre><code># Pick a random document from the test corpus and infer a vector from the model

#doc_id = random.randint(0, len(test_corpus) - 1)
doc_id = [index for index, text in enumerate(test_corpus)]

inferred_vector = model.infer_vector(test_corpus[doc_id])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

# Compare and print the most/median/least similar documents from the train corpus
print('Test Document ({}): {}\n'.format(doc_id, ' '.join(test_corpus[doc_id])))
print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
    print(u'%s %s: %s\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))
</code></pre>

<p>Error:</p>

<pre><code>TypeError: list indices must be integers or slices, not list
</code></pre>
",,2019-09-07 14:38:33,Testing the Model doc2vec in all test corpus,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22648,57903695,2019-09-12 09:28:22,,"<p>I want to use the read-only version of Gensim's FastText Embedding to save some RAM compared to the full model.</p>

<p>After loading the KeyVectors version, I get the following Error when fetching a vector:</p>

<p><code>IndexError: index 878080 is out of bounds for axis 0 with size 761210</code></p>

<p>The error occurs when using words that should be out-of-vocabulary e.g. ""lawyerxy"" instead of ""lawyer"". The full model returns a vector for both. </p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load(""model.kv"")
model .wv.__getitem__(""lawyerxy"")
</code></pre>

<p>So, my assumption is that the KeyedVectors do not offer FastText's out of vacabulary function - a key feature for my usecase. This limitation is not given in the documentation:
<a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Can anyone prove that assumption and/or name a fix to allow vectors for ""lawyerxy"" etc. ?</p>
",,2019-09-12 15:59:50,Gensim's FastText KeyedVector out of vocab,<gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22675,57943303,2019-09-15 10:36:08,,"<p>I am trying to compare Glove, Fasttext, Bert ,Elmo on basis on similarity between 2 words using pre-trained models of Wiki. Glove and Fasttext had pretrained models which could easily be used with gensim word2vec in python. Does Elmo and Bert have any such models ?</p>
",2019-09-16 07:25:33,2019-09-20 12:32:52,"Get similarity score between 2 words using Pre trained Bert, Elmo",<nlp><gensim><word2vec><word-embedding><elmo>,,,CC BY-SA 4.0,False,False,True,False,False
22679,57804554,2019-09-05 11:32:05,,"<p>I am running a dockerfile on a ubuntu base image as follows :</p>

<pre><code>FROM ubuntu:14.04

# Install dependencies
RUN apt-get update 
RUN apt-get install -y \
    software-properties-common
RUN add-apt-repository universe
RUN apt-get install -y python3.5 \
    python3-pip 

RUN apt-get install libav-tools -y

RUN apt-get update 

RUN apt-get upgrade

#RUN  apt-get install google-cloud-sdk

RUN pip3 install --upgrade pip 
RUN pip3 install pandas 
RUN pip3 install glob3

RUN     pip3 install --upgrade pip 
#RUN    pip3 install pandas 
RUN pip3 install glob3
#RUN    pip3 install json
RUN pip3 install numpy
RUN pip3 install fuzzywuzzy
RUN pip3 install gensim
</code></pre>

<p>I have python 3.5 installed on this machine, but still I am getting the error as follows :</p>

<pre><code>Collecting gensim
  Downloading https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz (23.4MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""'; __file__='""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info
         cwd: /tmp/pip-install-klg_2vmh/gensim/
    Complete output (5 lines):
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-klg_2vmh/gensim/setup.py"", line 23, in &lt;module&gt;
        raise Exception('This version of gensim needs Python 2.7, 3.5 or later.')
    Exception: This version of gensim needs Python 2.7, 3.5 or later.
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>

<p>Is there some specific version of Gensim that i need to download or this is some different error.</p>
",,2019-09-05 12:09:15,unable to install Gensim with Python 3.5 on ubuntu,<python><docker><ubuntu><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22689,57839264,2019-09-08 04:47:17,,"<p>I am using gensim to train a word2Vec model. Here I am passing one sentence at a time to the gensim.models.Word2Vec() method from my corpus to gradually train the model on my whole corpus. But I am confused what should the value of iter parameter be as I'm not sure whether it iterates over the passed sentence n times or the whole corpus.    </p>

<p>I have tried checking the documentation of gensim. it states the definition as follows:<br></p>

<blockquote>
  <p>iter (int, optional)  Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>But I am confused as I am not passing the whole corpus but only a single sentence on each iteration.</p>

<p>My line in the code that trains the model looks like this:<br>
<code>model = gensim.models.Word2Vec(data, min_count=2, window=arg.window_size, size=arg.dim_size, workers=4, sg=0, hs=0, negative=10, ns_exponent=0.75, alpha=0.025, iter=1)</code>
<br>Here ""data"" represents a single sentence passed at a time from a generator. </p>

<p>Suppose I have a corpus of 2 sentences. ""X is a variable. Y is a variable too."". The model receives data = ""X is a variable."" first and data = ""Y is a variable too."" in 2nd iteration. 
Now to clarify, my question is,<b> whether iter = 50 will train my model iterating though ""X is a variable."" 50 times &amp; ""Y is a variable too."" 50 times or will it iterating though ""X is a variable. Y is a variable too."" (my whole corpus) 50 times. 
</b></p>
",2019-09-08 04:52:37,2019-09-08 22:06:27,"Does the ""iter"" parameter of gensim.models.Word2Vec method iterate over the whole corpus or the sentence passed to it at a time?",<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22691,57856393,2019-09-09 14:47:55,,"<p>I'd like to calculate <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">Word Mover's Distance</a> with <a href=""https://tfhub.dev/google/universal-sentence-encoder/2"" rel=""nofollow noreferrer"">Universal Sentence Encoder on TensorFlow Hub</a> embedding. </p>

<p>I have tried the example on <a href=""https://spacy.io/universe/project/wmd-relax"" rel=""nofollow noreferrer"">spaCy for WMD-relax</a>, which loads 'en' model from spaCy, but I couldn't find another way to feed other embeddings. </p>

<p>In <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb"" rel=""nofollow noreferrer"">gensim</a>, it seems that it only accepts <code>load_word2vec_format</code> file (<code>file.bin</code>) or <code>load</code> file (<code>file.vec</code>). </p>

<p>As I know, someone has written a <a href=""https://github.com/Kyubyong/bert-token-embeddings"" rel=""nofollow noreferrer"">Bert to token embeddings based on pytorch</a>, but it's not generalized to other models on tf-hub.</p>

<p>Is there any other approach to transfer pretrained models on tf-hub to spaCy format or word2vec format?</p>
",,2020-06-29 16:54:38,Load pretrained model on TF-Hub to calculate Word Mover's Distance (WMD) on Gensim or spaCy,<tensorflow><nlp><gensim><spacy><tensorflow-hub>,,,CC BY-SA 4.0,False,True,True,False,False
22695,57894052,2019-09-11 17:23:26,,"<p>I switched from Windows development to Ubuntu 19.04 and after installing my libaries using pip I got the runtime warning about numpy.ufunc size changed.</p>

<p>I am using the following versions and my project is in PyCharm Professional (lastest version).</p>

<p>Spacy 2.1.8
numpy 1.17.2<br>
blis 0.4.0
thinc 7.0.8
gensim 2.8</p>

<p>I have uninstalled everything and installed SPACY last, I have uninstalled numpy and put version 1.15 up and various other tricks but I can't see to get it to install the correct version.</p>

<p>Does anyone know how to resolve this issue or which actual version of numpy/thinc/blis should be used in this case?</p>
",,2019-09-11 17:23:26,Spacy 3.8 numpy.ufunc size changed may indicate binary incompatibility,<python-3.x><numpy><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
22696,57895899,2019-09-11 19:49:13,,"<p>I am attempting to use Gensim's Mallet wrapper. When I run the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import os
import gensim

os.environ.update({
        'MALLET_HOME':
        r"":C\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8""
    })
lda_mallet = gensim.models.wrappers.LdaMallet(
        r""C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet"",
        corpus=corpus,
        num_topics=10,
        id2word=id_dict)
</code></pre>

<p>I am thrown the following errors:</p>

<pre><code>'C:\Users\me\OneDrive' is not recognized as an internal or external command,
operable program or batch file.

subprocess.CalledProcessError: Command 'C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\me\AppData\Local\Temp\17fe21_corpus.txt --output C:\Users\me\AppData\Local\Temp\17fe21_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>After exhaustive online searches, I have found many proposed solutions that unfortunately do not resolve my issue.</p>

<p>Since the first error message does not print the entire path, I believe the spaces are the cause of the issue. </p>

<p>Unfortunately, my company requires that I use this directory and I cannot change the name. Is there a way to ""escape"" the spaces in order to run my code?</p>
",,2019-09-18 23:22:48,How do I pass a file path containing spaces to the Gensim LDA Mallet wrapper?,<python><bash><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
22697,57896070,2019-09-11 20:04:04,,"<p>Given heavily cleaned input in the format</p>

<pre class=""lang-py prettyprint-override""><code>model_input = [['TWO people admitted fraudulently using bank cards (...)'],
               ['All tyrants believe forever',
                'But history especially People Power (...) first Bulatlat']]
</code></pre>

<p>word2vec is returning alongside the more obvious results super-specific vectors such as</p>

<pre class=""lang-py prettyprint-override""><code>{'A pilot shot dogfight Pakistani aircraft returned India Friday freed Islamabad called peace gesture following biggest standoff two countries years':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572828&gt;,
 'This story published content partnership POLITICO':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572a58&gt;,
 'Facebook says none 200 people watched live video New Zealand mosque shooting flagged moderators underlining challenge tech companies face policing violent disturbing content real time': 
    &lt;gensim.models.keyedvectors.Vocab at 0x12a93572ba8&gt;}
</code></pre>

<p>It appears to be occurring to more documents than not, and I have a hard time believing they each appear more than five times.</p>

<p>I'm using the following code to create my model:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 5 
DIMS = 250

vocab_model = gensim.models.Word2Vec(model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     min_count=MIN_COUNT)
</code></pre>

<p>What am I doing wrong that I'm getting such useless vectors?</p>
",2019-09-11 20:19:27,2019-09-11 22:35:47,Gensim's word2vec returning awkward vectors,<python><python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22704,57946734,2019-09-15 17:51:58,,"<p>this is my code</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there the hill have eyes"",""the_hill have_eyes new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1)
sent = ['the', 'mayor', 'of', 'new_york', 'was', 'there', 'the_hill', 'have_eyes']
print(bigram[sent])
</code></pre>

<p>i want it detects ""the_hill_have_eyes"" but the output is</p>

<pre><code>['the', 'mayor', 'of', 'new_york', 'was', 'there', 'the_hill', 'have_eyes']
</code></pre>
",,2019-09-17 04:20:40,find bigram using gensim,<python-3.x><gensim><phrase>,,,CC BY-SA 4.0,False,False,True,False,False
22708,57950325,2019-09-16 04:18:26,,"<p>I need to calculate the word vectors for each word of a sentence that is tokenized as follows:</p>

<pre><code>['my', 'aunt', 'give', 'me', 'a', 'teddy', 'ruxpin']. 
</code></pre>

<p>If I was using the pretrained [fastText][1] Embeddings: cc.en.300.bin.gz by facebook. I could get by OOV. However, when I use Google's word2vec from GoogleNews-vectors-negative300.bin, it returns an InvalidKey Error. My question is how to we calculate the word vectors that are OOV then? I searched online I could not find anything. Of course on way to do this is removing all the sentences that have words not listed in the google's word2vec. However, I noticed only 5550 out of 16134 have words completely in the embedding. </p>

<p>I did also </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True) 
model.train(sentences_with_OOV_words)
</code></pre>

<p>However, tensorflow 2 returns an error.</p>

<p>Any help would be greatly appreciate it. </p>
",,2019-09-16 18:41:30,Handling OOV words in GoogleNews-vectors-negative300.bin,<word2vec><oov>,,,CC BY-SA 4.0,False,False,True,False,False
22748,57939471,2019-09-14 21:41:55,,"<p>I would like to build an application to foreign languages learning, something based on creating decks with flashcards, words ordering, add voice (pronounciation) samples, add images etc. (something similar to ANKI app). </p>

<p>I want to use C# language and .NET platform to make it. I wish make both desktop and mobile app (maybe also website, but this is to be considered). And my question is what exactly technologies (type of projects) should I use to do it?
It means I should make desktop app using .net framework with WPF and Xamarin to mobile app and then somehow synchronize it? I would like to synchronize desktop and mobile apps together - if someone uses desktop version and then install mobile app then he/she could have access to the whole his/her previous decks/settings and so on, maybe should I consider creating accounts?
But I also wonder about the .net core and thanks to that I would make my app to be cross-platfrom.</p>

<p>I don't know how to correctly plan it ... Another thing is I would like to use some python libaries in that, exactly I mean gensim and generally topic-modeling, maybe also some other tools to neural networks. In general I would like to embed python things in my C# code.</p>

<p>Sumarizing, the aim is to create both desktop and mobile app using .net (.net framework or .net core), somehow makes the synchronization/connection between them and then (like a additional feature) use some python topic-modelling and neural network tools to enrich this whole app with some things. 
Would you recommend me something, some way to do it properly (some good approach to the topic), where should I start?
Till now I was using mainly wpf/win forms, also entity framework, ado.net and so on. I have never used .net core but I think it's time to get to know that - it's future I suppose :)</p>
",2019-09-14 21:48:07,2019-09-14 21:50:23,How to properly synchronize desktop and mobile app in .net?,<c#><.net><.net-core><project><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
22760,57921755,2019-09-13 10:19:32,,"<p>I would like to investigate how the TF-IDF score of particular words in document depends on the number of documents on which IDF is based. Unfortunately, the list of results that I receive vary in length and yet the number of words in the document is fixed... How to get TF-IDF results for all words in a document, regardless of the number of modeling documents?</p>

<p>I use the Gensim library to calculate the TF-IDF ratio. Here is my approach:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim import corpora
from gensim import models
from gensim.models import TfidfModel

# Suppose I have a list of words with a random distribution:
docs = [
    ['dog', 'cat', 'panda', 'deer', 'dog', 'elephant', 'panda', 'mouse', 'dog', 'panda', 'dog', 'python', 'penguin', 'lion', 'mouse'],
    ['cat', 'panda', 'rhino', 'lynx', 'panda', 'panda', 'panda', 'koala', 'mammoth', 'hamster', 'cat', 'koala', 'bear', 'fright'],
    ['dog', 'cat', 'elephant', 'panda', 'deer', 'deer', 'baloonfish', 'pig', 'owl', 'dove', 'camel', 'camel', 'camel'],
    ['dog', 'panda', 'mammoth', 'snake', 'lizard', 'elephant', 'partridge', 'alpaca', 'dog', 'dog', 'lizard', 'dog'],
    ['dog', 'owl', 'ostrich', 'porcupine', 'mouse', 'baloonfish', 'croc', 'lion', 'chimp',  'camel', 'doe']
]

# Each document has a certain number of tokens and unique types:
print([len(doc) for doc in docs])  # [15, 14, 13, 12, 11]
print([len(set(doc)) for doc in docs])  # [9, 9, 10, 8, 11]

# I create a dictionary that has 30 unique tokens...
dictionary = corpora.Dictionary(docs)  # ['cat', 'deer', 'dog', 'elephant', 'lion', ...]
# ...and corpus containing individual instances of 5 documents
corpus = [dictionary.doc2bow(doc) for doc in docs]  # [[(0, 1), (1, 1), (2, 4), (3, 1), ...], ...]

# now I'm training tfidf model and applying this model to all corpus documents then I check their length:
model = TfidfModel(corpus)
vector = model[corpus]
print([len(v) for v in vector])  # [9, 9, 10, 8, 11]
</code></pre>

<p>So far so good, but now I would like to compare these results with the results obtained for a smaller number of documents based on which the model is built. In order to do this I do the following:</p>

<pre class=""lang-py prettyprint-override""><code># now I'm training my tfidf model based only on first four documents in corpus:
new_model = TfidfModel(corpus[:4])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [8, 8, 9, 7, 6]

# based on first three:
new_model = TfidfModel(corpus[:3])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [7, 7, 8, 3, 6]

# based on first two:
new_model = TfidfModel(corpus[:2])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [7, 7, 3, 3, 3]
</code></pre>

<p>Can somebody explain to me why the number of results is decreasing? For example, the number of unique tokens in the first document is 9; but when the model is trained on fewer documents, the number of tokens suddenly drops to 8, 7, etc... And yet this document contains a fixed number of tokens. Why not all of them are included in the results? How to include them? Maybe I'm doing something wrong ... I'll be grateful for your help.</p>
",2019-09-13 10:39:38,2019-09-13 10:39:38,Evaluation of TF-IDF effectiveness in Gensim. Why are the results lists incomplete?,<python><keyword><gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
22768,58002791,2019-09-19 02:11:20,,"<p>I am trying to import pre trained wiki word embeddings. I am trying to read this file so I am facing the following error</p>

<pre><code>import gensim
from gensim.models import KeyedVectors
model = gensim.models.KeyedVectors.load_word2vec_format('C:\Users\PHQ-Admin\Downloads\enwiki_20180420_100d.txt')
</code></pre>

<p>Error:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('C:\Users\PHQ-Admin\Downloads\enwiki_20180420_100d.txt')
                                                           ^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
</code></pre>
",2019-09-19 10:29:10,2019-09-19 10:29:10,python programming for machine learning,<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22777,57969707,2019-09-17 07:53:15,,"<p>In gensim's word2vec python, I want to get the list of cosine similarity for ""price"".</p>

<p>I read the document of gensim word2vec, but document it describes <code>most_similar</code> and <code>n_similarity</code> function)()</p>

<p>I want the whole list of similarity between price and all others.</p>
",2019-09-17 19:05:43,2019-09-17 19:08:20,"Python3, word2vec, How can I get the list of similarity rank about ""price"" in my model",<python><gensim><word2vec><similarity><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22781,57961188,2019-09-16 16:45:21,,"<p>I would like take word ""book"" (for example) get its vector representation, call it v_1 and find all words whose vector representation is within ball of radius r of v_1 i.e. ||v_1 - v_i||&lt;=r, for some real number r.</p>

<p>I know gensim has <code>most_similar</code> function, which allows to state number of top vectors to return, but it is not quite what I need. I surely can use brute force search and get the answer, but it will be to slow. </p>
",,2019-09-17 01:53:18,Gensim find vectors/words in ball of radius r,<python><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
22805,58004978,2019-09-19 06:32:08,,"<p>After transforming my predicted labels from images into a list all_tags and later splitting them and finally storing into word_list which has all the labels stored in a sentence like structure.</p>

<p>All I want to do is use Google's Word2Vec pre-trained model (<a href=""https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a>) to to generate and print all the Word2Vec values of my predicted labels. Imported and mapped the pretrained weight of the model, yet I'm getting the error </p>

<blockquote>
  <p>KeyError: ""word '['cliff'' not in vocabulary""</p>
</blockquote>

<p>However, the word 'cliff' is available in the dictionary. Any insight will be well appreciated. 
Please check the code snippets below for reference. </p>

<pre><code>execution_path = os.getcwd()
TEST_PATH = '/home/guest/Documents/Aikomi'


prediction = ImagePrediction()
prediction.setModelTypeAsDenseNet()
prediction.setModelPath(os.path.join(execution_path, ""/home/guest/Documents/Test1/ImageAI-master/imageai/Prediction/Weights/DenseNet.h5""))
prediction.loadModel()

pred_array = np.empty((0,6), dtype=object)

predictions, probabilities = prediction.predictImage(os.path.join(execution_path, ""1.jpg""), result_count=5)

for img in os.listdir(TEST_PATH):
    if img.endswith('.jpg'):
        image = Image.open(os.path.join(TEST_PATH, img))
        image = image.convert(""RGB"")
        image = np.array(image, dtype=np.uint8)
        predictions, probabilities = prediction.predictImage(os.path.join(TEST_PATH, img), result_count=5)
        temprow = np.zeros((1,pred_array.shape[1]),dtype=object)
        temprow[0,0] = img
        for i in range(len(predictions)):
            temprow[0,i+1] = predictions[i]
        pred_array = np.append(pred_array, temprow, axis=0)


all_tags = list(pred_array[:,1:].reshape(1,-1))
_in_sent = ' '.join(list(map(str, all_tags)))


import gensim
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize, word_tokenize
import re
import random
import nltk
nltk.download('punkt')


word_list = _in_sent.split() 

from gensim.corpora.dictionary import Dictionary

# be sure to split sentence before feed into Dictionary
word_list_2 = [d.split() for d in word_list]
dictionary = Dictionary(word_list_2)
print(""\n"", dictionary, ""\n"")

corpus_bow = [dictionary.doc2bow(doc) for doc in word_list_2]

model = Word2Vec(word_list_2, min_count= 1)
model = gensim.models.KeyedVectors.load_word2vec_format('/home/guest/Downloads/Google.bin', binary=True)

print(*map(model.most_similar, word_list))
</code></pre>
",2019-09-19 06:46:01,2019-09-19 07:51:22,"KeyError(""word '%s' not in vocabulary"" % word)",<python><machine-learning><deep-learning><nlp><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22815,58081552,2019-09-24 13:34:22,,"<p>I am using enron emails dataset. I have to assign 5 labels to these classes, namely : appreciation, escalation, sending_document, request_for_document, meeting_invites. Now, I have used doc2vec to assign labels to it using:</p>

<pre><code>emails_df['tokenized_sents'] = emails_df.iloc[0:1000].apply(lambda row: nltk.word_tokenize(row['content']), axis=1)

common_texts = [
                ['We' ,'were', 'impressed', 'with' ,'the' ,'work', 'produced' ,'by' ,'you' ,'and' ,'you' ,'showed' ,'leadership', 'qualities' ,'that' 'the' ,'rest' ,'of' ,'the', 'team' ,'could' ,'look', 'up' ,'to'],

                ['Finish' ,'the' ,'financial' ,'analysis', 'report', 'that' ,'was' ,'started' ,'last' ,'week'],

                ['Please', 'find', 'attached'],

                ['Looking', 'forward', 'to' ,'hearing' ,'from', 'you'],

                ['The' , 'meeting', 'will', 'take', 'place', 'on', 'Wednesday'],

                ['forwarded', 'to', 'xx']



    ]
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]
labels = []
#print (documents)

model = Doc2Vec(documents, size=5, window=3, min_count=1, workers=4)
#Persist a model to disk:

from gensim.test.utils import get_tmpfile
fname = get_tmpfile(""my_doc2vec_model"")

#print (fname)
#output: C:\Users\userABC\AppData\Local\Temp\my_doc2vec_model

#load model from saved file
model.save(fname)
model = Doc2Vec.load(fname)  
# you can continue training with the loaded model!
#If youre finished training a model (=no more updates, only querying, reduce memory usage), you can do:

model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)

#Infer vector for a new document:
#Here our text paragraph just 2 words
c=0
for i in emails_df['tokenized_sents']: 
    vector = model.infer_vector(i)
    c=c+1
    import operator
    index, value = max(enumerate(vector), key=operator.itemgetter(1))
    labels.append(index)
</code></pre>

<p>Here, emails_df is simply the dataframe which I read emails.csv to. I don't need a perfect labeler but I need something worthwhile. Which direction should I go to from now on to improve it a bit? (considering this is the first time I came to know about doc2vec)</p>

<p>Edit : Explanation:
I have created common_texts as a feature vector that contains sentences belonging to each class. And then I apply doc2vec and then use it's function of infer_vector to generate similarities</p>
",2019-09-24 13:58:58,2019-09-24 18:39:12,How to use doc2vec to assign labels to enron dataset,<python><nltk><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22823,58045181,2019-09-22 00:41:48,,"<p>I have a medium-sized dataset of encrypted comments and their corresponding labels, which is either of positive or negative. I wonder what the best way is to treat the missing comments given that the missing comments rate is 1%. Below is a toy example of the dataset after applying an extensive data cleaning step.</p>

<pre><code>df=pd.DataFrame({'comments':['xxy uuicz', '', 'jiko bhht'], 'label':['positive', 'negative', 'negative']})

</code></pre>

<p>I am using Gensim (preprocess_string) and removing stopwords through building a customized list of stopwords. The goal is to fit a classifier to predict the sentiment of any given encrypted comment. </p>
",2019-09-22 00:54:00,2019-09-22 00:54:00,Treating missing values in sentiment analysis,<python><pandas><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
22849,58069421,2019-09-23 20:00:29,,"<p>I have a pre-trained word2vec bin file by using skipgram. The file is pretty big (vector dimension of 200 ), over 2GB. I am thinking some methods to make the file size smaller. This bin file contains vectors for punctuation, some stop words. So, I want to know what are the options to decrease the file size for this word2vec. Is it safe to delete those punctuation and stop words rows and what would be the most effective way ?</p>
",,2019-09-24 02:20:48,gensim word2vec extremely big and what are the methods to make file size smaller?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22853,58034474,2019-09-20 19:56:48,,"<p>I'm trying to modify an example from <a href=""http://dsgeek.com/2018/02/19/tfidf_vectors.html"" rel=""nofollow noreferrer"">this post</a>
that applies tf-idf. </p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from gensim.corpora import Dictionary
from gensim.models.tfidfmodel import TfidfModel
from gensim.matutils import sparse2full
import numpy as np
import spacy

nlp  = spacy.load('en_core_web_md')


def keep_token(t):
    return (t.is_alpha and 
            not (t.is_space or t.is_punct or 
                 t.is_stop or t.like_num))

def lemmatize_doc(doc):
    return [ t.lemma_ for t in doc if keep_token(t)]

sentences = ['Pro USB and Analogue Microphone']
docs = [lemmatize_doc(nlp(doc)) for doc in sentences]
docs_dict = Dictionary(docs)
docs_dict.filter_extremes(no_below=20, no_above=0.2)
docs_dict.compactify()
docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]
model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)
docs_tfidf  = model_tfidf[docs_corpus]
docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])
tfidf_emb_vecs = np.vstack([nlp(docs_dict[i]).vector for i in range(len(docs_dict))])
docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) 


But I'm getting this error: 

   282     _warn_for_nonsequence(tup)
--&gt; 283     return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
    284 
    285 

ValueError: need at least one array to concatenate
</code></pre>

<p>The reason is that this line is retuning an empty list:</p>

<pre><code>docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]
docs_corpus
</code></pre>

<p>This is because the dictionary is empty:</p>

<p><a href=""https://i.stack.imgur.com/bbblE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bbblE.png"" alt=""enter image description here""></a></p>

<p>But I'm feeding the dic with a non empty list</p>

<p><a href=""https://i.stack.imgur.com/MbMxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MbMxY.png"" alt=""enter image description here""></a></p>

<p>That's the part I'm not finding the reason for which it fails</p>
",,2019-09-23 07:07:49,Sentence returns empty dictionary from gensim.corpora,<python><nlp><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,True
22854,58099559,2019-09-25 13:28:56,,"<p>Imagine I have a fasttext model that had been trained thanks to the Wikipedia articles (like explained on the official website).
Would it be possible to train it again with another corpus (scientific documents) that could add new / more pertinent links between words? especially for the scientific ones ?</p>

<p>To summarize, I would need the classic links that exist between all the English words coming from Wikipedia. But I would like to enhance this model with new documents about specific sectors. Is there a way to do that ? And if yes, is there a way to maybe 'ponderate' the trainings so relations coming from my custom documents would be 'more important'.</p>

<p>My final wish is to compute cosine similarity between documents that can be very scientific (that's why to have better results I thought about adding more scientific documents)</p>
",2019-09-25 13:29:36,2019-09-25 21:42:46,Training a model from multiple corpus,<python><artificial-intelligence><gensim><training-data><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22857,58069724,2019-09-23 20:24:45,,"<p>I wonder how to deploy a doc2vec model in production to create word vectors as input features to a classifier. To be specific, let say, a doc2vec model is trained on a corpus as follows.</p>

<pre><code>dataset['tagged_descriptions'] = datasetf.apply(lambda x: doc2vec.TaggedDocument(
            words=x['text_columns'], tags=[str(x.ID)]), axis=1)

model = doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=150, workers=cores,
                                window=5, hs=0, negative=5, sample=1e-5, dm_concat=1)

corpus = dataset['tagged_descriptions'].tolist()

model.build_vocab(corpus)

model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

</code></pre>

<p>and then it is dumped into a pickle file. The word vectors are used to train a classifier such as random forests to predict movies sentiment. </p>

<p>Now suppose that in production, there is a document entailing some totally new vocabularies. That being said, they were not among the ones present during the training of the doc2vec model. I wonder how to tackle such a case. </p>

<p>As a side note, I am aware of <a href=""https://stackoverflow.com/questions/47775557/updating-training-documents-for-gensim-doc2vec-model"">Updating training documents for gensim Doc2Vec model</a> and <a href=""https://stackoverflow.com/questions/39252207/gensim-how-to-retrain-doc2vec-model-using-previous-word2vec-model"">Gensim: how to retrain doc2vec model using previous word2vec model</a>. However, I would appreciate more lights to be shed on this matter. </p>
",2019-09-24 12:46:09,2019-09-24 12:46:09,How to use doc2vec model in production?,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22863,58123189,2019-09-26 19:01:59,,"<p>I am extracting the word embeddings vector from a word2vec model using model.wv. What is the range of values for each element in this vector?</p>

<pre><code>import gensim

word2vec_model = gensim.models.Word2Vec.load(""testModel"")
word2vec_model.wv[""increase""] #What is range of values for each vector element?
</code></pre>

<p>Can't seem to find this information in the documentation.</p>
",2019-09-26 20:31:05,2019-09-26 22:07:46,Range for vector values in gensim model,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22865,58123369,2019-09-26 19:15:52,,"<p>Given a list of predefined terms that can be formed by one, two or even three words, the problem is to count their ocurrences in a set of documents with a free vocabulary (ie, much many words). </p>

<pre><code>terms= [
[t1],
[t2, t3],
[t4, t5, t6],
[t7],...]
</code></pre>

<p>and the documents where this terms needs to be recognized are in the form of:</p>

<pre><code>docs = [
[w1, w2, t1, w3, w4, t7],        #d1
[w1, w4, t4, t5, t6, wi, ...],   #d2
[wj, t7, ..] ..]                 #d3
</code></pre>

<p>The desired output should be</p>

<pre><code>[2, 1, 1, ...]
</code></pre>

<p>This is, the first doc has two terms of interest, the second has 1 (formed of three words) and so on.</p>

<p>If the terms needed to be accounted for where 1 word length, then I could easily order each document alphabetically, remove repeted terms (set) and then intersect with the terms of size 1 word. Counting repeated words are the searched result.</p>

<p>But with terms of length >=2 things get tricky.</p>

<p>I've been using gensim to form a bag of words and detect the indexes when using a new phrase </p>

<p>e.g.</p>

<pre><code>dict_terms = corpora.Dictionary(phrases)

sentence = unseen_docs[0]
idxs     = dict_terms[sentence]
</code></pre>

<p>And then count the seend idxs considering if the indexes are sequential, that would mean that a single term has been seen and not 2 o 3 of them.</p>

<p>Any suggestions.</p>
",2019-09-27 05:38:00,2019-09-27 05:38:00,How to identify terms from list in unseen documents,<python><nlp><information-retrieval>,,,CC BY-SA 4.0,False,False,True,False,False
22874,58053916,2019-09-22 22:42:31,,"<p>When running with Anaconda-python and apply gensim v3.4.0 can not use attribute <code>word2vec.KeyedVectors.load word2vec</code> format</p>

<p>How do I fix the problem?</p>

<pre class=""lang-py prettyprint-override""><code>model1 = word2vec.KeyedVectors.load_word2vec_format('text2_2.model.bin', binary=True)
</code></pre>

<p>Have error with comment: </p>

<pre><code>AttributeError: module 'gensim.models.word2vec' has no attribute 'KeyedVectors'
</code></pre>
",2019-09-22 22:44:18,2019-09-22 23:26:23,'gensim.models.word2vec' has no attribute 'KeyedVectors',<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22886,58074914,2019-09-24 07:13:10,,"<p>i have code </p>

<pre><code>import time
import multiprocessing
from datetime import timedelta
from gensim.models import word2vec
start_time = time.time()
print('Training Word2Vec Model...')
sentences = word2vec.LineSentence('data/data_text.txt')
id_w2v = word2vec.Word2Vec(sentences, size=300, workers=multiprocessing.cpu_count()-1)
id_w2v.save('model_terbaru/word2vec_300.model')
</code></pre>

<p>when i make model, i have an error</p>

<pre><code>Traceback (most recent call last):

File""&lt;ipython-input-10-fc7016864a34&gt;"", line 1, in &lt;module&gt;

        runfile('F:/pa reza/model.py', wdir='F:/pa reza')

File ""C:\ProgramData\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 704, in runfile
    execfile(filename, namespace)

File ""C:\ProgramData\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

File ""F:/pa reza/model.py"", line 13, in &lt;module&gt;
    iter=10)
</code></pre>

<p>File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 527, in <strong>init</strong>
    fast_version=FAST_VERSION)</p>

<pre><code> File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\base_any2vec.py"", line 335, in __init__
        self.build_vocab(sentences, trim_rule=trim_rule)

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\base_any2vec.py"", line 480, in build_vocab
    sentences, progress_per=progress_per, trim_rule=trim_rule)

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 1151, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 1073, in __iter__
    line = utils.to_unicode(line).split()

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\utils.py"", line 359, in any2unicode

return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe3 in position 87: invalid continuation byte
</code></pre>

<p>help me....</p>
",2019-09-24 07:26:18,2019-09-24 07:26:18,'utf-8' codec can't decode byte 0xe3 in position 87 word2vec gensim,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22900,58182293,2019-10-01 09:53:47,,"<p>Hosting a word2vec model with gensim on AWS lambda </p>

<p>using python 2.7
boto==2.48.0
gensim==3.4.0</p>

<p>and I have a few lines in my function.py file where I load the model directly from s3</p>

<pre><code>print('################### connecting to s3...')
s3_conn = boto.s3.connect_to_region(
        region,
        aws_access_key_id = Aws_access_key_id,
        aws_secret_access_key = Aws_secret_access_key,
        is_secure = True,
        calling_format = OrdinaryCallingFormat()
        )
print('################### connected to s3...')
bucket = s3_conn.get_bucket(S3_BUCKET)
print('################### got bucket...')
key = bucket.get_key(S3_KEY)
print('################### got key...')
model =  KeyedVectors.load_word2vec_format(key, binary=True)
print('################### loaded model...')
</code></pre>

<p>on the model loading line</p>

<pre><code>    model =  KeyedVectors.load_word2vec_format(key, binary=True)
</code></pre>

<p>getting a mysterious error without much details:</p>

<p>on the cloud watch can see all of my print messages til '################### got key...' inclusive, 
then I get: </p>

<pre><code>START RequestId: {req_id} Version: $LATEST 
</code></pre>

<p>then right after it [no time delays between these two messages]</p>

<pre><code>module initialization error: __exit__ 
</code></pre>

<p>please, is there a way to get a detailed error or more info?</p>

<p>More background details :
I was able to download the model from s3 to /tmp/ and it did authorize and retrieve the model file, but it went out of space [file is ~2GB, /tmp/ is 512MB]</p>

<p>so, switched to directly loading the model by gensim as above and now getting that mysterious error.</p>

<p>running the function with python-lambda-local works without issues </p>

<p>so, this probably narrows it down to an issue with gensim's smart open or aws lambda, would appreciate any hints, thanks! </p>
",2019-12-03 18:09:35,2019-12-03 18:09:35,AWS Lambda Boto gensim model module initialization error: __exit__,<python><amazon-web-services><aws-lambda><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22903,58055415,2019-09-23 03:47:51,,"<p>I have been trying to use Bio2Vec for a medical word embedding project using gensim. I have downloaded ""BioWordVec_PubMed_MIMICIII_d200.bin"" from the web however, i am unable to load it. This is the error message:</p>

<blockquote>
  <p>invalid literal for int() with base 10: '\x16O/'""</p>
</blockquote>

<p>I understand that there is some invalid character in the bin file because of which I am not able to load this. However, I am not sure how to correct it.</p>

<p>I am not able to open the bin file and edit anything. Can someone help?</p>

<p>This is the code that I am using:</p>

<pre class=""lang-py prettyprint-override""><code>model = KeyedVectors.load_word2vec_format(
    datapath('BioWordVec_PubMed_MIMICIII_d200.bin'),
    encoding='windows-1252', binary=True)
</code></pre>
",2019-09-23 12:49:49,2020-01-15 16:45:01,How to load Bio2Vec in gensim?,<python-3.x><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22921,58134062,2019-09-27 11:54:40,,"<p>I am training a skipgram model using gensim word2vec. I would like to exit the training before reaching the number of epochs passed in the parameters based on a specific accuracy test in a different set of data in order to avoid the overfitting of the model.</p>

<p>Is there a way in gensim to interrupt the train of word2vec from a callback function?</p>
",2020-02-12 15:51:03,2020-02-12 15:51:03,How to break the Word2vec training from a callback function?,<python><callback><gensim><word2vec><early-stopping>,,,CC BY-SA 4.0,False,False,True,False,False
22924,58186670,2019-10-01 14:12:27,,"<p>I'm building a Word2Vec model for a category-recommendation on a dataset consisting of ~35.000 sentences for a total of ~500.000 words but only ~3.000 distinct ones.
I build the model basically like this :</p>

<pre class=""lang-py prettyprint-override""><code>def train_w2v_model(df, epochs):
    w2v_model = Word2Vec(min_count=5,
                                 window=100,
                                 size=230,
                                 sample=0,
                                 workers=cores-1,
                                 batch_words=100)
    vocab = df['sentences'].apply(list)
    w2v_model.build_vocab(vocab)
    w2v_model.train(vocab, total_examples=w2v_model.corpus_count, total_words=w2v_model.corpus_total_words, epochs=epochs, compute_loss=True)
    return w2v_model.get_latest_training_loss()
</code></pre>

<p>I tried to find the right number of epochs for such a model like this :</p>

<pre class=""lang-py prettyprint-override""><code>print(train_w2v_model(1))
=&gt;&gt; 86898.2109375
print(train_w2v_model(100))
=&gt;&gt; 5025273.0
</code></pre>

<p>I find the results very counterintuitive.
I do not understand how increasing the number of epochs could lead to lower the performance.
It seems not to be a misunderstanding from the function <code>get_latest_training_loss</code> since I observe the results with the function <code>most_similar</code> way better with only 1 epoch :</p>

<p>100 epochs :</p>

<pre class=""lang-py prettyprint-override""><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('salesforce', 0.3464601933956146),
 ('marketing_relationnel', 0.3125850558280945),
 ('batiment', 0.30903393030166626),
 ('go', 0.29414454102516174),
 ('simulation', 0.2930642068386078),
 ('data_management', 0.28968319296836853),
 ('scraping', 0.28260597586631775),
 ('virtualisation', 0.27560457587242126),
 ('dataviz', 0.26913416385650635),
 ('pandas', 0.2685554623603821)]
</code></pre>

<p>1 epoch :</p>

<pre><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('data_science', 0.9953729510307312),
 ('data_mining', 0.9930223822593689),
 ('big_data', 0.9894922375679016),
 ('spark', 0.9881765842437744),
 ('nlp', 0.9879133701324463),
 ('hadoop', 0.9834049344062805),
 ('deep_learning', 0.9831978678703308),
 ('r', 0.9827396273612976),
 ('data_visualisation', 0.9805369973182678),
 ('nltk', 0.9800992012023926)]
</code></pre>

<p>Any insight on why it behaves like this ? I would have think that increasing the number of epochs would have for sure a positive effect on the <strong>training</strong> loss.</p>
",2019-10-02 08:09:53,2019-10-02 08:09:53,Gensim Word2Vec model getting worse by increasing the number of epochs,<python><machine-learning><nlp><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22935,58101800,2019-09-25 15:24:23,,"<p>I am trying to find categories for a website() via the content that is present on the same but my LDA model keeps returning very similar topics. Not sure if my corpus or dictionary are having issues </p>

<p>I have tried to follow a lot of online tutorials which show a very similar code but somehow my code is not working correctly. I have tried multi core LDA as well. I am thinking to use TF-IDF but I think this should have worked fine on its own. I am open to any suggestions , if LDA is not the way to go please let me know.</p>

<h1>I have not considered the entire website here, but that is the ultimate aim.</h1>

<pre><code>import random
import pandas as pd
import re
import nltk
from nltk import word_tokenize
from nltk.util import ngrams
from collections import Counter
from nltk.collocations import *
from nltk import FreqDist
import string
from sklearn.feature_extraction.text import CountVectorizer
import csv
import gensim
from gensim.corpora import Dictionary
from gensim.models import CoherenceModel, LdaModel, LdaMulticore
from gensim import corpora, models
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
from pprint import pprint
import matplotlib.pyplot as plt
import tokenize
from urllib.request import urlopen
from bs4 import BeautifulSoup
from nltk import word_tokenize
import requests
random.seed(1999)
np.random.seed(1999)

url_list=['https://www.aarp.org/caregiving/answers/info-2017/adult-day-care.html','https://www.aarp.org/caregiving/answers/info-2017/aging-parent-stop-driving.html','https://www.aarp.org/travel/vacation-ideas/cruises/info-2019/how-to-save.html',""https://www.aarp.org/caregiving/home-care/info-2018/hiring-caregiver.html"",""https://www.aarp.org/travel/vacation-ideas/cruises/info-2019/plan-by-budget.html""]
All_text=[]
myWords=[]
myWords_internal=[]
myWords_External=[]
custom_stopwords=[""share"", ""print"", ""en"", ""espaol"",""espanol"", ""register"",  ""login"",    ""search"",   ""join"",     ""discussion"",   ""please"",   ""leave "",   ""comment"", ""comments"", ""must"", ""logged"",    ""aarp"",     ""email"",    ""org"",  ""using"",    ""linkedin"",     ""twitter"",  ""facebook"",""javascript"", ""www"", ""qa""]
ps=nltk.WordNetLemmatizer()
stopwords = nltk.corpus.stopwords.words('english')
for i in custom_stopwords:
    stopwords.append(i)

for url in url_list:
    All_text=[]
    myWords=[]
    myWords_internal=[]

    answer = requests.get(url)

    html = urlopen(url)
    soup = BeautifulSoup(html, ""html5lib"")

    text = (''.join(s.findAll(text=True))for s in soup.findAll({'h1' : True,'h2' : True,'h3' : True,'h4' : True,'p' : True}))

    for y in text:
#   
      g=y.lower()
#      
      z=re.split('\W+',g)

      texts= "" "".join([word for word in z if word not in string.punctuation])

      tokens= re.split('\W+',texts)

      texts2= "" "".join([ps.lemmatize(word) for word in tokens if word not in stopwords])

      tokens= re.split('\W+',texts2)

      texts3= "" "".join([word for word in tokens if word.isnumeric() == False])
      tokens= re.split('\W+',texts3)

      texts4="" "".join([word for word in tokens if word != ""  ""])
      tokens= re.split('\W+',texts4)
      texts5="" "".join([word for word in tokens if word != """"])
      tokens= re.split('\W+',texts5)
      str_list = list(filter(None, tokens))

      myWords_internal.append(str_list)

      myWords = [item for sublist in myWords_internal for item in sublist]

    myWords_External.append(myWords)

dictionary = corpora.Dictionary(myWords_External)

corpus1 = [dictionary.doc2bow(i) for i in myWords_External]

limit=15
start=1 
step=1
coherence_values = []
model_list = []
for num_topics in range(start, limit, step):
  print(num_topics)
  lda_model = LdaModel(corpus=corpus1, num_topics=num_topics, id2word=dictionary)
#  lda_model = gensim.models.LdaMulticore(corpus1, num_topics=num_topics, id2word=dictionary, passes=1, workers=2)

  print(lda_model.show_topics())

  model_list.append(lda_model)

  coherencemodel = CoherenceModel(model=lda_model, texts=myWords_External, dictionary=dictionary, coherence='c_v')
  coherence_values.append(coherencemodel.get_coherence())

print(coherence_values)                               #Cross-Verifying the Coherence value
print(*coherence_values, sep = ""\n"")
</code></pre>

<p>Below is a sample of results if i take the entire website into consideration.</p>

<pre><code>1 [(0, '0.010*""receive"" + 0.008*""member"" + 0.008*""benefit"" + 0.008*""save"" + 0.005*""people"" + 0.004*""say"" + 0.004*""related"" + 0.004*""make"" + 0.004*""age"" + 0.004*""year""')] 
2 [(0, '0.008*""member"" + 0.007*""receive"" + 0.007*""save"" + 0.006*""benefit"" + 0.005*""people"" + 0.005*""say"" + 0.005*""one"" + 0.005*""age"" + 0.004*""related"" + 0.004*""year""'), (1, '0.013*""receive"" + 0.009*""member"" + 0.009*""benefit"" + 0.008*""save"" + 0.006*""enter"" + 0.005*""valid"" + 0.005*""people"" + 0.005*""make"" + 0.004*""social"" + 0.004*""free""')] 
3 [(0, '0.011*""receive"" + 0.010*""member"" + 0.009*""benefit"" + 0.008*""save"" + 0.006*""people"" + 0.005*""subscription"" + 0.005*""valid"" + 0.005*""provider"" + 0.004*""social"" + 0.004*""leave""'), (1, '0.008*""save"" + 0.007*""member"" + 0.007*""receive"" + 0.007*""benefit"" + 0.004*""age"" + 0.004*""make"" + 0.004*""one"" + 0.004*""say"" + 0.004*""related"" + 0.004*""people""'), (2, '0.010*""receive"" + 0.007*""member"" + 0.007*""benefit"" + 0.006*""save"" + 0.005*""people"" + 0.005*""say"" + 0.005*""year"" + 0.005*""one"" + 0.004*""provider"" + 0.004*""get""')] 
4 [(0, '0.008*""member"" + 0.007*""receive"" + 0.006*""save"" + 0.005*""year"" + 0.005*""benefit"" + 0.005*""people"" + 0.004*""related"" + 0.004*""one"" + 0.004*""subscription"" + 0.004*""also""'), (1, '0.010*""receive"" + 0.008*""member"" + 0.008*""save"" + 0.007*""benefit"" + 0.006*""people"" + 0.005*""age"" + 0.005*""free"" + 0.005*""volunteering"" + 0.005*""provider"" + 0.005*""make""'), (2, '0.011*""receive"" + 0.011*""benefit"" + 0.010*""member"" + 0.009*""save"" + 0.006*""social"" + 0.006*""valid"" + 0.006*""enter"" + 0.005*""say"" + 0.005*""age"" + 0.005*""people""'), (3, '0.008*""receive"" + 0.006*""member"" + 0.006*""people"" + 0.006*""save"" + 0.005*""benefit"" + 0.005*""say"" + 0.004*""year"" + 0.004*""related"" + 0.004*""also"" + 0.004*""make""')] 
5 [(0, '0.008*""member"" + 0.008*""receive"" + 0.007*""benefit"" + 0.006*""save"" + 0.005*""make"" + 0.005*""people"" + 0.004*""subscription"" + 0.004*""insurance"" + 0.004*""volunteering"" + 0.004*""leave""'), (1, '0.009*""receive"" + 0.008*""member"" + 0.006*""say"" + 0.006*""save"" + 0.006*""benefit"" + 0.005*""people"" + 0.005*""way"" + 0.005*""make"" + 0.004*""year"" + 0.004*""one""'), (2, '0.012*""receive"" + 0.008*""member"" + 0.007*""save"" + 0.007*""benefit"" + 0.006*""age"" + 0.006*""home"" + 0.005*""related"" + 0.005*""people"" + 0.005*""year"" + 0.005*""one""'), (3, '0.011*""benefit"" + 0.011*""receive"" + 0.010*""member"" + 0.010*""save"" + 0.008*""enter"" + 0.007*""valid"" + 0.006*""social"" + 0.006*""people"" + 0.005*""related"" + 0.005*""location""'), (4, '0.008*""receive"" + 0.007*""save"" + 0.007*""member"" + 0.006*""benefit"" + 0.005*""people"" + 0.004*""year"" + 0.004*""say"" + 0.004*""make"" + 0.004*""provider"" + 0.004*""volunteering""')] 
6 [(0, '0.012*""enter"" + 0.012*""valid"" + 0.007*""location"" + 0.007*""save"" + 0.007*""receive"" + 0.006*""room"" + 0.006*""member"" + 0.006*""date"" + 0.005*""people"" + 0.004*""benefit""'), (1, '0.009*""member"" + 0.008*""receive"" + 0.007*""save"" + 0.006*""benefit"" + 0.005*""year"" + 0.005*""say"" + 0.005*""people"" + 0.005*""make"" + 0.005*""one"" + 0.004*""leave""'), (2, '0.009*""receive"" + 0.008*""member"" + 0.008*""save"" + 0.008*""benefit"" + 0.005*""people"" + 0.005*""one"" + 0.005*""related"" + 0.004*""make"" + 0.004*""provider"" + 0.004*""say""'), (3, '0.012*""benefit"" + 0.009*""receive"" + 0.008*""social"" + 0.008*""member"" + 0.007*""save"" + 0.006*""security"" + 0.005*""age"" + 0.005*""say"" + 0.005*""year"" + 0.005*""make""'), (4, '0.015*""receive"" + 0.011*""member"" + 0.009*""benefit"" + 0.009*""save"" + 0.006*""age"" + 0.006*""people"" + 0.006*""related"" + 0.006*""subscription"" + 0.005*""provider"" + 0.005*""insurance""'), (5, '0.009*""receive"" + 0.007*""member"" + 0.007*""save"" + 0.006*""say"" + 0.005*""people"" + 0.005*""benefit"" + 0.005*""day"" + 0.004*""make"" + 0.004*""one"" + 0.004*""volunteering""')] 
7 [(0, '0.008*""receive"" + 0.008*""member"" + 0.006*""save"" + 0.006*""benefit"" + 0.005*""get"" + 0.004*""social"" + 0.004*""say"" + 0.004*""make"" + 0.004*""also"" + 0.004*""one""'), (1, '0.012*""valid"" + 0.011*""enter"" + 0.008*""save"" + 0.008*""receive"" + 0.008*""location"" + 0.007*""member"" + 0.006*""room"" + 0.006*""benefit"" + 0.006*""date"" + 0.004*""people""'), (2, '0.008*""benefit"" + 0.008*""receive"" + 0.007*""age"" + 0.007*""say"" + 0.006*""people"" + 0.006*""social"" + 0.005*""member"" + 0.005*""save"" + 0.005*""medicare"" + 0.005*""year""'), (3, '0.008*""receive"" + 0.007*""member"" + 0.006*""save"" + 0.005*""one"" + 0.005*""benefit"" + 0.005*""people"" + 0.004*""subscription"" + 0.004*""year"" + 0.004*""family"" + 0.004*""home""'), (4, '0.009*""receive"" + 0.008*""member"" + 0.008*""save"" + 0.006*""benefit"" + 0.005*""people"" + 0.005*""related"" + 0.004*""community"" + 0.004*""say"" + 0.004*""u"" + 0.004*""new""'), (5, '0.014*""receive"" + 0.012*""member"" + 0.010*""save"" + 0.009*""benefit"" + 0.006*""provider"" + 0.006*""people"" + 0.006*""volunteering"" + 0.006*""confirm"" + 0.006*""social"" + 0.006*""leave""'), (6, '0.011*""receive"" + 0.010*""benefit"" + 0.009*""member"" + 0.008*""save"" + 0.006*""people"" + 0.005*""age"" + 0.005*""say"" + 0.005*""make"" + 0.005*""provider"" + 0.004*""related""')] 
8 [(0, '0.006*""receive"" + 0.006*""member"" + 0.006*""save"" + 0.005*""benefit"" + 0.005*""people"" + 0.004*""one"" + 0.004*""make"" + 0.004*""year"" + 0.004*""age"" + 0.004*""also""'), (1, '0.007*""say"" + 0.006*""receive"" + 0.006*""member"" + 0.006*""save"" + 0.006*""year"" + 0.005*""people"" + 0.005*""enter"" + 0.005*""benefit"" + 0.005*""time"" + 0.005*""one""'), (2, '0.011*""receive"" + 0.011*""member"" + 0.010*""benefit"" + 0.008*""save"" + 0.006*""social"" + 0.006*""related"" + 0.006*""volunteering"" + 0.006*""people"" + 0.006*""age"" + 0.006*""leave""'), (3, '0.009*""benefit"" + 0.008*""receive"" + 0.006*""people"" + 0.006*""save"" + 0.006*""member"" + 0.006*""say"" + 0.005*""get"" + 0.005*""year"" + 0.004*""way"" + 0.004*""make""'), (4, '0.010*""receive"" + 0.009*""member"" + 0.008*""save"" + 0.006*""benefit"" + 0.006*""drug"" + 0.005*""people"" + 0.005*""health"" + 0.005*""say"" + 0.004*""year"" + 0.004*""make""'), (5, '0.010*""receive"" + 0.009*""member"" + 0.008*""valid"" + 0.008*""enter"" + 0.008*""save"" + 0.007*""benefit"" + 0.005*""location"" + 0.005*""home"" + 0.005*""room"" + 0.004*""people""'), (6, '0.013*""receive"" + 0.011*""member"" + 0.010*""save"" + 0.010*""benefit"" + 0.006*""provider"" + 0.005*""related"" + 0.005*""people"" + 0.005*""age"" + 0.005*""confirm"" + 0.005*""subscription""'), (7, '0.009*""receive"" + 0.006*""benefit"" + 0.006*""save"" + 0.005*""member"" + 0.005*""social"" + 0.005*""related"" + 0.005*""use"" + 0.004*""security"" + 0.004*""subscription"" + 0.004*""provider""')] 
9 [(0, '0.010*""enter"" + 0.009*""valid"" + 0.006*""member"" + 0.006*""receive"" + 0.006*""save"" + 0.005*""room"" + 0.005*""date"" + 0.005*""location"" + 0.004*""say"" + 0.004*""also""'), (1, '0.011*""valid"" + 0.011*""receive"" + 0.010*""enter"" + 0.010*""member"" + 0.007*""benefit"" + 0.007*""save"" + 0.007*""location"" + 0.006*""date"" + 0.006*""people"" + 0.005*""room""'), (2, '0.012*""receive"" + 0.011*""member"" + 0.011*""save"" + 0.010*""benefit"" + 0.006*""subscription"" + 0.006*""people"" + 0.006*""related"" + 0.006*""leave"" + 0.006*""provider"" + 0.005*""volunteering""'), (3, '0.006*""receive"" + 0.006*""people"" + 0.005*""save"" + 0.005*""year"" + 0.004*""say"" + 0.004*""benefit"" + 0.004*""home"" + 0.004*""get"" + 0.004*""age"" + 0.004*""make""'), (4, '0.011*""receive"" + 0.010*""member"" + 0.008*""save"" + 0.008*""benefit"" + 0.006*""provider"" + 0.006*""make"" + 0.006*""medicare"" + 0.005*""people"" + 0.005*""related"" + 0.004*""volunteering""'), (5, '0.008*""receive"" + 0.005*""save"" + 0.005*""member"" + 0.005*""say"" + 0.005*""home"" + 0.005*""benefit"" + 0.004*""one"" + 0.004*""get"" + 0.004*""people"" + 0.004*""related""'), (6, '0.009*""benefit"" + 0.009*""receive"" + 0.008*""save"" + 0.007*""member"" + 0.007*""social"" + 0.005*""age"" + 0.005*""year"" + 0.005*""security"" + 0.004*""community"" + 0.004*""make""'), (7, '0.009*""receive"" + 0.008*""member"" + 0.008*""benefit"" + 0.007*""say"" + 0.006*""people"" + 0.006*""save"" + 0.006*""year"" + 0.005*""make"" + 0.005*""age"" + 0.004*""related""'), (8, '0.007*""receive"" + 0.007*""benefit"" + 0.006*""member"" + 0.006*""save"" + 0.005*""people"" + 0.004*""home"" + 0.004*""say"" + 0.004*""make"" + 0.004*""one"" + 0.004*""year""')] 
10 [(0, '0.013*""receive"" + 0.011*""member"" + 0.010*""save"" + 0.009*""benefit"" + 0.006*""people"" + 0.006*""provider"" + 0.005*""age"" + 0.005*""subscription"" + 0.005*""free"" + 0.005*""related""'), (1, '0.009*""receive"" + 0.007*""member"" + 0.006*""save"" + 0.005*""benefit"" + 0.005*""make"" + 0.004*""people"" + 0.004*""one"" + 0.004*""related"" + 0.004*""also"" + 0.004*""volunteering""'), (2, '0.006*""say"" + 0.006*""receive"" + 0.006*""save"" + 0.006*""member"" + 0.005*""get"" + 0.005*""benefit"" + 0.005*""year"" + 0.005*""people"" + 0.004*""make"" + 0.004*""one""'), (3, '0.009*""receive"" + 0.007*""member"" + 0.006*""save"" + 0.006*""benefit"" + 0.006*""year"" + 0.005*""make"" + 0.005*""people"" + 0.005*""provider"" + 0.005*""valid"" + 0.004*""enter""'), (4, '0.010*""receive"" + 0.010*""save"" + 0.008*""benefit"" + 0.008*""member"" + 0.006*""age"" + 0.005*""year"" + 0.005*""subscription"" + 0.005*""say"" + 0.004*""provider"" + 0.004*""social""'), (5, '0.011*""receive"" + 0.008*""benefit"" + 0.008*""member"" + 0.007*""save"" + 0.006*""people"" + 0.006*""health"" + 0.005*""care"" + 0.005*""also"" + 0.005*""home"" + 0.005*""volunteering""'), (6, '0.013*""receive"" + 0.011*""save"" + 0.010*""member"" + 0.008*""benefit"" + 0.006*""enter"" + 0.006*""valid"" + 0.005*""provider"" + 0.005*""location"" + 0.005*""related"" + 0.005*""people""'), (7, '0.007*""member"" + 0.007*""valid"" + 0.006*""enter"" + 0.006*""receive"" + 0.006*""save"" + 0.005*""say"" + 0.005*""people"" + 0.004*""day"" + 0.004*""year"" + 0.004*""new""'), (8, '0.015*""benefit"" + 0.013*""member"" + 0.011*""receive"" + 0.009*""social"" + 0.009*""save"" + 0.008*""security"" + 0.006*""people"" + 0.006*""age"" + 0.006*""year"" + 0.005*""related""'), (9, '0.008*""enter"" + 0.008*""valid"" + 0.008*""receive"" + 0.006*""save"" + 0.006*""member"" + 0.005*""say"" + 0.005*""location"" + 0.005*""benefit"" + 0.005*""date"" + 0.004*""related""')] 
11 [(10, '0.015*""enter"" + 0.015*""valid"" + 0.010*""receive"" + 0.009*""location"" + 0.009*""save"" + 0.008*""member"" + 0.008*""room"" + 0.007*""date"" + 0.007*""benefit"" + 0.004*""volunteering""'), (5, '0.010*""receive"" + 0.008*""member"" + 0.007*""benefit"" + 0.006*""medicare"" + 0.006*""care"" + 0.006*""health"" + 0.006*""insurance"" + 0.005*""people"" + 0.005*""drug"" + 0.005*""save""'), (1, '0.014*""benefit"" + 0.013*""social"" + 0.011*""security"" + 0.010*""receive"" + 0.009*""member"" + 0.008*""save"" + 0.005*""also"" + 0.005*""people"" + 0.005*""day"" + 0.004*""make""'), (8, '0.008*""receive"" + 0.007*""member"" + 0.006*""benefit"" + 0.005*""also"" + 0.005*""people"" + 0.005*""save"" + 0.004*""say"" + 0.004*""subscription"" + 0.004*""age"" + 0.004*""hearing""'), (4, '0.007*""receive"" + 0.007*""benefit"" + 0.007*""age"" + 0.007*""member"" + 0.007*""save"" + 0.007*""people"" + 0.006*""make"" + 0.005*""related"" + 0.005*""provider"" + 0.005*""say""'), (9, '0.013*""receive"" + 0.010*""member"" + 0.009*""save"" + 0.008*""benefit"" + 0.005*""one"" + 0.005*""say"" + 0.005*""related"" + 0.005*""people"" + 0.005*""provider"" + 0.005*""subscription""'), (3, '0.006*""member"" + 0.006*""say"" + 0.006*""receive"" + 0.005*""one"" + 0.004*""save"" + 0.004*""year"" + 0.004*""benefit"" + 0.004*""u"" + 0.004*""time"" + 0.004*""home""'), (0, '0.013*""receive"" + 0.010*""member"" + 0.008*""benefit"" + 0.007*""save"" + 0.007*""people"" + 0.006*""provider"" + 0.005*""subscription"" + 0.005*""say"" + 0.005*""related"" + 0.005*""confirm""'), (2, '0.011*""receive"" + 0.010*""save"" + 0.009*""member"" + 0.006*""people"" + 0.005*""benefit"" + 0.005*""leave"" + 0.005*""volunteering"" + 0.005*""help"" + 0.005*""age"" + 0.005*""make""'), (7, '0.008*""say"" + 0.007*""benefit"" + 0.006*""receive"" + 0.006*""year"" + 0.005*""save"" + 0.005*""member"" + 0.005*""people"" + 0.004*""like"" + 0.004*""time"" + 0.004*""one""')] 
12 [(0, '0.009*""benefit"" + 0.009*""social"" + 0.008*""receive"" + 0.006*""save"" + 0.006*""security"" + 0.006*""member"" + 0.004*""medicare"" + 0.004*""make"" + 0.004*""tax"" + 0.004*""year""'), (7, '0.013*""receive"" + 0.008*""save"" + 0.008*""member"" + 0.007*""benefit"" + 0.005*""time"" + 0.005*""subscription"" + 0.004*""provider"" + 0.004*""related"" + 0.004*""leave"" + 0.004*""insurance""'), (8, '0.012*""receive"" + 0.008*""save"" + 0.007*""member"" + 0.007*""benefit"" + 0.006*""insurance"" + 0.006*""age"" + 0.005*""people"" + 0.005*""health"" + 0.005*""help"" + 0.005*""make""'), (2, '0.015*""benefit"" + 0.011*""receive"" + 0.009*""save"" + 0.008*""social"" + 0.008*""member"" + 0.006*""security"" + 0.006*""age"" + 0.005*""year"" + 0.005*""people"" + 0.005*""related""'), (6, '0.009*""receive"" + 0.007*""member"" + 0.006*""benefit"" + 0.006*""save"" + 0.005*""medicare"" + 0.004*""people"" + 0.004*""provider"" + 0.004*""age"" + 0.004*""say"" + 0.004*""also""'), (10, '0.019*""enter"" + 0.018*""valid"" + 0.010*""location"" + 0.009*""room"" + 0.009*""date"" + 0.007*""save"" + 0.006*""member"" + 0.006*""receive"" + 0.005*""benefit"" + 0.005*""flight""'), (3, '0.007*""member"" + 0.007*""receive"" + 0.005*""benefit"" + 0.005*""say"" + 0.005*""people"" + 0.004*""one"" + 0.004*""way"" + 0.004*""save"" + 0.004*""related"" + 0.004*""year""'), (1, '0.013*""receive"" + 0.013*""member"" + 0.012*""save"" + 0.010*""benefit"" + 0.008*""people"" + 0.007*""related"" + 0.006*""provider"" + 0.006*""say"" + 0.006*""volunteering"" + 0.006*""confirm""'), (5, '0.007*""member"" + 0.007*""receive"" + 0.006*""year"" + 0.006*""percent"" + 0.005*""people"" + 0.005*""save"" + 0.005*""say"" + 0.004*""job"" + 0.004*""u"" + 0.004*""benefit""'), (11, '0.012*""member"" + 0.008*""receive"" + 0.008*""save"" + 0.007*""benefit"" + 0.005*""get"" + 0.004*""also"" + 0.004*""people"" + 0.004*""volunteering"" + 0.004*""confirm"" + 0.004*""free""')] 
13 [(2, '0.015*""benefit"" + 0.014*""receive"" + 0.013*""social"" + 0.011*""security"" + 0.011*""save"" + 0.010*""member"" + 0.007*""age"" + 0.006*""people"" + 0.005*""provider"" + 0.005*""year""'), (0, '0.009*""receive"" + 0.009*""member"" + 0.007*""save"" + 0.006*""people"" + 0.006*""benefit"" + 0.006*""home"" + 0.006*""year"" + 0.005*""say"" + 0.005*""age"" + 0.005*""related""'), (1, '0.009*""enter"" + 0.008*""receive"" + 0.007*""member"" + 0.007*""valid"" + 0.007*""save"" + 0.006*""room"" + 0.005*""benefit"" + 0.005*""date"" + 0.004*""make"" + 0.004*""year""'), (5, '0.020*""valid"" + 0.017*""enter"" + 0.012*""location"" + 0.009*""room"" + 0.008*""date"" + 0.008*""save"" + 0.008*""receive"" + 0.008*""member"" + 0.007*""benefit"" + 0.005*""flight""'), (8, '0.008*""member"" + 0.008*""receive"" + 0.007*""save"" + 0.006*""benefit"" + 0.005*""people"" + 0.005*""health"" + 0.005*""drug"" + 0.005*""say"" + 0.004*""related"" + 0.004*""also""'), (7, '0.010*""receive"" + 0.008*""save"" + 0.007*""member"" + 0.005*""benefit"" + 0.005*""make"" + 0.004*""related"" + 0.004*""volunteering"" + 0.004*""car"" + 0.004*""say"" + 0.004*""provider""'), (11, '0.008*""receive"" + 0.008*""say"" + 0.008*""member"" + 0.007*""save"" + 0.007*""benefit"" + 0.006*""year"" + 0.006*""people"" + 0.005*""make"" + 0.005*""subscription"" + 0.005*""age""'), (12, '0.010*""receive"" + 0.008*""benefit"" + 0.007*""people"" + 0.007*""member"" + 0.006*""save"" + 0.005*""one"" + 0.005*""related"" + 0.005*""say"" + 0.005*""provider"" + 0.004*""age""'), (6, '0.014*""member"" + 0.014*""receive"" + 0.011*""save"" + 0.009*""benefit"" + 0.006*""related"" + 0.006*""provider"" + 0.005*""subscription"" + 0.005*""leave"" + 0.005*""confirm"" + 0.005*""people""'), (3, '0.010*""receive"" + 0.008*""benefit"" + 0.007*""save"" + 0.007*""medicare"" + 0.007*""insurance"" + 0.006*""member"" + 0.006*""people"" + 0.005*""health"" + 0.005*""get"" + 0.005*""volunteering""')] 
14 [(2, '0.009*""member"" + 0.009*""receive"" + 0.008*""benefit"" + 0.008*""medicare"" + 0.006*""save"" + 0.005*""people"" + 0.005*""related"" + 0.005*""year"" + 0.005*""home"" + 0.004*""volunteering""'), (5, '0.009*""receive"" + 0.008*""benefit"" + 0.008*""member"" + 0.007*""save"" + 0.005*""year"" + 0.004*""day"" + 0.004*""one"" + 0.004*""subscription"" + 0.004*""say"" + 0.004*""make""'), (4, '0.008*""save"" + 0.007*""receive"" + 0.006*""valid"" + 0.006*""car"" + 0.005*""location"" + 0.005*""enter"" + 0.005*""member"" + 0.005*""benefit"" + 0.004*""related"" + 0.004*""say""'), (0, '0.008*""receive"" + 0.007*""year"" + 0.007*""job"" + 0.007*""benefit"" + 0.006*""say"" + 0.006*""people"" + 0.006*""member"" + 0.005*""make"" + 0.004*""work"" + 0.004*""one""'), (9, '0.007*""receive"" + 0.006*""benefit"" + 0.006*""member"" + 0.005*""eye"" + 0.005*""people"" + 0.005*""age"" + 0.005*""make"" + 0.004*""say"" + 0.004*""one"" + 0.004*""may""'), (11, '0.033*""valid"" + 0.032*""enter"" + 0.018*""location"" + 0.017*""room"" + 0.016*""date"" + 0.008*""member"" + 0.008*""flight"" + 0.007*""save"" + 0.007*""receive"" + 0.005*""seat""'), (7, '0.009*""member"" + 0.007*""benefit"" + 0.007*""receive"" + 0.007*""save"" + 0.005*""one"" + 0.004*""people"" + 0.004*""make"" + 0.004*""insurance"" + 0.004*""say"" + 0.004*""volunteering""'), (8, '0.008*""receive"" + 0.008*""benefit"" + 0.007*""save"" + 0.007*""age"" + 0.006*""people"" + 0.006*""social"" + 0.005*""member"" + 0.004*""make"" + 0.004*""one"" + 0.004*""also""'), (10, '0.011*""receive"" + 0.008*""health"" + 0.008*""save"" + 0.008*""member"" + 0.006*""people"" + 0.006*""age"" + 0.005*""community"" + 0.005*""help"" + 0.005*""benefit"" + 0.005*""subscription""'), (1, '0.012*""benefit"" + 0.011*""receive"" + 0.010*""member"" + 0.009*""save"" + 0.006*""related"" + 0.006*""people"" + 0.006*""social"" + 0.006*""provider"" + 0.005*""confirm"" + 0.005*""volunteering""')]
</code></pre>
",2019-09-25 15:58:08,2019-09-25 15:58:08,LDA Topics have no variations,<python><nlp><databricks><lda>,,,CC BY-SA 4.0,True,False,True,False,True
22944,58238043,2019-10-04 14:08:54,,"<p>After creating a FastText model using Gensim, I want to load it but am running into errors seemingly related to callbacks. </p>

<p>The code used to create the model is</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 50
DIMS = 256

vocab_model = gensim.models.FastText(sentences=model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     workers=6,
                                     min_count=MIN_COUNT,
                                     callbacks=[EpochSaver(""./ftchkpts/"")])

vocab_model.save('ft_256_min_50_model_30eps')
</code></pre>

<p>and the callback <code>EpochSaver</code> is defined as</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.callbacks import CallbackAny2Vec

class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0
        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, f""ft256_{self.epoch}e"")
        model.save(savepath)
        print(f""Epoch saved: {self.epoch + 1}"")
        if os.path.isfile(os.path.join(self.savedir, f""ft256_{self.epoch-1}e"")):
            os.remove(os.path.join(self.savedir,  f""ft256_{self.epoch-1}e""))
            print(""Previous model deleted "")
        self.epoch += 1
</code></pre>

<p>Aside from the type of model, this is identical to my process for Word2Vec which worked without issue. However when I open another file and try to load the model with</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
vocab = FastText.load(r'vocab/ft_256_min_50_model_30eps')
</code></pre>

<p>I'm greeted with the error</p>

<blockquote>
  <p><code>AttributeError: Can't get attribute 'EpochSaver' on &lt;module '__main__'&gt;</code></p>
</blockquote>

<p>What can I do to get the vocabulary to load so I can create the embedding layer for my keras model? If it's relevant, this is happening in JupyterLab.</p>
",2019-10-04 16:02:37,2019-10-04 16:08:37,Loading Gensim FastText Model with Callbacks Fails,<python><callback><gensim><jupyter-lab><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
22953,58171114,2019-09-30 15:36:53,,"<p>I'm trying to implement a topic detection function with the HdpModel of Gensim. I choose the HdpModel since it is a model that does not require to know, a priori, the number of topics to detect. That's really cool. </p>

<p>The problem is that the method to generate the topics (print_topics) receives an argument to indicate the number of topics. The docs say that the num_topics param (which is optional) will indicate the number of topics to be selected, and that passing the -1 value will result on all topics be retrieved by significance. </p>

<p>But when I set -1 it retrieves no topics. If I try not defining the parameter, just calling print_topics(), the default number of topics is always returned (20 topics). So, how can I retrieve all the possible events? This is supposed to be the main contribution of Hdp.</p>

<p>Thanks!</p>
",,2019-09-30 15:36:53,Get all the possible topics with Gensim HdpModel,<gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
22962,58153359,2019-09-29 07:57:23,,"<p>I've created a dictionary with the document-topic probabilities from a Gensim LDA model. Each iteration over the dictionary (even with the same exact code) produces slightly different values. Why is this? (Note, when the same code is copied and pasted in another jupyter cell)</p>

<pre><code>for r in doc_topics[:2]:
    print(r)

</code></pre>

<p>First time produces:</p>

<pre><code>[(5, 0.46771166), (8, 0.09964698), (12, 0.08084056), (55, 0.16801219), (58, 0.07947531), (97, 0.04642806)]
[(8, 0.7273078), (69, 0.06939292), (78, 0.062151615), (101, 0.119957164)]
</code></pre>

<p>Second run produces:</p>

<pre><code>[(5, 0.47463417), (8, 0.105600394), (12, 0.06531593), (55, 0.16066092), (58, 0.06662597), (97, 0.054465853)]
[(8, 0.7306167), (69, 0.054978732), (78, 0.06831972), (84, 0.025588958), (101, 0.10244013)]
</code></pre>

<p>Third:</p>

<pre><code>[(5, 0.4771855), (8, 0.09988891), (12, 0.088423), (55, 0.15682992), (58, 0.058175407), (97, 0.053951494)]
[(8, 0.75193375), (69, 0.059308972), (78, 0.0622621), (84, 0.020040851), (101, 0.09659243)]
</code></pre>

<p>And so on...</p>
",,2019-09-29 08:06:02,Why do different runs of the same iteration produce different results?,<python><pandas><loops><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
22963,58155131,2019-09-29 12:16:48,,"<p>I am wondering which steps I have to execute on my corpus to pre-process it the same style like google did for their massive, pre-trained word2vec model (<a href=""https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a> )</p>

<p>According to the website they did the following:</p>

<ul>
<li>bigrams/ trigrams</li>
<li>removal of some stop words (only common ones like: a, and, of)</li>
<li>removal of some numbers (only without surrounding letters)</li>
</ul>

<p>Is there any source which details all steps?</p>

<p>Did they also  e.g. ...</p>

<ul>
<li>remove some punctuation</li>
<li>lowercase some letters</li>
<li>stem or lemmatize
?</li>
</ul>
",,2019-09-29 12:16:48,How to pre-process texts to match Googles pre-trained word2vec model?,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22970,58190771,2019-10-01 18:59:58,,"<p>Code : </p>

<pre><code>from gensim.models.word2vec import Word2Vec
w2v = Word2Vec()
training_data = w2v.generate_training_data(settings, corpus)
</code></pre>

<p>Error :</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-45-bae554564046&gt; in &lt;module&gt;
      1 w2v = Word2Vec()
      2 # Numpy ndarray with one-hot representation for [target_word, context_words]
----&gt; 3 training_data = w2v.generate_training_data(settings, corpus)

AttributeError: 'Word2Vec' object has no attribute 'generate_training_data'
</code></pre>

<p>I even tried importing gensim.models.word2vec and tried every possibility but couldn't get it done. 
Can someone help me with it?
Thanks in advance !</p>
",,2019-10-02 01:26:36,'Word2Vec' object has no attribute 'generate_training_data',<nltk><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
22974,58206571,2019-10-02 17:39:01,,"<p>Hi I am trying find similar sentence using doc2vec. What I am not able to find is actual sentence that is matching from the trained sentences.</p>

<p>Below is the code from <a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">link</a> </p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [""I love machine learning. Its awesome."",
        ""I love coding in python"",
        ""I love building chatbots"",
        ""they chat amagingly well""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(""d2v.model"")
print(""Model Saved"")

model= Doc2Vec.load(""d2v.model"")
#to find the vector of a document which is not in training data
test_data = word_tokenize(""I love building chatbots"".lower())
v1 = model.infer_vector(test_data)
print(""V1_infer"", v1)

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)


# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data
print(model.docvecs['1'])
</code></pre>

<p>But the above code only gives me vectors or numbers. But how can I get the actual sentence matched from training data. For Eg - In this case I am expecting the result as ""I love building chatbots"".</p>
",,2020-07-22 06:25:31,Doc2Vec find the similar sentence,<python><nlp><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,True,False,True,False,False
22980,58195364,2019-10-02 04:49:30,,"<p>I am doing text classification using gensim and doc2vec. I am using two data-sets for testing this, one being a stack exchange data-set and a Reddit data-set. I am trying to classify between posts from one subreddit/stackexchange site on a particular subject and then using posts from other unrelated subreddit/stackexchange sites as negative examples.</p>

<p>I am using a data-set of 10k posts to train the model and a testing set of 5k divided in to 50% positive examples and 50% negative. I then use the infer_vector and most_similar functions to classify the entry as positive or negative. Before training the model I pre-process the data to remove any words, symbols, links etc just leaving the most significant words to train the model. Below is the code used to train the model.</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""fulltrainingset.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100,200,300,400]
size_list = [1,5,10,15,25,50,100,200,300]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        minimum_count = 1
        mode = 0
        window_ = 15
        negative_sampling = 5
        subsampling = 1e-5
        alpha = 0.025
        minalpha = 0.00025

        model = Doc2Vec(alpha=alpha, min_alpha=minalpha, vector_size=vec_size, dm=mode, min_count=minimum_count, window =window_, sample=subsampling ,hs =negative_sampling)
        model.build_vocab(tagged_data)

        for epoch in range(max_epochs):
            print('iteration {0}'.format(epoch))
            model.train(tagged_data,
                        total_examples=model.corpus_count,
                        epochs=model.epochs)#self.epochs
            model.alpha -= 0.0002
            model.min_alpha = model.alpha


        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>

<p>This method is working and I can get results from it, but I would like to know if there is a different way of training to achieve better results. Currently I am just training many models with different epochs and vector_sizes, then using the infer_vector and most_similar functions to see if the vector score returned from the most_similar entry is greater than a certain number, but is there a way to improve upon this in the aspect of training the model?</p>

<p>Also, aiming to get better results I trained another model in the same way with a larger data-set (100k+ entries). When I used this model on the same data-set it produced similar but worse results to the models trained on smaller data-sets. I thought that more training data would have improved the results not made them worse, does anyone know a reason for this ?</p>

<p>Also, to further test I created a new but bigger test-set (15k entries) which did even worse then the original test-set. The data in this test-set although being unique is the same type of data used in the original test-set yet produces worse results, what may be the reason for this ?</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""all_sec_tweets.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100]
size_list = [1,5,10,15,25,50,100]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        mode = 0
        window_ = 5
        subsampling = 1e-5

        model = Doc2Vec(vector_size=vec_size, dm=mode, window =window_, sample=subsampling,epochs=max_epochs)
        model.build_vocab(tagged_data)

        model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)

        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>
",2019-10-03 02:21:03,2019-10-03 02:21:03,text classification model using doc2vec and gensim,<python><machine-learning><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
22983,58245826,2019-10-05 06:08:32,,"<p>I have already pretrained word2vec in gensim. In keras , I want to  use Word vector for word get from pretrained word2vec combined with that word's POS tag feature that i encode in one hot vector. In Keras, I  think use embedding matrix  So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>
",,2019-12-01 03:21:55,How to combine POS tag feature with associated word vector for word get from Pretrained gensim word2vec ans use in embedding layer in keras,<python-3.x><keras><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
22986,58248337,2019-10-05 12:34:15,,"<p>I have tried from un-installing to re-installing of jupyter notebook and also separately installed pycharm and tried working using it but still it shows error and does not install gensim properly.I also installed virtual studio and vc2015+ libraries and other support libraries from visual studio.
most of the time the usage of simple libraries is good but i ' m not understanding after using pip,easy,conda and installing the libraries and updating it is still not working but scipy, sklearn are working properly</p>

<pre><code> ValueError                                Traceback (most recent call last)
    &lt;ipython-input-1-1d2184025e54&gt; in &lt;module&gt;()
    ----&gt; 1 import nltk

    ~\Anaconda301\lib\site-packages\nltk\__init__.py in &lt;module&gt;()
        127 # Import top-level functionality into top-level namespace
        128 
    --&gt; 129 from nltk.collocations import *
        130 from nltk.decorators import decorator, memoize
        131 from nltk.featstruct import *

    ~\Anaconda301\lib\site-packages\nltk\collocations.py in &lt;module&gt;()
         38 from nltk.util import ngrams
         39 # these two unused imports are referenced in collocations.doctest
    ---&gt; 40 from nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures, QuadgramAssocMeasures
         41 from nltk.metrics.spearman import ranks_from_scores, spearman_correlation
         42 

    ~\Anaconda301\lib\site-packages\nltk\metrics\__init__.py in &lt;module&gt;()
         14 """"""
         15 
    ---&gt; 16 from nltk.metrics.scores import (
         17     accuracy,
         18     precision,

    ~\Anaconda301\lib\site-packages\nltk\metrics\scores.py in &lt;module&gt;()
         16 
         17 try:
    ---&gt; 18     from scipy.stats.stats import betai
         19 except ImportError:
         20     betai = None

    ~\Anaconda301\lib\site-packages\scipy\stats\__init__.py in &lt;module&gt;()
        343 from __future__ import division, print_function, absolute_import
        344 
    --&gt; 345 from .stats import *
        346 from .distributions import *
        347 from .morestats import *

    ~\Anaconda301\lib\site-packages\scipy\stats\stats.py in &lt;module&gt;()
        169 import scipy.special as special
        170 import scipy.linalg as linalg
    --&gt; 171 from . import distributions
        172 from . import mstats_basic
        173 from ._distn_infrastructure import _lazywhere

    ~\Anaconda301\lib\site-packages\scipy\stats\distributions.py in &lt;module&gt;()
          8 from __future__ import division, print_function, absolute_import
          9 
    ---&gt; 10 from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,
         11                                     rv_frozen)
         12 

    ~\Anaconda301\lib\site-packages\scipy\stats\_distn_infrastructure.py in &lt;module&gt;()
         14 import warnings
         15 
    ---&gt; 16 from scipy.misc import doccer
         17 from ._distr_params import distcont, distdiscrete
         18 from scipy._lib._util import check_random_state, _lazywhere, _lazyselect

    ~\Anaconda301\lib\site-packages\scipy\misc\__init__.py in &lt;module&gt;()
         66 from numpy import who as _who, source as _source, info as _info
         67 import numpy as np
    ---&gt; 68 from scipy.interpolate._pade import pade as _pade
         69 from scipy.special import (comb as _comb, logsumexp as _lsm,
         70         factorial as _fact, factorial2 as _fact2, factorialk as _factk)

    ~\Anaconda301\lib\site-packages\scipy\interpolate\__init__.py in &lt;module&gt;()
        173 from __future__ import division, print_function, absolute_import
        174 
    --&gt; 175 from .interpolate import *
        176 from .fitpack import *
        177 

    ~\Anaconda301\lib\site-packages\scipy\interpolate\interpolate.py in &lt;module&gt;()
         30 from . import _ppoly
         31 from .fitpack2 import RectBivariateSpline
    ---&gt; 32 from .interpnd import _ndim_coords_from_arrays
         33 from ._bsplines import make_interp_spline, BSpline
         34 

    interpnd.pyx in init scipy.interpolate.interpnd()

    ~\Anaconda301\lib\site-packages\scipy\spatial\__init__.py in &lt;module&gt;()
         92 from __future__ import division, print_function, absolute_import
         93 
    ---&gt; 94 from .kdtree import *
         95 from .ckdtree import *
         96 from .qhull import *

    ~\Anaconda301\lib\site-packages\scipy\spatial\kdtree.py in &lt;module&gt;()
          6 import numpy as np
          7 from heapq import heappush, heappop
    ----&gt; 8 import scipy.sparse
          9 
         10 __all__ = ['minkowski_distance_p', 'minkowski_distance',

    ~\Anaconda301\lib\site-packages\scipy\sparse\__init__.py in &lt;module&gt;()
        239 
        240 # For backward compatibility with v0.19.
    --&gt; 241 from . import csgraph
        242 
        243 __all__ = [s for s in dir() if not s.startswith('_')]

    ValueError: source code string cannot contain null bytes
</code></pre>
",2019-10-06 00:45:56,2019-10-06 00:45:56,getting error while using nltk library python even after updating nltk in python 3.7.3,<python><python-3.x><nltk>,,,CC BY-SA 4.0,True,False,True,False,True
22988,58109056,2019-09-26 03:10:57,,"<p>I have got a python-dictionary stored as Vector file with Pickle method (through Bert-as-Service and Google's pretrained model) like:</p>

<p>(key)Phrase : (value)Phrase_Vector_from_Bert = 
woman cloth : 1.3237 -2.6354 1.7458 ....</p>

<p>But I have no idea to get phrases' similarity with the vector files from Bert-as-Service model as I do with Gensim Word2Vec, since the later is equipped with .similarity method.</p>

<p>Would you please give an advice to get phrases/keywords similarity or to cluster them with my python-Pickle-dictionary vector file?</p>

<p>Or maybe is there an better idea to cluster keywords with Bert-as-Service?</p>

<p>The following codes show how I get the vectors for phrases/keywords:</p>

<pre><code>import Myutility
# the file Myutility includes the function save_model and load_model

import BertCommand
# the file Bertcommand includes the function to start Bert-as-service 
  client

WORD_PATH = 'E:/Works/testwords.txt'
WORD_FEATURE = 'E:/Works/word.google.vector'

word_vectors = {}

with open(WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None

for word in word_vectors:
    try:
        v = bc.encode([word])
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,WORD_FEATURE)
</code></pre>
",,2019-09-26 06:19:34,How to cluster keywords or get keywords similarity when I have their vectors,<nlp><word2vec><similarity><cosine-similarity><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
22992,58278111,2019-10-07 22:52:36,,"<p>I have a TF Estimator that uses Feature Columns at its input layer. One of these is and <code>EmbeddingColumn</code> which I have been initializing randomly (the default behaviour). </p>

<p>Now I would like to pre-train my embeddings in gensim and transfer the learned embeddings into my TF model. The <code>embedding_column</code> accepts an initializer argument which expects a callable that can be <a href=""https://stackoverflow.com/questions/51237419/feature-column-pre-trained-embedding"">created</a> using <code>tf.contrib.framework.load_embedding_initializer</code>.</p>

<p>However, that function expects a saved TF checkpoint, which I don't have, because I trained my embeddings in gensim.</p>

<p>The question is: how do I save gensim word vectors (which are numpy arrays) as a tensor in the TF checkpoint format so that I can use that to initialize my embedding column?</p>
",2019-10-14 01:27:12,2019-10-14 01:27:12,Importing pre-trained embeddings into Tensorflow's Embedding Feature Column,<tensorflow><gensim><word2vec><transfer-learning>,,,CC BY-SA 4.0,False,False,True,False,False
23005,58253405,2019-10-06 00:41:33,,"<p>I'm trying to use <a href=""https://github.com/idio/wiki2vec/"" rel=""nofollow noreferrer"">this</a> 1000 dimension wikipedia word2vec model to analyze some documents.</p>

<p>Using introspection I found out that the vector representation of a word is a 1000 dimension numpy.ndarray, however whenever I try to create an ndarray to find the nearest words I get a value error:</p>

<pre><code>ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>and from what I can tell by looking around online 32 is indeed the maximum supported number of dimensions for an ndarray - so what gives? How is gensim able to output a 1000 dimension ndarray?</p>

<p>Here is some example code:</p>

<pre><code>doc = [model[word] for word in text if word in model.vocab]
out = []
n = len(doc[0])
print(n)
print(len(model[""hello""]))
print(type(doc[0]))
for i in range(n):
    sum = 0
    for d in doc:
        sum += d[i]
    out.append(sum/n)
out = np.ndarray(out)
</code></pre>

<p>which outputs:</p>

<pre><code>1000
1000
&lt;class 'numpy.ndarray'&gt;
ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>The goal here would be to compute the average vector of all words in the corpus in a format that can be used to find nearby words in the model so any alternative suggestions to that effect are welcome.</p>
",,2019-10-06 22:13:12,Gensim word2vec model outputs 1000 dimension ndarray but the maximum number of ndarray dimensions is 32 - how?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23016,58320218,2019-10-10 09:55:24,,"<p>I am practicing with building an article summarizer. I built something using the script below. I would like to export the model and use it for deployment but can't find a way around it.</p>

<p>Here is the script for the analyzer. </p>

<pre><code>#import necessary libraries
import re
import gensim
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

file = open(""somefile.txt"",""r"") 
data=file.readlines() 
file.close()

#define preprocessing steps
#lower case
#remove everything inside []
#remove 's
#fetch only ascii characters

def preprocessor(text):
    newString = text.lower()
    newString = re.sub(""[\(\[].*?[\)\]]"", """", newString)
    newString = re.sub(""'s"","""",newString)
    newString = re.sub(""[^'0-9.a-zA-Z]"", "" "", newString)
    tokens=newString.split()
    return ("" "".join(tokens)).strip()

#call above function
text=[]
for i in data:
    text.append(preprocessor(i))

all_sentences=[]    
for i in text:
    sentences=i.split(""."")       
    for i in sentences:
        if(i!=''):
            all_sentences.append(i.strip())


# tokenizing the sentences for training word2vec
tokenized_text = [] 
for i in all_sentences:
    tokenized_text.append(i.split()) 


#define word2vec model
model_w2v = gensim.models.Word2Vec(
            tokenized_text,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 0, # 1 for cbow model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)


#train word2vec
model_w2v.train(tokenized_text, total_examples= len(tokenized_text), epochs=model_w2v.epochs)

#define function to obtain sentence embedding
def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model_w2v[word].reshape((1, size))
            count += 1.
        except KeyError: # handling the case where the token is not in vocabulary

            continue
    if count != 0:
        vec /= count
    return vec


#call above function
wordvec_arrays = np.zeros((len(tokenized_text), 200))
for i in range(len(tokenized_text)):
    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 200)



# similarity matrix
sim_mat = np.zeros([len(wordvec_arrays), len(wordvec_arrays)])

#compute similarity score
for i in range(len(wordvec_arrays)):
  for j in range(len(wordvec_arrays)):
    if i != j:
      sim_mat[i][j] = cosine_similarity(wordvec_arrays[i].reshape(1,200), wordvec_arrays[j].reshape(1,200))[0,0]

#Generate a graph
nx_graph = nx.from_numpy_array(sim_mat)



#compute pagerank scores
scores = nx.pagerank(nx_graph)

#sort the scores
sorted_x = sorted(scores.items(), key=lambda kv: kv[1],reverse=True)

sent_list=[]
for i in sorted_x:
    sent_list.append(i[0])


#extract top 10 sentences
num=10
summary=''
for i in range(num):
    summary=summary+all_sentences[sent_list[i]]+'. '
print(summary)

</code></pre>

<p>I want to have an exported model that I can pass to a flask API later. I need help with that.</p>
",2019-10-11 03:47:36,2019-10-11 03:47:36,How can I save this text analyser model?,<python><numpy><nlp>,,,CC BY-SA 4.0,False,False,True,False,True
23019,58301450,2019-10-09 09:49:46,,"<p>I just want to be able to see the values in my word2vec model.</p>

<p>I have a  very small corpus. I just want to see exactly what happens in each step for this particular corpus.</p>

<p>A section of my code is below.</p>

<pre class=""lang-py prettyprint-override""><code>word2vec = Word2Vec(corpus, min_count=1)
word_vectors = word2vec.wv 

termsim_index = WordEmbeddingSimilarityIndex(word_vectors)


dictionary = corpora.Dictionary(food)
bow_corpus = [dictionary.doc2bow(doc) for doc in food]


similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  
docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)

</code></pre>

<p>So I want to see what exactly is in <code>word_vectors</code>,<code>termsim_index</code>,<code>similarity_matrix</code> , <code>docsim_index</code></p>
",,2019-10-09 19:53:26,How to view word2vec model,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23041,58286505,2019-10-08 12:36:41,,"<p>I am a newbie in python and ML.
I found a nice script  (<a href=""https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/</a>) on how to get attributed topics to each document for LDA and I changed it to be able to use it with LSI as well.
The original code is:</p>

<pre><code>def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()
    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list            
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
</code></pre>

<p>In order to use it for LSI, I changed it to:</p>

<pre><code>def format_topics_sentences_lsi(LsiModel=None, corpus=corpus, texts=data):
    """"""
    Extract all the information needed such as most predominant topic assigned to document and percentage of contribution
    LsiModel= model to be used
    corpus = corpus to be used
    texts = original text to be classify (for topic assignment)
    """"""
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(LsiModel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = LsiModel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
</code></pre>

<ul>
<li>Is this the correct way?</li>
<li>As LSI is not based on probabilities the ""Perc_Contrib"" is above 100%. How should I interpret this number? </li>
<li>Apart from the script above, since LSI does not have get_document_topics, which function can I use to see the topic with the highest score?</li>
</ul>
",,2019-10-08 12:36:41,How can I get the topic scores attributed to a document on gensim LSI?,<python><gensim><topic-modeling><latent-semantic-indexing>,,,CC BY-SA 4.0,False,False,True,False,False
23066,58291846,2019-10-08 18:07:27,,"<p>I have a set of documents (3000) which each contain a short description. I want to use Word2Vec model to see if I can cluster these documents based on the description. </p>

<p>I'm doing it the in the following way, but I am not sure if this is a ""good"" way to do it. Would love to get feedback.</p>

<p>I'm using Google's trained w2v model.</p>

<pre><code>wv = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,encoding=""ISO-8859-1"", limit = 100000)
</code></pre>

<p>Each document is split into words where stop words are removed, and I have used stemming as well.</p>

<p>My initial idea was to fetch the word vector for each word in each documents description, average it, and then cluster based on this. </p>

<pre><code>doc2vecs = []
for i in range(0, len(documents_df['Name'])):
    vec = [0 for k in range(300)] 
    for j in range(0, len(documents_df['Description'][i])):
        if documents_df['Description'][i][j] in wv:
            vec += wv[documents_df['Description'][i][j]]
    doc2vecs.append(vec/300)
</code></pre>

<p>I'm then finding similarities using</p>

<pre><code>similarities = squareform(pdist(doc2vecs, 'cosine'))
</code></pre>

<p>Which returns a matrix of the cosine between each vector in <code>doc2vec</code>.</p>

<p>I then try to cluster the documents. </p>

<pre><code>num_clusters = 2
km = cluster.KMeans(n_clusters=num_clusters)
km.fit(doc2vecs)
</code></pre>

<p>So basically what I am wondering is:</p>

<p>Is this method of clustering the average word vector for each word in the document a reasonable way to cluster the documents?</p>
",2019-10-08 18:13:18,2019-10-08 23:54:05,How do you correctly cluster document names & find similarities between documents based on Word2Vec model?,<python><nlp><cluster-analysis><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23111,58397381,2019-10-15 14:48:54,,"<p>I would like to create a LDA model (i.e. an instance of gensim.models.LdaModel) returning a pre-determined topic-word distribution. The reason for doing this is that I would like to leverage Gensim, pyLDAvis, etc to display the results on some topic-word distribution that I obtain from other algorithms. </p>

<p>Is that possible? Below I show some incomplete code (because I find the ML community intimidating):</p>

<pre><code>import gensim
texts=[['a','a','b'], ['a','b','c'],['b','c','c']]
d = gensim.corpora.Dictionary(texts)
bow = [d.doc2bow(doc) for doc in texts]

my_topics=[[2/3, 1/3, 0],[0, 2/3, 1/3]]
model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=2, 
        eta=..., alpha..., passes=0, iterations=0, random_state=1)
model.show_topics(num_words=3)
# Hopefully return my_topics above
#[(0, '0.666*""a"" + 0.333*""b""'),
# (1, '0.666*""b"" + 0.333*""c""')]
</code></pre>

<p>I was hoping to achieve this by initializing eta in the LDA model and by setting iterations to 0. In my attempts (not shown) the resulting topics are not the same as 'my_topics'. </p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel"" rel=""nofollow noreferrer"">Gensim docs</a>:</p>

<blockquote>
  <p>eta ({float, np.array, str}, optional) </p>
  
  <p>A-priori belief on word probability, this can be:</p>

<pre><code>    scalar for a symmetric prior over topic/word probability,
    vector of length num_words to denote an asymmetric user defined probability for each word,
    matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,
    the string auto to learn the asymmetric prior from the data.
</code></pre>
</blockquote>

<p>EDIT: This (calling reset on the model's state) seems to work, but I am not sure it makes theoretically sense</p>

<pre><code>model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=2, 
    eta=my_topics, alpha=1, passes=0, iterations=0, random_state=1)
model.state.reset()
model.show_topics(num_words=3)
#array([[0.6666667 , 0.33333334, 0.        ],
#   [0.        , 0.6666667 , 0.33333334]], dtype=float32)
</code></pre>

<p>However, the topic distribution for a given document is not trained and I do not think it can be keeping the topics constant:</p>

<pre><code>model.get_document_topics(bow[2])
model[bow[2]]
#[(0, 0.55569696), (1, 0.44430298)]
</code></pre>
",2019-10-28 11:03:17,2019-10-28 11:03:17,Initialize Gensim LDA model with pre-determined priors,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
23156,58393090,2019-10-15 10:58:26,,"<p>I have two lists, A is a list of words, for example [""hello"",""world"",......], Len(A) is 10000. List B contains the all pre-trained vectors corresponding to A, which is a [10000,512], 512 is the vector dimension. I want to convert two lists into gensim word2vec model format in order to load the model in later, such as <code>model = Word2Vec.load(""word2vec.model"")</code> how should I do this? </p>
",2019-10-15 12:48:05,2019-10-15 18:18:29,How to save as a gensim word2vec file?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23173,58493250,2019-10-21 19:54:03,,"<p>I am trying to code for a LDA Mallet Model...I ran this a couple months ago and it ran fine but it is no longer. There have been other posts on the same subject but the solutions have not yet helped me. Can anyone figure out what is wrong in my code and/or other solutions to fix the problem? The first two cells run fine. The third is where it breaks and it says it returns a non-zero exit status 1.</p>

<p><a href=""https://i.stack.imgur.com/0Uwij.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Uwij.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/5ap8D.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5ap8D.jpg"" alt=""enter image description here""></a></p>
",,2019-10-21 20:24:11,Lda Mallet returned non-zero exit status 1,<python><gensim><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
23214,58497442,2019-10-22 05:10:55,,"<p>I am trying to use doc2vec to do text classification based on document subject, for example, I want to classify all documents about sports as 1 and all other documents as 0. I want to do this by first training a doc2vec model with training data and then use a classification model such as logistic regression to classify the texts as positive or negative.</p>

<p>I have seen various examples online to do this [<a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">1</a>,<a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">2</a>] which employ different methods and I am unclear about some of the details as to why they are using certain methods, and which method is the best for text classification.</p>

<ol>
<li><p>Firstly Using the example above, is it better to train the model using just documents related to sports or documents on all subjects. My thinking was by training just on sports documents you could classify documents based on document similarity(although this wouldnt produce vectors for non sports documents to use to train the next model). Also, i feel like if training the model on all documents you would need a huge amount of documents to represent everything other than sports to get good classification. </p></li>
<li><p>Secondly, which features are actually used to train the logistic regression model. If training the model on all documents I assume you would track the documents using an index of some sort and then train the logistic regression model using the vectors with a class label, is this correct ? </p></li>
<li><p>Thirdly, I have seen various uses of TaggedDocument where a unique id is put for each document and also where a shared id is used to represent the same class, eg., 1 = sports 0 = non sports. From what I have read a shared id means the model has a single vector representing each class, while using a unique id provides unique vectors for each document, is this correct ?. If so, assuming that I need unique labeled vectors for training the logistic regression model what is the point of using a shared id ? Wouldnt this provide terrible classification results ?</p></li>
</ol>

<p>If anyone can help me with the questions above and generally what is the best way to do text classification using doc2vec vectors it would be greatly appreciated.</p>
",,2019-10-22 17:56:30,best training methods for binary text classification using doc2vec gensim,<machine-learning><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23221,58545704,2019-10-24 16:39:34,,"<p>I'm running gensim 3.8.0 on an Anaconda 5.1.0 distribution of Python 2.7.14 on my laptop.  I've prepared my term-document matrix and vocabulary files just as I've done in the past, but am getting a ValueError message when I try to run the LsiModel function on my corpus.  I've read through all the tutorials and other documentation, but can't figure out what I'm missing.  Can anyone provide some insight?  Thanks in advance.  :)
<br><br></p>

<p>--> Here is a copy-paste of my QtConsole terminal: </p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import numpy as numpy
import scipy
from scipy.misc import logsumexp
from scipy.sparse import csc_matrix
from scipy.special import gammaln, psi
from gensim import corpora, interfaces, matutils, models, similarities, utils
</code></pre>

<p><em>2019-10-24 09:11:11,499 : INFO : 'pattern' package not found; tag filters are not available for English</em></p>

<pre><code>corpus = corpora.bleicorpus.BleiCorpus('file:~/NameOfFile.txt', fname_vocab='file:~/NameOfFile.vocab')
</code></pre>

<p><em>2019-10-24 09:11:23,430 : INFO : loading corpus from file:~/NameOfFile.txt</em></p>

<pre><code>dict = corpus.id2word
mymodel = models.LsiModel(corpus, id2word=dict, num_topics=5)
</code></pre>

<p><em>2019-10-24 09:11:36,127 : INFO : using serial LSI version on this node<br>
2019-10-24 09:11:36,128 : INFO : updating model with new documents<br>
ValueError: invalid format in file:~/NameOfFile.txt: '1 8:14 17:19 23:21 38:20 41:2 52:7 54:3 70:1 80:2 81:3 90:1 101:10 \n'</em>
<br><br></p>

<p>--> Here is the full error message:</p>

<pre><code>ValueErrorTraceback (most recent call last)
&lt;ipython-input-4-7cd66919d5d1&gt; in &lt;module&gt;()
----&gt; 1 mymodel = models.LsiModel(corpus, id2word=dict, num_topics=5)
/anaconda2/lib/python2.7/site-packages/gensim/models/lsimodel.pyc in __init__(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)
    443 
    444         if corpus is not None:
--&gt; 445             self.add_documents(corpus)
    446 
    447     def add_documents(self, corpus, chunksize=None, decay=None):
/anaconda2/lib/python2.7/site-packages/gensim/models/lsimodel.pyc in add_documents(self, corpus, chunksize, decay)
    489                     logger.info('initializing %s workers', self.numworkers)
    490                     self.dispatcher.reset()
--&gt; 491                 for chunk_no, chunk in enumerate(utils.grouper(corpus, chunksize)):
    492                     logger.info(""preparing a new chunk of documents"")
    493                     nnz = sum(len(doc) for doc in chunk)
/anaconda2/lib/python2.7/site-packages/gensim/utils.pyc in chunkize_serial(iterable, chunksize, as_numpy, dtype)
   1168             wrapped_chunk = np.array(doc, dtype=dtype) for doc in itertools.islice(it, int(chunksize))
   1169         else:
-&gt; 1170             wrapped_chunk = list(itertools.islice(it, int(chunksize)))
   1171         if not wrapped_chunk0:
   1172             break
/anaconda2/lib/python2.7/site-packages/gensim/corpora/bleicorpus.pyc in __iter__(self)
     91         with utils.open(self.fname, 'rb') as fin:
     92             for lineno, line in enumerate(fin):
---&gt; 93                 yield self.line2doc(line)
     94         self.length = lineno + 1
     95 
/anaconda2/lib/python2.7/site-packages/gensim/corpora/bleicorpus.pyc in line2doc(self, line)
    110         parts = utils.to_unicode(line).split()
    111         if int(parts0) != len(parts) - 1:
--&gt; 112             raise ValueError(""invalid format in %s: %s"" % (self.fname, repr(line)))
    113         doc = part.rsplit(':', 1) for part in parts1:
    114         doc = (int(p1), float(p2)) for p1, p2 in doc
ValueError: invalid format in file:~/NameOfFile.txt: '1 8:14 17:19 23:21 38:20 41:2 52:7 54:3 70:1 80:2 81:3 90:1 101:10 \n'
</code></pre>
",,2019-10-24 16:39:34,gensim ValueError: invalid format in file when running LsiModel function on Blei corpus,<python><python-2.7><nlp><anaconda><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23230,58407649,2019-10-16 07:10:25,,"<p>I'm trying to load one of the FastText pre-trained models that has a form of a .bin file. The size of .bin file is 2.8GB and I have 8GB RAM and 8GB swap file. Unfortunately, the model starts loading and it occupies almost 15GB and then it breaks with the following error:</p>

<p><code>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</code></p>

<p>By observing the system monitor, I can see that RAM and swap are fully occupied, so I think it breaks because it is out of memory.</p>

<p>I'm trying to load the file using Gensim wrapper for FastText</p>

<p><code>from gensim.models.wrappers import FastText
 model = FastText.load_fasttext_format('../model/java_ftskip_dim100_ws5')</code></p>

<hr>

<p>My questions are the following:</p>

<p>1) Is there any way to fit this model in the current memory of my system?</p>

<p>2) Is it possible to reduce the size of this model? I tried the quantization using the following code</p>

<p><code>./fasttext quantize -output java_ftskip_dim100_ws5 -input unused_argument.txt</code></p>

<p>And I'm getting the following error:</p>

<p><code>terminate called after throwing an instance of 'std::invalid_argument'
  what():  For now we only support quantization of supervised models
Aborted (core dumped)</code></p>

<p>I would really appreciate your help!</p>
",,2019-10-16 15:37:48,"FastText .bin file cannot fit in memory, even though I have enough RAM",<python><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
23254,58451218,2019-10-18 12:44:40,,"<p>I have a list of sentences. I want to cluster my sentences on similarity using the WMD (word mover's distance). I am using a word2vec model from gensim to create embeddings for my words.</p>

<p>The clustering algorithms I know (nltk, sklearn) use number vectors as input so I need to give the sentences as an array (or list) of the embeddings of the words in them. I think I can use the nltk clustering methods with a custom distance function. I want to use the WMD as his custom function. But the WMD function of gensim uses a 2 lists of strings as input. </p>

<p>Is there a prebuild WMD function that uses the embeddings and not the strings as input? Or is there a clustering (kmeans or something else) that can handle lists of strings as input and can have the WMD as custom distance function?</p>

<p>Thanks</p>
",,2020-09-08 17:27:47,Use wmd function of gensim for sentence clustering,<python><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
23289,58540089,2019-10-24 11:12:40,,"<p>I failed to use the callbacks to save model according to the [official documents]:<a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/callbacks.html</a></p>

<p>AttributeError: Can't pickle local object 'train_model..shf'</p>

<pre class=""lang-py prettyprint-override""><code>    from random import shuffle
    from gensim.models.callbacks import CallbackAny2Vec
    from gensim.test.utils import get_tmpfile

    class shf(CallbackAny2Vec):
        def __init__(self, x, path_prefix):
            self.epoch = 0
            self.x = x
            self.path_prefix = path_prefix
        def on_epoch_begin(self, model):
            shuffle(self.x)

        def on_epoch_end(self, model):
            print(""epoch:%s""%self.epoch)
            if self.epoch % 10 == 0:
                output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
                model.save(output_path)
            self.epoch += 1
    model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)
    model_dm.build_vocab(x_train + x_test)

    fun = shf(x_train, ""\models"")
    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=100, callbacks=[fun])
</code></pre>
",,2019-10-24 16:27:42,"Pickle error when I save a doc2vec model, AttributeError",<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23293,58412763,2019-10-16 11:58:44,,"<p>can anybody tell me which default values are used in <code>Doc2Vec()</code> for <code>alpha</code> and <code>min_alpha</code>? </p>
",2019-10-21 11:32:35,2019-10-21 11:32:35,Default values of doc2vec for alpha and min_alpha,<python><scikit-learn><gensim><doc2vec><hyperparameters>,,,CC BY-SA 4.0,False,False,True,False,True
23311,58556924,2019-10-25 10:37:05,,"<p>I am new to NLP and Word Embeddings and still need to learn many concepts within these topics, so any pointers would be appreciated. This question is related to <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">this</a> and <a href=""https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages?noredirect=1&amp;lq=1"">this</a>, and I think there may have been developments since these questions had been asked. Facebook <a href=""https://arxiv.org/pdf/1710.04087.pdf"" rel=""nofollow noreferrer"">MUSE</a> provides aligned, supervised <a href=""https://github.com/facebookresearch/MUSE#multilingual-word-embeddings"" rel=""nofollow noreferrer"">word embeddings for 30 languages</a>, and it can be used to calculate word similarity across different languages. As far as I understand, The embeddings provided by MUSE satisfy the requirement of <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">coordinate space compatibilty</a>. It seems that it is possible to <a href=""https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"">load these embeddings into libraries such as Gensim</a>, but I wonder: </p>

<ol>
<li>Is it possible to load multiple-language word embeddings 
into Gensim (or other libraries), and if so:</li>
<li>What type of similarity measure
might fit in this use case?</li>
<li>How to use these loaded word embeddings
to calculate cross-lingual similarity score of phrases* instead of
words?</li>
</ol>

<p>*e.g., ""<em>PNV</em>"" in German vs ""<em>Trasporto pubblico locale</em>"" in Italian for the English term ""<em>Public Transport</em>"". </p>

<p>I am open o any implementation (libraries/languages/embeddings) though I may need some time to learn this topic. Thank you in advance.</p>
",2019-10-25 10:43:30,2020-09-21 14:40:30,"Calculate Cross-Lingual Phrase Similarity (using e.g., MUSE and Gensim)",<python><nlp><multilingual><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
23324,58619716,2019-10-30 06:28:27,,"<p>My data has more than 1 million rows and while training gensim similarity model, it is making multiple .sav files (model.sav, model.sav.0, model.sav.1 and so on..). Problem is while loading, it is loading only one sub-part, instead of all the sub-parts, hence performing horribly in prediction. Parameters/options are not working as per gensim documentation.</p>

<p>As per the gensim documentation - <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/similarities/docsim.html</a>
Saving as file handle and giving the following params should have worked - : </p>

<ol>
<li>model.save(fname_or_handle, separately = None)</li>
<li>model.load(filepath, mmap = 'r')</li>
</ol>

<p>Even tried to -</p>

<ol>
<li>pickle the .sav files ( this pickles the 1st shard only i.e. model.sav)</li>
<li>compressing all sub-parts as .gz file ( this compresses one shard only , not all the sub-parts) and also gives some sort of pickle error.</li>
</ol>

<pre><code>tf_idf = gensim.models.TfidfModel(corpus)

sims = gensim.similarities.Similarity('./models/model.sav',tf_idf[corpus],
                                          num_features=len(dictionary))
sims.save('./models/model.sav')

sims1 = gensim.similarities.Similarity.load(./models/model.sav)
</code></pre>

<p>Expected results should give all matching documents from corpus, but this gives only from model.sav (the file mentioned while loading). It does NOT even execute the other shards. I checked result from each shard.</p>

<p>Question: How do I use all the sub-files of gensim model to predict similarity of my test document, WITHOUT looping through every sub-file individually and then presenting union of those results.</p>
",,2019-11-04 17:31:33,saving and loading multiple shards made by gensim similarity model,<python><model><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23332,58522356,2019-10-23 11:57:18,,"<p>I'm using anaconda enviroment python 3.7, gensim 3.8.0,  basically. I have my data as a dataframe tha tI separated in a test and training set, they both have this structure:</p>

<p>X_test and Xtrain dataframe format :</p>

<pre><code>        id                                            alltext  
1710  3264537  [exmodelo, karen, mcdougal, asegura, mantuvo, ...   
8211  3272079  [grupo, socialista, pionero, supone, apoyar, n...   
1885  3263933  [parte, entrenador, zaragoza, javier, aguirre,...   
2481  3263744  [fans, hielo, fuego, saga, literaria, dio, pie...   
2975  3265302  [actividad, busca, repetir, tres, ediciones, a... 
</code></pre>

<p>already preprocessed. </p>

<p>This is the code I use for creating my model</p>

<pre><code>id2word = corpora.Dictionary(X_train[""alltext""])   
texts = X_train[""alltext""]
corpus = [id2word.doc2bow(text) for text in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=20,
                                       random_state=100, 
                                       update_every=1, 
                                       chunksize=400, 
                                       passes=10, 
                                       alpha='auto',
                                       per_word_topics=True)enter code here
</code></pre>

<p>Until here, everything works fine. I can effectively use </p>

<pre><code>pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>to get my topics.</p>

<p>The problem comes, when I try to compare similarity between a new document and the corpus. Here is the code I'm using</p>

<pre><code>newddoc = X_test[""alltext""][2730] #I get a particular instance of the test_set
new_doc_freq_vector = id2word.doc2bow(newddoc)  #vectorize its list of words
model_vec= lda_model[new_doc_freq_vector] #run the trained model on it
index = similarities.MatrixSimilarity(lda_model[corpus]) # error
sims = index[model_vec] #error
</code></pre>

<p>In the last two lines, I get this error: </p>

<pre><code>-------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-110-352248c464f8&gt; in &lt;module&gt;
      4 
      5 #index = Similarity('model/indexes/similarity_index_01', lda_model[corpus], num_features=len(id2word)) #the first argument, the place where the
----&gt; 6 index = similarities.MatrixSimilarity(lda_model[corpus]) # funciona si en vez de lda_model[corpus] usamos solo corpus
      7 index = similarities.MatrixSimilarity(model_vec)
      8 #sims = index[model_vec] #funciona si usamos index[new_doc_freq_vector] en vez de model_vec

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\similarities\docsim.py in __init__(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)
    776                 ""scanning corpus to determine the number of features (consider setting `num_features` explicitly)""
    777             )
--&gt; 778             num_features = 1 + utils.get_max_id(corpus)
    779 
    780         self.num_features = num_features

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\utils.py in get_max_id(corpus)
    734     for document in corpus:
    735         if document:
--&gt; 736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
    737     return maxid
    738 

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\utils.py in &lt;genexpr&gt;(.0)
    734     for document in corpus:
    735         if document:
--&gt; 736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
    737     return maxid
    738 

ValueError: too many values to unpack (expected 2
</code></pre>

<p>No idea how to solve this, I have been trying to debug this for 3 hours now. , I believe I followed the same code many other people use fot getting similarity.</p>

<p>Things I have tried to solve this: </p>

<p>1)  Using </p>

<p><code>Similarity('model/indexes/similarity_index_01', lda_model[corpus], num_features=len(id2word))</code>.</p>

<p>But it did not work. Same error code was obtained.</p>

<p>2)  If I replace lda_model[corpus] with corpus, and index[model_vec] with index[new_doc_freq_vector], similarities.MatrixSimilarity() works. But I believe it does not give the proper result because, it does not have the model information in there. The fact that it works it tells me it has something to do with data types (?), if I print lda_model[corpus] I get </p>

<pre><code>&lt;gensim.interfaces.TransformedCorpus object at 0x00000221ECA8E148&gt;
</code></pre>

<p>no Idea what this means though. </p>
",2019-10-24 06:51:40,2019-11-10 14:47:41,"Error ""too many values to unpack"" when trying to get similiraties in Gensim using LDA model",<python><gensim><similarity><recommendation-engine><lda>,,,CC BY-SA 4.0,False,False,True,False,False
23334,58635642,2019-10-31 01:07:27,,"<p>I am training doc2vec with corpus file, which is very huge.     </p>

<pre><code>model = Doc2Vec(dm=1, vector_size=200, workers=cores, comment='d2v_model_unigram_dbow_200_v1.0')
model.build_vocab(corpus_file=path)
model.train(corpus_file=path, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I want to know how to get value of total_words.</p>

<p>Edit:</p>

<pre><code>total_words=model.corpus_total_words
</code></pre>

<p>Is this right?</p>
",2019-10-31 01:16:47,2019-10-31 22:08:43,total_words must be provided alongside corpus_file argument,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23359,58610689,2019-10-29 15:35:10,,"<p>I've produced GloVe vectors using the code provided by <a href=""https://github.com/stanfordnlp/GloVe/blob/master/demo.sh"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/demo.sh</a> using my own corpus. So, I have both the .bin file and .txt file vectors. I'm trying to import these files into gensim so I can work with them like I can word2vec vectors.</p>

<p>I've tried changing to load using both the binary format and text file format but only ended up getting a pickling error: </p>

<pre><code>models = gensim.models.Word2Vec.load(file)
</code></pre>

<p>I've tried ignoring the unicode error, which didn't work. I still got the unicode error. </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=True, unicode_errors='ignore')
</code></pre>

<p>This is what I have for my code right now:</p>

<pre><code>from gensim.models import KeyedVectors
import gensim
from gensim.models import word2vec

file = 'vectors.bin'
model = KeyedVectors.load_word2vec_format(file, binary=True, unicode_errors='ignore')  
model.wv.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>

<p>This is the error message I keep getting: </p>

<pre><code>Traceback (most recent call last):
  File ""glove_to_word2vec.py"", line 6, in &lt;module&gt;
    model = KeyedVectors.load_word2vec_format(file, binary=True)  # C  binary format
  File ""/home/users/epair/.local/lib/python3.6/site- packages/gensim/models/keyedvectors.py"", line 1498, in load_word2vec_format
    limit=limit, datatype=datatype)
  File ""/home/users/epair/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 343, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/home/users/epair/.local/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
    return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe6 in position 0:  invalid continuation byte
</code></pre>

<p>The pickling error was something like this: <a href=""https://stackoverflow.com/questions/44022180/unpickling-error-while-using-word2vec-load"">Unpickling Error while using Word2Vec.load()</a></p>

<p><a href=""https://i.stack.imgur.com/celtK.png"" rel=""nofollow noreferrer"">Text file format</a></p>
",2019-10-30 17:07:07,2019-10-30 17:07:07,Importing GloVe vectors into gensim. UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe6 in position 0: invalid continuation byte,<python><gensim><word2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
23370,58574772,2019-10-26 21:12:45,,"<p>Getting the error below when I try to install <code>gensim</code> on <code>Python 3.8.0</code>. How can I make it work, is there any workaround?</p>

<p>Here is the full error stacktrace:</p>

<pre><code>The following command was executed:

C:\Users\talha\AppData\Local\Programs\Python\Python38\Scripts\pipenv.exe update --dev

The exit code: 1
The error output of the command:

Running $ pipenv lock then $ pipenv sync.
Installing dependencies from Pipfile.lock (ec3971)
Installing initially failed dependencies

Locking [dev-packages] dependencies
Locking [packages] dependencies

[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...Success! 
Updated Pipfile.lock (ec3971)!
An error occurred while installing scipy==1.3.1 --hash=sha256:0baa64bf42592032f6f6445a07144e355ca876b177f47ad8d0612901c9375bef --hash=sha256:243b04730d7223d2b844bda9500310eecc9eda0cba9ceaf0cde1839f8287dfa8 --hash=sha256:2643cfb46d97b7797d1dbdb6f3c23fe3402904e3c90e6facfe6a9b98d808c1b5 --hash=sha256:396eb4cdad421f846a1498299474f0a3752921229388f91f60dc3eda55a00488 --hash=sha256:3ae3692616975d3c10aca6d574d6b4ff95568768d4525f76222fb60f142075b9 --hash=sha256:435d19f80b4dcf67dc090cc04fde2c5c8a70b3372e64f6a9c58c5b806abfa5a8 --hash=sha256:46a5e55850cfe02332998b3aef481d33f1efee1960fe6cfee0202c7dd6fc21ab --hash=sha256:75b513c462e58eeca82b22fc00f0d1875a37b12913eee9d979233349fce5c8b2 --hash=sha256:7ccfa44a08226825126c4ef0027aa46a38c928a10f0a8a8483c80dd9f9a0ad44 --hash=sha256:89dd6a6d329e3f693d1204d5562dd63af0fd7a17854ced17f9cbc37d5b853c8d --hash=sha256:a81da2fe32f4eab8b60d56ad43e44d93d392da228a77e229e59b51508a00299c --hash=sha256:a9d606d11eb2eec7ef893eb825017fbb6eef1e1d0b98a5b7fc11446ebeb2b9b1 --hash=sha256:ac37eb652248e2d7cbbfd89619dce5ecfd27d657e714ed049d82f19b162e8d45 --hash=sha256:cbc0611699e420774e945f6a4e2830f7ca2b3ee3483fca1aa659100049487dd5 --hash=sha256:d02d813ec9958ed63b390ded463163685af6025cb2e9a226ec2c477df90c6957 --hash=sha256:dd3b52e00f93fd1c86f2d78243dfb0d02743c94dd1d34ffea10055438e63b99d! Will try again.
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 2604, in do_sync
[pipenv.exceptions.InstallError]:       do_init(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 1246, in do_init
[pipenv.exceptions.InstallError]:       do_install_dependencies(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 858, in do_install_dependencies
[pipenv.exceptions.InstallError]:       batch_install(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 763, in batch_install
[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 681, in _cleanup_procs
[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)
[pipenv.exceptions.InstallError]: ['Collecting scipy==1.3.1', '  Using cached https://files.pythonhosted.org/packages/ee/5b/5afcd1c46f97b3c2ac3489dbc95d6ca28eacf8e3634e51f495da68d97f0f/scipy-1.3.1.tar.gz', '  Installing build dependencies: started', '  Installing build dependencies: still running...', ""  Installing build dependencies: finished with status 'done'"", '  Getting requirements to build wheel: started', ""  Getting requirements to build wheel: finished with status 'done'"", '    Preparing wheel metadata: started', ""    Preparing wheel metadata: finished with status 'error'""]
[pipenv.exceptions.InstallError]: ['ERROR: Command errored out with exit status 1:', ""     command: 'd:\\.virtualenvs\\pyemoji-fylffy3g\\scripts\\python.exe' 'd:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\talha\\AppData\\Local\\Temp\\tmp88ndh32u'"", '         cwd: C:\\Users\\talha\\AppData\\Local\\Temp\\pip-install-ppv6gock\\scipy', '    Complete output (172 lines):', '    lapack_opt_info:', '    lapack_mkl_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries mkl_rt not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    openblas_lapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries openblas not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", ""    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'"", '    customize GnuFCompiler', '    Could not locate executable g77', '    Could not locate executable f77', '    customize IntelVisualFCompiler', '    Could not locate executable ifort', '    Could not locate executable ifl', '    customize AbsoftFCompiler', '    Could not locate executable f90', '    customize CompaqVisualFCompiler', '    Could not locate executable DF', '    customize IntelItaniumVisualFCompiler', '    Could not locate executable efl', '    customize Gnu95FCompiler', '    Could not locate executable gfortran', '    Could not locate executable f95', '    customize G95FCompiler', '    Could not locate executable g95', '    customize IntelEM64VisualFCompiler', '    customize IntelEM64TFCompiler', '    Could not locate executable efort', '    Could not locate executable efc', '    customize PGroupFlangCompiler', '    Could not locate executable flang', ""    don't know how to compile Fortran code on platform 'nt'"", '      NOT AVAILABLE', '    ', '    openblas_clapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries openblas,lapack not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    atlas_3_10_threads_info:', '    Setting PTATLAS=ATLAS', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries tatlas,tatlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries tatlas,tatlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_3_10_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries satlas,satlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries satlas,satlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_threads_info:', '    Setting PTATLAS=ATLAS', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries ptf77blas,ptcblas,atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries ptf77blas,ptcblas,atlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries f77blas,cblas,atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries f77blas,cblas,atlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;"", '      NOT AVAILABLE', '    ', '    lapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries lapack not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    lapack_src_info:', '      NOT AVAILABLE', '    ', '      NOT AVAILABLE', '    ', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\ma\\core.py:4462: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?', '      if self.shape is ():', ""    setup.py:386: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\\Users\\talha\\AppData\\Local\\Temp\\pip-modern-metadata-kh8x6t2r'), proceeding with generating Cython sources and expanding templates"", '      warnings.warn(""Unrecognized setuptools command (\'{}\'), proceeding with ""', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\misc_util.py:464: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?', ""      return is_string(s) and ('*' in s or '?' is s)"", '    Running from scipy source directory.', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Atlas (http://math-atlas.sourceforge.net/) libraries not found.', '        Directories to search for the libraries can be specified in the', '        numpy/distutils/site.cfg file (section [atlas]) or by setting', '        the ATLAS environment variable.', '      self.calc_info()', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Lapack (http://www.netlib.org/lapack/) libraries not found.', '        Directories to search for the libraries can be specified in the', '        numpy/distutils/site.cfg file (section [lapack]) or by setting', '        the LAPACK environment variable.', '      self.calc_info()', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Lapack (http://www.netlib.org/lapack/) sources not found.', '        Directories to search for the sources can be specified in the', '        numpy/distutils/site.cfg file (section [lapack_src]) or by setting', '        the LAPACK_SRC environment variable.', '      self.calc_info()', '    Traceback (most recent call last):', '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 257, in &lt;module&gt;', '        main()', '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 240, in main', ""        json_out['return_val'] = hook(**hook_input['kwargs'])"", '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 110, in prepare_metadata_for_build_wheel', '        return hook(metadata_directory, config_settings)', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 156, in prepare_metadata_for_build_wheel', '        self.run_setup()', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 236, in run_setup', '        super(_BuildMetaLegacyBackend,', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 142, in run_setup', ""        exec(compile(code, __file__, 'exec'), locals())"", '      File ""setup.py"", line 505, in &lt;module&gt;', '        setup_package()', '      File ""setup.py"", line 501, in setup_package', '        setup(**metadata)', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\core.py"", line 135, in setup', '        config = configuration()', '      File ""setup.py"", line 403, in configuration', '        raise NotFoundError(msg)', '    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.', '    ----------------------------------------', ""ERROR: Command errored out with exit status 1: 'd:\\.virtualenvs\\pyemoji-fylffy3g\\scripts\\python.exe' 'd:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\talha\\AppData\\Local\\Temp\\tmp88ndh32u' Check the logs for full command output.""]
ERROR: ERROR: Package installation failed...

Error Running Pipenv
</code></pre>

<p>p.s. Using <code>pipenv</code> as the packaging tool.</p>
",2019-10-26 21:21:36,2019-10-27 18:03:04,Unable to install 'gensim' on Python 3.8.0,<python-3.x><scipy><gensim><pipenv><python-3.8>,,,CC BY-SA 4.0,False,False,True,False,False
23391,58710000,2019-11-05 11:04:03,,"<p>I want to get the list of similar words. Since Spacy doesn't have a built-in support for this I want to convert the spacy model to gensim word2vec and get the list of similar words.</p>

<p>I have tried to use the below method. But it is time consuming.</p>

<pre class=""lang-py prettyprint-override""><code>def most_similar(word):
    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)
    return [w.orth_ for w in by_similarity[:10]]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_md')
nlp.to_disk(filename)
nlp.vocab.vectors.to_disk(filename)
</code></pre>

<p>This does not save the model to a text file. Hence, I am not able to use the following method.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('test_glove.txt')
tmp_file = get_tmpfile(""test_word2vec.txt"")

_ = glove2word2vec(glove_file, tmp_file)
</code></pre>
",2019-11-05 11:15:35,2019-11-05 16:11:16,Is there a way to load spacy trained model into gensim?,<python-3.x><nlp><gensim><spacy><similarity>,,,CC BY-SA 4.0,False,True,True,False,False
23392,58710305,2019-11-05 11:21:13,,"<p>Currently I have run into a problem where when I convert my keyword list into a dictionary, I cannot multiple the frequency in the original data set to the frequency when I convert the list to the dictionary. </p>

<p>rowid   Keyword Frequency</p>

<p>1   dermatology 1151</p>

<p>2   psychiatry  1068</p>

<p>3   obgyn   1017</p>

<p>4   internal medicine   883</p>

<p>5   mental health   865</p>

<p>6   optometry   763</p>

<p>7   pediatrician    678</p>

<p>8   pediatrics  622</p>

<p>I am trying to cluster some search keywords using LDA and tfidfmodel. In my data set, I have a list of keywords along with its frequency. I am trying to cluster topics based on those keywords using the frequency #.</p>

<pre><code>data_text = data[['Keyword']]
data_text['index']=data_text.index
documents = data_text

#Pre-Processing steps lemmatize and stemming

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))

#def lemmatize_stemming(text):
#    return stemmer.stem(text)

def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)&gt;3:
            result.append(lemmatize_stemming(token))
    return result

doc_sample = documents[documents['index']==4310].values[0][0]
print('original document:')
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print('\n\n tokenize and lemmatize document: ')
print(preprocess(doc_sample))

documents['Keyword']=documents['Keyword'].astype(str)

processed_docs = documents['Keyword'].map(preprocess)
processed_docs[:]

dictionary = gensim.corpora.Dictionary(processed_docs)

count=0
for k,v in dictionary.iteritems():
    print(k,v)
    count +=1
    if count &gt; 10: 
        break

[[(dictionary[id], freq*) for id, freq in cp] for cp in bow_corpus[:1]]
print(bow_corpus)

#create dic reporting how many words and how many times those words appear
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
bow_corpus
</code></pre>

<p>Not sure what's the best way to prep these keywords to better cluster and find topics. Please advise.</p>
",,2019-11-05 11:21:13,How to customize tfidf based word count,<python><jupyter-notebook><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
23397,58682909,2019-11-03 17:43:49,,"<p>I'm working my way through LDA models for text analysis; I've heard that the Mallet implementation is the best. However, it seems to generate very poor results when I compare it with the Gensim version, so I think I may be doing something wrong. Can anyone explain the discrepancy?</p>

<pre><code>import gensim
from gensim.corpora.dictionary import Dictionary
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim  


## Generate a toy corpus:

dog = list(np.repeat('dog', 500)) + list(np.repeat('cat', 20)) + list(np.repeat('bird', 20))
cat = list(np.repeat('dog', 20)) + list(np.repeat('cat', 500)) + list(np.repeat('bird', 20))
bird = list(np.repeat('dog', 20)) + list(np.repeat('cat', 20)) + list(np.repeat('bird', 500))

texts = [dog, cat, bird]

id2word = corpora.Dictionary(texts)

corpus = [id2word.doc2bow(i) for i in texts]

### Gensim model

lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,
                                        id2word=id2word,
                                           num_topics=3, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)


vis = pyLDAvis.prepared_data_to_html(vis)

with open(""LDA_output.html"", ""w"") as file:
    file.write(vis)
</code></pre>

<p>This gives the following plausible inference of topics:</p>

<p><a href=""https://i.stack.imgur.com/mwenL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mwenL.png"" alt=""Gensim topic inference""></a></p>

<p>However, things work very differently for the Mallet implementation:</p>

<pre><code>mallet_path = '/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, iterations=1000, workers = 4, id2word=id2word)

model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(model, corpus, id2word)

vis = pyLDAvis.prepared_data_to_html(vis)

with open(""LDA_output.html"", ""w"") as file:
    file.write(vis)
</code></pre>

<p>Here, there is very little difference between the topics that the model infers. </p>

<p><a href=""https://i.stack.imgur.com/oG1PT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oG1PT.png"" alt=""Mallet version""></a></p>

<p>Now, it seems to me that I'm making an elementary blunder here--possibly by not specifying a relevant model parameter the correct way. However, I'm baffled as to what that might be. I'd be grateful for any advice!</p>
",2019-11-03 20:50:00,2019-11-03 20:50:00,Why does Mallet LDA give poor results when then Gensim version doesn't?,<python><nlp><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
23409,58712856,2019-11-05 13:55:28,,"<p>Let's say, <strong>word2vec.model</strong> is my trained word2vec model. When a out-of-vocabulary word (<strong>oov_word</strong>) occurs, I compute a vector <strong>vec</strong> using <em>compute_vec(oov_word)</em> method. Now, I want to add/append <strong>oov_word</strong> and its corresponding vector <strong>vec</strong> to my already trained model <strong>word2vec.model</strong>.</p>

<p>I have already checked the below links. But they do not answer my question.</p>

<p><a href=""https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models"">Combining/adding vectors from different word2vec models</a></p>

<p><a href=""https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"">https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words</a></p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add</a></p>
",,2019-11-06 09:32:33,How to add words and vectors manually to Word2vec gensim?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23425,58714746,2019-11-05 15:40:56,,"<p>I've got a question during following the simple gensim tutorial on <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim website</a>,</p>

<pre><code>&gt;&gt;&gt; from gensim.test.utils import common_texts, get_tmpfile
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""word2vec.model"")
&gt;&gt;&gt;
&gt;&gt;&gt; model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
&gt;&gt;&gt; model.save(""word2vec.model"")
&gt;&gt;&gt; model = Word2Vec.load(""word2vec.model"")
&gt;&gt;&gt; model.train([[""hello"", ""world""]], total_examples=1, epochs=1)

&gt;&gt;&gt; from gensim.models import KeyedVectors
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""wordvectors.kv"")
&gt;&gt;&gt;
</code></pre>

<p>And when I tried below,</p>

<pre><code>&gt;&gt;&gt; model.wv.save(path)
&gt;&gt;&gt; wv = KeyedVectors.load(""model.wv"", mmap='r')
</code></pre>

<p>I've got a following error :</p>

<pre><code>---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-81-eee6865b677b&gt; in &lt;module&gt;
      1 path = get_tmpfile('wordvectors.kv')
      2 model.wv.save(path)
----&gt; 3 KeyedVectors.load(""model.wv"",mmap='r')

/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    210     @classmethod
    211     def load(cls, fname_or_handle, **kwargs):
--&gt; 212         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    213 
    214     def similarity(self, entity1, entity2):

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    420         compress, subname = SaveLoad._adapt_by_suffix(fname)
    421 
--&gt; 422         obj = unpickle(fname)
    423         obj._load_specials(fname, mmap, compress, subname)
    424         logger.info(""loaded %s"", fname)

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1356 
   1357     """"""
-&gt; 1358     with smart_open(fname, 'rb') as f:
   1359         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1360         if sys.version_info &gt; (3, 0):

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    179         raise TypeError('mode should be a string')
    180 
--&gt; 181     fobj = _shortcut_open(uri, mode, **kw)
    182     if fobj is not None:
    183         return fobj

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in _shortcut_open(uri, mode, **kw)
    299     #
    300     if six.PY3:
--&gt; 301         return open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
    302     elif not open_kwargs:
    303         return open(parsed_uri.uri_path, mode, buffering=buffering)

FileNotFoundError: [Errno 2] No such file or directory: 'model.wv'
</code></pre>

<p>Does anyone know the reason for this message? How can I know that I do have 'model.wv' file?</p>

<p>Thank you in advance!</p>
",,2019-11-05 17:11:31,Gensim -- [Errno 2] No such file or directory: 'model.wv',<python><model><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23431,58685936,2019-11-04 00:41:06,,"<p>With gensim, I can solve the equation ""king + woman - man = queen"" with this line:</p>

<pre class=""lang-py prettyprint-override""><code>model.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>But how can I pass in <code>('king', 'queen')</code> and output an interpolation like this:</p>

<pre><code>positive=['woman'], negative=['man']
</code></pre>

<p>It would be awesome to pass in two dissimilar words like ""rain"" and ""mouse"" to see what words need to be added/subtracted to transform one into the other!</p>

<p>If gensim can't do this, are there any tools for this?</p>
",,2019-11-04 00:41:06,How can I use gensim keyedvectors to find the connecting words between two given words?,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23445,58749620,2019-11-07 13:19:46,,"<p>There are a lot of examples of LDA Mallet topic modelling however non of them shows how to add dominant topic, percent contribution and topic keywords to the original dataframe. 
Let's assume this is the dataset and my code</p>

<h1>Dataset:</h1>

<pre><code>Document_Id   Text
1             'Here goes one example sentence that is generic'
2             'My car drives really fast and I have no brakes'
3             'Your car is slow and needs no brakes'
4             'Your and my vehicle are both not as fast as the airplane'
</code></pre>

<h1>Code</h1>

<pre><code># Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import pandas as pd
df = pd.read_csv('data_above.csv')
data = df.Text.values.tolist() 
# Assuming I have done all the preprocessing, lemmatization and so on and ended up with data_lemmatized:

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
model = gensim.models.ldamodel.LdaModel(corpus=corpus, 
        id2word=id2word, 
        num_topics=50,random_state=100, 
        chunksize = 1000, update_every=1, 
        passes=10, alpha='auto', per_word_topics=True)
</code></pre>

<p>I tried something like this but it doesn't work...</p>

<pre><code>def format_topics_sentences(ldamodel, corpus, df):
    # Init output
    sent_topics_df = pd.DataFrame()
    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    # Add original text to the end of the output
    contents = df
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)
</code></pre>
",2019-11-07 13:35:59,2020-07-27 06:39:56,"How to return dominant topic, percent contribution and topic keywords to original model",<python><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
23455,58730230,2019-11-06 12:30:19,,"<p>I used gimsm for LSA as per this tutorial
<a href=""https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python</a></p>

<p>and I got the following output after running it for a list of text</p>

<pre><code>
[(1, '-0.708*""London"" + 0.296*""like"" + 0.294*""go"" + 0.287*""dislike"" + 0.268*""great"" + 0.200*""romantic"" + 0.174*""stress"" + 0.099*""lovely"" + 0.082*""good"" + -0.075*""Tower"" + 0.072*""see"" + 0.063*""nice"" + 0.061*""amazing"" + -0.053*""Palace"" + 0.053*""walk"" + -0.050*""Eye"" + 0.046*""eat"" + -0.042*""Bridge"" + 0.041*""Garden"" + 0.040*""Covent"" + -0.040*""old"" + -0.039*""visit"" + 0.039*""really"" + 0.035*""spend"" + 0.034*""watch"" + 0.034*""get"" + -0.032*""Buckingham"" + 0.032*""Weather"" + -0.032*""Museum"" + -0.032*""Westminster""')]

</code></pre>

<p>What does -0.708 London indicate?</p>
",2019-11-06 16:58:20,2019-11-06 16:58:20,What does the score indicate in topic modelling,<python><nlp><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
23485,58735585,2019-11-06 17:28:04,,"<p>I am doing my research with fasttext pre-trained model and I need word frequency to do further analysis. Does the .vec or .bin files provided on fasttext website contain the info of word frequency? if yes, how do I get?</p>

<p>I am using load_word2vec_format to load the model tried using model.wv.vocab[word].count, which only gives you the word frequency rank not the original word frequency.</p>
",2019-11-06 19:54:03,2019-11-06 19:54:03,Gensim: Any chance to get word frequency in Word2Vec format?,<python-3.6><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
23487,58736548,2019-11-06 18:36:06,,"<p>I want to develop an NER model where I want to use word-embedding features to train CRF model. Code perfectly working without word-embedding features but when I insert embedding as features for CRF training, got error messages. Here is the part of snippet of my code: </p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('ggplot')

from itertools import chain

import nltk
import sklearn
import scipy.stats
from sklearn.metrics import make_scorer
#from sklearn.cross_validation import cross_val_score
#from sklearn.grid_search import RandomizedSearchCV

import sklearn_crfsuite
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics
import pickle
from gensim.models import KeyedVectors
import numpy as np
# Load vectors directly from the file
model1 = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) ### Loading pre-trainned word2vec model
### Embedding function 
def get_features(word):
    word=word.lower()
    vectors=[]
    try:
        vectors.append(model1[word])
    except:
        pass
    #vectors=np.array(vectors)
    #vectors=vectors[0]
    return vectors

def word2features(sent, i):
    word = sent[i][0]
    wordembdding=get_features(word)   ## word embedding vector 
    wordembdding=np.array(wordembdding) ## vectors 
    #wordembdding= 
    #wordembdding=wordembdding[0]
    postag = sent[i][1]
    tag1=sent[i][2]
    tag2=sent[i][4]
    tag3 = sent[i][5]


    features = {
        'bias': 1.0,
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'wordembdding': wordembdding,
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'postag': postag,
        'postag[:2]': postag[:2],
        'tag1': tag1,
        'tag1[:2]': tag1[:2],
        'tag2': tag2,
        'tag2[:2]': tag2[:2],
        'tag3': tag3,
        'tag3[:2]': tag3[:2],
        'wordlength': len(word),
        'wordinitialcap': word[0].isupper(),
        'wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
        'wordallcap': len([x for x in word if x.isupper()])==len(word),
        'distfromsentbegin': i
    }
    if i &gt; 0:
        word1 = sent[i-1][0]
        wordembdding1= get_features(word1)
        wordembdding1=np.array(wordembdding1)
        #wordembdding1=f2(wordembdding1)
        postag1 = sent[i-1][1]
        tag11=sent[i-1][2]
        tag22=sent[i-1][4]
        tag33 = sent[i-1][5]
        features.update({
            '-1:word.lower()': word1.lower(),
            '-1:word.istitle()': word1.istitle(),
            '-1:word.isupper()': word1.isupper(),
            '-1:wordembdding': wordembdding1,   # word embedding features 
            '-1:postag': postag1,
            '-1:postag[:2]': postag1[:2],
            '-1:tag1': tag1,
            '-1:tag1[:2]': tag1[:2],
            '-1:tag2': tag2,
            '-1:tag2[:2]': tag2[:2],
            '-1:tag3': tag3,
            '-1:tag3[:2]': tag3[:2],
            '-1:wordlength': len(word),
            '-1:wordinitialcap': word[0].isupper(),
            '-1:wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
            '-1:wordallcap': len([x for x in word if x.isupper()])==len(word),
        })
    else:
        features['BOS'] = True

    if i &lt; len(sent)-1:
        word1 = sent[i+1][0]
        wordembdding1= get_features(word1)
        wordembdding1= get_features(word1)
        wordembdding1=np.array(wordembdding1) ## word embedding features 
        #wordembdding1=f2(wordembdding)
        postag1 = sent[i+1][1]
        tag11=sent[i+1][2]
        tag22=sent[i+1][4]
        tag33 = sent[i+1][5]
        features.update({
            '+1:word.lower()': word1.lower(),
            '+1:word.istitle()': word1.istitle(),
            '+1:word.isupper()': word1.isupper(),
            '+1:wordembdding': wordembdding1,
            '+1:postag': postag1,
            '+1:postag[:2]': postag1[:2],
            '+1:tag1': tag1,
            '+1:tag1[:2]': tag1[:2],
            '+1:tag2': tag2,
            '+1:tag2[:2]': tag2[:2],
            '+1:tag3': tag3,
            '+1:tag3[:2]': tag3[:2],
            '+1:wordlength': len(word),
            '+1:wordinitialcap': word[0].isupper(),
            '+1:wordmixedcap': len([x for x in word[1:] if x.isupper()])&gt;0,
            '+1:wordallcap': len([x for x in word if x.isupper()])==len(word),
        })
    else:
        features['EOS'] = True

    return features


def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]

def sent2labels(sent):
    return [label for token, postag, tag1, label, tag2, tag3 in sent]

def sent2tokens(sent):
    return [token for token, postag, tag1, label, tag2, tag3, tag4, tag5 in sent]



X_train = [sent2features(s) for s in train_sents]
y_train = [sent2labels(s) for s in train_sents]

X_test = [sent2features(s) for s in test_sents]
y_test = [sent2labels(s) for s in test_sents]


%%time
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=True
)
crf.fit(X_train, y_train)   ### Error message when try to train
</code></pre>

<p>When I want to train the CRF model I got this error messages:</p>

<p><strong><code>TypeError: only size-1 arrays can be converted to Python scalars</code></strong></p>

<p>Can anyone suggest me how to use word embedding vectors to train CRF model ?</p>
",,2019-11-08 15:48:18,How to use word embedding as features for CRF (sklearn-crfsuite) model training,<python><word-embedding><crfsuite><python-crfsuite>,,,CC BY-SA 4.0,True,False,True,False,True
23491,58786596,2019-11-10 07:30:53,,"<p>I am struggling with the following problem:</p>

<p>I downloaded a <strong>pre-trained word embedding model</strong> for <strong>Spanish</strong> (over 1 million words 300-dimensional word vectors for Spanish)
I loaded it successfully and I even managed to undertake a couple of experiments, such as most similar words and basic analogies in Spanish (A is to B as C is to what), but when I try the following:</p>

<pre><code> for pais in 'Italia', 'Francia', 'India', 'China':
      print(' is the capital of '  
      (A_is_to_B_as_C_is_to('Alemania','Berln',pais),pais))
</code></pre>

<p>It raises the error:</p>

<pre><code>KeyError: ""word 'Berln' not in vocabulary""
</code></pre>

<p>I already checked that the word is actually in the word embedding. I have also eliminated the possibility of an encoding error. </p>

<p>Based on my research, this type of error is produced when the token/word is supposed to be wrapped in a list [], however I dont know how to apply that to this specific problem. Besides, this block of code is the same code used in <strong>Deep Learning Cookbook</strong> in chapter 3 (Word2vecMath)</p>

<p>This is the complete script:</p>

<pre><code>import os
from keras.utils import get_file
import gensim

from gensim.models.keyedvectors import KeyedVectors

import subprocess
import numpy as np
import matplotlib.pyplot as plt
from IPython.core.pylabtools import figsize


from sklearn.manifold import TSNE
import json
from collections import Counter
from itertools import chain

from keras.models import load_model
path = (""D:\Pretrained_wordEmbeddings_ESP\embeddings-l-model.vec"")


model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False)


data=model.most_similar(positive=[""muerte""])

print(data[:])


def A_is_to_B_as_C_is_to(a, b, c, topn=1):
    a, b, c = map(lambda x:x if type(x) == list else [x], (a, b, c))
    res = model.most_similar(positive=b + c, negative=a, topn=topn)
    if len(res):
        if topn == 1:
            return res[0][0]
        return [x[0] for x in res]
    return None

A_is_to_B_as_C_is_to('hombre', 'mujer', 'rey')

## for pais in 'Italia', 'Francia', 'India', 'China':
##    print(' is the capital of '  
##          (A_is_to_B_as_C_is_to('Alemania', 'Berln', pais), pais))
</code></pre>

<p>Thank you from your support</p>
",,2019-11-10 19:14:34,"using a Spanish pretrained model with Gensim causes raise KeyError(""word '%s' not in vocabulary"" % word)",<python><deep-learning><gensim><word-embedding><keyerror>,,,CC BY-SA 4.0,False,False,True,False,True
23493,58754450,2019-11-07 17:50:21,,"<p>I'm using Gensim to train a skip-gram word2vec model. The dataset has 1 million sentences, but the vocabulary is of size 200. I would like to see the model accuracy over iterations, so I used <code>model.wv.similar_by_word</code> in the callback function to see the scores. But the returned values were not updated over iterations.</p>

<p>The <code>iter</code> was set to be <code>100</code>.
I tried to change the values of <code>window</code> and <code>size</code>, but it has no effect.</p>

<p>The model was initialized with callbacks:</p>

<pre class=""lang-py prettyprint-override""><code>Word2Vec(self.train_corpus, workers=multiprocessing.cpu_count(), compute_loss=True, callbacks=[A_CallBack], **word2vec_params)
</code></pre>

<p>In the class <code>A_CallBack</code>, I have something like this:</p>

<pre class=""lang-py prettyprint-override""><code>def on_epoch_end(self, model):
    word, score = model.wv.similar_by_word(word='target_word', topn=1)[0]
    print(word, score)
</code></pre>

<p>The <code>word</code> and <code>score</code> were printed out for every epoch, but the values have never changed.</p>

<p>I was expecting the values of them to be updated over iterations, which should make sense?</p>

<p>I'm new to machine learning and word2vec. Thanks a lot for the help.</p>
",,2019-11-08 14:15:28,'similar_by_word' did not improve over iterations,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23500,58816895,2019-11-12 10:38:53,,"<p>I am trying to train a Doc2Vec model using gensim.</p>

<p>The dataset i am using is the 20 newsgroups dataset [1] which is included in sklearn's datasets module.</p>

<p>I have used the example in the gensim documentation to create the model.</p>

<pre class=""lang-py prettyprint-override""><code>docs = newsgroups_train['data']
enumerated_docs = enumerate(docs)
documnets= [TaggedDocument(doc.split(),i) for i, doc in enumerated_docs]
model = Doc2Vec(documnets, vector_size=20, window=2, min_count=30, workers=4)
</code></pre>

<p>I checked every line of code, all seems to be working up to the line which initializes the model.</p>

<p>I  get a type error:
<code>TypeError: 'int' object is not iterable</code></p>

<p>[1] <a href=""https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html</a></p>
",,2019-11-12 10:47:20,Type error when trying to create a doc2vec model in gensim,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
23503,58718429,2019-11-05 19:47:10,,"<p>Ran into the following error msg: </p>

<p>ValidationError: 
 * Not all rows (distributions) in topic_term_dists sum to 1.</p>

<pre><code>import pyLDAvis.gensim
import warnings

warnings.filterwarnings(""ignore"",category=DeprecationWarning)
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model_tfidf,corpus,id2word)
vis
</code></pre>
",,2019-11-05 19:47:10,How do I fix pyLDAvis.gensim error msg when there's 0 in my dictionary,<python><jupyter-notebook><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23506,58723979,2019-11-06 06:04:50,,"<p>I ran Gensim to train Doc2vec of the corpus. I need to extract the vector of each document as input data for Logical regression in spark.    </p>
",,2019-11-06 17:21:39,Recall Doc2Vec in Spark and input vectors to Logical regression machine learning,<apache-spark><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23510,58789484,2019-11-10 14:13:54,,"<p>I am trying to lemmatize documents with the following codes. Lemmatization works. It produces byte string. Therefore, the next part of the codes produces ""cant concan byte to str"" error. Then I have changed tokens as str() as given in below codes. The output of the code is as given below;(I am using Python 3.7 (64 bit))</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-223-cb505389f802&gt; in &lt;module&gt;
      1 #Build a Vocabulary
----&gt; 2 model.build_vocab(train_demo_corpus)

~\Anaconda3\lib\site-packages\gensim\models\doc2vec.py in build_vocab(self, documents, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    727         """"""
    728         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 729             documents, self.docvecs, progress_per=progress_per, trim_rule=trim_rule)
    730         self.corpus_count = corpus_count
    731         report_values = self.vocabulary.prepare_vocab(

~\Anaconda3\lib\site-packages\gensim\models\doc2vec.py in scan_vocab(self, documents, docvecs, progress_per, trim_rule)
    807         for document_no, document in enumerate(documents):
    808             if not checked_string_types:
--&gt; 809                 if isinstance(document.words, string_types):
    810                     logger.warning(
    811                         ""Each 'words' should be a list of words (usually unicode strings). ""

AttributeError: 'str' object has no attribute 'words'
</code></pre>

<p>here is my code;</p>

<pre class=""lang-py prettyprint-override""><code>train_demo_corpus = list(lemmat(lee_train_demo_file))

def lemmat(fname, tokens_only=False):
    with smart_open.smart_open(fname, encoding=""iso-8859-1"") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.lemmatize(line)
            if tokens_only:
                yield str(tokens)
            else:
                # For training data, add tags
                yield str(gensim.models.doc2vec.TaggedDocument(tokens, [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_demo_corpus)
</code></pre>

<p>Best regards,</p>
",2019-11-10 21:15:46,2019-11-10 21:15:46,Gensim Lemmatization Remove Postag b',<gensim><lemmatization>,,,CC BY-SA 4.0,False,False,True,False,False
23515,58771410,2019-11-08 17:40:47,,"<p>I'm looking for test datasets to optimize my Word2Vec model. I have found a good one from gensim:</p>

<p>gensim/test/test_data/questions-words.txt </p>

<p>Does anyone know other similar datasets?</p>

<p>Thank you!</p>
",,2019-11-12 19:37:54,Question pairs (ground truth) datasets for Word2Vec model testing?,<machine-learning><nlp><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
23522,58666699,2019-11-01 22:45:31,,"<p>Using the <code>Word2Vec</code> implementation of the module <code>gensim</code> in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word <code>happy</code> is defined in the vocabulary, getting the error <code>KeyError: ""word 'happy' not in vocabulary""</code>. Tried to apply the given the answers to <a href=""https://stackoverflow.com/questions/41133844/keyerror-word-word-not-in-vocabulary-in-word2vec"">a similar question</a>, but did not work. Hence, posted my own question.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    # When I debug, both of the words 'happy' and 'birthday' exist in the variable 'data'
    word2vec = Word2Vec(data, min_count=5, size=10000, window=5, workers=4)

    # Print result
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru word2vec: {word2vec.similarity(word_1, word_2)}')
except Exception as err:
    print(f'An error happened! Detail: {str(err)}')
</code></pre>
",,2019-11-01 23:27:33,"word2vec - KeyError: ""word X not in vocabulary""",<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
23523,58666807,2019-11-01 23:02:35,,"<p>I'm using <code>fastText</code> implementation of the module <code>gensim</code>. Despite getting no reasons, my program throws an exception.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    fastText = FastText(data, min_count=1, size=10000, window=5, workers=4)

    # Print results
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru fastText: {fastText.similarity(word_1, word_2)}')
except Exception as err:
    print(f'\n!!!!! An error happened! Detail: {str(err)}')
</code></pre>

<p>The end of the output:</p>

<pre><code>!!!!! An error happened! Detail: 
</code></pre>
",,2019-11-01 23:34:16,fastText - Throws exception without any reasons,<python-3.x><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
23527,58804099,2019-11-11 15:18:43,,"<p>Going through the gensim source, I noticed the <code>simple_preprocess</code> utility function clears all punctuations except those with words starting with an underscore, <code>_</code>. Is there a reason for this?</p>

<pre class=""lang-py prettyprint-override""><code>def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):
    tokens = [
        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')
        if min_len &lt;= len(token) &lt;= max_len and not token.startswith('_')
    ]
    return tokens

</code></pre>
",,2019-11-11 18:15:07,Why does gensim ignore underscores during preprocessing?,<nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
23548,58793692,2019-11-10 22:56:49,,"<p>Using Latent Dirichlet Allocation, (with gensim or sklearn in python), how can I use the topics distribution for each document with their associated classification (let's say we have movie reviews labeled positive or negative) to use in a supervised learning algorithm to classify unseen documents</p>

<p>Are there any resources or work other people have done you recommend I take a look at?</p>

<p><strong>Here's a break down of the problem:</strong></p>

<ol>
<li><p>Let's suppose I have a corpus such that each document is a movie review.  Each document is labeled either positive or negative (for positive or negative review).</p></li>
<li><p>Using Latent Dirichlet Allocation (topic modeling), I'd like to generate a topic model for this corpus such that each document is associated with some distribution of topics.</p></li>
<li><p>Then using the topic distribution and classification (positive or negative) I'd like to train a supervised machine learning algorithm (like Neural Networks or Decision Trees) so we can classify future movie reviews that have not been seen at all by the model to either be positive or negative.</p></li>
</ol>
",,2019-11-10 22:56:49,Topic Modeling - use the topics distribution for each document with their associated classification to use in a supervised learning algorithm,<python><scikit-learn><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
23551,58836322,2019-11-13 11:40:57,,"<p>I am trying to implement a semantic search to retrieve similar documents from a dataset of unstructured French documents.</p>

<ul>
<li>These documents are not categorized and are templates with 300 - 3000 words per document.</li>
<li>I am using doc2vec using gensim to find the paragraph embeddings with 300 dimensions and a window of 5 of the dataset.</li>
<li>I am then converting the search query which is a maximum of 5 words to the vector with 300 dimensions and comparing the cosine distance to find the document close to the search queries.</li>
</ul>

<p>I am not getting good results. Please suggest some strategies to do the semantic search. I was trying to reduce the number of words in my dataset by doing rake keyword extraction.</p>
",2019-11-13 15:46:33,2019-11-25 20:17:33,How to do language representation on huge documents of 3000-4000 word for query-based retrieval?,<search><nlp><gensim><cosine-similarity><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23553,58839049,2019-11-13 14:12:14,,"<p>So, I have a keyword list lowercase. Let's say </p>

<pre><code>keywords = ['machine learning', 'data science', 'artificial intelligence']
</code></pre>

<p>and a list of texts in lowercase. Let's say</p>

<pre><code>texts = [
  'the new machine learning model built by google is revolutionary for the current state of artificial intelligence. it may change the way we are thinking', 
  'data science and artificial intelligence are two different fields, although they are interconnected. scientists from harvard are explaining it in a detailed presentation that could be found on our page.'
]
</code></pre>

<p>I need to transform the texts into:</p>

<pre><code>[[['the', 'new',
   'machine_learning',
   'model',
   'built',
   'by',
   'google',
   'is',
   'revolutionary',
   'for',
   'the',
   'current',
   'state',
   'of',
   'artificial_intelligence'],
  ['it', 'may', 'change', 'the', 'way', 'we', 'are', 'thinking']],
 [['data_science',
   'and',
   'artificial_intelligence',
   'are',
   'two',
   'different',
   'fields',
   'although',
   'they',
   'are',
   'interconnected'],
  ['scientists',
   'from',
   'harvard',
   'are',
   'explaining',
   'it',
   'in',
   'a',
   'detailed',
   'presentation',
   'that',
   'could',
   'be',
   'found',
   'on',
   'our',
   'page']]]
</code></pre>

<p>What I do right now is checking if the keywords are in a text and replace them with the keywords with _. But this is of complexity m*n and it is really slow when you have 700 long texts and 2M keywords as in my case.</p>

<p>I was trying to use Phraser, but I can't manage to build one with only my keywords.</p>

<p>Could someone suggest me a more optimized way of doing it?</p>
",2019-11-13 22:18:55,2019-11-14 19:27:32,Python connect composed keywords in texts,<python><data-science><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23571,58822292,2019-11-12 15:56:58,,"<p><strong>Context</strong></p>

<p>There exists severals questions about how to train <code>Word2Vec</code> using <code>gensim</code> with streamed data. Anyhow, these questions don't deal with the issue that streaming cannot use multiple workers since there is no array to split between threads.</p>

<p>Hence I wanted to create a generator providing such functionality for gensim. My results look like:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec as w2v

#The data is stored in a python-list and unsplitted.
#It's too much data to store it splitted, so I have to do the split while streaming.
data = ['this is document one', 'this is document two', ...]

#Now the generator-class
import threading

class dataGenerator:
    """"""
    Generator for batch-tokenization.
    """"""

    def __init__(self, data: list, batch_size:int = 40):
        """"""Initialize generator and pass data.""""""

        self.data = data
        self.batch_size = batch_size
        self.lock = threading.Lock()


    def __len__(self):
        """"""Get total number of batches.""""""
        return int(np.ceil(len(self.data) / float(self.batch_size)))


    def __iter__(self) -&gt; list([]):
        """"""
        Iterator-wrapper for generator-functionality (since generators cannot be used directly).
        Allows for data-streaming.
        """"""
        for idx in range(len(self)):
            yield self[idx]


    def __getitem__(self, idx):

        #Make multithreading thread-safe
        with self.lock:

            # Returns current batch by slicing data.
            return [arr.split("" "") for arr in self.data[idx * self.batch_size : (idx + 1) * self.batch_size]]


#And now do the training
model = w2v(
             sentences=dataGenerator(data),
             size=300,
             window=5,
             min_count=1,
             workers=4
            )
</code></pre>

<p>This results in the error </p>

<blockquote>
  <p>TypeError: unhashable type: 'list'</p>
</blockquote>

<p>Since <code>dataGenerator(data)</code> would work if I'd just yield a single splitted document, I assume that gensims <code>word2vec</code> wraps the generator within an extra list. In this case the <code>__iter__</code> would look like:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>Hence, my batch would also be wrapped resulting in something like <code>[[['this', '...'], ['this', '...']], [[...], [...]]]</code> (=> list of list of list) which cannot be processed by gensim.</p>

<p><br>
<br>
<br>
<strong>My question:</strong></p>

<p><em>Can I ""stream""-pass batches in order to use multiple workers?
How can I change my code accordingly?</em></p>
",2020-01-09 22:18:00,2020-07-17 23:38:48,Batch-train word2vec in gensim with support of multiple workers,<python><nlp><batch-processing><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23584,58797101,2019-11-11 07:24:27,,"<p>I'm trying to store data in a global variable inside a Redis Queue (RQ) worker so that this data remains pre-loaded, i.e. it doesn't need to be loaded for every RQ job.</p>

<p>Specifically, I'm working with Word2Vec vectors and loading them using gensim's KeyedVectors.</p>

<p>My app is in Python Flask, running on a Linux server, containerized using Docker.</p>

<p>My goal is to reduce processing time by keeping a handful of large vectors files loaded in memory at all times. </p>

<p>I first tried storing them in global variables in Flask, but then <em>each</em> of my 8 gunicorn workers loads the vectors, which eats up a lot of RAM.</p>

<p>I only need <em>one</em> worker to store a particular vectors file.</p>

<p>I've been told that one solution is to have a set number of RQ workers holding the vectors in a global variable, so that I can control which workers get which vectors files loaded in.</p>

<p>Here is what I have so far:</p>

<p><em>RQ_worker.py</em></p>

<pre><code>from rq import Worker, Connection
from gensim.models.keyedvectors import KeyedVectors
from my_common_methods import get_redis

W2V = KeyedVectors.load_word2vec_format('some_path/vectors.bin', binary=True)

def rq_task(some_args):
    # use some_args and W2V to do some processing, e.g.:
    with open(some_args_filename, 'w') as f_out:
        f_out.write(str(W2V['word']))

if __name__ == '__main__':
    with Connection(get_redis()):
        worker = Worker(['default'])
        worker.work()
</code></pre>

<p><em>app.py</em></p>

<pre><code>from rq import Queue, Connection
from RQ_worker import rq_task

@app.route(""/someroute"", methods=['POST'])
def some_route():
    # test Redis Queue
    with Connection(get_redis()):
        q = Queue()
        task = q.enqueue(rq_task, some_args)
</code></pre>

<p><em>docker-stack.yml</em></p>

<pre><code>version: '3.7'

services:
  nginx:
    image: nginx:mainline-alpine
    deploy: ...
    configs: ...
    networks: ...

  flask:
    image: ...
    deploy: ...
    environment: ...
    networks: ...
    volumes: ...

  worker:
    image: ...
    command: python2.7 RQ_worker.py
    deploy:
      replicas: 1
    networks: ...
    volumes:
      - /some_path/data:/some_path/data

configs:
  nginx.conf:
    external: true
    name: nginx.conf

networks:
  external:
    external: true
  database:
    external: true
</code></pre>

<p>(I redacted a bunch of stuff from Docker, but can provide more details, if relevant.)</p>

<p>The above generally works, <strong>except</strong> that the RQ worker seems to load W2V <strong>from scratch</strong> each time it gets a new job, which defeats the whole purpose. It should keep the vectors stored in W2V as a global variable, so they don't need to be reloaded each time.</p>

<p>Am I missing something? Should I set it up differently? </p>

<p>I've been told that it might be possible to use mmap to load the vectors file into a global variable that the RQ worker sits on, but I'm not sure how that would work with KeyedVectors.</p>

<p>Any advice would be much appreciated!</p>
",,2019-11-11 18:46:45,How to store gensim's KeyedVectors object in a global variable inside a Redis Queue worker,<docker><flask><redis><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23596,58671002,2019-11-02 12:14:43,,"<p>I have around 150,000 rows in csv file and getting 'Memory Error' in the <code>for sims in index</code> statement.<br>
Could you please advise me how to extract values in Similarity object without getting the memory error.</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in terms_list]
similarity_matrix = []

index = gensim.similarities.Similarity('E:\\cm_test',corpus,len(dictionary))

for sims in index:

    similarity_matrix.append(sims)

similarity_array = np.array(similarity_matrix)
</code></pre>
",2019-11-15 06:37:10,2019-11-15 06:37:10,Memory error for Similarity matrix for large number of rows (gensim),<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23610,58895222,2019-11-16 21:05:11,,"<p>I'm topic modeling a corpus of English 20th century correspondence using LDA and I've been using <strong>topic coherence</strong> (as well as silhouette scores) to evaluate my topics. I use gensim's <code>CoherenceModel</code> with <code>c_v</code> coherence and the highest I've ever gotten was a <code>0.35</code> score in all the models I've tested, even in the topics that make the most sense to me in qualitative evaluation, even after extensive pre-processing and hyperparameter comparison.</p>

<p>So I basically accepted that that's the best I'd get, but in order to write about it now I've been reading up on topic coherence and I've understood it's a pipeline and it models human judgement. One thing I can't seen to find clear info on, though: Is it based exclusively on calculations made on <strong>my</strong> corpus, or is it based on some external data as well? Like trained on external corpora that might have nothing to do with my domain? Should I use <code>u_mass</code> instead?</p>
",,2020-01-14 04:03:32,Is topic coherence (gensim CoherenceModel) calculated based exclusively on my corpus or external data as well?,<data-science><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
23612,58876630,2019-11-15 12:01:11,,"<p>I'm trying to export the fasttext model created by gensim to a binary file. But the docs are unclear about how to achieve this. 
What I've done so far: </p>

<pre><code>model.wv.save_word2vec_format('model.bin')
</code></pre>

<p>But this does not seems like the best solution. Since later when I want to load the model using the :</p>

<pre><code>fasttext.load_facebook_model('model.bin')
</code></pre>

<p>I get into an infinite loop. While loading the <code>fasttext.model</code> created by <code>model.save('fasttext.model)</code> function gets completed in around 30 seconds.</p>
",,2020-05-15 20:50:12,"How to export a fasttext model created by gensim, to a binary file?",<python><nlp><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
23634,58883170,2019-11-15 19:03:36,,"<p>Saving word2vec in the word2vec text format gives a file with weird characters in it. </p>

<p><img src=""https://i.stack.imgur.com/dL1Vc.png"" alt=""What the saved word2vec text file looks like""></p>

<p>The contents of the file word2vec is making vectors from.</p>

<p><img src=""https://i.stack.imgur.com/XUUxl.png"" alt=""Cleaned and tokenized text""></p>

<p>I get no errors until I try and use the vector files in an analogy test. The text originally comes from an East African online newspaper.</p>

<p>My code:</p>

<pre><code>word2vec = gensim.models.Word2Vec(all_words, min_count=3, workers = 2)
save_as_1 = ""daily_nation_"" + str(subject) + ""_"" + str(startyr) + ""_"" + str(endyr) + ""_vectors.txt""
save_as_2 = ""daily_nation_"" + str(subject) + ""_"" + str(startyr) + ""_"" + str(endyr) + ""_vectors.bin""
word2vec.wv.save_word2vec_format(save_as_1, binary = ""FALSE"")
word2vec.wv.save_word2vec_format(save_as_2, binary = ""TRUE"")
vocabulary = word2vec.wv.vocab
print(""Vectors: "")
print(vocabulary)
sim_words = word2vec.wv.most_similar('woman')
print(""Words most similar to woman are: "" + str(sim_words))
</code></pre>

<p>I  want to  create proper text files of the embeddings.</p>
",2019-11-16 11:03:30,2019-11-16 11:03:30,Saving word2vec model results in messed up file,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23645,58965192,2019-11-20 23:55:57,,"<p>If I have some documents like this:</p>

<pre><code>doc1 = ""hello hello this is a document""
doc2 = ""this text is very interesting""
documents = [doc1, doc2]
</code></pre>

<p>And I compute a TF-IDF matrix for this in Gensim like this:</p>

<pre><code># create dictionary
dictionary = corpora.Dictionary([simple_preprocess(line) for line in documents])
# create bow corpus
corpus = [dictionary.doc2bow(simple_preprocess(line)) for line in documents]
# create the tf.idf matrix
tfidf = models.TfidfModel(corpus, smartirs='ntc')
</code></pre>

<p>Then for each document, I get a TF-IDF like this:</p>

<pre><code>Doc1: [(""hello"", 0.5), (""a"", 0.25), (""document"", 0.25)]
Doc2: [(""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>But I want the TF-IDF vector for each document to include words with 0 TF-IDF values (i.e. include every word mentioned in the corpus):</p>

<pre><code>Doc1: [(""hello"", 0.5), (""this"", 0), (""is"", 0), (""a"", 0.25), (""document"", 0.25), (""text"", 0), (""very"", 0), (""interesting"", 0)]
Doc2: [(""hello"", 0), (""this"", 0), (""is"", 0), (""a"", 0), (""document"", 0), (""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>How can I do this in Gensim? Or maybe there is some other library that can compute a TF-IDF matrix in this fashion (although like Gensim, it needs to be able to handle very large data sets, e.g. I achieved this result in Sci-kit on a small data set, but Sci-kit has memory problems on a large data set).</p>
",,2019-11-22 20:13:49,How do you include all words from the corpus in a Gensim TF-IDF?,<python><nlp><gensim><text-classification><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
23647,58869364,2019-11-15 02:10:01,,"<p>I have a dataset with 4.7 million questions, and I want to compare their tf-idf vectors and retrieve the most similar pair for each question.</p>
<p>According to the gensim documentation,</p>
<blockquote>
<p>There is also a special syntax for when you need similarity of documents in the index</p>
<p>to the index itself (i.e. queries=indexed documents themselves). This special syntax</p>
<p>uses the faster, batch queries internally and <strong>is ideal for all-vs-all pairwise similarities</strong>:</p>
<p><code>for similarities in index:  # yield similarities of the 1st indexed document, then 2nd... ...</code></p>
<p><code>pass</code></p>
</blockquote>
<p>However, since I have about 4.7 million documents, <code>similarities</code> should be a numpy array with length 4.7 million, which is very large too and I cannot store on memory.</p>
<pre><code>index = Similarity.load('out/corpus.index')
idx1 = 0
for similarities in index: # &lt;---- this part is slow
  idx1 += 1
  # and other stuff
</code></pre>
<p>Is there a way that I can get the most similar pair for each question?</p>
",2020-06-20 09:12:55,2019-11-15 05:52:46,Gensim Similarity with very large dataset (~4.7 million),<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23651,58930298,2019-11-19 09:10:59,,"<p>I am building a machine learning model which will process documents and extract some key information from it. For this, I need to use word embedding for OCRed output. I have several different options for the embedding (Google's word2vec, Stanford's, Facebook's FastText) but my main concern is OOV words, as the OCR output will have a lot of misspelled words. For example, I want the embeddings such that the output for <strong>Embedding</strong> and <strong>Embdding</strong> (e missed by the OCR) should have a certain level of similarity. I don't care much about the associated contextual information. </p>

<p>I chose Facebook's FastText as it gives the embeddings for OOV words as well. My only concern is the size of the embeddings. The vector size of the FastText's model is of length 300. Is there a way to reduce the size of the returned word vector(I am thinking of using PCA or any other dimensionality reduction technique, but given the size of word vectors, it can be a time-consuming task)? </p>
",2019-11-20 00:49:26,2020-04-30 18:02:23,Reducing size of Facebook's FastText Word2Vec,<data-science><gensim><word2vec><dimensionality-reduction><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
23655,58883173,2019-11-15 19:03:56,,"<p>I want to create a topic model from data provided by Jstor (e.g. <a href=""https://www.jstor.org/dfr/about/sample-datasets"" rel=""nofollow noreferrer"">https://www.jstor.org/dfr/about/sample-datasets</a>). However, because of copyright, they do not allow full text access. Instead, I can request a list of unigrams followed by their frequencies in the document (supplied in plain <code>.txt</code>). e.g:</p>

<pre><code>his         295
old         181
he          165
age         152
p           110
from         79
life         74
de           71
petrarch     58
book         51
courtier     47
</code></pre>

<p>This should be easy to convert to a bag-of-words vector. However, I have only found examples of Gensim LDA models being built from fulltext. Would it be possible to pass it these vectors instead?</p>
",2019-11-17 14:55:29,2020-01-14 04:19:55,Creating LDA model using gensim from bag-of-words vectors,<vector><lda><topic-modeling><jstor>,,,CC BY-SA 4.0,False,False,True,False,False
23658,58886579,2019-11-16 01:04:05,,"<p>I use GenSim and CBOW for training the corpus. How can I get the most similar words from a set of input words?</p>

<p>For example:
Given a set of input words: [""David"", ""Mary"", ""married""]. Can I infer some output words like: ""wedding"", ""husband"", ""wife"", ""couple"", etc?</p>
",,2019-11-16 01:07:40,How to find the most similar words from a set of input words by CBOW (GenSim)?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23674,58871071,2019-11-15 05:51:44,,"<p>I'm trying to find out the similarity between 2 documents i.e 'document_1' and 'document_2'.
I'm using <strong>Doc2Vec Gensim's <em>keyedvectors.py</em></strong> for finding similarity score.</p>

<pre><code>score = model.docvecs.similarity_unseen_docs(trainedModel, document_1, document_2)
print(score)
</code></pre>

<p>Where score is negative.</p>

<p>Here document_1 and document_2 are result of <em>NLTK's word_tokenize()</em></p>

<p><strong>What does Negative score mean when we try to find similarity between two ""<em>tokenized</em>"" documents?</strong></p>

<p><strong>P.S:</strong> Trained the model on 10 documents(2 Pages each)=20 Pages MS
word documents. </p>
",2019-11-15 14:34:24,2019-11-15 14:34:24,"Getting negative score for model.docvecs.similarity_unseen_docs(document_1, document_2)",<python><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
23689,58938729,2019-11-19 16:28:32,,"<p>i have run this code for more than 4 days without any output also without any error, is there something i didnt do to get the topics from gensim model from the bag of words that i've created?</p>

<p>my dataset only consist of 5000+ documents</p>

<pre><code>from pprint import pprint
import gensim
from gensim import corpora, models
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
import pandas as pd
import nltk
nltk.download('wordnet')

df = pd.read_csv('F:/1Steam Data/SteamCSV/realdata/gamesdescription.csv')
datatext = df[['Description']]
datatext['index'] = df.index
text3 = datatext
#print (datatext.head())

stemmer = SnowballStemmer('english')

def lemmatize_stemming(text):
  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
  result = []
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
        result.append(lemmatize_stemming(token))
  return result



processed_docs = text3['Description'].map(preprocess)

#print (processed_docs[:10])

dictionary = gensim.corpora.Dictionary(processed_docs)
dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

print (dictionary)

bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
print('hihi')
lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, 
workers=3)
print('hihi')
for idx, topic in lda_model.print_topics(-1):
  print('Topic: {} \nWords: {}'.format(idx, topic))

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=8, id2word=dictionary, 
passes=2, workers=3)
for idx, topic in lda_model_tfidf.print_topics(-1):
   print('Topic: {} Word: {}'.format(idx, topic))
</code></pre>
",2019-11-19 17:18:43,2019-11-19 17:18:43,Extremely slow LDA training model using python gensim libraries,<python><pandas><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
23701,58986684,2019-11-22 02:38:46,,"<p>I have been doing clustering of a certain corpus, and obtaining results that group sentences together by obtaining their <em>tf-idf</em>, checking similarity weights > a certain threshold value from the gensim model. </p>

<pre class=""lang-py prettyprint-override""><code>tfidf_dic = DocSim.get_tf_idf()
ds = DocSim(model,stopwords=stopwords, tfidf_dict=tfidf_dic)
sim_scores = ds.calculate_similarity(source_doc, target_docs)
</code></pre>

<p>The problem is that despite putting high threshold values, sentences of similar topics but <strong>opposite polarities</strong> get clustered together as such:</p>

<p><img src=""https://i.stack.imgur.com/K1uIn.png"" alt=""cluster results.""></p>

<blockquote>
  <p>Here is an example of the similarity weights obtained between ""don't like it"" &amp; ""i like it""</p>
</blockquote>

<p><img src=""https://i.stack.imgur.com/8sFvK.png"" alt=""similarity results."">
Are there any other methods, libraries or alternative models that can differentiate the polarities effectively by assigning them very low similarities or opposite vectors?</p>

<p>This is so that the outputs ""i like it"" and ""dont like it"" are in separate clusters.</p>

<p>PS: Pardon me if there are any conceptual errors as I am rather new to NLP. Thank you in advance!</p>
",2019-11-25 07:08:49,2019-11-25 07:08:49,Text representations : How to differentiate between strings of similar topic but opposite polarities?,<nlp><cluster-analysis><gensim><similarity>,,,CC BY-SA 4.0,False,False,True,False,False
23715,58975407,2019-11-21 12:44:46,,"<p>Good day, fellow humans (?).</p>

<p>I have a methodological question that is confused by a deep research in a tiny amount of time.</p>

<p>The question arises from the following problem(s): I need to apply semi-supervised or unsupervised clustering on documents. I have ~300 documents classified with multi-labels and approximately 3400 documents not classified. The number of unsupervised documents could become ~10'000 in the next days.</p>

<p>The main idea is that of applying semi-supervised clustering based on the labels at hands. Alternatively, that of going fully unsupervised for soft clustering.</p>

<p>We thought of creating embeddings for the whole documents, but here lies the confusion: which library is the best for such a task? </p>

<p>I guess the utmost importance needs to lie in the context of the whole document. As far as I know, BERT and FastText provide context-dependent word embedding, but not whole document embedding. On the other hand, Gensim's Doc2Vec is context-agnostic, right?</p>

<p>I think I saw a way to train sentence embeddings with BERT, via the HuggingFace API, and was wondering whether it could be useful to consider the whole document as a single sentence.</p>

<p>Do you have any suggestion? I'm probably exposing my utter ignorance and confusion on the matter, but my brain is melted.</p>

<p>Thank you very much for your time.</p>

<p>Viva!</p>

<p>Edit to answer to @gojomo:</p>

<p>My documents are on average ~180 words. The original task was that of multi-label text classification, i.e. each document can have from 1 to N labels, with the number of labels now being N=18.  They are highly imbalanced.
Having only 330 labeled documents so far due to several issues, we asked the documents' provider to give also unlabeled data, that should reach the order of the 10k.
I used FastText classification mode, but the result is obviously atrocious. I also run a K-NN with Doc2Vec document embedding, but the result is obviously still atrocious.
I was going to use biomedical BERT-based models (like BioBERT and SciBERT) to produce a NER tagging (trained on domain-specific datasets) on the documents to later apply a classifier.
Now that we have unlabeled documents at disposal, we wanted to adventure into semi-supervised classification or unsupervised clustering, just to explore possibilities. I have to say that this is just a master thesis.</p>
",2019-11-21 21:27:00,2019-11-21 21:27:00,NLP - Best document embedding library,<nlp><document><gensim><embedding><bert-language-model>,,,CC BY-SA 4.0,False,False,True,False,False
23731,59009670,2019-11-23 16:19:38,,"<p>I am trying to reimplement wor2vec in pytorch. I implemented subsamping according to the <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L407"" rel=""nofollow noreferrer"">code</a> of the original paper. However, I am trying to understand how subsampling is implemented in Gensim. I looked at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">source code</a>, but I did not manage to grasp how it reconnects to the original paper.</p>

<p>Thanks a lot in advance.</p>
",,2019-11-25 19:42:23,How does Gensim implement subsampling in Word2Vec?,<gensim><word2vec><subsampling>,,,CC BY-SA 4.0,False,False,True,False,False
23732,59024220,2019-11-25 01:48:29,,"<p>I am a beginner in NLP and it's my first time to do Topic Modeling. I was able to generate my model however I cannot produce the coherence metric.</p>

<p>Converting the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus</p>

<pre><code>sparse_counts = scipy.sparse.csr_matrix(data_dtm)
corpus = matutils.Sparse2Corpus(sparse_counts)
corpus
</code></pre>

<p><a href=""https://i.stack.imgur.com/EU6kl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EU6kl.png"" alt=""enter image description here""></a></p>

<pre><code>df_lemmatized.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/NUk6h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NUk6h.png"" alt=""enter image description here""></a></p>

<pre><code># Gensim also requires dictionary of the all terms and their respective location in the term-document matrix
tfidfv = pickle.load(open(""tfidf.pkl"", ""rb""))
id2word = dict((v, k) for k, v in tfidfv.vocabulary_.items())
id2word
</code></pre>

<p><a href=""https://i.stack.imgur.com/vN9Dm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vN9Dm.png"" alt=""enter image description here""></a></p>

<p>This is my model:</p>

<pre><code>lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=15, passes=10, random_state=43)
lda.print_topics()
</code></pre>

<p><a href=""https://i.stack.imgur.com/VXdpA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXdpA.png"" alt=""enter image description here""></a></p>

<p>And finally, here is where I attempted to get Coherence Score Using Coherence Model:</p>

<pre><code># Compute Perplexity
print('\nPerplexity: ', lda.log_perplexity(corpus))  

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda, texts=df_lemmatized.long_title, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<p>This is the error:</p>

<p><em>---> 57     if not dictionary.id2token:  # may not be initialized in the standard gensim.corpora.Dictionary
     58         setattr(dictionary, 'id2token', {v: k for k, v in dictionary.token2id.items()})
     59 
AttributeError: 'dict' object has no attribute 'id2token'</em></p>
",,2019-11-25 09:07:15,Error in Computing the Coherence Score  AttributeError: 'dict' object has no attribute 'id2token',<python><scipy><nlp><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
23742,59039366,2019-11-25 20:07:56,,"<p>Assume I have the following variables:</p>

<pre><code>import gensim
from gensim.models import KeyedVectors
wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
dict_dict = {
    ""abc"": ('dog', 'cat', 'bat'),
    ""def"": ('fat', 'hat', 'rat')
}
</code></pre>

<p>In this situation, <code>wv</code> is a word2vec model. </p>

<p>I want to take the values of each key in <code>dict_dict</code>, extract the value's vector (<code>e.g. wv['dog']</code>), and have the value now acts as a key to a sub dictionary:</p>

<pre><code>dict_dict = {
    ""abc"": ({'dog': array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]), {'cat':array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]), array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01]):'bat')}
</code></pre>

<p>Would I have to create a new dictionary to do this?</p>
",,2019-11-25 20:15:14,How to add numpy arrays as values in a dictionary of dictionaries?,<python><numpy><dictionary><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23747,58993767,2019-11-22 11:54:50,,"<p>Can we make gensim lda model to use pre-determined topic distribution, while determining topics for new docs?</p>

<p>Ex:</p>

<pre><code>import gensim
texts=[['a','a','a'], ['b','b','b'],['c','c','c']]
d = gensim.corpora.Dictionary(texts)
bow = [d.doc2bow(doc) for doc in texts]
import numpy as np
user_topics=np.array([[1, 0, 0],[0, 1, 0],[0,0,1]])
model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=3,  random_state=1,eta=user_topics)
model.get_topics()

</code></pre>

<p>Returns</p>

<pre><code>array([[0.9728407 , 0.01204113, 0.01511812],
       [0.01140388, 0.9742677 , 0.01432837],
       [0.02468761, 0.00788806, 0.9674243 ]], dtype=float32)

</code></pre>

<p>Is it possible for <code>model.get_topics()</code> to return same distribution as 'eta'?</p>

<p>i.e. </p>

<pre><code>[[1, 0, 0],[0, 1, 0],[0,0,1]]

</code></pre>
",,2020-01-14 03:47:38,Initialize Gensim LDA model with pre-determined topic distribution,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
23762,58961983,2019-11-20 19:30:20,,"<p>In Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"" rel=""nofollow noreferrer"">documentation</a>, it says:</p>

<blockquote>
  <p>You can save trained models to disk and later load them back, either to continue training on new training documents or to transform new documents.</p>
</blockquote>

<p>I would like to do this with a dictionary, corpus and tf.idf model. However, the documentation seems to say that it is possible, without explaining how to save these things and load them back up again.</p>

<p>How do you do this?</p>

<hr>

<p>I've been using Pickle, but don't know if this is right...</p>

<pre><code>import pickle
pickle.dump(tfidf, open(""tfidf.p"", ""wb""))
tfidf_reloaded = pickle.load(open(""tfidf.p"", ""rb""))
</code></pre>
",2019-11-20 19:38:13,2020-04-29 20:17:51,"How do you save a model, dictionary and corpus to disk in Gensim, and then load them again?",<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
23788,59079173,2019-11-27 22:16:11,,"<p>I have been training a gensim doc2vec model for a couple days on 10 epochs. It's been running smoothly until it got to 79.29% on the last Epoch, and it suddenly stopped logging anything new. The last message logged was:</p>

<blockquote>
  <p>INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 79.29% examples, 8566 words/s, in_qsize 0, out_qsize 0</p>
</blockquote>

<p>There is a line of code that saves the model after the 10 epochs, but it hasn't saved anything. Any idea what could be the issue?</p>
",,2019-11-27 22:16:11,Gensim doc2vec training stalled,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23799,58999509,2019-11-22 18:01:00,,"<p>Is there any way I can map generated topic from LDA to the list of documents and identify to which topic it belongs to ? I am interested in clustering documents using unsupervised learning and segregating it into appropriate cluster. </p>

<p>Example, I have 10 topics after running LDA model with the best hyperparameter. So, it should return a number of Topic is already defined withe pre-trained LDA model with new sentence or document that user input. </p>

<p>I am waiting you guys good solution. :)</p>

<p>Ps. I am using Gensim for NLP.</p>
",,2019-11-25 15:49:10,How to map topic to a document after topic modeling is done with LDA?,<nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
23804,59032757,2019-11-25 13:17:46,,"<p>The program should be returning the second text in the list for most similar, as it is same word to word. But its not the case here.</p>

<pre><code>import gensim
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument


data = [""I love machine learning. Its awesome."",
        ""I love coding in python"",
        ""I love building chatbots"",
        ""they chat amagingly well""]


tagged_data=[TaggedDocument(word_tokenize(_d.lower()),tags=[str(i)]) for i,_d in enumerate(data)]

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                negative=0,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    #print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(""d2v.model"")


loaded_model=Doc2Vec.load(""d2v.model"")
test_data=[""I love coding in python"".lower()]

v1=loaded_model.infer_vector(test_data)

similar_doc=loaded_model.docvecs.most_similar([v1])
print similar_doc
</code></pre>

<p>Output:</p>

<pre><code>[('0', 0.17585766315460205), ('2', 0.055697083473205566), ('3', -0.02361609786748886), ('1', -0.2507985532283783)]
</code></pre>

<p>Its showing the first text in the list as most similar instead of the second text. Can you please help with this ?</p>
",,2019-11-25 19:23:33,Doc2Vec infer_vector not working as expected,<python><text-classification><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
23846,59067555,2019-11-27 10:01:20,,"<p>I am getting KeyError:0 when running this code in python:</p>

<pre><code>full_pipeline.fit(X_train, y_train)
</code></pre>

<p>Here is the completed code:</p>

<pre><code>from gensim.sklearn_api import D2VTransformer
from sklearn.pipeline import FeatureUnion, Pipeline 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

name_pipeline = Pipeline( steps = [ 
                              ( 'feature_selector', FeatureSelector(['name']) ),
                              ( 'feature_transformer', D2VTransformer() ) ] )

description_pipeline = Pipeline( steps = [ 
                              ( 'feature_selector', FeatureSelector(['description']) ),
                              ( 'feature_transformer', D2VTransformer() ) ] )

X_pipeline = FeatureUnion( transformer_list = [ 
                                                  ( 'name_pipeline', name_pipeline ), 
                                                  ( 'description_pipeline', description_pipeline ) ] )

#Split up the train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)

clf = LogisticRegression(random_state=0, class_weight='balanced', solver='lbfgs', max_iter=1000, multi_class='multinomial')

full_pipeline = Pipeline( steps = 
                         [ ( 'pipeline', X_pipeline),
                          ( 'model', clf ) ] )

full_pipeline.fit(X_train, y_train)
</code></pre>

<p>And here is the error I'm getting:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2896             try:
-&gt; 2897                 return self._engine.get_loc(key)
   2898             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
19 frames
&lt;ipython-input-14-0ddbaedffb67&gt; in &lt;module&gt;()
     25                           ( 'model', clf ) ] )
     26 
---&gt; 27 full_pipeline.fit(X_train, y_train)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    350             This estimator
    351         """"""
--&gt; 352         Xt, fit_params = self._fit(X, y, **fit_params)
    353         with _print_elapsed_time('Pipeline',
    354                                  self._log_message(len(self.steps) - 1)):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params)
    315                 message_clsname='Pipeline',
    316                 message=self._log_message(step_idx),
--&gt; 317                 **fit_params_steps[name])
    318             # Replace the transformer of the step with the fitted
    319             # transformer. This is necessary when loading the transformer

/usr/local/lib/python3.6/dist-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    353 
    354     def __call__(self, *args, **kwargs):
--&gt; 355         return self.func(*args, **kwargs)
    356 
    357     def call_and_shelve(self, *args, **kwargs):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    714     with _print_elapsed_time(message_clsname, message):
    715         if hasattr(transformer, 'fit_transform'):
--&gt; 716             res = transformer.fit_transform(X, y, **fit_params)
    717         else:
    718             res = transformer.fit(X, y, **fit_params).transform(X)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    910             sum of n_components (output dimension) over transformers.
    911         """"""
--&gt; 912         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
    913         if not results:
    914             # All transformers are None

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _parallel_func(self, X, y, fit_params, func)
    940             message=self._log_message(name, idx, len(transformers)),
    941             **fit_params) for idx, (name, transformer,
--&gt; 942                                     weight) in enumerate(transformers, 1))
    943 
    944     def transform(self, X):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self, iterable)
   1001             # remaining jobs.
   1002             self._iterating = False
-&gt; 1003             if self.dispatch_one_batch(iterator):
   1004                 self._iterating = self._original_iterator is not None
   1005 

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    832                 return False
    833             else:
--&gt; 834                 self._dispatch(tasks)
    835                 return True
    836 

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in _dispatch(self, batch)
    751         with self._lock:
    752             job_idx = len(self._jobs)
--&gt; 753             job = self._backend.apply_async(batch, callback=cb)
    754             # A job can complete so quickly than its callback is
    755             # called before we get here, causing self._jobs to

/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    199     def apply_async(self, func, callback=None):
    200         """"""Schedule a func to be run""""""
--&gt; 201         result = ImmediateResult(func)
    202         if callback:
    203             callback(result)

/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in __init__(self, batch)
    580         # Don't delay the application, to avoid keeping the input
    581         # arguments in memory
--&gt; 582         self.results = batch()
    583 
    584     def get(self):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--&gt; 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in &lt;listcomp&gt;(.0)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--&gt; 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    714     with _print_elapsed_time(message_clsname, message):
    715         if hasattr(transformer, 'fit_transform'):
--&gt; 716             res = transformer.fit_transform(X, y, **fit_params)
    717         else:
    718             res = transformer.fit(X, y, **fit_params).transform(X)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    391                 return Xt
    392             if hasattr(last_step, 'fit_transform'):
--&gt; 393                 return last_step.fit_transform(Xt, y, **fit_params)
    394             else:
    395                 return last_step.fit(Xt, y, **fit_params).transform(Xt)

/usr/local/lib/python3.6/dist-packages/sklearn/base.py in fit_transform(self, X, y, **fit_params)
    554         else:
    555             # fit method of arity 2 (supervised transformation)
--&gt; 556             return self.fit(X, y, **fit_params).transform(X)
    557 
    558 

/usr/local/lib/python3.6/dist-packages/gensim/sklearn_api/d2vmodel.py in fit(self, X, y)
    158 
    159         """"""
--&gt; 160         if isinstance(X[0], doc2vec.TaggedDocument):
    161             d2v_sentences = X
    162         else:

/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   2993             if self.columns.nlevels &gt; 1:
   2994                 return self._getitem_multilevel(key)
-&gt; 2995             indexer = self.columns.get_loc(key)
   2996             if is_integer(indexer):
   2997                 indexer = [indexer]

/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897                 return self._engine.get_loc(key)
   2898             except KeyError:
-&gt; 2899                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2900         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2901         if indexer.ndim &gt; 1 or indexer.size &gt; 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0
</code></pre>

<p>Does anyone know why might this happen? I think it has to do with D2VTransformer because when I'm running the code below I'm getting the same error:</p>

<pre><code>model = D2VTransformer(min_count=1, size=5)
docvecs = model.fit_transform(X_train) 
</code></pre>

<p>But when trying to select only one column from the dataframe:</p>

<pre><code>docvecs = model.fit_transform(X_train['name']) 
</code></pre>

<p>it doesn't throw an error and that is why when I created the pipelines I've only used one column, but still getting the error.</p>

<p>This is how <a href=""https://i.stack.imgur.com/87d5Q.png"" rel=""nofollow noreferrer"">X_train</a> looks.</p>

<pre><code>name    description
9107    way great entrepreneur push limit help succeed  way great entrepreneur push limit
7706    dit het team week week  dit het team week week
3995    decorate home jewel tone    feel bold colour choice inspire fill home abun...
5220    attic meat district attic meat district
3412    tee apparel choose design item clothe accessory piece inde...
... ... ...
3830    marque web designer mode    marque web designer
3261    design holiday rest bite try lear magazine dai...   design holiday rest bite try lear
2415    hallucinatory house father spirit   music room hold tower season rug produce early...
7223    jacket rise jacket rise
4697    cupcake bake explorer   love love chocolate cupcake top kind easy foll...
</code></pre>

<p>And some more details about X_train:</p>

<pre><code>X_train.shape
(7159, 2)

X_train.dtypes
name           object
description    object
dtype: object
</code></pre>
",2019-11-28 22:44:34,2019-11-30 18:00:47,Python KeyError: 0 when using D2VTransformer,<python><dataframe><pipeline><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
23856,59050644,2019-11-26 12:07:13,,"<p>I am trying to train the word2vec model from Wikipedia text data, for that I am using following code.</p>

<pre><code>import logging
import os.path
import sys
import multiprocessing

from gensim.corpora import  WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments

    if len(sys.argv) &lt; 3:
        print (globals()['__doc__'])
        sys.exit(1)
    inp, outp = sys.argv[1:3]

    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())

    # trim unneeded model memory = use (much) less RAM
    model.init_sims(replace=True)

    model.save(outp)
</code></pre>

<p>But after 20 minutes of program running, I am getting following error</p>

<p><a href=""https://i.stack.imgur.com/6BjOz.png"" rel=""nofollow noreferrer"">Error message</a></p>
",2019-11-26 12:20:08,2019-11-26 17:40:21,MemoryError: unable to allocate array with shape and data type float32 while using word2vec in python,<python><multiprocessing><python-multiprocessing><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23877,59056631,2019-11-26 17:42:33,,"<p>I have a TF IDF vocabulary I already get from gensim or tfidfvectorizer. Is there any specific metric or method to drop tails of TF IDF vocabulary?  I mean tails at Zipf diagram. How to visualize it?
I would like to see how accuracy changes when I drop number of words in vocabulary. For instance, I have vocabulary that has 175000 of words. </p>
",2019-11-26 17:49:00,2019-11-27 11:39:51,Is there any specific metric or method to drop tails of TF IDF vocabulary?,<python><machine-learning><nlp><data-science><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
23878,59120553,2019-11-30 21:47:57,,"<p><strong>The problem to solve:</strong> Given a sentence, return the intent behind it (Think chatbot)</p>

<p><em>Reduced example dataset (Intent on the left of dict):</em></p>

<pre><code>data_raw    = {""mk_reservation"" : [""i want to make a reservation"",
                                   ""book a table for me""],
               ""show_menu""      : [""what's the daily menu"",
                                   ""do you serve pizza""],
               ""payment_method"" : [""how can i pay"",
                                   ""can i use cash""],
               ""schedule_info""  : [""when do you open"",
                                   ""at what time do you close""]}
</code></pre>

<p>I have stripped down the sentences with spaCy, and tokenized each word by using the <strong>word2vec</strong>  algorithm provided by the gensim library. </p>

<p>This is what resulted from the use of word2vec model GoogleNews-vectors-negative300.bin:</p>

<pre><code>[[[ 5.99331968e-02  6.50703311e-02  5.03010787e-02 ... -8.00536275e-02
    1.94782894e-02 -1.83010306e-02]
  [-2.14406010e-02 -1.00447744e-01  6.13847338e-02 ... -6.72588721e-02
    3.03986594e-02 -4.14126664e-02]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]

 [[ 4.48647663e-02 -1.03907576e-02 -1.78682189e-02 ...  3.84555124e-02
   -2.29179319e-02 -2.05144612e-03]
  [-5.39291985e-02 -9.88398306e-03  4.39085700e-02 ... -3.55276838e-02
   -3.66208404e-02 -4.57760505e-03]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]]
</code></pre>

<ul>
<li>This is a List of sentences, and each sentence is a list of words (<strong>[sentences[sentence[word]]]</strong>) </li>
<li>Each sentence (list) must be of size 10 words (I am padding the remaining with zeroes)</li>
<li>Each word (list) has 300 elements (word2vec dimensions)</li>
</ul>

<p>By following some tutorials i transformed this to a TensorDataset. </p>

<p>At this moment, i am very confused on how to use the word2vec and probably i have just been wasting time, as of now i believe the <strong>embeddings layer</strong> from an LSTM configuration should be composed by importing the word2vec model weights using:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors)    
word_embeddings = nn.Embedding.from_pretrained(weights)
</code></pre>

<p>This is not enough as pytorch is saying it does not accept embeddings where indices are not INT type  . </p>

<p><strong>EDIT: I found out that importing the weight matrix from gensim word2vec is not straightforward, one has to import the word_index table as well.</strong> </p>

<p>As soon as i fix this issue i'll post it here.</p>
",2019-12-01 00:45:34,2019-12-01 10:31:35,How to use gensim with pytorch to create an intent classifier (With LSTM NN)?,<python><pytorch><lstm><word2vec><torch>,,,CC BY-SA 4.0,False,True,True,False,False
23891,59093325,2019-11-28 16:46:19,,"<p>I am trying to annotate the vocabulary in the corpus.</p>

<ol>
<li><p>I have trained the word2vec model on the corpus</p></li>
<li><p>I have grouped the words which are related based on the score as key as the first word as the key and remaining words as a list of 2-tuple of word and scores with respect to the key </p></li>
</ol>

<p>example:
'coffee'---key 
values are </p>

<pre><code>[('tea', 0.8139282),
 ('latte', 0.76456803),
 ('coffe', 0.7607962),
 ('lattes', 0.756057),
 ('starbucks', 0.7158153),
 ('espresso', 0.71386236),
 ('mocha', 0.69999266),
 ('coffees', 0.6816252),
 ('frappucino', 0.67192864),
 ('cuppa', 0.66720986),
 ('cappucino', 0.6664002),
 ('chai', 0.6623157),
 ('decaf', 0.65980726),
 ('frappuccino', 0.65150374),
 ('venti', 0.6486204),
 ('expresso', 0.6369579),
 ('macchiato', 0.6280453),
 ('scone', 0.62476856),
 ('sippy', 0.6236704),
 ('cappuccino', 0.61718297),
 ('iced', 0.6130485),
 ('hazelnut', 0.6023698),
 ('mug', 0.6004759),
'
'
'
'
'
</code></pre>

<p>as i  know the coffee is releated to latte ,green_tea ,espresso,starbucks.. from the above data
I would like to label each word as below </p>

<p>latte [COHYPO] green_tea [COHYPO] espresso [HYPO] Starbucks [RELATED] tim_horton [RELATED] </p>

<p>COHYPO-<a href=""https://en.wiktionary.org/wiki/cohyponym"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/cohyponym</a></p>

<p>[HYPO] -<a href=""https://en.wiktionary.org/wiki/hyponyme"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/hyponyme</a></p>

<p>[RELATED] -the word is repeated </p>

<p>[MORPHO]-Morphological variant (example :Computer and computers )</p>

<p>[Partof]- indicates that the annotated word is a part of the word of interest</p>

<p>Any suggestion or ideas by which I can approach this problem </p>
",2019-11-28 23:29:28,2019-11-28 23:29:28,Annotating the vocabulary using Word2vec model,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23917,58925659,2019-11-19 02:14:35,,"<p>I got a dataset over 40G. The program of my tokenizer is killed due to limited memory, so I try to split my dataset. How can I train the word2vec model incrementally, that is, how can I use separate datasets to train one word2vec model?</p>

<p>My current word2vec code is:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=1, workers=10)
model.train(documents,total_examples=len(documents),epochs=epochs)
model.save(""./word2vec150d/word2vec_{}.model"".format(epochs))
</code></pre>

<p>Any help would be appreciated!</p>
",2019-11-19 06:38:04,2019-11-19 06:38:04,How to incrementally train a word2vec model with new vocabularies,<python><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
23918,58927877,2019-11-19 06:27:33,,"<p>I trained word embedding word2vec model using gensim and then used most_similar method to find the most associated words.</p>

<pre><code>Word to search:  forest 
</code></pre>

<p>The result is below:</p>

<pre><code>Most similar words:  [('wood', 0.2495424747467041), ('trees', 0.24147865176200867), ('distant', 0.2403097301721573), ('island', 0.2402323037)]
</code></pre>

<p>I wonder why the coefficient is very low, even the top word is less than 0.25.</p>

<p>Thank you!</p>
",,2019-11-19 06:27:33,Gensim most_similar method coefficients are very low,<nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
23957,59129793,2019-12-01 20:43:42,,"<p>As the title says, I would like to load custom word vectors built from <code>gensim</code> to the <code>SpaCy</code> Vector class. </p>

<p>I have found several other questions where folks have successfully loaded vectors to the <code>nlp</code> object itself, but I have a current project where I would like to have a separate Vectors object. </p>

<p>Specifically, I am using BioWordVec to generate my word vectors which serializes the vectors using methods from <code>gensim.models.Fastext</code>.</p>

<p>On the <code>gensim</code> end I am:</p>

<ul>
<li>calling <code>model.wv.save_word2vec_format(output/bin/path, binary=True)</code></li>
<li>saving the model -> <code>model.save(path/to/model)</code></li>
</ul>

<p>On the <code>SpaCy</code> side:</p>

<ul>
<li>I can either use the <code>from_disk</code> or <code>from_bytes</code> methods to load the word vectors </li>
<li>there is also a <code>from_glove</code> method that expects a vocab.txt file and a binary file (which I already have a binary file</li>
</ul>

<p>Link to <a href=""https://spacy.io/api/vectors"" rel=""nofollow noreferrer"">Vectors Documentation</a></p>

<p>just for reference, here is my code to test the load process:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.vectors import Vectors 

vecs = Vectors()
path = '/home/medmison690/pyprojects/BioWordVec/pubmed_mesh_test.bin'
dir_path = '/home/medmison690/Desktop/tuned_vecs'


vecs.from_disk(dir_path)


print(vecs.shape)
</code></pre>

<p>I have tried various combinations of <code>from_disk</code> and <code>from_bytes</code> with no success. Any help or advice would be greatly appreciated!</p>
",,2019-12-02 22:35:52,Load word vectors from Gensim to SpaCy Vectors class,<python><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
23965,59208184,2019-12-06 06:46:39,,"<p>this my code you can see i am tokonize the sentence to word but i am still have a problem when i apply 
word2vec model in my sentences i use Arabic text 
anaconda version 4.7.12</p>

<pre><code>sentences = nltk.sent_tokenize(str(sentences1))
sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
for i in range(len(sentences)):
sentences[i] = [word for word in sentences[i] if word not in stopwords.words('arabic')]

sentences = re.sub(r'[^\w\s]','',(str(sentences)))
sentences = re.sub(""\d+"", """", sentences)
sentences =sentences.strip()
sentences = nltk.word_tokenize(sentences)
from gensim.models import Word2Vec
model = Word2Vec(sentences, min_count=1)
words1 = model.wv.vocab
</code></pre>

<p>in words1 the vocab just shown the letters    </p>
",,2019-12-06 19:20:15,when apply word2vec just characters shown not a word ,<python><nlp><text-mining><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
23974,59232589,2019-12-08 05:05:46,,"<p>I've been trying to apply TFIDF vectorizer on a gensim LDA model with no success. It looks like I have to use any() or all() but I'm not sure what is going on with the vectorizer. The data has been cleaned and pre-processed.</p>

<p>Data:</p>

<pre><code>text_data=
0         [new, leaked, treasury, document, full, sugges...
1         [tommy, robinson, endorsing, boris, johnson's,...
2         [thanks, already, watched, corbyn, catch, tv, ...
3         [treasury, document, check, boris, johnson, to...
</code></pre>

<p>Code:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

# Create dummy function to initialize

def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  

# Fit and transform on text_data
tfidf_corpus = tfidf.fit_transform(text_data)

# Use LDA model to find 10 topics in the data, based on TFIDF vectorizer

ldamodel_tfidf = gensim.models.ldamodel.LdaModel(corpus=tfidf_corpus,
                                       #id2word=id2word,
                                       num_topics=10, 
                                       random_state=42,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)
</code></pre>

<p>The error is asking that I use any() or all(), but why is it asking for this?</p>

<pre><code>Traceback (most recent call last)
    &lt;ipython-input-25-57489833d281&gt; in &lt;module&gt;
          9                                            passes=10,
         10                                            alpha='auto',
    ---&gt; 11                                            per_word_topics=True)

    /opt/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py in __init__(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)
        431         if self.id2word is None:
        432             logger.warning(""no word id mapping provided; initializing from corpus, assuming identity"")
    --&gt; 433             self.id2word = utils.dict_from_corpus(corpus)
        434             self.num_terms = len(self.id2word)
        435         elif len(self.id2word) &gt; 0:

    /opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py in dict_from_corpus(corpus)
        824 
        825     """"""
    --&gt; 826     num_terms = 1 + get_max_id(corpus)
        827     id2word = FakeDict(num_terms)
        828     return id2word

    /opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py in get_max_id(corpus)
        733     maxid = -1
        734     for document in corpus:
    --&gt; 735         if document:
        736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
        737     return maxid

    /opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py in __bool__(self)
        285             return self.nnz != 0
        286         else:
    --&gt; 287             raise ValueError(""The truth value of an array with more than one ""
        288                              ""element is ambiguous. Use a.any() or a.all()."")
        289     __nonzero__ = __bool__

    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().
</code></pre>
",2019-12-08 05:10:17,2019-12-08 05:40:16,TFIDF vectorizer: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),<python><nlp><tfidfvectorizer>,,,CC BY-SA 4.0,False,False,True,False,True
23981,59281409,2019-12-11 08:17:32,,"<p>I'm trying to execute simple code to lemmatize string, but there's an error about iteration.
I have found some solutions which are about reinstalling web.py, but this not worked for me.</p>

<p>python code</p>

<pre><code>from gensim.utils import lemmatize
lemmatize(""gone"")
</code></pre>

<p>error is</p>

<pre><code>---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
I:\Anaconda\lib\site-packages\pattern\text\__init__.py in _read(path, encoding, comment)
    608             yield line
--&gt; 609     raise StopIteration
    610 

StopIteration: 

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-9daceee1900f&gt; in &lt;module&gt;
      1 from gensim.utils import lemmatize
----&gt; 2 lemmatize(""gone"")

-------------------------------------------------------------------------------------

I:\Anaconda\lib\site-packages\pattern\text\__init__.py in &lt;genexpr&gt;(.0)
    623     def load(self):
    624         # Arnold NNP x
--&gt; 625         dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
    626 
    627 #--- FREQUENCY -------------------------------------------------------------------------------------

RuntimeError: generator raised StopIteration
</code></pre>
",,2019-12-11 08:25:50,gensim lemmatize error generator raised StopIteration,<python><nlp><gensim><lemmatization>,,,CC BY-SA 4.0,False,False,True,False,False
24000,59282572,2019-12-11 09:29:46,,"<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebooks native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebooks native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
",,2019-12-11 19:54:18,Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,<python><nlp><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24011,59168273,2019-12-04 02:53:11,,"<p>I have adapted an existing implementation of doc2vec model in gensim library. The implementation I used can be found on Github <a href=""https://github.com/gojomo/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">here</a>. I have gotten this to work but in this version they only use error rate as a performance metric. I would like to know the accuracy, precision and F1 score of the models also. </p>

<p>I have been trying to adapt the existing code to generate accuracy, precision and F1 score also but have not been successful. Below I have put the function that generates the error rate, can anyone help me with how I can adapt this to also generate the previously mentioned metrics? It would be much appreciated</p>

<pre><code>def error_rate_for_model(test_model, train_set, test_set):
    """"""Report error rate on test_doc sentiments, using supplied model and train_docs""""""

    train_targets = [doc.sentiment for doc in train_set]
    train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]
    train_regressors = sm.add_constant(train_regressors)
    predictor = logistic_predictor_from_data(train_targets, train_regressors)

    test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_set]
    test_regressors = sm.add_constant(test_regressors)

    # Predict &amp; evaluate
    test_predictions = predictor.predict(test_regressors)
    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_set])
    errors = len(test_predictions) - corrects
    error_rate = float(errors) / len(test_predictions)
    return (error_rate, errors, len(test_predictions), predictor)```

</code></pre>
",,2019-12-04 02:53:11,generating performance metrics for gensim doc2vec model,<python-3.x><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24033,59268044,2019-12-10 13:12:54,,"<p>I am trying to measure the Word Mover's Distance between a lot of texts using Gensim's Word2Vec tools in Python. I am comparing each text with all other texts, so I first use itertools to create pairwise combinations like <code>[1,2,3] -&gt; [(1,2), (1,3), (2,3)]</code>. For memory's sake, I don't do the combinations by having all texts repeated in a big dataframe, but instead make a reference dataframe <code>combinations</code> with indices of the texts, which looks like:</p>

<pre><code>    0   1
0   0   1
1   0   2
2   0   3
</code></pre>

<p>And then in the comparison function I use these indices to look up the text in the original dataframe. The solution works fine, but I am wondering whether I would be able to it with big datasets. For instance I have a 300.000 row dataset of texts, which gives me about a 100 year's worth of computation on my laptop:</p>

<pre><code>C2(300000) = 300000! / (2!(3000002))!
           = 300000299999 / 2 * 1
           = 44999850000 combinations
</code></pre>

<p>Is there any way this could be optimized better?</p>

<p>My code right now:</p>

<pre><code>import multiprocessing
import itertools
import numpy as np
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
from gensim.models.word2vec import Word2Vec
from gensim.corpora.wikicorpus import WikiCorpus

def get_distance(row):
    try: 
        sent1 = df.loc[row[0], 'text'].split()
        sent2 = df.loc[row[1], 'text'].split()
        return model.wv.wmdistance(sent1, sent2)  # Compute WMD
    except Exception as e:
        return np.nan

df = pd.read_csv('data.csv')

# I then set up the gensim model, let me know if you need that bit of code too.

# Make pairwise combination of all indices
combinations = pd.DataFrame(itertools.combinations(df.index, 2))

# To dask df and apply function
dcombinations = dd.from_pandas(combinations, npartitions= 2 * multiprocessing.cpu_count())
dcombinations['distance'] = dcombinations.apply(get_distance, axis=1)
with ProgressBar():
    combinations = dcombinations.compute()
</code></pre>
",2020-07-23 12:45:31,2020-07-23 12:45:31,Can I optimize this Word Mover's Distance look-up function?,<python><python-multiprocessing><dask><gensim><wmd>,,,CC BY-SA 4.0,False,False,True,False,False
24038,59237691,2019-12-08 16:58:26,,"<p>I'm doing topic modeling with LDAmallet and it gives me some meaningful topics. The only problem is I can't visualize the output on PyLDAvis. I tried doing this </p>

<pre><code>ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_1,
            num_topics=14, id2word=id2word_1)
model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)
model.save('ldamallet.gensim')` 
</code></pre>

<p>but it didn't work. </p>
",2019-12-08 21:10:05,2019-12-08 21:10:05,How to convert LDAmallet model to a basic gensim ldamodel for pyldavis?,<lda>,,,CC BY-SA 4.0,False,False,True,False,False
24049,59286382,2019-12-11 12:58:26,,"<p>I am trying to parallelize calculating word2vec distances between a big list of sentences. I have a dataframe with the paried up in two rows. I am wondering how one would go about distributing this with dask. It seems like the normal ways to use dask don't work in this situation, perhaps because having just the one word2vec model bottlenecks the different workers. Is there a way to ""create a model"" for each of the workers? Also, how would I go about doing this with several machines - would they all need to have the model trained locally, and how would this look in python? Thanks! </p>

<pre><code>from gensim.models.word2vec import Word2Vec
import dask.dataframe as dd
from dask.distributed import Client


def get_distance(row):
    sent1 = df.loc[row[0], 'text'].split()
    sent2 = df.loc[row[1], 'text'].split()
    return model.wv.wmdistance(sent1, sent2)


model = Word2Vec.load('my-model')

client = Client('127.0.0.1:8786')

df = dd.read_csv('data.csv')

# df looks like

#    +-----+-------------------+-------------------+
#    |     |         0         |         1         |
#    +-----+-------------------+-------------------+
#    |  1  | ""first sentence""  | ""second sentence"" |
#    |  2  | ""first sentence""  | ""third sentence""  |
#    | ... | ...               | ...               |
#    |  3  | ""second sentence"" | ""first sentence""  |
#    |  4  | ""second sentence"" | ""third sentence""  |
#    |  5  | ""second sentence"" | ""fourth sentence"" |
#    +-----+-------------------+-------------------+

combinations['distance'] = combinations.apply(get_distance, axis=1)
combinations = combinations.compute()
</code></pre>
",,2019-12-11 12:58:26,How would you distribute calls to find word2vec distance between words using dask?,<python><parallel-processing><multiprocessing><dask><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24061,59270197,2019-12-10 15:06:41,,"<p>I got a file regarding pre-trained data from this link: (.bin file)<a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>and I want to use this file on input layer of neural network(maybe its' forms vector)</p>

<p>I want pre-trained data learn more of training data. ( let' call training data as data.txt.)</p>

<p>and I have loaded pre-trained model using fasttext 
and genism library however I don't know how to train them more.</p>

<pre><code>import fasttext
model = fasttext.load_model('model.bin')
#whatever using data.txt
</code></pre>

<p>or</p>

<pre><code>from gensim.models import FastText
model = FastText.load_fasttext_format('model.bin')
#whatever using data.txt
</code></pre>

<p>please advice.</p>
",2019-12-13 11:17:10,2019-12-13 11:17:10,How can fastText or Gensim train additional data from pre-trained data (.bin)?,<gensim><bin><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24062,59270202,2019-12-10 15:07:15,,"<p>I have been looking at the documentation for text2vec, but am finding hard to see how I might make a sample matrix of all my documents and their distance, such as :</p>

<pre><code>+------+------+------+-------------+
|      | doc1 | doc2 | doc3 Column |
+------+------+------+-------------+
| doc1 |    1 |  0.6 |         0.1 |
| doc2 |  0.2 |    1 |         0.8 |
| doc4 |  0.1 |  0.9 |           1 |
+------+------+------+-------------+
</code></pre>

<p>Where the values of the matrix is the distance between the two docs.</p>

<p>I have been looking at some example code, such as </p>

<pre><code>## Not run: 
data(""movie_review"")
tokens = word_tokenizer(tolower(movie_review$review))
v = create_vocabulary(itoken(tokens))
v = prune_vocabulary(v, term_count_min = 5, doc_proportion_max = 0.5)
it = itoken(tokens)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer)
tcm = create_tcm(it, vectorizer, skip_grams_window = 5)
glove_model = GloVe$new(word_vectors_size = 50, vocabulary = v, x_max = 10)
wv = glove_model$fit_transform(tcm, n_iter = 10)
# get average of main and context vectors as proposed in GloVe paper
wv = wv + t(glove_model$components)
rwmd_model = RWMD$new(wv)
rwmd_dist = dist2(dtm[1:100, ], dtm[1:10, ], method = rwmd_model, norm = 'none')
head(rwmd_dist)

## End(Not run)
</code></pre>

<p>From <a href=""https://rdrr.io/cran/text2vec/man/RelaxedWordMoversDistance.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>My problem is that I end up with a matrix that I find hard to interpret, and not something like what I provided. I am able to do this with normal WMD with tools such as gensim buch just calling a similarity function, but I am unsure how this can be done with RWMD. The reason I am trying this is because I want to get faster speeds through <a href=""https://arxiv.org/abs/1711.07227"" rel=""nofollow noreferrer"">LC-RWMD</a>, which seems to be implemented text2vec. </p>

<p>Thanks a lot!</p>
",,2019-12-10 15:07:15,How can I do pairwise comparisons of all documents with relaxed WMD with text2vec in R,<gensim><word2vec><wmd><text2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
24073,59318935,2019-12-13 08:45:43,,"<p>I already have a training model for fastText with gensim, and<br>
I can get the distance between two sentence as described below,  </p>

<pre><code>sentence_1 = ""Today is very cold.""  
sentence_2 = ""I'd like something to drink.""    

print(model.wv.wmdistance(sentence_1.split("" ""), sentence_2.split("" "")))
# 0.8446287678977793  # for example
</code></pre>

<p>but how does <code>vmdistance</code> calculate this value?<br>
I'd like to know the formula.  </p>

<p>API documents: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance</a></p>
",,2019-12-14 23:08:40,How does gensim.models.FatText.wv.wmdistance calculate between two documents?,<python-3.x><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24078,59183624,2019-12-04 19:46:06,,"<p>I want to use fasttext pre-trained models to compute similarity
a sentence between a set of sentences.
can anyone help me?
what is the best approach?</p>

<p>I computed the similarity between sentences by train a tfidf model. write code like this.
is it possible to change it and use fasttext pre-trained models? for example use vectors to train a tfidf model?</p>

<pre><code>def generate_tfidf_model(sentences):
    print(""generating TfIdf model"")
    texts = [[sentence for sentence in doc.split()] for doc in sentences]
    dictionary = gensim.corpora.Dictionary(texts)    
    feature_cnt = len(dictionary.token2id)
    mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]
    tfidf_model = gensim.models.TfidfModel(mycorpus)
    index = gensim.similarities.SparseMatrixSimilarity(tfidf_model[mycorpus]
                                                        , num_features = feature_cnt)
    return tfidf_model, index, dictionary

def query_search(query, tfidf_model, index, dictionary):
    query = normal_stemmer_sentence(query)
    query_vector = dictionary.doc2bow(query.split())
    similarity = index[tfidf_model[query_vector]]
    return similarity
</code></pre>
",,2019-12-06 16:38:35,fasttext pre trained sentences similarity,<python><nlp><information-retrieval><fasttext><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24087,59289611,2019-12-11 15:57:00,,"<p>Can we take the stand that the words around the target word have an impact on it in a positive or negative way or is there any other kind of interpretation.</p>

<p>A t-SNE graph is given at the end of the page in the link provided for reference. Thank you.</p>

<p><a href=""https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial?source=post_page-----a38bf1906483----------------------#Getting-Started"" rel=""nofollow noreferrer"">https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial?source=post_page-----a38bf1906483----------------------#Getting-Started</a>     </p>
",2019-12-11 16:22:02,2019-12-11 16:22:02,Can anybody explain what a t-SNE visualization or graph on a word2vec model signifies?,<data-visualization><data-science><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
24088,59292179,2019-12-11 18:46:09,,"<p><strong>I have this code</strong></p>

<p>from gensim import models
import numpy as np</p>

<p><strong>Create the TF-IDF model</strong>
tfidf = models.TfidfModel(corpus, smartirs= ""ntc)</p>

<p><strong>Show the TF-IDF weights</strong>
for doc in tfidf[corpus]:
    print([[dictionary[id], np.around(freq, decimals=7)] for id, freq in doc])</p>

<p><strong>Then, I have this result</strong> </p>

<p>[['brocolli', 0.5491093], ['brother', 0.1237955], ['but', 0.1237955], ['eat', 0.568619], ['good', 0.3660728], ['like', 0.2843095], ['not', 0.1830364], ['rata', 0.1237955], ['saluut', 0.2843095]]
[['brother', 0.1647724], ['rata', 0.1647724], ['around', 0.3784174], ['basebal', 0.3784174], ['drive', 0.1647724], ['lot', 0.3784174], ['mother', 0.2436224], ['practic', 0.3784174], ['spend', 0.3784174], ['time', 0.3784174]]<code>enter code here</code>
[['rata', 0.1335974], ['drive', 0.1335974], ['and', 0.3068207], ['blood', 0.3068207], ['caus', 0.3068207], ['expert', 0.3068207], ['health', 0.197529], ['increas', 0.3068207], ['may', 0.3068207], ['pressur', 0.197529], ['some', 0.3068207], ['suggest', 0.3068207],['tension',0.3068207], ['that', 0.197529]]</p>

<p>I want to remove all the tuples &lt; 0.2 
For exaple the word ""DRIVE""= 0.1335 I want to remove this tuple. How can I do this?</p>
",,2019-12-11 18:46:09,How can I remove a tuple from my corpus TF-IDF?,<python><tuples><gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
24094,59293936,2019-12-11 20:55:57,,"<p>Is there a standard process that <a href=""https://github.com/rare-technologies/gensim"" rel=""nofollow noreferrer"">gensim</a> uses to compile the cython bits?  I've forked the project and edited the cython code, but it doesn't seem to recompile when I run <code>python3 setup.py install --force</code>.</p>

<p>I'm not getting any errors, it just doesn't seem to even attempt to compile the cython code.  I have a <a href=""https://github.com/rare-technologies/gensim"" rel=""nofollow noreferrer"">fresh clone of the <code>develop</code> branch</a></p>

<p>How do I compile gensim ""from source""?</p>

<p>OS: Ubuntu 16.04<br>
gcc: 5.4.0</p>
",2019-12-12 19:27:24,2019-12-12 19:27:24,How to compile the cython modules for gensim?,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24104,59355733,2019-12-16 11:28:10,,"<p>I got the following code from DARIAH project website to do topic modelling in Python. When I run the script in the command shell, it starts reading the files but always stucks at:</p>

<pre><code>**reading files ...

Traceback (most recent call last):

File ""C:\topmodel.py"", line 131, in &lt;module&gt;
    dictionary, corpus, doc_labels = preprocessing(path, columns, pos_tags, doc_size, doc_split, stopwordlist)

  File ""C:\topmodel.py"", line 64, in preprocessing
    for file in os.listdir(path=path):

OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '[C:\\topmodel.py]'**
</code></pre>

<p>Any help is appreciated. Thanks in advance.</p>

<pre><code>#!/usr/bin/env python

from gensim.corpora import MmCorpus, Dictionary
from gensim.models import LdaMulticore, LdaModel
import pandas as pd
import os
import sys
import csv

#import logging
#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


#########################
# CONFIGURATION

# input
columns = ['ParagraphId', 'TokenId', 'Lemma', 'CPOS']   #, 'NamedEntity']   # columns to read from csv file
pos_tags = ['ADJ', 'NN', 'V']                        # parts-of-speech to include into the model, following dkpro's
                                            # coarse grained tagset: ADJ, ADV, ART, CARD, CONJ, N (NP, NN), O, PP, PR, V, PUNC

# stopwords
stopwordlist = ""stopwords.txt""              # path to text file, e.g. stopwords.txt in the same directory as the script

# document size (in words)
#doc_size = 1000000                             # set to arbitrarily large value to use original doc size
doc_size = 1000                                 # the document size for LDA commonly ranges from 500-2000 words
doc_split = 0                                   # set to 1 to use the pipeline's ParagraphId feature instead of doc_size

# model parameters, cf. https://radimrehurek.com/gensim/models/ldamodel.html
no_of_topics = 20                               # no. of topics to be generated
no_of_passes = 100                              # no. of lda iterations - the more the better, but increases computing time

eval = 1                                        # perplexity estimation every n chunks - the smaller the better, but also increases computing time
chunk = 10                                      # documents to process at once

alpha = ""auto""                             # ""symmetric"", ""asymmetric"", ""auto"", or array (default: a symmetric 1.0/num_topics prior)
                                                # affects sparsity of the document-topic (theta) distribution

# custom alpha may increase topic coherence, but may also produce more topics with zero probability
#alpha = np.array([ 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.05,
#                   0.05, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02])

eta = None                                      # can be a number (int/float), an array, or None
                                                # affects topic-word (lambda) distribution - not necessarily beneficial to topic coherence


#########################
# PRE-PROCESSING

def preprocessing(path, columns, pos_tags, doc_size, doc_split, stopwordlist):
    docs = []
    doc_labels = []
    stopwords = """"

    print(""reading files ...\n"")

    try:
        with open(stopwordlist, 'r') as f: stopwords = f.read()
    except OSError:
        pass
    stopwords = sorted(set(stopwords.split(""\n"")))

    for file in os.listdir(path=path):
        if not file.startswith("".""):
            filepath = path+""/""+file
            print(filepath)

            df = pd.read_csv(filepath, sep=""\t"", quoting=csv.QUOTE_NONE)
            #df = pd.read_csv(filepath)
            df = df[columns]
            df = df.groupby('CPOS')

            doc = pd.DataFrame()
            for p in pos_tags:                          # collect only the specified parts-of-speech
                doc = doc.append(df.get_group(p))

            """"""
            df = df.groupby('NamedEntity')              # add named entities to stopword list
            names = df.get_group('B-PER')['Lemma'].values.astype(str)
            names += df.get_group('I-PER')['Lemma'].values.astype(str)
            """"""
            #names = df.get_group('NP')['Lemma'].values.astype(str)
            #stopwords += names.tolist()

            # construct documents
            if doc_split:                               # size according to paragraph id
                doc = doc.groupby('ParagraphId')
                for para_id, para in doc:
                    docs.append(para['Lemma'].values.astype(str))
                    doc_labels.append(file.split(""."")[0]+"" #""+str(para_id))     # use filename + doc id as plot label
            else:                                       # size according to doc_size
                doc = doc.sort(columns='TokenId')
                i = 1
                while(doc_size &lt; doc.shape[0]):
                    docs.append(doc[:doc_size]['Lemma'].values.astype(str))
                    doc_labels.append(file.split(""."")[0]+"" #""+str(i))
                    doc = doc.drop(doc.index[:doc_size])        # drop doc_size rows
                    i += 1
                docs.append(doc['Lemma'].values.astype(str))    # add the rest
                doc_labels.append(file.split(""."")[0]+"" #""+str(i))

    #for doc in docs: print(str(len(doc)))              # display resulting doc sizes
    #print(stopwords)

    print(""\nnormalizing and vectorizing ...\n"")        # cf. https://radimrehurek.com/gensim/tut1.html

    texts = [[word for word in doc if word not in stopwords] for doc in docs]       # remove stopwords

    all_tokens = sum(texts, [])                                                     # remove words that appear only once
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
    texts = [[word for word in text if word not in tokens_once] for text in texts]

    dictionary = Dictionary(texts)                      # vectorize
    corpus = [dictionary.doc2bow(text) for text in texts]

    return dictionary, corpus, doc_labels


#########################
# MAIN

if len(sys.argv) &lt; 2:
    print(""usage: {0} [folder containing csv files]\n""
          ""parameters are set inside the script."".format(sys.argv[0]))
    sys.exit(1)

path = sys.argv[1]
foldername = path.split(""/"")[-1]

dictionary, corpus, doc_labels = preprocessing(path, columns, pos_tags, doc_size, doc_split, stopwordlist)

print(""fitting the model ...\n"")

model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=no_of_topics, passes=no_of_passes,
                 eval_every=eval, chunksize=chunk, alpha=alpha, eta=eta)

#model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=no_of_topics, passes=no_of_passes,
#                 eval_every=eval, chunksize=chunk, alpha=alpha, eta=eta)

print(model, ""\n"")

topics = model.show_topics(num_topics=no_of_topics)

for item, i in zip(topics, enumerate(topics)):
    print(""topic #""+str(i[0])+"": ""+str(item)+""\n"")


print(""saving ...\n"")

if not os.path.exists(""out""): os.makedirs(""out"")

with open(""out/""+foldername+""_doclabels.txt"", ""w"") as f:
    for item in doc_labels: f.write(item+""\n"")

with open(""out/""+foldername+""_topics.txt"", ""w"") as f:
    for item, i in zip(topics, enumerate(topics)):
        f.write(""topic #""+str(i[0])+"": ""+str(item)+""\n"")

dictionary.save(""out/""+foldername+"".dict"")
MmCorpus.serialize(""out/""+foldername+"".mm"", corpus)
model.save(""out/""+foldername+"".lda"")

</code></pre>
",2019-12-16 11:31:00,2019-12-16 11:32:26,"Python WindowsError: [Error 123] The filename, directory name, or volume label syntax is incorrect",<python><lda><topic-modeling><windowserror>,,,CC BY-SA 4.0,False,False,True,False,False
24105,59358828,2019-12-16 14:39:13,,"<p>I am getting this error in the following code</p>

<pre><code>from sklearn.externals import joblib
from gensim.models import Word2Vec 


filename2 = 'Models/mpl_model.sav'
loaded_model2 = joblib.load(filename2)
modell= Word2Vec.load(""Models/cbow.model"")
</code></pre>

<p>I am trying to load model from Models folder. I keep getting the error</p>

<pre><code>TypeError: 'errstate' object is not callable
</code></pre>

<p>Traceback</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py"", line 605, in load
    obj = _unpickle(fobj, filename, mmap_mode)
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py"", line 529, in _unpickle
    obj = unpickler.load()
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1088, in load
    dispatch[key[0]](self)
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1376, in load_global
    klass = self.find_class(module, name)
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1426, in find_class
    __import__(module, level=0)
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/numpy/random/_pickle.py"", line 2, in &lt;module&gt;
    from .philox import Philox
  File ""philox.pyx"", line 1, in init numpy.random.philox
  File ""bit_generator.pyx"", line 397, in init numpy.random.bit_generator
TypeError: 'errstate' object is not callable
</code></pre>

<p>Update :</p>

<p>The error was solved by upgrading numpy. Model was dumped using a new version and 
I was loading it using an older version.</p>
",2019-12-16 15:57:31,2019-12-16 15:57:31,Python Error : TypeError: 'errstate' object is not callable,<python><scikit-learn><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
24127,59312001,2019-12-12 20:08:37,,"<p>Getting <code>TypeError: unhashable type: 'list'</code> and <code>AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found</code> errors when I create my model based on <code>Word2Vec</code> implementation of the <code>gensim</code> module.</p>

<p><strong>Each entry has three parts</strong> which are presented within a list. And, <strong>the model contains three entries</strong> for the sake of demonstration.</p>

<p>Here is what I have tried:</p>

<pre><code>model = Word2Vec(sentences=features, size=100, sg=1, window=3, min_count=1, iter=10, workers=Pool()._processes)

model.build_vocab(features)

model.train(features)
</code></pre>

<p>The value of the <code>features</code> is: </p>

<pre><code>  [
    [
      ['permission.ACCESS_WIFI_STATE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.CHANGE_WIFI_STATE'],
       ['intent.action.MAIN', 'intent.action.BATTERY_CHANGED_ACTION', 'intent.action.SIG_STR', 'intent.action.BOOT_COMPLETED'],
       []
    ],
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.INSTALL_PACKAGES', 'permission.SEND_SMS', 'permission.DELETE_PACKAGES'],
      ['intent.action.BOOT_COMPLETED', 'intent.action.USER_PRESENT', 'intent.action.PHONE_STATE', 'intent.action.MAIN'],
      []
    ], 
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_FINE_LOCATION', 'permission.INTERNET', 'permission.READ_PHONE_STATE', 'permission.ACCESS_COARSE_LOCATION', 'permission.CALL_PHONE', 'permission.READ_CONTACTS', 'permission.READ_SMS'], 
      ['intent.action.PHONE_STATE', 'intent.action.MAIN'], 
      []
    ]
  ]
</code></pre>

<p><strong>Edit:</strong> The error stack trace after correcting the form of the feature vector according to the comment of @gojomo.</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 3 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 2 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 1 more threads
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 0 more threads
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - EPOCH - 10 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - training on a 60 raw words (2 effective words) took 0.1s, 21 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:WARNING - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
2019-12-13 12:24:34,521:gensim.utils:INFO - saving Word2Vec object under model/word2vec_model, separately None
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
    debugger.enable_tracing(apply_to_all_threads=True)  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
      File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
2019-12-13 12:24:34,521:gensim.utils:INFO - not storing attribute vectors_norm
        func = self.__getitem__(name)func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__

2019-12-13 12:24:34,522:gensim.utils:INFO - not storing attribute cum_table
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
    func = self._FuncPtr((name_or_ordinal, self))AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found

AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found
func = self._FuncPtr((name_or_ordinal, self))
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
AttributeError: dlsym(0x7fed18d2ff70, AttachDebuggerTracing): symbol not found
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,524:gensim.utils:INFO - saved model/word2vec_model
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
        pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
result = lib.AttachDebuggerTracing(  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18b09320, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
</code></pre>
",2019-12-13 09:29:17,2019-12-13 09:29:17,"Word2Vec - How to rid of ""TypeError: unhashable type: 'list'"" and ""AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found""?",<gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
24136,59297344,2019-12-12 03:34:19,,"<p>My aim here is text summarization, not sure if I'm doing it correctly but here's the plan. I've got a dataframe called train_data. Each cell in every row contains messages. Now, I am looking to iterate over each cell or each message in the dataframe column to get the keywords from each message, using the gensim.summarization.keyword package. </p>

<p>I understand that the keyword function takes text as an input and I can't pass the whole df column inside so tried to iterate each cell over the keyword function as text but it doesn't seem to work. What am I missing here? Here's my code.</p>

<pre><code>cols = train_data.new_msg
for col in cols:
    cols

train_data['keywords'] = keywords(col)

</code></pre>

<p>I then plan to count the length of original vs new message(ie keyword column) to get the compression rate/ratio. </p>
",,2019-12-12 07:52:08,Getting keywords from messages,<python><nlp><nltk><gensim><text-classification>,,,CC BY-SA 4.0,True,False,True,False,False
24139,59322409,2019-12-13 12:14:22,,"<p>I am using PyLDAvis to visualise the results of the LDA from Mallet. </p>

<p>Before I can do that, I need the wrapper of the gensim library:</p>

<pre><code>model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model_list[8])
</code></pre>

<p>When I print the found topics, they are ordered from 0-10.</p>

<p>However when I am using the pyLDAvis to visualise the Topics, the Topic order (0-10), does not align with printed topics.</p>

<p>Example:  </p>

<pre><code>(5,
  '0.042*""euro"" + 0.030*""smartpho"" + 0.022*""camera"" + 0.020*""display"" + '
  '0.018*""model"" + 0.016*""picture"" + 0.012*""price"" + 0.010*""android""')
</code></pre>

<p>As you can see this topic is about smartphones.</p>

<p>However when I visualise the model with pyLDAvis, Topic 5 is not about smartphones, but about another Topic (cars for example). The smartphone topic is not 5 anymore but topic 1.</p>

<p>Example1: </p>

<p><a href=""https://i.stack.imgur.com/XeS6s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XeS6s.png"" alt=""enter image description here""></a></p>

<p>Example2:
<a href=""https://i.stack.imgur.com/p1zUU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p1zUU.png"" alt=""enter image description here""></a></p>

<p>Is this a known error or is this the normal? 
Somebody can help?</p>
",,2020-06-06 06:53:51,PyLDAvis visualisation does not align with generated topics,<python><gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
24147,59345285,2019-12-15 15:24:28,,"<p>i'm new to python and trying to predict a new text by using Doc2vec and logisticRegression</p>

<p>For example ""i am happy"" return POS , ""i am sad"" return NEG</p>

<p>Below is my example code download from <a href=""https://github.com/nisarg64/Sentiment-Analysis-Word2Vec/blob/master/doc2vec_sentiment.py"" rel=""nofollow noreferrer"">https://github.com/nisarg64/Sentiment-Analysis-Word2Vec/blob/master/doc2vec_sentiment.py</a> </p>

<pre><code>from gensim.models import Doc2Vec
import numpy
from sklearn.linear_model import LogisticRegression

model = Doc2Vec.load('./imdb.d2v')

train_arrays = numpy.zeros((25000, 128))
train_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]
    train_labels[i] = 1
    train_labels[12500 + i] = 0


test_arrays = numpy.zeros((25000, 128))
test_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_test_pos = 'TEST_POS_' + str(i)
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]
    test_labels[i] = 1
    test_labels[12500 + i] = 0

# for i in range(50000):
#     prefix_unsup = 'TEST_POS_' + str(i)
#     test_arrays[i] = model.docvecs[prefix_test_pos]
#     test_arrays[12500 + i] = model.docvecs[prefix_test_neg]
#     test_labels[i] = 1
#     test_labels[12500 + i] = 0


classifier = LogisticRegression()
classifier.fit(train_arrays, train_labels)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)


#unsup = classifier.predict(s)

score = classifier.score(test_arrays, test_labels)
from nltk.tokenize import word_tokenize



print(score)
#here is where i want to predict the text
classifier.predict(""input text here"")
</code></pre>

<p>i had tried this before</p>

<pre><code>v1= model.infer_vector([""I"",""love"",""chatbot""])
classifier.predict(v1)
</code></pre>

<p>but this is the error code</p>

<pre><code>File ""D:\Anaconda3\lib\site-packages\gensim\models\doc2vec.py"", line 543, in infer_vector
    doctag_vectors, doctag_locks = self.trainables.get_doctag_trainables(doc_words, self.docvecs.vector_size)

AttributeError: 'DocvecsArray' object has no attribute 'vector_size'
</code></pre>
",,2019-12-15 15:24:28,How to predict a new text with doc2vec and LogisticRegression,<nlp><logistic-regression><sentiment-analysis><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,True
24175,59348206,2019-12-15 21:27:14,,"<p>I'm a complete novice to NLP and would like to load a zipped XLM file of the Hungarian Wikipedia corpus (807 MB). I downloaded the dumpfile and started parsing it in Python with Gensim, but after 4 hours my laptop crashed, complaining that I had run out of RAM. I have a fairly old laptop (4GB RAM) and was wondering whether there is any way I could solve this problem by </p>

<ul>
<li>(1) either  tinkering with my code, e.g, by reducing the corpus by taking, say, a 1/10th random sample of it; </li>
<li>(2) or using some cloud platform to enhance my CPU power. I read in <a href=""https://stackoverflow.com/questions/32543235/python-gensim-memory-error"">this SO post</a> that AWS can be used for such puposes, but I am unsure which service I should select (Amazon EC2?). I also checked Google Colab, but got confused that it lists hardware acceleration options (GPU and CPU) in the context of Tensorflow, and I am not sure if that is suitable for NLP. I didn't find any posts about that. </li>
</ul>

<p>Here's my Jupyter Notebook code that I've tried after downloading the wikipedia dumps from <a href=""https://dumps.wikimedia.org/huwiki/latest/huwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">here</a>: </p>

<pre><code>! pip install gensim 
from nltk.stem import SnowballStemmer
from gensim.corpora import WikiCorpus
from gensim.models.word2vec import Word2Vec

hun_stem = SnowballStemmer(language='hungarian')

%%time
hun_wiki = WikiCorpus(r'huwiki-latest-pages-articles.xml.bz2')
hun_articles = list(hun_wiki.get_texts())
len(hun_articles)
</code></pre>

<p>Any guidance would be much appreciated. </p>
",2019-12-15 22:57:23,2019-12-16 01:47:50,Loading Wikipedia XML files into Gensim,<python><nlp><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
24178,59331278,2019-12-14 00:17:18,,"<p>In python, I'm building ngrams with gensim and passing the words into spacy for lemmatization. I'm finding that spacy is not working very well as it's keeping many words as plurals that shouldn't be.</p>

<p>It looks like this is mostly happening when it's mistakenly tagging nouns as proper nouns.</p>

<pre><code>import spacy
nlp = spacy.load('en', disable=['parser','ner'])

doc = nlp(u""bed_bugs bed bug beds bedbug bugs bed_bug nymph nymphs nintendo"")

for token in doc:
    print(""original: {}, Lemma: {}, POS: {}"".format(token, token.lemma_, token.pos_))
</code></pre>

<p>output:</p>

<pre><code>original: bed_bugs, Lemma: bed_bugs, POS: PROPN
original: bed, Lemma: bed, POS: NOUN
original: bug, Lemma: bug, POS: NOUN
original: beds, Lemma: bed, POS: VERB
original: bedbug, Lemma: bedbug, POS: PROPN
original: bugs, Lemma: bugs, POS: PROPN
original: bed_bug, Lemma: bed_bug, POS: X
original: nymph, Lemma: nymph, POS: PROPN
original: nymphs, Lemma: nymphs, POS: PROPN
original: nintendo, Lemma: nintendo, POS: PROPN
</code></pre>

<p>My preferred output would have these changes -</p>

<pre><code>bed_bugs -&gt; bed_bug
nymphs -&gt; nymph
bugs -&gt; bug
</code></pre>

<p>Is there a way to accomplish this with spacy or some other tool?</p>
",,2019-12-16 19:15:30,"Improve spacy lemmatization with bigrams, proper nouns, and plurals?",<python><nlp><gensim><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
24179,59331778,2019-12-14 02:11:36,,"<p>I want to compare the two different tech-word's similarity <br/></p>

<p>for example </p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('/mnt/3CE35B99003D727B/input/jabfair2019/recommender_system/GoogleNews-vectors-negative300.bin', binary=True)
model.similarity('AI','java')
</code></pre>

<p>for many different tech words <br/></p>

<blockquote>
  <p>AWS DynamoDB','Swift','JavaScript','React Native','ReactJS',
                       'TypeScript','Vue.js','Webpack','Amazon Web Services(AWS)','Kubernetes',
                       'PHP','Node.js','REST API','Go','Redux-Saga',
                       'Redux.js','Babel','GraphQL','Jenkins','Django',
                        'Git','AWS EC2','CSS','HTML','Docker',
                        'Kotlin','jQuery','RxJS','AngularJS','XML',
                       'Jekyll','Selenium','Consul','Nexus','Backbone.js',
                       'PostCSS','GatsbyJS','Google API','Apache HTTP Server','Grunt',
                       'ThreeJS','Responsive Web','D3.js','OkHttp','BitBucket',
                       'PKI(Public key infrastructure)','NestJS','JIRA','Less.js','Puppeteer',
                       'AWS Simple Queue Service(AWS SQS)','Gradle','GitLab','DRF(Django REST framework)','AWS X-Ray'
                       ,'Rust','Bootstrap','web3.js','Oracle','ExpressJS','ActionScript','Nginx','Flask'
                        ,'RxSwift','HTML5','Ajax','TCP/IP','Next.js','i18n', 'CSS3','</p>
</blockquote>

<p>is there well trained and all included pre-trained wordvector for this job?</p>
",,2019-12-14 02:11:36,is there pre-trained word-vecotor for different tech words?,<deep-learning><nlp><word2vec><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24182,59333165,2019-12-14 07:28:08,,"<p>When I run my .py file containing the following code </p>

<pre><code>if not os.path.exists('model_out'):
    model1 = gensim.models.Word2Vec(l, min_count = 1, size = 100, window = 5)
    model1.save('model_out')
model1.load('model_out')
model11 = gensim.models.keyedvectors.KeyedVectors.load(model1)
max_size = len(model.wv.vocab)-1
</code></pre>

<p>The following error is generated</p>

<blockquote>
  <p>Traceback (most recent call last):   File ""assignment.py"", line 35, in
  
      model11 = gensim.models.keyedvectors.KeyedVectors.load(model1)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 1540, in load
      model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)   File
  ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py"",
  line 228, in load
      return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 424, in load
      compress, subname = SaveLoad._adapt_by_suffix(fname)   File ""/Users/harshpanwar/Desktop/Enthire_assignment/myenv/lib/python3.6/site-packages/gensim/utils.py"",
  line 513, in _adapt_by_suffix
      compress, suffix = (True, 'npz') if fname.endswith('.gz') or fname.endswith('.bz2') else (False, 'npy') AttributeError: 'Word2Vec'
  object has no attribute 'endswith'</p>
</blockquote>
",,2019-12-14 09:33:32,AttributeError: 'Word2Vec' object has no attribute 'endswith',<python><machine-learning><nlp><artificial-intelligence><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
24185,59397949,2019-12-18 18:15:49,,"<p>I'm training some embeddings on a large corpus.  I gather from <code>gensim</code>'s documentation that it builds the vocabulary before beginning training.  In my case, building the vocabulary takes many hours.  I'd like to save time by re-using the vocabulary from the first model.  How can I do this?  the <code>.build_vocab</code> method can't take the <code>vocabulary</code> object from another model.  </p>

<p>Here's a dummy example:</p>

<pre><code>from gensim.models import FastText, Word2Vec
sentences = [""where are my goats"", ""yay i found my goats""]
m1 = Word2Vec(sentences, size  = 3)
m2 = Word2Vec(size = 4)
m2.build_vocab(m1.vocabulary) # doesn't work
</code></pre>
",2019-12-18 18:37:13,2019-12-19 18:23:47,How to initialize a gensim model with the vocabulary from another model?,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24190,59316635,2019-12-13 05:29:57,,"<p>I need to find similarities between documents and give them a score as the normal essay marking. So can anyone please give me some hint or reference ?</p>

<p>Thank you so much.</p>
",,2019-12-13 05:29:57,Can I use gensim for AES(Automatic Essay Scoring) project?,<python><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
24204,59368232,2019-12-17 05:48:21,,"<p>I wonder what does <code>.build_vocab_from_freq()</code> function from gensim actually do? What is the difference when I'm not using it? Thank you!</p>
",,2019-12-17 06:08:36,Gensim Word2Vec or FastText build vocab from frequency,<python><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24208,59418433,2019-12-20 00:14:31,,"<p>I just study gensim for topic modeling. when I use </p>

<pre><code>lda_model = gensim.models.ldamodel.LdaModel(...)
</code></pre>

<p>the result lda_model has two functions: get_topics() and get_document_topics(). I can find the topic-word and document-topics by them. But, I want to try:</p>

<pre><code>hdp_lda_model = gensim.models.hdpmodel.HdpModel(...)
</code></pre>

<p>I can only find there is get_topics() in its result, no something like get_document_topics(). So I cannot find the relation of document and topics. But it should be somewhere. I read some instruction from <a href=""https://radimrehurek.com/gensim/models/hdpmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/hdpmodel.html</a>. But I did not find any (maybe I miss something?). So is there a function in hdp model, which is like get_document_topics() in lda model?</p>
",2019-12-21 13:25:22,2020-03-17 11:20:32,How to get document-topics using models.hdpmodel  Hierarchical Dirichlet Process in gensim,<document><gensim><word><lda><hdp>,,,CC BY-SA 4.0,False,False,True,False,False
24210,59419123,2019-12-20 02:16:48,,"<p>I am new to gensim topic modeling. Here is my sample code:</p>

<pre><code>import nltk
nltk.download('stopwords')
import re
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)
# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
train=pd.DataFrame({'text':['find the most representative document for each topic',
                            'topic distribution across documents',
                            'to help with understanding the topic',
                            'one of the practical application of topic modeling is to determine']})
text=pd.DataFrame({'text':['how to find the optimal number of topics for topic modeling']})

data =  train.loc[:,'text'].values.tolist()

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

data_words = list(sent_to_words(data))
id2word = corpora.Dictionary(data_words)
corpus = [id2word.doc2bow(text) for text in data_words]
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=3)
</code></pre>

<p>So far so good. But I want to use lda_model to predict text. I at least need to know topic distribution over text and all the topic-word relation. </p>

<p>I think prediction is very common and important function for lda. But I do not know where I can find such function in gensim. Some answers says doc_lda = model[doc_bow] is prediction (<a href=""https://stackoverflow.com/questions/40924185/calculating-topic-distribution-of-an-unseen-document-on-gensim"">Calculating topic distribution of an unseen document on GenSim</a>). But I am not sure about it.</p>
",2019-12-21 13:25:57,2020-02-11 10:49:42,How to use gensim topic modeling to predict new document?,<document><gensim><predict><lda>,,,CC BY-SA 4.0,True,True,True,False,False
24213,59385399,2019-12-18 04:18:30,,"<p>I use the <code>gensim</code> wrapper, <code>LdaMallet()</code> <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""nofollow noreferrer"">[link]</a>, to run <code>MALLET</code>.</p>

<p>Gensim library provide a parameter <code>workers</code> to assign the <code>--num-threads</code> argument in <code>MALLET</code>.<br>
(Ref: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/wrappers/ldamallet.py"" rel=""nofollow noreferrer"">Gensim Code - line274</a>)</p>

<p>But I found the <code>workers</code> seems not working, here is the different setting and running time:</p>

<pre><code> `workers=1` -&gt; run time: 7.32 sec   # &lt;--
 `workers=2` -&gt; run time: 2min 25s
 `workers=4` -&gt; run time: 2min 38s
 `workers=16` -&gt; run time: 3min 13s  # &lt;--
</code></pre>

<p>No matter I run this on my computer:</p>

<pre><code>openjdk version ""1.8.0_162""
OpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.16.04.2-b12)
OpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)
</code></pre>

<p>or on the Colab:</p>

<pre><code>openjdk version ""11.0.4"" 2019-07-16
OpenJDK Runtime Environment (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3)
OpenJDK 64-Bit Server VM (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3, mixed mode, sharing)
</code></pre>

<p>the results are similar, more workers spent more time.
(and I have also tried <code>mallet-2.0.8</code> &amp; <code>mallet-2.0.7</code>)  </p>

<p>Dose it means I am not using a proper way to run MALLET LDA in parallel?  </p>

<p>Thanks!   </p>

<hr>

<p>reference code:</p>

<pre><code># code in gensim (python)
# (i tried with different `workers`)

workers = 16
gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, 
                                 optimize_interval=1, iterations=6000, workers=workers)
</code></pre>

<pre><code># the equivalent commands in mallet (key in shell, ignore the I/O setting):

$ bin/mallet train-topics --num-threads 16
</code></pre>
",,2019-12-18 04:18:30,Parallel problem in MALLET LDA (gensim wrapper),<gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
24244,59423553,2019-12-20 10:17:37,,"<p>Dear all, I have trained word2vec in gensim using Wikipedia data and saved using following program.</p>

<pre><code>model = Word2Vec(LineSentence(inp), size=300, window=5, min_count=5, max_final_vocab=500000,
        workers=multiprocessing.cpu_count())

model.save(""outp1"")
</code></pre>

<p>I want use this model in keras for multi-class Text Classification, What changes I need to do in the following code</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, 
batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accr = model.evaluate(X_test,Y_test) 
</code></pre>

<p>Actually I am new and trying to learn.</p>
",,2019-12-20 16:07:31,Using pretrained gensim Word2vec embedding along with data set in keras,<python-3.x><machine-learning><keras><artificial-intelligence><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24249,59465247,2019-12-24 07:20:17,,"<p>I am doing topic modeling using gensim (in jupyter notebook). I successfully created a model and visualized it. Below is the code:</p>

<pre><code>import time
start_time = time.time()
import re
import spacy
import nltk
import pyLDAvis
import pyLDAvis.gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)
# nlp = spacy.load('en')
stop_word_list = nltk.corpus.stopwords.words('english')
stop_word_list.extend(['from', 'subject', 're', 'edu', 'use'])
df = pd.read_csv('Topic_modeling.csv')
data = df.Articles.values.tolist()

# Remove Emails
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]

# Remove new line characters
data = [re.sub('\s+', ' ', sent) for sent in data]

# Remove distracting single quotes
data = [re.sub(""\'"", """", sent) for sent in data]


def sent_to_words(sentences):
    for sentence in sentences:
        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations


data_words = list(sent_to_words(data))

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_word_list] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    for sent in texts:
        doc = nlp("" "".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out


# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ'])

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]


# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics= 3,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=20,
                                           alpha=0.4,
                                           eta=0.2,
                                           per_word_topics=True)

print(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>Now I want to find dominant topics in each sentence. So I am using the below code:</p>

<pre><code>def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)
</code></pre>

<p>however, I am getting below error: </p>

<blockquote>
  <p>TypeError                                 Traceback (most recent call
  last)  in 
       22 
       23 
  ---> 24 df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)
       25 
       26 # Format</p>
  
  <p> in format_topics_sentences(ldamodel,
  corpus, texts)
        5     # Get main topic in each document
        6     for i, row in enumerate(ldamodel[corpus]):
  ----> 7         row = sorted(row, key=lambda x: (x[1]), reverse=True)
        8         # Get the Dominant topic, Perc Contribution and Keywords for each document
        9         for j, (topic_num, prop_topic) in enumerate(row):</p>
  
  <p>TypeError: '&lt;' not supported between instances of 'int' and 'tuple'</p>
</blockquote>

<p>I don't understand what is the problem. 
Can anyone help? </p>

<p>Thanks in advance!</p>
",,2020-03-20 04:30:28,Type error while finding dominant topics in each sentence in Gensim,<jupyter-notebook><typeerror><python-3.7><gensim><topic-modeling>,,,CC BY-SA 4.0,True,True,True,False,False
24275,59469032,2019-12-24 12:44:25,,"<p>I have built a semantic similarity engine based on LSI (Gensim). I am receiving results with similarity score high as .55 to .50 when the input text is garbage text. How to interpret this score. </p>

<p>I used cosine similarity on vectors obtained from LSI. </p>

<pre><code>def cosine_similarity(v1, v2):
    mag1 = np.linalg.norm(v1)
    mag2 = np.linalg.norm(v2)
    if (not mag1) or (not mag2):
        return 0
    return np.dot(v1, v2) / (mag1 * mag2)
</code></pre>

<p>Results </p>

<pre><code>{
      ""result"": [
        {      
          ""journal_title"": ""Animal Production Science"", 
          ""matched_text"": ""The objective was to compare the behavioural and productive response of cows to either abrupt or two-step weaning with nose flaps. Calves were fitted with nose flaps during the 14 days before separation from their dams (NF group),then were abruptly and permanently separated (AW group), or remained as non-weaned controls (NW group). The behaviour of the cows was recorded before and after nose-flap insertion and permanent separation. Milk yield and composition were determined. After permanent separation, milk yield of NW cows was greater than that of NF and AW cows. AW cows paced and vocalised more than NF and NW cows, and NF cows more times than NW cows. The two-step weaning method with nose flaps was positive for the wellbeing of cows, as it reduced the main behavioural changes that indicate distress and decreased the bodyweight loss. Weaning, either in one or two steps, decreased milk yield 1 week after permanent separation."", 
          ""score"": 0.5508012771606445
        }, 
        {     
          ""journal_title"": ""Journal of the Academy of Nutrition and Dietetics"", 
          ""matched_text"": ""Introduction. Health at Every Size (HAES) is a weight-neutral approach focused on promoting healthy behaviors in people with different body sizes and on enhancing pleasure derived from consuming food to achieve sustainable healthy eating outcomes. However, to the best of our knowledge, there are no studies in the literature assessing the effects of the HAES approach on perceptions of eating pleasure. Objective. We qualitatively investigated the perceptions of obese women about eating pleasure before and after a new interdisciplinary, nonprescriptive intervention based on the HAES approach. Design. The intervention was a randomized controlled clinical trial, designated as Health and Wellness in Obesity, conducted over 7 months at University of Sao Paulo (Brazil). We used a qualitative approach to data construction and analysis of perceptions about eating pleasure. Participants were randomized to either the intervention (I-HAES) group or the control (CTRL) group. The I-HAES group featured individual nutritional counseling, group practice of enjoyable physical activity, and philosophical workshops. The CTRL group was a traditional HAES intervention group (lecture-based model). Focus group discussions eliciting perceptions of pleasure around eating were conducted at baseline and post-study. Focus group transcripts were analyzed by exploratory content analysis. Participants. Forty-three women aged 25 to 50 years with body mass index (measured in kilograms per square meter) between 30 and 39. 9 completed the intervention and the focus groups, with 32 in the I-HAES group and 11 in the CTRL group. Results. Lack of guilt about experiencing pleasure while eating and increased reflection on their own desires increased in participants of both groups after the study. The I-HAES group also displayed a greater sense of autonomy related to eating, increased pleasure in commensality, familiarity with the practice of cooking, and decreased automatic eating. Conclusion HAES-based intervention featuring nutritional counseling, appreciation for physical activity, and philosophical engagement was shown to stimulate pleasure around eating without leading to indiscriminate eating. All rights reserved, Elsevier."", 
          ""score"": 0.5286298990249634
        }, 
        {
          ""journal_title"": ""International Journal of Eating Disorders"", 
          ""matched_text"": ""Objective. Studies have demonstrated that negative affect increases prior to food intake in individuals diagnosed with an eating disorder. Mindfulness has been supported empirically to treat experiential avoidance stemming from anxiety. Thus, the current objective in this study is to empirically compare mindfulness vs. thought suppression invention during a food exposure in both clinical and nonclinical samples. Method. In a 2 (Group: clinical vs. nonclinical) &amp;#x00d7; 2 (Intervention: mindfulness vs. distraction) counterbalanced within treatment design, the current investigation sought to determine the differential effectiveness of a brief mindfulness intervention vs. a brief distraction intervention in women diagnosed with AN and BN in a clinical and nonclinical sample during a food exposure. Results. Results indicated that the eating disorder group reported a significant increase in negative affect after the mindfulness intervention as compared to the distraction intervention, whereas the nonclinical group reported a significant decrease in negative affect after the mindfulness intervention as compared to the distraction intervention. Discussion. Preliminary findings suggest that clinicians may want to proceed cautiously when using mindful eating in those with severe eating disorders during the early stages of food exposure. Limitations and future directions are discussed. &amp;#x00a9; 2013 Wiley Periodicals, Inc. (Int J Eat Disord 2013; 46:582-585)."", 
          ""score"": 0.5221812725067139
        }, 
        {
          ""journal_title"": ""Food and Nutrition in China"", 
          ""matched_text"": ""Objective. To investigate the effect of long-term milk-drinking on reproductive functioning of generation male rats. Method. Totally 4w Spraque-Dawley rats (20 male and 20 female) were randomly divided into milk group and control group according to their body mass and were fed with milk and normal diet for 2 generation, respectively. Indexes including body weight,number of pups, blood hormone level, reproduction organ, anogenital distance, preputial separation (PPS) and sperm parameters were examined. Result. The E2 of milk group in P and PRL in F1 were significantly higher than control. Besides, the number of F2 b in milk group was significantly lower than control. There was no significant differences on the other indexes such as weight of organs, number and quality of sperm, anogenital distance and PPS between milk group and control group. Conclusion. Long-term milk-drinking did not show significant influence on the reproductive functioning in the two-generation reproduction study. Further study was needed to determine the effects of hormone and number of pups."", 
          ""score"": 0.5116945505142212
        }, 
        {
          ""journal_title"": ""Arquivo Brasileiro de Medicina Veterinaria e Zootecnia"", 
          ""matched_text"": ""The aim of the study was to determine the effect of &lt;i&gt;Saccharomyces cerevisiae&lt;/i&gt;, multienzyme composition supplementation on milk yield, quality, blood biochemical parameters of Lithuanian-Black-and-White cows. For this reason 28 cows were divided into four groups (three experimental and one control) each with seven cows fed balanced ration (control group) and following experimental groups: addition of 40g supplement of live yeast with organic selenium (group A); 40g supplement of live yeast with aromatic additives (group B); and 0.2g supplement of multienzyme composition (group C) during a 90 days period. The study showed that milk yield was 2.64%, 1.75%, 1.4% higher in groups A, B, C respectively, comparing with the control group. The milk SCC in experimental groups were lower comparing to the control group. The percentage of milk fat was significantly higher in group A - 0.33%, B - 0.31% and C - 0.16% comparing with the control group. All used additives ensure positive dynamics of investigated biochemical parameters in cattle blood. The results indicated that probiotic additives and multienzyme composition supplementation to dairy cows increased cows productivity and milk fat as well. Probiotic additives supplementation decreased SCC values in milk."", 
          ""score"": 0.5060627460479736
        }, 
        {
          ""journal_title"": ""Asian-Australasian Journal of Animal Sciences"", 
          ""matched_text"": ""Objective. This study was conducted to investigate the effect of transport stress on physiological and hematological responses and milk performance in lactating dairy cows. Methods. Ten lactating dairy cows were randomly divided into 2 groups. The treatment group (TG) was transported 200 km for 4 h by truck, and the control group (NTG) was restrained by stanchion for 4 h in Konkuk University farm. Blood and milk samples were collected at 24 h pre-transport; 1, 2, and 4 h during transport; and 2, 24, and 48 h post-transport. Milk yields were measured at 24 h pre-transport, 0 h during transport, and 24, 48, and 72 h post-transport. Results. Leukocyte, neutrophil, and monocyte numbers in the TG were significantly higher than those of the NTG at each experimental time point. Lymphocyte numbers in the TG were significantly (p&amp;lt;0.05) higher than those of the NTG at 48 h post-transport. Additionally, the neutrophil:lymphocyte ratio of the TG was 45% and 46% higher than that of the NTG at 4 h during transport and 2 h post-transport, respectively. There were no significant differences in erythrocyte numbers, hemoglobin concentrations, platelet numbers, and hematocrit percentages between two groups. Cortisol levels in the TG were significantly (p&amp;lt;0.05) higher than those in the NTG. Milk yields in the TG were lower than those in the NTG. The somatic cell count (SCC) of the TG was significantly (p&amp;lt;0.05) higher than that of the NTG at 1 and 2 h during transport; that of the TG increased dramatically at 1 h during transport and gradually decreased subsequently. Conclusion. Transport stress increased blood parameters including leucocyte, neutrophil, and monocyte numbers by increased cortisol levels, but did not affect erythrocytes, hemoglobin and hematocrit levels. Additionally, transport resulted in a decrease in milk yield and reduced milk quality owing to an increase in milk SCC."", 
          ""score"": 0.504709005355835
        }, 
        {
          ""journal_title"": ""Appetite"", 
          ""matched_text"": ""This randomized-controlled trial aims to test the efficacy of a group intervention (Kg-Free) for women with overweight or obesity based on mindfulness, ACT and compassion approaches. The intervention aimed to reduce weight self-stigma and unhealthy eating patterns and increase quality-of-life (QoL). Seventy-three women, aged between 18 and 55 years old, with BMI \u00e2\u2030\u00a525 without binge-eating seeking weight loss treatment were randomly assigned to intervention or control groups. Kg-Free comprises 10 weekly group sessions plus 2 booster fortnightly sessions, of 2h30 h each. The control group maintained Treatment as Usual (TAU). Data was collected at baseline and at the end of the Kg-Free intervention. Overall, participants enrolled in Kg-Free found the intervention to be very important and helpful when dealing with their weight-related unwanted internal experiences. Moreover, when compared with TAU, the Kg-Free group revealed a significant increased health-related QoL and physical exercise and a reduction of weight self-stigma, unhealthy eating behaviors, BMI, self-criticism, weight-related experiential avoidance and psychopathological symptoms at post-treatment. Results for self-compassion showed a trend towards significance, whereas no significant between-groups differences were found for mindfulness. Taken together, evidence was found for Kg-Free efficacy in reducing weight-related negative experiences and promoting healthy behaviors, psychological functioning, and QoL. All rights reserved, Elsevier."", 
          ""score"": 0.5014874935150146
        } 
      ], 
      ""text"": ""zxvcbnzcn nbzcx bZcxb bZxbvbvbv sgfauy ert we""
    }
</code></pre>

<p>Only the word ""we"" appears in one sentence. The other input text tokens are garbage. </p>

<p>Semantic similarity function</p>

<pre><code>def get_semantic_similarity(
    query_vec, 
    data, 
    vectors,
    df):
    """"""
        Run the query against each document in corpus

        RETURN
        --------
        the top N documents found
    """"""

    results = []

    for idx, d in enumerate(data):
        doc_vec = vectors[idx].ravel()
        similarity = cosine_similarity(query_vec, doc_vec)
        results.append((similarity, d[:200], idx))
</code></pre>
",,2019-12-24 12:44:25,LSI returning high similarity score for garbage input,<python><nlp><gensim><latent-semantic-indexing>,,,CC BY-SA 4.0,False,False,True,False,False
24283,59510075,2019-12-28 10:32:18,,"<p>We have a <code>flask</code> application where we need to load a pretrained model located at the path <code>'/root/apps/mlapi/resources/emoji2vec.bin'</code> using <code>gensim</code>. While running the code I am getting below error</p>

<pre><code>File ""mlapi.py"", line 26, in &lt;module&gt;
    e2v_model = ModelEmoji2Vec()
  File ""/home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/models/susheels/text2emoji/vector_model/modelE2V.py"", line 19, in __init__
    self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/keyedvectors.py"", line 1498, in load_word2vec_format
    limit=limit, datatype=datatype)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 342, in _load_word2vec_format
    with utils.open(fname, 'rb') as fin:
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 308, in open
    errors=errors,
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 517, in _shortcut_open
    return _builtin_open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
PermissionError: [Errno 13] Permission denied: '/root/apps/mlapi/resources/emoji2vec.bin'
</code></pre>

<p>Basically, error is occurring at </p>

<pre><code>#modelE2V.py, line 19
self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
</code></pre>

<p>It says that permission denied. But if I try to load the model using simple python script outside flask app it works fine, why this error is occurring inside flask application</p>
",,2019-12-28 18:24:51,Unable to load a file in flask application,<python><python-3.x><flask><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24284,59425670,2019-12-20 12:57:26,,"<p>I have a strange problem while trying to evaluate my Word2Vec model in Italian. 
I'm using this data <a href=""http://www.leviants.com/ira.leviant/MultilingualVSMdata.html"" rel=""nofollow noreferrer"">http://www.leviants.com/ira.leviant/MultilingualVSMdata.html</a></p>

<p>I'm getting this error `</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code> model.wv.evaluate_word_pairs(pairs= ""...MWS353_Italian_tab.txt"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1281, in evaluate_word_pairs
    ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1281, in &lt;listcomp&gt;
    ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
KeyError: 'di'</code></pre>
</div>
</div>
</p>

<p>Now the strange part is that the word ""di"" is neither in my vocabulary nor in my txt file. What is wrong here? I thought it was a separator problem, tried tsv, tried to give the argument <code>=""\t""</code>, nothing changed. </p>

<p>Any ideas to solve this? </p>
",,2019-12-20 12:57:26,Evaluating word2vec using SimLex-999 and wordsim - key error,<nlp><word2vec><word-embedding><evaluate>,,,CC BY-SA 4.0,False,False,True,False,False
24297,59391403,2019-12-18 11:50:00,,"<p>I want to ask how to improve the score of <code>WmdSimilarity</code> from <code>gensim</code>? Is it the query or the corpus I trained on word embedding? Thanks. I'm really grateful if I can read the references about it.</p>
",,2019-12-18 11:50:00,Improving Word Mover's Distance Similarity Score,<python><gensim><word2vec><fasttext><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24329,59478986,2019-12-25 13:45:50,,"<p>I trained a doc2vec model using <code>python gensim</code> on a corpus of 40,000,000 documents. This model is used for infering docvec on millions of documents everyday. To ensure stability, I set <code>alpha</code> to a small value and a large <code>steps</code> instead of setting a constant random seed:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec.load('doc2vec_dm.model')
doc_demo = ['a','b']
# model.random.seed(0)
model.infer_vector(doc_demo, alpha=0.1, min_alpha=0.0001, steps=100)
</code></pre>

<p><code>doc2vec.infer_vector()</code> accepts only one documents each time and it takes almost 0.1 second to infer each docvec. Is there any <code>API</code> that can handle a series of documents in each infering step?</p>
",,2019-12-25 21:19:29,How to perform doc2vec.infer_vector() on millions of documents?,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24357,59548233,2019-12-31 20:28:45,,"<p>I've got several hundred pandas data frames, each of which has a column of very long strings that need to be processed/sentencized and finally tokenized before modeling with word2vec.</p>

<p>I can store them in any format on the disk, before I build a stream to pass them to gensim's word2vec function. </p>

<p>What format would be best, and why?  The most important criterion would be performance vis-a-vis training (which will take many days), but coherent structure to the filesystem would also be nice.  </p>

<p>Would it be crazy to store several million or maybe even a few billion text files containing one sentence each?  Or perhaps some sort of database?  If this was numerical data I'd use hdf5.  But it's text.  The cleanest would be to store them in the original data frames, but that seems less ideal from an i/o perspective, because I'd have to load each data frame (largish) every epoch.  </p>

<p>What makes the most sense here?</p>
",,2020-01-02 05:00:26,Best way to store processed text data for streaming to gensim?,<stream><nlp><storage><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24368,59568131,2020-01-02 18:20:04,,"<p>I am new to Machine Learning and it is the first time that I am using python's gensim in order to extract topics from text.</p>

<p>I successfully trained a model (for 100 topics) and then I had the idea to use that model in an HTTP API that I created using python flask. The endpoint gives as back terms for a given text.</p>

<p>Btw model is loaded when I initialize the API. </p>

<p>After trying this out on production, memory (on a small VM ~ 1GB Ram) exhausted and finally I got an error:</p>

<pre><code>tags = tags + lda.topic_words(topic_index, num_of_keywords_for_topic, model, words)
  File ""/var/app/tagbee/lda.py"", line 64, in topic_words
    x2 = model.get_topic_terms(topicid=topic_index, topn=number_of_keywords)
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 1224, in get_topic_terms
    topic = self.get_topics()[topicid]
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 1204, in get_topics
    topics = self.state.get_lambda()
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 269, in get_lambda
    return self.eta + self.sstats
MemoryError: Unable to allocate 96.6 MiB for an array with shape (100, 253252) and data type float32
</code></pre>

<p>So I have some questions:</p>

<ul>
<li>Can a gensim LDA model be used that way, mean in an HTTP API?</li>
<li>If yes, what is the trick to make it happen? If it needs at least 90MB of memory per request, how does it scale?</li>
<li>Is there any alternative approach?</li>
</ul>

<p>Thank you in advance!</p>
",2020-01-03 20:03:26,2020-01-22 06:09:35,LDA gensim model in a flask HTTP API - Memory issues,<http><flask><out-of-memory><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
24373,59521204,2019-12-29 16:29:13,,"<p>I'm specifically using <code>gensim</code> to build a <code>TfidfModel</code> but I believe this is more of a general TF-IDF question...</p>

<p>Let's say I build a TF-IDF model with 10 documents. How can I use this model to detect words that are high-value in the model, but under-represented from a specific seen or unseen document?</p>

<p>For example, if documents 1-9 all use the word ""banana"" frequently, how can I discover that document 10 (or a document not used to build the model) doesn't use it at all?</p>

<p>I know that I could just pull a dictionary of words and values from the model and do my own kind of comparison but I'm wondering if there's a better way.</p>
",,2019-12-29 16:29:13,"How to use a TF-IDF model to find ""missing"" or under-represented words from a document?",<python><nlp><data-science><gensim><tf-idf>,,,CC BY-SA 4.0,False,False,True,False,False
24378,59501121,2019-12-27 12:55:42,,"<p>I am confused as to how I can use <strong>Doc2Vec(using Gensim)</strong> for IMDB sentiment classification dataset. I have got the Doc2Vec embeddings after training on my corpus and built my Logistic Regression model using it. How do I use it to make predictions for new reviews? sklearn TF-IDF has a <em>transform</em> method that can be used on test data after training on training data, what is its equivalent in Gensim Doc2Vec?</p>
",,2019-12-27 17:27:22,Sentiment Classification using Doc2Vec,<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,True
24391,59634935,2020-01-07 19:31:03,,"<p>I construct sentences using 3 words ""1"", ""2"", ""3"", in different ways, and observe that the word vectors are unchanged for each of these words.</p>

<p>Following are the different sentences</p>

<p>Type 1: [[""1"", ""2""], [""1"", ""3""]]</p>

<p>Type 2: [[""1"", ""2"", ""3""]]</p>

<p>Type 3: [[""1"", ""2""], [""3""]]</p>

<p>I am training <code>Word2Vec</code> model as follows</p>

<pre><code>model = Word2Vec(sentences,min_count=1,size=2)
print (model.wv.most_similar(""1""))
print (model.wv.most_similar(""2""))
print (model.wv.most_similar(""3""))
print (model.wv['1'])
print (model.wv['2'])
print (model.wv['3'])
</code></pre>

<p>And results are same on changing the sentence type</p>

<pre><code>[('3', 0.5377859473228455), ('2', -0.5831003785133362)]
[('1', -0.5831003189086914), ('3', -0.9985027313232422)]
[('1', 0.5377858281135559), ('2', -0.9985026717185974)]
[-0.24893647 -0.24495095]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>Also when I change word ""1"" to suppose ""101"", the result changes</p>

<pre><code>[('3', 0.5407046675682068), ('2', -0.5859125256538391)]
[('101', -0.5859125256538391), ('3', -0.9985027313232422)]
[('101', 0.540704607963562), ('2', -0.9985026717185974)]
[-0.05898098 -0.0576357 ]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>I wanted to know </p>

<ol>
<li><p>Why the results didn't change when I changed the sentences?</p></li>
<li><p>Why results changed when I just updated the value?</p></li>
</ol>
",,2020-01-08 01:04:28,Understanding gensim Word2Vec most_similar results for 3 words,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24393,59570511,2020-01-02 21:54:47,,"<p>I downloaded <a href=""https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M-subword.bin.zip"" rel=""nofollow noreferrer"">wiki-news-300d-1M-subword.bin.zip</a> and loaded it as follows:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim
print(gensim.__version__)
model = gensim.models.fasttext.load_facebook_model('./wiki-news-300d-1M-subword.bin')
print(type(model))
model_keyedvectors = model.wv
print(type(model_keyedvectors))
model_keyedvectors.save('./wiki-news-300d-1M-subword.keyedvectors')
</code></pre>

<p>As expected, I see the following output:</p>

<pre class=""lang-sh prettyprint-override""><code>3.8.1
&lt;class 'gensim.models.fasttext.FastText'&gt;
&lt;class 'gensim.models.keyedvectors.FastTextKeyedVectors'&gt;
</code></pre>

<p>I also see the following three numpy arrays serialized to the disk:</p>

<pre class=""lang-sh prettyprint-override""><code>$ du -h wiki-news-300d-1M-subword.keyedvectors*
127M    wiki-news-300d-1M-subword.keyedvectors
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors_ngrams.npy
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors.npy
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors_vocab.npy
</code></pre>

<p>I understand <code>vectors_vocab.npy</code> and <code>vectors_ngrams.npy</code>, however, what is <code>vectors.npy</code> is used for internally in <code>gensim.models.keyedvectors.FastTextKeyedVectors</code>? If I look at the source code for finding out <a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.1/gensim/models/keyedvectors.py#L2090"" rel=""nofollow noreferrer"">word vector</a>, I do not see how attribute <code>vectors</code> is being used anywhere. I see the attributes <code>vectors_vocab</code> and <code>vectors_ngrams</code> bing used. However, if I remove <code>vectors.npy</code> file, I am not able to load the model using <code>gensim.models.keyedvectors.FastTextKeyedVectors.load</code> method.</p>

<p>Can someone please explain where this variable is used? Can I remove it if all I am interested is in looking word vectors (to reduce memory footprint)?</p>

<p>Thanks. </p>
",,2020-01-07 00:23:11,"FastTextKeyedVectors difference between vectors, vectors_vocab and vectors_ngrams instance variables",<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24395,59482140,2019-12-25 22:01:34,,"<p>Gensim Doc2Vec infer_vector on paragraphs with unseen words generates vectors that differ based on the characters in the unsween words.</p>

<pre><code>for i in range(0, 2):
    print(model.infer_vector([""zz""])[0:2])
    print(model.infer_vector([""zzz""])[0:2])
    print(model.infer_vector([""zzzz""])[0:2])
    print(""\n"")

[ 0.00152548 -0.00055992]
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]


[ 0.00152548 -0.00055992] # same as in previous iteration
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]
</code></pre>

<p>I am trying understand how unseen words affect initialization of the infer_vector. It looks like different characters will produce different vectors. Trying to understand why.</p>
",,2019-12-25 23:16:57,Gensim Doc2Vec infer_vector on unseen words differs based on characters in these words,<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24410,59573454,2020-01-03 05:07:44,,"<p>I am trying to find a simple way to calculate soft cosine similarity between two sentences.</p>

<p>Here is my attempt and learning:</p>

<pre><code>from gensim.matutils import softcossim

sent_1 = 'Dravid is a cricket player and a opening batsman'.split()
sent_2 = 'Leo is a cricket player too He is a batsman,baller and keeper'.split()

print(softcossim(sent_1, sent_2, similarity_matrix))
</code></pre>

<p>I'm unable to understand about <code>similarity_matrix</code>. Please help me find so, and henceforth the soft cosine similarity in python.</p>
",2020-02-20 15:09:37,2020-05-09 07:04:36,Soft Cosine Similarity between two sentences,<python><gensim><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24444,59605023,2020-01-05 23:12:48,,"<p>I would like to load a gensim pretrained model and continue training it.</p>

<p>This example fails...I've tried many variants and gensim API, but what I'm trying to do doesn't seem possible.</p>

<pre><code>import gensim, logging, os
from gensim.models import KeyedVectors
from gensim.models import Word2Vec

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

# this fails - TypeError: 'int' object is not iterable
model = Word2Vec(wv)

class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences('all-tokenized-sentences.txt')

# Training the model with list of sentences (with 4 CPU cores)
model.train(sentences, workers=4)
</code></pre>
",,2020-01-05 23:12:48,What is the process to convert gensim KeyedVector to model?,<python><model><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24453,59592326,2020-01-04 15:52:27,,"<p>I wonder which algorithm is the best for semantic similarity? Can anyone explain why? </p>

<p>Thank you!</p>
",,2020-01-04 18:55:13,Word Mover's Distance vs Cosine Similarity,<python><nlp><gensim><semantics><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24456,59654203,2020-01-08 21:42:41,,"<p>'''</p>

<pre><code>mallet_path = '/Users/saira/Scicom/SENTIMENT Analysis/DELTA/Topic Modelling/mallet-2.0.8/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>

<p>'''</p>

<p>My error is:</p>

<p>CalledProcessError: Command '/Users/saira/Scicom/SENTIMENT Analysis/DELTA/Topic Modelling/mallet-2.0.8/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\saira\AppData\Local\Temp\fb153e_corpus.txt --output C:\Users\saira\AppData\Local\Temp\fb153e_corpus.mallet' returned non-zero exit status 1.</p>
",,2020-01-08 21:42:41,CalledProcessError:non-zero exit status 1,<python-3.x><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
24463,59690985,2020-01-11 01:48:35,,"<p>I'm trying to train a gensim Word2Vec model with bigrams. To get the bigrams, I run the following code, with <code>sentences</code> standing for a long list of split sentences using <code>nltk.sent_tokenize</code>, lemmatized with Spacy and then lowercased:</p>

<pre><code>from gensim.models import Word2Vec, Phrases

bigrams = Phrases(sentences, min_count=20, threshold=10)
</code></pre>

<p>This could only include bigrams which occur >= 20 times. But when I run <code>bigrams.vocab</code>, I get:</p>

<pre><code>defaultdict(int,
             b'inflated': 237,
             b'the_inflated': 34,
             b'inflated_bag': 1,
             b'let': 6841,
             b'bag_let': 1,
             b'let_-pron-': 3723,
             ...)
</code></pre>

<p>From what I understand, <code>inflated_bag</code> and <code>let_-pron-</code> should not be present. Is there something I'm doing wrong? Or am I misinterpreting the output?</p>
",,2020-01-11 09:00:28,gensim Phrases not observing min_count parameter,<python><gensim>,,,CC BY-SA 4.0,True,True,True,False,False
24517,59710100,2020-01-13 01:15:39,,"<p>I'm running the following code to process a list of documents, basically it's just two for loops.</p>

<pre><code>from nltk.tokenize import TreebankWordTokenizer
from gensim.models import KeyedVectors
from nlpia.loaders import get_data
word_vectors = get_data('w2v', limit=200000)

def tokenize_and_vectorize(dataset):
    tokenizer = TreebankWordTokenizer()
    vectorized_data = []
    expected = []
    for sample in dataset:
        tokens = tokenizer.tokenize(sample[1])
        sample_vecs = []
        for token in tokens:
            try:
               sample_vecs.append(word_vectors[token])

            except KeyError:
               pass  

        vectorized_data.append(sample_vecs)
        #print(1)
    return vectorized_data
</code></pre>

<p>then I call the function to process the top 25k elements</p>

<pre><code>vectorized_data=tokenize_and_vectorize(dataset[0:25000])
</code></pre>

<p>However, this code seems taking forever running as the * sign never disappear. (Note: I did try running only 50 samples and results came back pretty fast)</p>

<p>In order to see where it got stuck, I naively added <code>print(1)</code> ahead of <code>return vectorized_data</code> so for every cycle of loop it returns me a 1. After 1min36sec, I got all results returned. </p>

<p>A side observation of the computer memory usage. In the case without adding print(1), I did observe that the memory usage were high at the beginning and dropped back to normal level after couple mins, not sure if this indicates the process is done though * sign is still showing.</p>

<p>What caused this issue and how do I fix it?</p>
",2020-01-13 03:49:21,2020-01-13 03:49:21,Python code non-stop when processing text documents,<python><nlp>,,,CC BY-SA 4.0,True,False,True,False,False
24530,59631259,2020-01-07 15:20:33,,"<p><a href=""https://i.stack.imgur.com/SZxpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SZxpG.png"" alt=""enter image description here""></a></p>

<p>Does it mean that I must provide tokenized words of a document as list of strings or simply a document as a list of string for the input doc_words. Please clarify</p>
",,2020-01-07 17:59:36,Understanding of the parameter model.infer_vector for doc2vec gensim,<python><gensim><doc2vec>,2020-01-08 07:55:52,,CC BY-SA 4.0,False,False,True,False,False
24537,59765941,2020-01-16 08:53:24,,"<p>I am using LDA for a Topic Modelling task. As suggested in various forums online, I have trained my model on a fairly large corpus : NYTimes news dataset (~ 200 MB csv file) which has reports regarding a wide variety of news topics.
Surprisingly the topics predicted out of it are mostly related to US politics and when I test it on a new document regarding 'how to educate children and parenting stuff' it predicts the most likely topic as this :</p>

<p>['two', 'may', 'make', 'company', 'house', 'things', 'case', 'use']</p>

<p>Kindly have a look at my model :</p>

<pre><code>def ldamodel_english(filepath, data):
  data_words = simple_preprocess(str(data), deacc=True)

  # Building the bigram model and removing stopwords
  bigram = Phrases(data_words, min_count=5, threshold=100)
  bigram_mod = Phraser(bigram)
  stop_words_english = stopwords.words('english')
  data_nostops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words_english] 
for doc in data_words]
  data_bigrams = [bigram_mod[doc] for doc in data_nostops]
  data_bigrams = [x for x in data_bigrams if x != []]

  # Mapping indices to words for computation purpose
  id2word = corpora.Dictionary(data_bigrams)
  corpus = [id2word.doc2bow(text) for text in data_bigrams]

  # Building the LDA model. The parameters 'alpha' and 'eta' handle the number of topics per document and words per topic respectively
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=10, iterations=100,
                                            update_every=1, chunksize=1000, passes=8, alpha=0.09, per_word_topics=True, eta=0.8)
  print('\nPerplexity Score: ' + str(lda_model.log_perplexity(corpus)) + '\n')
  for i, topic in lda_model.show_topics(formatted=True, num_topics=20, num_words=10):
      print('TOPIC #' + str(i) + ': ' + topic + '\n')
  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams, dictionary=id2word, coherence='c_v')
  print('\nCoherence Score: ', coherence_model_lda.get_coherence())
  saved_model_path = os.path.join(filepath, 'ldamodel_english')
  lda_model.save(saved_model_path)

return saved_model_path, corpus, id2word
</code></pre>

<p>The 'data' part comes from the 'Content' section of the NYTimes News dataset and I used GENSIM library for LDA.</p>

<p>My question is if a well trained LDA model predicts so badly why there is such a hype and what is an effective alternative method?</p>
",2020-01-16 09:01:45,2020-01-20 16:14:41,LDA Topic Modelling : Topics predicted from huge corpus make no sense,<python><data-science><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
24543,59712626,2020-01-13 08:06:41,,"<p>I am trying some unknown word and it give out 0% by putting ""Polytechnic"", ""Diploma"" that dictionary does not even have and i try to find sources that are able to add words into dictionary that i find are not able to find</p>

<p>Here is my function of code that i am calling</p>

<pre><code>def similarityChecker(txt1, txt2):
   result = 0.00
   file_docs = []

   tokens1 = sentence(txt1)
   for line in tokens1:
       file_docs.append(line)

   print(""Number of sentence:"",len(file_docs))

   gen_docs = [[w.lower() for w in removestop(text)]
            for text in file_docs]

   dictionary = gensim.corpora.Dictionary(gen_docs)

   corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]
   # This tf_idf cannot add unknown words?
   tf_idf = gensim.models.TfidfModel(corpus)

   # Gives out an empty array [] for using words not in the english dictionary
   for doc in tf_idf[corpus]:
       print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])

   # building the index
   sims = gensim.similarities.Similarity('/', tf_idf[corpus], num_features=len(dictionary))

   file2_docs = []

   tokens2 = sentence(txt2)
   for line in tokens2:
       file2_docs.append(line)

   print(""Number of sentence:"", len(file2_docs))

   avg_sims = []

   for line in file2_docs:
       # tokenize words
       query_doc = [w.lower() for w in removestop(line)]
       # create bag of words
       query_doc_bow = dictionary.doc2bow(query_doc)
       # find similarity for each document
       query_doc_tf_idf = tf_idf[query_doc_bow]
       # print (document_number, document_similarity)
       print('Comparing Result:', sims[query_doc_tf_idf])
       # calculate sum of similarities for each query doc
       sum_of_sims = (np.sum(sims[query_doc_tf_idf], dtype=np.float32))
       # calculate average of similarity for each query doc
       avg = sum_of_sims / len(file_docs)
       # print average of similarity for each query doc
       print(f'avg: {sum_of_sims / len(file_docs)}')
       # add average values into array
       avg_sims.append(avg)

   total_avg = np.sum(avg_sims, dtype=np.float)

   result = round(float(total_avg) * 100)

   if result &gt;= 100:
       result = 100

   return result
</code></pre>

<p>Some function i added is to call nltk which is working.
And I am new to this gensim coding i really need help.</p>
",2020-01-15 08:06:46,2020-01-15 08:06:46,Adding Unknown words to Gensim dictionary and teaching the model,<python><nltk><gensim><tf-idf>,,,CC BY-SA 4.0,True,False,True,False,False
24561,59736023,2020-01-14 14:41:59,,"<p>I have a huge data frame that doesn't fit into memory. Thus I access it in Python via <code>dask</code> (distributed).</p>

<p>I want to train a Word2Vec/Doc2Vec model with the package <code>gensim</code> based on the entries of one column in the data frame, that's why I built an iterator like in <a href=""https://stackoverflow.com/questions/56681210/convert-a-column-in-a-dask-dataframe-to-a-taggeddocument-for-doc2vec"">this question</a>.</p>

<p>Now, <code>gensim</code> trains using multiple cores whose number I need to specify, and similarly <code>dask</code> allows me to use multiple cores, too. So far I gave all available cores to <code>dask</code> and the same number of cores to <code>gensim</code>. My reasoning would be that fetching data and training on the data are exclusive tasks that cannot be done at the same time, so <code>gensim</code> and <code>dask</code> shouldn't fight over the cores.</p>

<p>Indeed, there are no error messages, but still, training seems to be quite slow, and I suspect there's a better way to distribute the work. Does anyone have experience in this matter?</p>
",,2020-01-17 03:22:18,Efficient use of multiple cores with dask distributed and gensim,<python><multithreading><dask><gensim><dask-distributed>,,,CC BY-SA 4.0,False,False,True,False,False
24569,59667894,2020-01-09 16:03:17,,"<p>I have created a k means cluster from Gensim word2vec where the value of <code>k</code> is <code>3</code>. Now I want to retrieve the cluster and the values where the frequency is the most. </p>

<pre><code>import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.cluster import KMeans
import numpy as np
text = ""Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for &lt;pre&gt; your suggestions. We hope to improve this feature in the future. In case you experience any &lt;pre&gt; further problems with the app, please don't hesitate to contact me again.""
sentences = sent_tokenize(text)
word_text = [[text for text in sentences.split()] for sentences in sentences]
model = Word2Vec(word_text, min_count=1)
x = model[model.wv.vocab]
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters)
kmeans = kmeans.fit(x)
</code></pre>
",2020-01-09 16:38:01,2020-01-09 19:45:11,how to select cluster with maximum frequency in k means,<python><k-means><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
24588,59820942,2020-01-20 10:06:34,,"<p>I want to add a full stop after every line of sentence that I am getting from my cleaned text after performing text cleaning in order to perform summarisation using heapq or gensim. If I don't get a full stop, heapq or Gensim will not understand the different sentences and would take all the sentences as one. I am using the following code :</p>

<pre><code>import en_core_web_sm
nlp = en_core_web_sm.load()
text = nlp(str1_clean_summary)

for sent in text.sents:
  print(sent.string.strip())
</code></pre>

<p>str1_clean_summary looks like this :</p>

<pre><code>many price increase options 
still believe us need prove consistently 
aim please delay end displeasingich
responds wuickly
</code></pre>

<p>This gives me sentences in a different lines but I need to add a full stop after each sentence so they are treated separately. </p>
",2020-01-20 10:55:13,2020-01-20 11:02:35,Adding a full stop '.' after every sentence line while using spacy NLP to perform summarisation,<python><python-3.x><nlp><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
24590,59823688,2020-01-20 12:51:43,,"<p>Trying to build gensim word2vec model. Corpus contains 1 M sentences.
I am using callback to print loss after every epochs. After few epochs loss becomes zero.
Any idea why loss becomes 0?</p>

<pre><code>Loss after epoch 0: 17300302.0
Loss after epoch 1: 11381698.0
Loss after epoch 2: 8893964.0
Loss after epoch 3: 7105532.0
           ...
           ...
           ...
Loss after epoch 54: 1283432.0
Loss after epoch 55: 1278048.0
Loss after epoch 56: 316968.0
Loss after epoch 57: 0.0
Loss after epoch 58: 0.0
Loss after epoch 59: 0.0
Loss after epoch 60: 0.0
Loss after epoch 61: 0.0
Loss after epoch 62: 0.0
Loss after epoch 63: 0.0
Loss after epoch 64: 0.0
Loss after epoch 65: 0.0
Loss after epoch 66: 0.0
</code></pre>
",2020-01-20 15:22:03,2020-01-27 10:04:08,Gensim word2vec model loss becomes 0 after few epochs,<deep-learning><gensim><word2vec><loss-function>,,,CC BY-SA 4.0,False,False,True,False,False
24620,59775594,2020-01-16 18:15:10,,"<p>Is there a way for Gensim to generate strictly the bigrams, trigrams in a list of words? </p>

<p>I can successfully generate the unigrams, bigrams, trigrams but I would like to extract only the bigrams, trigrams. </p>

<p>For example, in the list below:</p>

<pre><code>words = [['the', 'mayor', 'of', 'new', 'york', 'was', 'there'],[""i"",""love"",""new"",""york""],[""new"",""york"",""is"",""great""]]
</code></pre>

<p>I use </p>

<pre><code>bigram = gensim.models.Phrases(words, min_count=1, threshold=1)
bigram_mod = gensim.models.phrases.Phraser(bigram)
words_bigram = [bigram_mod[doc] for doc in words]
</code></pre>

<p>This creates a list of unigrams and bigrams as follows:</p>

<pre><code>[['the', 'mayor', 'of', 'new_york', 'was', 'there'],
 ['i', 'love', 'new_york'],
 ['new_york', 'is', 'great']]
</code></pre>

<p>My question is, is there a way (other than regular expressions) to extract strictly the bigrams, so that in this example only ""new_york"" would be a result?</p>
",,2020-01-16 19:24:19,How to generate bigram/trigram corpus only,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24624,59827389,2020-01-20 16:28:01,,"<p>I'm trying to set-up a pipeline that increases summarization results. I would like to do the following:</p>

<pre><code>1. Obtain summarization results from Gensim.
2. Run Gensim summarization results through spaCy.
4. Run spaCy summarization results through NLTK.
5. Run NLTK results through Sumy Lexrank.
</code></pre>

<p>I'm already able summarize with each of these algorithms (<a href=""https://endlessmetrics.substack.com/p/inflation"" rel=""nofollow noreferrer"">see repo</a>). However, hoping this process can produce a more accurate consolidated summary.</p>

<p>Does anyone know how to make this happen?</p>
",2020-01-21 17:19:54,2020-01-21 17:19:54,"How to run Gensim results through spaCy, spaCy results through nltk, and nltk results through Sumy",<nlp><stanford-nlp><spacy><spacy-pytorch-transformers>,,,CC BY-SA 4.0,True,True,True,True,False
24625,59827730,2020-01-20 16:48:53,,"<p>I have been following the following example for using doc2vec for text classification:</p>

<p><a href=""https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb"" rel=""nofollow noreferrer"">https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb</a></p>

<p>I ran this notebook on my datasets and want to apply one of the doc2vec models to a 3rd dataset (eg, the overall dataset the test/train model was built on).  I tried:</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)
X_train = label_sentences(X_train, 'Train')
X_test = label_sentences(X_test, 'Test')

#added
big_text = label_sentences(big_text, 'Test') #big_text = larger dataframe

#old
#all_data = X_train + X_test

#new
all_data = X_train + X_test + big_text 
</code></pre>

<p>1 - this is not really practical for applied purposes.  The data that one wants to predict might not be available at the time of train/testing.</p>

<p>2 - the model performance decreased as a result</p>

<p>So how can I save once of the models and applying to a completely different dataset?  It would seems that I would need to update the doc2vec model with docs of the other dataset as well.</p>
",2020-01-20 16:54:48,2020-01-21 06:06:24,save/reuse doc2vec based model for further predictions,<machine-learning><scikit-learn><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
24627,59794837,2020-01-17 21:04:35,,"<p>I'm working on an LDA model using <code>Gensim</code> and <code>spacy</code>. </p>

<p>Generically:</p>

<pre><code>ldamodel = Lda(doc_term_matrix, num_topics=4, random_state = 100, update_every=3, chunksize = 50, id2word = dictionary, passes=100, alpha='auto')
ldamodel.print_topics(num_topics=4, num_words=6)
</code></pre>

<p>I'm at the point where I have some output and I'd like to append my original Dataframe (from which the text came from) with the topics and a percent contribution for each document.</p>

<p>The original df looks like this</p>

<pre><code>id  group text
234 1     here is some text
837 7     here is some text
494 2     here is some text
223 1     here is some text
</code></pre>

<p>I do some standard preprocessing including lemmatization, removing stop words, etc. and then compute percent contributions for each document.</p>

<p>my output looks like this</p>

<pre><code>   Document_No  Dominant_Topic  ...                                           Keywords Text
0            0             1.0  ...  RT, new, work, amp, year, today, people, look,...    0
1            1             0.0  ...  like, time, good, know, day, find, research, a...    1
2            2             1.0  ...  RT, new, work, amp, year, today, people, look,...    2
3            3             3.0  ...  study, t, change, use, want, Trump, love, stud...    3
4            4             3.0  ...  study, t, change, use, want, Trump, love, stud...    4
</code></pre>

<p>I thought I could just concat the 2 dfs back together like so:</p>

<pre><code>results = pd.concat([df, results])
</code></pre>

<p>but when I do that the indices don't match and I'm left with a sort of Frankenstein df that looks like this</p>

<pre><code>id  group text                Document_No  Dominant_Topic  ...                                           
NaN NaN   NaN                 0            1.0             ...
NaN NaN   NaN                 1            0.0             ...
494 2     here is some text   NaN          NaN             ...
223 1     here is some text   NaN          NaN             ...
</code></pre>

<p>Happy to post fuller code if that would be helpful, but I'm hoping someone just knows a better way to do this from same point as I might print topics.</p>
",2020-01-17 21:18:50,2020-01-17 21:18:50,Append/Merge Dataframe with LDA output,<python><python-3.x><pandas><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24639,59845191,2020-01-21 16:17:13,,"<p>I plan to use NLTK, Gensim and Scikit Learn for some NLP/text mining. But i will be using these libraries to work with my org data. The question is while using these libraries 'do they make API calls to process the data' or is the data taken out of the python shell to be processed. It is a security question, so was wondering if someone has any documentation for reference.</p>

<p>Appreciate any help on this.</p>
",,2020-01-22 21:22:25,"API calls from NLTK, Gensim, Scikit Learn",<python><api><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,True
24658,59863559,2020-01-22 15:53:08,,"<p>I'm trying to predict big 5 personality traits (Extraversion, Neuroticism,  Agreeableness,  Conscientiousness, Openness) based on text analysis written by user. 
Here is my preprocessed data set:</p>

<p><img src=""https://i.stack.imgur.com/N6QHO.png"" alt=""df_processed""></p>

<p>And here is my word2vec model:</p>

<pre><code>model_wv = Word2Vec(df_processed['TEXT'], sg=1, size=300, window=10, min_count=1)
</code></pre>

<p>Vocabulary consists of 26126 words. </p>

<pre><code>features = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']
X = df_processed['TEXT']
y = df_processed[features]
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)

model = Sequential()
model.add(Embedding(max_features, 100)) 
model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(5, activation='softmax'))

model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs, validation_data=(X_test, y_test))
</code></pre>

<p>My question is, how can I use word2vec vectors in tensorflow model? </p>

<p>Should I replace every word in every row in ""X"" with the vector from word2vec model? I think it will be quite expensive for calculation but what will be another possibility? </p>

<p>Sorry if my questions sounds dummy, I' m just really new in NLP and tensorflow.</p>
",2020-01-22 17:01:36,2020-01-22 17:01:36,How to use Word2Vec for classification problem in Tensorflow,<python><tensorflow><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24667,59813664,2020-01-19 19:33:28,,"<p>I'm getting an <strong>AttributeError</strong> while trying to implement with embedding_vector:</p>

<pre><code>from gensim.models import KeyedVectors
embeddings_dictionary = KeyedVectors.load_word2vec_format('model', binary=True)

embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
</code></pre>

<blockquote>
  <p>AttributeError: 'Word2VecKeyedVectors' object has no attribute 'get'</p>
</blockquote>
",2020-01-19 19:34:20,2020-01-19 19:57:02,Error while implementing Word2Vec model with embedding_vector,<python><machine-learning><keras><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24673,59721715,2020-01-13 17:57:52,,"<p>I've generated a PySpark Word2Vec model and save it like:</p>

<pre><code>from pyspark.ml.feature import Word2Vec

w2v = Word2Vec(minCount = 1000, seed=42, inputCol=""item_name"", outputCol=""features"")
model = w2v.fit(sample)
model.save('w2v_pyspark')
</code></pre>

<p>Why i use PySpark W2V? Because i collected my sample from the <em>Hive</em> table and it s very huge sample, thats why i didn't transformed spark dataframe to pandas dataframe. So why i need <em>gensim</em>? Because i want to make beautiful visualisation. I need to cluster my word2vec model and visualise it, but pyspark functions haven't got so much functions and i can't use TensorBoard. So i tried to load model by gensim and it's didn't work, because no test data</p>

<pre><code>from gensim.test.utils import datapath
from gensim.models import KeyedVectors

gensim_model = KeyedVectors.load_word2vec_format(datapath('w2v_pyspark'), binary=False)

IOError: [Errno 2] No such file or directory:u'/data/anaconda2/lib/python2.7/site-packages/gensim/test/test_data/w2v_pyspark'
</code></pre>

<p>I put my data from pyspark model to this path and it also didn't work. How can i solve this problem?
Also, if you know some ideas about how to cluster and visualise by PySpark, you are welcome!</p>
",2020-01-21 13:21:41,2020-01-21 13:21:41,How to convert PySpark Word2Vec model and load it how gensim Word2Vec model?,<python><machine-learning><pyspark><cluster-analysis><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24681,59878593,2020-01-23 12:28:35,,"<p>I trained a gensim hdp model on a large corpus (50GB of text, 160 million lines). The result was a set of 20 topics that all were nearly the same with a slight difference in words.</p>

<p>My pipeline for preprocessing input text includes trivial methods like <em>text normalization</em>, <em>stopword removal</em>, <em>calculate bigrams</em>, <em>tf-idf</em> and also <em>ignoring all the sentences with less than 20 words</em>. </p>

<p>Is there any straight forward approach for topic modeling with hdp or any similar method that gives more accurate results?</p>
",,2020-01-23 12:28:35,All topics in gensim hdp model converge to one topic,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
24692,59835900,2020-01-21 07:16:40,,"<p>I am working on Word2Vec model. Is there any way to get the ideal value for one of its parameter i.e <code>iter</code>. Like the way we used do in K-Means (Elbo curve plot) to get the K value.Or is there any other way for parameter tuning on this model.</p>
",,2020-01-21 19:33:13,Gensim Word2vec model parameter tuning,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24696,59865719,2020-01-22 18:00:05,,"<p>I am trying to get textual representation(or the closest word) of given word embedding using BERT. Basically I am trying to get similar functionality as in gensim:</p>

<pre><code>&gt;&gt;&gt; your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
&gt;&gt;&gt; model.most_similar(positive=[your_word_vector], topn=1))
</code></pre>

<p>So far, I have been able to generate contextual word embedding using <a href=""https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding"" rel=""nofollow noreferrer"">bert-as-service</a> but can't figure out how to get closest words to this embedding. I have used pre-trained bert model (uncased_L-12_H-768_A-12) and haven't done any fine tuning.</p>
",,2020-03-22 20:59:20,How to find the closest word to a vector using BERT,<nlp><word-embedding><bert-language-model>,,,CC BY-SA 4.0,False,False,True,False,False
24712,59917878,2020-01-26 11:28:58,,"<p>I am trying to find <strong>cosine similarity</strong> between <strong>two sentences</strong> of unequal length using word2vec google news corpus but i am getting the error: <code>AxisError: axis 1 is out of bounds for array of dimension 1</code></p>

<p>Below is my code:</p>

<pre><code>from gensim.models import KeyedVectors
EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

vocab = word2vec.vocab.keys()
wordsInVocab = len(vocab)

import numpy as np

def sent_vectorizer(sent, model):
    sent_vec = np.zeros(50)
    numw = 0
    for w in sent:
        try:
            vc=model[w]
            vc=vc[0:50]

            sent_vec = np.add(sent_vec, vc) 
            numw+=1
        except:
            pass
    return sent_vec / np.sqrt(sent_vec.dot(sent_vec))

a = sent_vectorizer('Football is played in Brazil',word2vec)
b =sent_vectorizer('Cricket is played in India',word2vec)

word2vec.cosine_similarities(b,a)
</code></pre>

<p>I am converting the sentences into vectors as cosine_similarity takes vector array as input.
How do i resolve this?</p>
",,2020-02-17 07:20:52,Cosine similarity between sentences using Google news corpus word2vec model python,<python-3.x><nlp><word2vec><cosine-similarity><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
24719,59949098,2020-01-28 12:46:50,,"<p>I try to build a gensim LDA model for topic modeling <a href=""https://i.stack.imgur.com/lmnU6.png"" rel=""nofollow noreferrer"">output of my trained model</a>. I need to give a real time input to check that document come under which topic. this model for question classification like Qunts, Reasoning and verbal question. Please help me.</p>
",,2020-01-28 12:46:50,How to classifiy document the Topics using Gensim LDA model after the Training(How to deploy the code)?,<input><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
24726,59919462,2020-01-26 14:51:28,,"<p>I'm trying to build a Word2vec (or FastText) model using Gensim on a massive dataset which is composed of 1000 files, each contains ~210,000 sentences, and each sentence contains ~1000 words. The training was made on a 185gb RAM, 36-core machine.
I validated that</p>

<pre><code>gensim.models.word2vec.FAST_VERSION == 1
</code></pre>

<p>First, I've tried the following:</p>

<pre><code>files = gensim.models.word2vec.PathLineSentences('path/to/files')
model = gensim.models.word2vec.Word2Vec(files, workers=-1)
</code></pre>

<p>But after 13 hours I decided it is running for too long and stopped it.</p>

<p>Then I tried building the vocabulary based on a single file, and train based on all 1000 files as follows:</p>

<pre><code>files = os.listdir['path/to/files']
model = gensim.models.word2vec.Word2Vec(min_count=1, workers=-1)
model.build_vocab(corpus_file=files[0])
for file in files:
    model.train(corpus_file=file, total_words=model.corpus_total_words, epochs=1)
</code></pre>

<p>But I checked a sample of word vectors before and after the training, and there was no change, which means no actual training was done.</p>

<p>I can use some advise on how to run it quickly and successfully. Thanks!</p>

<p><strong>Update #1:</strong></p>

<p>Here is the code to check vector updates:</p>

<pre><code>file = 'path/to/single/gziped/file'
total_words = 197264406 # number of words in 'file'
total_examples = 209718 # number of records in 'file'
model = gensim.models.word2vec.Word2Vec(iter=5, workers=12)
model.build_vocab(corpus_file=file)
wv_before = model.wv['9995']
model.train(corpus_file=file, total_words=total_words, total_examples=total_examples, epochs=5)
wv_after = model.wv['9995']
</code></pre>

<p>so the vectors: <code>wv_before</code> and <code>wv_after</code> are exactly the same</p>
",2020-01-28 12:18:10,2020-01-28 12:18:10,Speed Up Gensim's Word2vec for a Massive Dataset,<gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
24749,59872029,2020-01-23 05:25:42,,"<p>I am using word2vec (and doc2vec) to get embeddings for sentences, but i want to completely ignore word order.
I am currently using gensim, but can use other packages if necessary.</p>

<p>As an example, my text looks like this:</p>

<pre><code>[
['apple', 'banana','carrot','dates', 'elderberry', ..., 'zucchini'],
['aluminium', 'brass','copper', ..., 'zinc'],
...
]
</code></pre>

<p>I intentionally want 'apple' to be considered as close to 'zucchini' as it is to 'banana' so I have set the window size to a very large number, say 1000.
I am aware of 2 problems that may arise with this.</p>

<p>Problem 1:
The window might <em>roll</em> in at the start of a sentence creating the following training pairs:
<code>('apple', ('banana')), ('apple', ('banana', 'carrot')), ('apple', ('banana', 'carrot', 'date'))</code> before it eventually gets to the correct <code>('apple', ('banana','carrot', ..., 'zucchini'))</code>.
This would seem to have the effect of making 'apple' closer to 'banana' than 'zucchini',
since their are so many more pairs containing 'apple' and 'banana' than there are pairs containing 'apple' and 'zucchini'.</p>

<p>Problem 2:
I heard that pairs are sampled with inverse proportion to the distance from the target word to the context word- This also causes an issue making nearby words more seem more connected than I want them to be.</p>

<p>Is there a way around problems 1 and 2?
Should I be using cbow as opposed to sgns? Are there any other hyperparameters that I should be aware of?
What is the best way to go about removing/ignoring the order in this case?</p>

<p>Thank you</p>
",,2020-01-23 18:48:16,word2vec window size at sentence boundaries,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24765,59924168,2020-01-27 00:56:17,,"<p>I need to combine Word2Vec with my <code>CNN</code> model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (<em>a.k.a.</em> target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of <code>Word2Vec</code> as I need both of them in order to train my deep neural network?</p>

<p>p.s. I'm using <code>Gensim</code> implementation of <code>Word2Vec</code>.</p>

<p>p.s. My corpus has <strong>6,925</strong> sentences, and <code>Word2Vec</code> produces <strong>5,260</strong> vectors.</p>

<p><strong>Edit:</strong> More detail regarding my corpus (as requested):</p>

<p>The structure of the corpus is as follows:</p>

<ol>
<li><p>sentences (label: <code>positive</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
<li><p>sentences (label: <code>negative</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
</ol>

<p>Then all the sentences were given as the input to <code>Word2Vec</code>.</p>

<pre><code>word2vec = Word2Vec(all_sentences, min_count=1)
</code></pre>

<p>I'll feed my CNN with the extracted features (which is the <code>vocabulary</code> in this case) and the <code>targets</code> of sentences. So, I need these labels of the sentences as well.</p>
",2020-01-27 19:06:27,2020-01-27 19:06:27,Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?,<deep-learning><gensim><word2vec><one-hot-encoding><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
24772,59925207,2020-01-27 04:37:37,,"<p>I have created a word2vec model and have made a visualization of the top n similar words for a particular term using TSNE and matplotlib. What I do not understand is that when I run it multiple times, the same words are plotted to different positions even though the words and vectors are the same each time. Why is this the case? I have a feeling it has to do with the way TSNE reduces the dimensionality of the vectors. If this is the case is it really reliable to use this method of visualization since it will be different every time?</p>

<pre><code>model = Word2Vec.load(""a_w2v_model"")

topn_words_list = [x[0] for x in model.wv.most_similar(""king"",topn=3)]
topn_vectors_list = model[topn_words_list]

tsne = TSNE(n_components=2, verbose=1, perplexity=27, n_iter=300)
Y = tsne.fit_transform(topn_vectors_list)

fig, ax = plt.subplots()
ax.plot(Y[:, 0], Y[:, 1], 'o')
ax.set_yticklabels([]) #Hide ticks
ax.set_xticklabels([]) #Hide ticks

for i, word in enumerate(topn_words_list):
    plt.annotate(word, xy=(Y[i, 0], Y[i, 1]))
plt.show()
</code></pre>
",,2020-03-03 16:21:42,random points when visualizing word2vec embeddings using TSNE,<python><matplotlib><pca><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24780,59889710,2020-01-24 02:33:11,,"<p>I am trying to use gensim's file-based training (example from documentation below):</p>

<pre><code>from multiprocessing import cpu_count
from gensim.utils import save_as_line_sentence
from gensim.test.utils import get_tmpfile
from gensim.models import Word2Vec, Doc2Vec, FastText
 # Convert any corpus to the needed format: 1 document per line, words delimited by "" ""
corpus = api.load(""text8"")
corpus_fname = get_tmpfile(""text8-file-sentence.txt"")
save_as_line_sentence(corpus, corpus_fname)
 # Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()
 # Train models using all cores
w2v_model = Word2Vec(corpus_file=corpus_fname, workers=num_cores)
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores)
ft_model = FastText(corpus_file=corpus_fname, workers=num_cores)
</code></pre>

<p>However, my actual corpus contains many documents, each containing many sentences.
For example, let's assume my corpus is the plays of Shakespeare - Each play is a document, each document has many many sentences, and I would like to learn embeddings for each play, but the word embeddings only from within the same sentence.
Since the file-based training is meant to be one document per line, I assume that I should put one play per line. However, the documentation for file-based-training doesn't have an example of any documents with multiple sentences.
Is there a way to peek inside the model to see the documents and word context pairs that have been found before they are trained?</p>

<p>What is the correct way to build this file, maintaining sentence boundaries?</p>

<p>Thank you</p>
",,2020-01-25 02:35:19,Correct way to represent documents containing multiple sentences in gensim file-based training,<gensim><corpus><doc2vec><sentence>,,,CC BY-SA 4.0,False,False,True,False,False
24791,59909099,2020-01-25 12:13:50,,"<p>I'am using the word2vec model and I have a problem with storing and reading it.</p>

<pre><code>import gensim.models.keyedvectors as w2v
from gensim.models import KeyedVectors

word_vectors = w2v.wv
word_vectors.save(filepath + ""Vectors.bin"")

m = word2vec.KeyedVectors.load_word2vec_format(filepath + ""Vectors.bin"", binary=True)
</code></pre>

<p>I get following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte

</code></pre>

<p>In the following way the laoding will work:</p>

<pre><code>vectors = KeyedVectors.load(filepath + ""Vectors.bin"", mmap='r')
</code></pre>

<p>But If I then call</p>

<pre><code>vectors.similar_by_word(""cat"")
</code></pre>

<p>I get following error: 
TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'</p>

<p>What am I doing wrong?
How can I use the save_word2vec_format() function?</p>
",2020-01-25 20:26:11,2020-01-25 20:26:11,Gensim framework: Saving and storing word2vec keyed vectors,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24801,59938853,2020-01-27 21:01:18,,"<p>I am trying to tokenize the following sentence type: </p>

<p><code>""The item at issue is no. 3553.""</code></p>

<p>Every tokenizer I've tried so far returns the following (including a Punkt tokenizer trained on my corpus):</p>

<p><code>[[""the"", ""item"", ""at"", ""issue"", ""is"", ""no.""], [""3553.""]]</code></p>

<p>Adding a ""no"" abbreviation to the tokenizer would be a problem for sentences ending in ""no.""</p>
",2020-01-27 21:31:27,2020-02-04 06:42:24,"Stop sentence tokenizer from splitting sentence on ""no."" abbreviation",<python><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
24809,59926638,2020-01-27 07:22:16,,"<p>This is the code for my model using <strong>Gensim.i</strong> run it and it returned a tuple. I wanna know that which one is the number of tokens?</p>

<pre><code>model = gensim.models.Word2Vec(mylist5,size=100, sg=0, window=5, alpha=0.05, min_count=5, workers=12, iter=20, cbow_mean=1, hs=0, negative=15)

model.train(mylist5, total_examples=len(mylist5), epochs=10)
</code></pre>

<p>The value that was returned by my model is: I need to know what is this?</p>

<pre><code> (167131589, 208757070)
</code></pre>

<p>I wanna know what is the number of tokens?</p>
",2020-01-27 07:30:45,2020-01-27 20:03:11,How to find number of tokens in gensim model,<python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
24831,59891168,2020-01-24 06:05:47,,"<p>I have a set of text documents(2000+) with labels (Liked/Disliked).Each document consists of 200+ words.
I am trying to do a supervised learning with these documents.
<strong>My approach would be:</strong></p>

<ol>
<li>Vectorize each document in the corpus. Say we have 2347 docs.</li>
<li>I can have 2347 rows with labels viz. Like as 1 and Dislike as 0.</li>
<li>Using any ML classification supervised model train above dataset with 2347 rows.</li>
</ol>

<p><strong>How to vectorize and create such dataset?</strong></p>
",2020-01-24 12:03:14,2020-01-24 12:50:18,How to do supervised learning with Gensim/Word2Vec/Doc2Vec having large corpus of text documents?,<python><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
24834,60043276,2020-02-03 16:25:06,,"<p>I wrote a text classification program. When I run the program it crashes with an error as seen in this screenshot:</p>

<p><a href=""https://i.stack.imgur.com/wDoXv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wDoXv.jpg"" alt=""as seen in this screenshot""></a></p>

<blockquote>
  <p>ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.</p>
</blockquote>

<p>Here is my code:</p>

<pre><code>from sklearn.model_selection import train_test_split
from gensim.models.word2vec import Word2Vec
from sklearn.preprocessing import scale
from sklearn.linear_model import SGDClassifier
import nltk, string, json
import numpy as np

def cleanText(corpus):
    reviews = []
    for dd in corpus:
        #for d in dd:
        try:
            words = nltk.word_tokenize(dd['description'])
            words = [w.lower() for w in words]
            reviews.append(words)
            #break
        except:
            pass
    return reviews

with open('C:\\NLP\\bad.json') as fin:
    text = json.load(fin)
    neg_rev = cleanText(text)

with open('C:\\NLP\\good.json') as fin:
    text = json.load(fin)
    pos_rev = cleanText(text)

#1 for positive sentiment, 0 for negative
y = np.concatenate((np.ones(len(pos_rev)), np.zeros(len(neg_rev))))

x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_rev, neg_rev)), y, test_size=0.2)

n_dim = 1104
#Initialize model and build vocab
imdb_w2v = Word2Vec(size=n_dim, min_count=10)
imdb_w2v.build_vocab(x_train)

#Train the model over train_reviews
imdb_w2v.train(x_train, total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.iter)

#Build word vector for training set
def buildWordVector(text, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in text:
        try:
            vec += imdb_w2v[word].reshape((1, size))
            count += 1.
        except KeyError:
            continue
    if count != 0:
        vec /= count
    return vec

train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
train_vecs = scale(train_vecs)

#Train word2vec on test reviews
imdb_w2v.train(x_test, total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.iter)

#Build test vectors then scale
test_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_test])
test_vecs = scale(test_vecs)

#Use classification algorithm 
lr = SGDClassifier(loss='log', penalty='l1')
lr.fit(train_vecs, y_train)

print('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))
</code></pre>

<p>The data I am using is available here:</p>

<ol>
<li><p><a href=""https://github.com/SilverYar/TransportDataMiner/blob/master/bad.json"" rel=""nofollow noreferrer"">Bad</a>;</p></li>
<li><p><a href=""https://github.com/SilverYar/TransportDataMiner/blob/master/good.json"" rel=""nofollow noreferrer"">Good</a> </p></li>
</ol>

<p>How would I go about fixing this error?</p>
",2020-02-03 17:58:33,2020-08-11 11:30:10,"ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters",<python><scikit-learn><nlp>,,,CC BY-SA 4.0,True,False,True,False,True
24887,60096180,2020-02-06 13:32:30,,"<p>I adapted the following code from Susan Li's <a href=""https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"" rel=""nofollow noreferrer"">post</a>, but incurred an error when the code tries to tokenize text using <code>NLTK</code>'s resources (or, there could be something wrong with ""keyed vectors"" loaded from the web). The error occurred on the 5th code block (see below, might take a while to load from the web):</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>## 1. load packages and data

import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk import sent_tokenize
STOPWORDS = set(stopwords.words('english'))
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import re
from bs4 import BeautifulSoup
%matplotlib inline

df = pd.read_csv('https://www.dropbox.com/s/b2w7iqi7c92uztt/stack-overflow-data.csv?dl=1')
df = df[pd.notnull(df['tags'])]

my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']

## 2. cleaning

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):

    text = BeautifulSoup(text, ""lxml"").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text
    
df['post'] = df['post'].apply(clean_text)

## 3. train test split

X = df.post
y = df.tags
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)

## 4. load keyed vectors from the web: will take a while to load

import gensim
word2vec_path = ""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz""
wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
wv.init_sims(replace=True)


## 5. this is where it goes wrong

def w2v_tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text, language='english'):
        for word in nltk.word_tokenize(sent, language='english'):
            if len(word) &lt; 2:
                continue
            tokens.append(word)
    return tokens
    
train, test = train_test_split(df, test_size=0.3, random_state = 42)

test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values
train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values

X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)


## 6. perform logistic regression test

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg = logreg.fit(X_train_word_average, train['tags'])
y_pred = logreg.predict(X_test_word_average)
print('accuracy %s' % accuracy_score(y_pred, test.tags))
print(classification_report(test.tags, y_pred,target_names=my_tags))</code></pre>
</div>
</div>
</p>

<p><strong>Update on part 5</strong> (per <code>@luigigi</code>'s comments)</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>## 5. download nltk and use apply() function without using lambda

import nltk
nltk.download()
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk import sent_tokenize

    def w2v_tokenize_text(text):
        tokens = []
        for sent in nltk.sent_tokenize(text, language='english'):
            for word in nltk.word_tokenize(sent, language='english'):
                if len(word) &lt; 2:
                    continue
                tokens.append(word)
        return tokens
        
    train, test = train_test_split(df, test_size=0.3, random_state = 42)

    def w2v_tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text, language='english'):
        for word in nltk.word_tokenize(sent, language='english'):
            if len(word) &lt; 2:
                continue
            tokens.append(word)
    return tokens
    
train, test = train_test_split(df, test_size=0.3, random_state = 42)

test_tokenized = test['post'].apply(w2v_tokenize_text).values

train_tokenized = train['post'].apply(w2v_tokenize_text).values

    X_train_word_average = word_averaging_list(wv,train_tokenized)
    X_test_word_average = word_averaging_list(wv,test_tokenized)

## now run the test

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg = logreg.fit(X_train_word_average, train['tags'])
y_pred = logreg.predict(X_test_word_average)
print('accuracy %s' % accuracy_score(y_pred, test.tags))
print(classification_report(test.tags, y_pred,target_names=my_tags))</code></pre>
</div>
</div>
</p>

<p>This should work.</p>
",2020-02-06 15:38:43,2020-02-06 15:38:43,"Error in loading NLTK resources: ""Please use the NLTK Downloader to obtain the resource:\n\n""",<python><nltk><tokenize><word2vec>,,,CC BY-SA 4.0,True,False,True,False,True
24906,60077757,2020-02-05 14:14:57,,"<p>I am using gensim's LDA implementation, and I would like to test the mallet LDA implementation. My problem is that I run gensim from a container, so how can specify the path to the mallet executable in such a case? I have seen that in hub docker there is a container for mallet, so that is not a problem. </p>

<p>Does any one know how to specify the path to mallet in a containerized application?</p>
",2020-02-05 16:58:16,2020-02-05 16:58:16,gensim wrapper for mallet lda with container image,<containers><gensim><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
24918,60051877,2020-02-04 06:40:27,,"<pre><code>corpus = corpus_tfidf
from gensim.models import CoherenceModel
coherence_score=[]
for i in range(2,16):
        model=gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary,num_topics=i)
        coherence_model=CoherenceModel(model,texts=preprocessed_texts,dictionary=dictionary
        ,coherence='c_v')
        coherence_lda=coherence_model.get_coherence()
        print('n =', i,'Score :', coherence_lda)
        coherence_score.append(coherence_lda)
</code></pre>

<p><code></code> `</p>

<p>the result is like that : </p>

<p>n = 2 Score : 0.6117061117743858</p>

<p>n = 3 Score : 0.6117061117743858</p>

<p>n = 4 Score : 0.6117061117743858</p>

<p>n = 5 Score : 0.6117061117743858</p>

<p>n = 6 Score : 0.6117061117743859</p>

<p>n = 7 Score : 0.6117061117743859</p>

<p>n = 8 Score : 0.6117061117743858</p>

<p>n = 9 Score : 0.6117061117743858</p>

<p>n = 10 Score : 0.6117061117743858</p>

<p>n = 12 Score : 0.6117061117743859</p>

<p>n = 13 Score : 0.6117061117743859</p>

<p>n = 14 Score : 0.6117061117743859</p>

<p>n = 15 Score : 0.6117061117743859</p>

<p>All result is same. What is the problem? </p>
",,2020-02-04 06:40:27,python topicmodeling error : coherence error,<python><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
24922,60081431,2020-02-05 17:50:25,,"<p>I am trying to load a fasttext .bin model in spanish, donwloaded from <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a> and continue training it with new sentences from the specific domain I am interested in.  </p>

<p>System: Anaconda, Jupyter Notebook, Python 3.6, Upgraded Gensim </p>

<p>My code (toy example):</p>

<pre><code>from gensim.models.fasttext import load_facebook_model
import os
os.chdir('path/to/directory')
model = load_facebook_model('cc.es.300.bin')

'enmadrarse' in model.wv.vocab
&gt;&gt;&gt; False
old_vector = np.copy(model.wv['enmadrarse'])

new_sentences = [['complexidad', 'cataratas', 'enmadrarse'],
['enmadrarse', 'cataratas', 'increibles'], 
['unidad','enmadrarse','complexa']]

model.build_vocab(new_sentences, update = True)
model.train(new_sentences, total_examples = len(new_sentences), epochs=model.epochs)

new_vector = np.copy(model.wv['enmadrarse'])
np.allclose(old_vector, new_vector, atol=1e-4)
&gt;&gt;&gt; True

'enmadrarse' in model.wv.vocab
&gt;&gt;&gt; False (still)
</code></pre>

<p>The old and new vectors of the word are equal and it remains out of the vocab so the model learnt nothing. What am I doing wrong? </p>
",,2020-02-05 17:50:25,Fasttext model loaded with gensim won't continue training with new sentences,<python><gensim><fasttext><resuming-training>,,,CC BY-SA 4.0,False,False,True,False,False
24923,60082606,2020-02-05 19:10:33,,"<p>I modified some <code>Python</code> code from <code>github</code> to run logistic regression on a subset of consumer complaints data using the following code, the text vectorization and classification parts work smoothly. But I am wondering if it's possible to also include non-text, binary numerical indicators, such as <code>timely_response</code> and <code>consumer_disputed.</code> as features (alongside text vectors)?
However, when I did this, <code>Python</code> returns an error saying that I have <code>input variables with inconsistent numbers of samples</code>.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>%% load packages and data
import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
STOPWORDS = set(stopwords.words('english'))
from bs4 import BeautifulSoup
from IPython.core.interactiveshell import InteractiveShell

df = pd.read_csv('https://www.dropbox.com/s/obbs000w7knjmys/example_complaints.csv?dl=1')
df = df[pd.notnull(df['consumer_complaint_narrative'])]

df['product'].value_counts()

%% cleaning text

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """"""
        text: a string
        
        return: modified initial string
    """"""
    text = BeautifulSoup(text, ""lxml"").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text
    
df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(clean_text)

%% include only text as features

X = df['consumer_complaint_narrative']
y = df['product']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)

%% fit and test with logistic regression

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import classification_report

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(n_jobs=1, C=1e5)),
               ])
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
my_tags = ['Debt collection','Mortgage','Credit reporting','Credit card','Bank account or service','Consumer Loan','Student loan','Payday loan','Money transfers','Other financial service','Prepaid card']

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))


%% including binary numerical indicators as additional features

new_X = df[['consumer_complaint_narrative', 'timely_response', 'consumer_disputed.']]
y = df['product']
X_train, X_test, y_train, y_test = train_test_split(new_X, y, test_size=0.25, random_state = 42)

%% fit and test again

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(n_jobs=1, C=1e5)),
               ])
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))</code></pre>
</div>
</div>
</p>

<p>which returns the following error message</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    &lt;ipython-input-28-455c8fd83ba4&gt; in &lt;module&gt;
          8                 ('clf', LogisticRegression(n_jobs=1, C=1e5)),
          9                ])
    ---&gt; 10 logreg.fit(X_train, y_train)
         11 
         12 y_pred = logreg.predict(X_test)

    ~\Anaconda3\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
        265         Xt, fit_params = self._fit(X, y, **fit_params)
        266         if self._final_estimator is not None:
    --&gt; 267             self._final_estimator.fit(Xt, y, **fit_params)
        268         return self
        269 

    ~\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py in fit(self, X, y, sample_weight)
       1286 
       1287         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=""C"",
    -&gt; 1288                          accept_large_sparse=solver != 'liblinear')
       1289         check_classification_targets(y)
       1290         self.classes_ = np.unique(y)

    ~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
        764         y = y.astype(np.float64)
        765 
    --&gt; 766     check_consistent_length(X, y)
        767 
        768     return X, y

    ~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_consistent_length(*arrays)
        233     if len(uniques) &gt; 1:
        234         raise ValueError(""Found input variables with inconsistent numbers of""
    --&gt; 235                          "" samples: %r"" % [int(l) for l in lengths])
        236 
        237 

    ValueError: Found input variables with inconsistent numbers of samples: [3, 529]</code></pre>
</div>
</div>
</p>

<p>Will be really grateful if someone could shed some lights on this.</p>
",2020-02-06 05:23:16,2020-02-06 13:10:14,Incorporating additional numeric features into text classification model,<python><nltk><logistic-regression><feature-engineering>,,,CC BY-SA 4.0,True,False,True,False,True
24944,60055813,2020-02-04 10:54:13,,"<p>I used the following code to using glove vectors for word embeddings</p>

<pre><code>from gensim.scripts.glove2word2vec import glove2word2vec    #line1
glove_input_file = 'glove.840B.300d.txt'  #line2
word2vec_output_file = 'glove.word2vec'   #line3
glove2word2vec(glove_input_file, word2vec_output_file)  #line4
from gensim.models import KeyedVectors  #line5
glove_w2vec = KeyedVectors.load_word2vec_format('glove.word2vec', binary=False) #line6
</code></pre>

<p>I understand this chunk of code is for using glove pretrained vectors for your word embeddings. But I am not sure of what is happening in each line. Why to convert glove to word2vec format ? What does KeyedVectors.load_word2vec_format does exactly ?</p>
",,2020-02-05 02:19:48,Understanding usage of glove vectors,<python><nlp><word2vec><word-embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
24951,60085407,2020-02-05 23:06:39,,"<p>I got an error:</p>

<pre><code>Error when checking input: 
expected embedding_1_input to have shape (50,) but got array with shape (1,)
</code></pre>

<p>When I change the input parameter <code>input_length</code> to 1, the error becomes:</p>

<pre><code>Error when checking input: 
expected embedding_1_input to have shape (1,) but got array with shape (50,)
</code></pre>

<p>My code is as below:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
import numpy as np
import os
from keras import metrics
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, BatchNormalization, Activation, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D
from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.preprocessing import sequence, text
import pandas as pd
from gensim import corpora
from gensim import models

maxlen = 50
batch_size = 128
np.random.seed(7)

df = pd.read_csv('C:/Users/DMY/Peer-logic-master/newdata/topnine.csv',encoding='utf-8')

x = df[""REVIEW""].fillna(""na"").values  
y = df[""TAG""]
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)

word_list = []

for i in range(len(x_train)):
    word_list.append(x_train[i].split(' '))


dictionary = corpora.Dictionary(word_list)
corpus = [dictionary.doc2bow(text) for text in word_list]


tfidf = models.TfidfModel(corpus)

X_train_id = [] 
word_id_dict = dictionary.token2id
for i in range(len(word_list)):
    sen_id = []
    word_sen = word_list[i]
    for j in range(len(word_sen)):       
        id = word_id_dict.get(word_sen[j])
        if id is None:
            id = 0 
        sen_id.append(id)
    X_train_id.append(sen_id)

X_train_tfidf_vec = []  
for i in range(len(x_train)):
    temp = {}
    string = x_train[i]
    string_bow = dictionary.doc2bow(string.lower().split()) 
    string_tfidf = tfidf[string_bow]

    for j in range(len(string_tfidf)):
#         print(string_tfidf[j][0])
        temp[string_tfidf[j][0]] = string_tfidf[j][1]
#         print(temp)
    X_train_tfidf_vec.append(temp)

X_train_tfidf = []  
for i in range(len(X_train_id)):
    sen_id = X_train_id[i]
    sen_id_tfidf = X_train_tfidf_vec[i]
    sen = []
    for j in range(len(sen_id)):
        word_id = sen_id[j]
        word_tfidf = sen_id_tfidf.get(word_id)
        if word_tfidf is None:
            word_tfidf = 0
        sen.append(word_tfidf)
    X_train_tfidf.append(sen)

x_train_tfidf = sequence.pad_sequences(X_train_tfidf, maxlen=maxlen,dtype='float64')
#print(len(x_train_tfidf))
#print(x_train_tfidf)

model4 = Sequential()
model4.add(Embedding(len(x_train_tfidf)+1, 100, input_length = ))#input_dim,output_dim,input_length
model4.add(Dropout(0.6))
model4.add(LSTM(100, recurrent_dropout=0.6))
model4.add(Dropout(0.6))
model4.add(Dense(1, activation='sigmoid'))
model4.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
model4_history = model4.fit(x_train_tfidf, y_train, batch_size=batch_size, epochs=7,
                            validation_split=0.1)
score4, acc4 = model4.evaluate(x_test, y_test,
                               batch_size=batch_size)
print('Test accuracy for LSTM Model is:', acc4)
y_pred4 = model4.predict(x_test)
y_pred4 = (y_pred4 &gt; 0.5)
print(classification_report(y_test, y_pred4))
</code></pre>
",2020-02-06 00:13:06,2020-02-06 00:13:06,"Error when checking input: expected embedding_1_input to have shape (50,) but got array with shape (1,)",<python><keras><lstm><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
24968,60087463,2020-02-06 03:50:41,,"<p>im following this tutorials <a href=""https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"" rel=""nofollow noreferrer"">https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0</a> and find problem. so my purpose on this code to make iterate it over the range of topics, alpha, and beta parameter values. so I can determine the optimal number of topics from the coherence score generated by alpha and beta</p>

<pre><code>def compute_coherence_values(corpus, dictionary, k, a, b):

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=10, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       alpha=a,
                                       eta=b,
                                       per_word_topics=True)

coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

return coherence_model_lda.get_coherence()
</code></pre>

<p>and then</p>

<pre><code>import numpy as np
import tqdm
grid = {}
grid['Validation_Set'] = {}
# Topics range
min_topics = 2
max_topics = 11
step_size = 1
topics_range = range(min_topics, max_topics, step_size)
# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.3))
alpha.append('symmetric')
alpha.append('asymmetric')
# Beta parameter
beta = list(np.arange(0.01, 1, 0.3))
beta.append('symmetric')
# Validation sets
num_of_docs = len(corpus)
corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), 
               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), 
               gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), 
               corpus]
corpus_title = ['75% Corpus', '100% Corpus']
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': []
                }
# Can take a long time to run
if 1 == 1:
    pbar = tqdm.tqdm(total=540)

    # iterate through validation corpuses
    for i in range(len(corpus_sets)):
        # iterate through number of topics
        for k in topics_range:
            # iterate through alpha values
            for a in alpha:
                # iterare through beta values
                for b in beta:
                    # get the coherence score for the given parameters
                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, 
                                                  k=k, a=a, b=b)
                    # Save the model results
                    model_results['Validation_Set'].append(corpus_title[i])
                    model_results['Topics'].append(k)
                    model_results['Alpha'].append(a)
                    model_results['Beta'].append(b)
                    model_results['Coherence'].append(cv)

                    pbar.update(1)
    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)
    pbar.close()
</code></pre>

<p>come out this error <strong>ValueError: Stop argument for islice() must be None or an integer: 0 &lt;= x &lt;= sys.maxsize.</strong></p>
",,2020-02-13 18:57:15,ValueError: Stop argument for islice() must be None or an integer: 0 <= x <= sys.maxsize on topic coherence,<python><python-3.x><long-integer><itertools><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25005,60108919,2020-02-07 07:30:54,,"<p>I have created a bigram model using gensim and the try to get the bigram sentences but it's not picking all bigram sentences why?</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.phrases import Phrases, Phraser
phrases = Phrases(sentences, min_count=1, threshold=1)
bigram_model = Phraser(phrases)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>
<p>Can anyone explain how to get all bigrams.</p>
<h1>Why only 'new_york' not 'the_mayor' and others?</h1>
",2020-06-20 09:12:55,2020-02-07 18:22:49,Why aren't all bigrams created in gensim's `Phrases` tool?,<python><nlp><gensim><n-gram><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
25007,60171782,2020-02-11 15:01:15,,"<p>I have issues with my memory and I was wondering if anybody has any idea about what could be happening. What I am trying to do is the following: I am processing some texts, so I am creating a vocabulary and the removing all words in the corpus that are not in the vocabulary. </p>

<p>So I have 2 big pandas series (train_x and test_x) that occupies around 4.3GB together (train_x is around 75%).</p>

<pre><code>&gt;&gt;&gt;train_x.head()
  1    list of blank space
  2    separated words that
  3    represent my processed text

&gt;&gt;&gt;type(train_x)
pandas.core.series.Series
</code></pre>

<p>Then, I create a vocabulary out of these texts with my own function. This increases the memory consumption up to 4.75GB. (Not going to post the function since is long and not really relevant for the issue)</p>

<pre><code>&gt;&gt;&gt;vocab
  {""list"", ""of"", ""blank"", ""space"",...}

&gt;&gt;&gt;type(vocab)
  set
</code></pre>

<p>Finally, I just want to create a list of lists, where each element is a word that appears in the vocabulary (the vocabulary size is only 20k, so there are actually a lot of words that are going to be removed). Therefore, I used the following function.</p>

<pre><code>def fitVocabulary(corpus, vocabulary):
    return [[w for w in line.split() if w in vocabulary] for line in corpus]

&gt;&gt;&gt;train_words = fitVocabulary(train_x, vocab)
&gt;&gt;&gt;test_words = fitVocabulary(test_x, vocab)
&gt;&gt;&gt;del train_x, test_x, vocab

&gt;&gt;&gt;train_words
  [[""list"", ""of"", ""blank"", ""space""], [""separated"", ""words"",...],...]


from sys import getsizeof
&gt;&gt;&gt;getsizeof(train_words)
  25105992                 #I think this is bytes??
</code></pre>

<p>However, when I perform this operation my memory consumption leaps to 11.5GB. I am not sure what could be causing this much consumption since I am trying to delete the variables I don't use anymore (I know Python do not necessarily collect garbage) and there is no more code in the environment for the moment, so no more reference to those variables.</p>

<p>Any idea about what is happening or suggestions about better ways of doing these operations are welcomed. (Ideally the final output is the list of lists, since I need to use this as an input for the Gensim Word2Vec model to create vectors for each word)</p>
",,2020-02-11 15:01:15,Transforming dataframe into a list of lists is bursting my memory - Python,<python><memory-management><memory-leaks><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
25023,60142106,2020-02-09 22:52:43,,"<p>Gensim <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">LDAModel</a> has parameters <code>iterations</code> and <code>passes</code> to control the number of training epochs, and callbacks to get information about convergence, but is there a possibility to stop the training when difference between two epochs is very small i.e. early stopping?</p>
",2020-02-12 15:53:35,2020-02-18 09:14:57,gensim LDAModel early stopping,<python><python-3.x><gensim><lda><early-stopping>,,,CC BY-SA 4.0,False,False,True,False,False
25037,60129419,2020-02-08 17:42:22,,"<p>I have a movie review data set which has two columns Review(Sentences) and Sentiment(1 or 0).</p>

<p>I want to create a classification model using word2vec for the embedding and a CNN for the classification.</p>

<p>I've looked for tutorials on youtube but all they do is create vectors for every words and show me the similar words. Like this-</p>

<pre><code>model= gensim.models.Word2Vec(cleaned_dataset, min_count = 2,  size = 100, window = 5)     
words= model.wv.vocab
simalar= model.wv.most_similar(""bad"")
</code></pre>

<p>I already have my dependent variable(y) which is my 'Sentiment' column all I need is the independent variable(X) which I can pass on to my CNN model. </p>

<p>Before using word2vec I used the Bag Of Words(BOW) model which generated a sparse matrix which was my independent(X) variable. How can I achieve something similar using word2vec?</p>

<p>Kindly correct me if I'm doing something wrong. </p>
",,2020-02-08 22:38:19,How to generate independent(X) variable using Word2vec?,<python><word2vec><sentiment-analysis>,,,CC BY-SA 4.0,False,False,True,False,False
25076,60246570,2020-02-16 08:03:17,,"<p>I created a Gensim LDA Model as shown in this tutorial: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></p>

<pre><code>lda_model = gensim.models.LdaMulticore(data_df['bow_corpus'], num_topics=10, id2word=dictionary, random_state=100, chunksize=100, passes=10, per_word_topics=True)
</code></pre>

<p>And it generates 10 topics with a log_perplexity of: </p>

<blockquote>
  <p>lda_model.log_perplexity(data_df['bow_corpus']) = -5.325966117835991</p>
</blockquote>

<p>But when I run the coherence model on it to calculate coherence score, like so:</p>

<pre><code>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_df['bow_corpus'].tolist(), dictionary=dictionary, coherence='c_v')
with np.errstate(invalid='ignore'):
    lda_score = coherence_model_lda.get_coherence()
</code></pre>

<p>My LDA-Score is nan. What am I doing wrong here?</p>
",,2020-02-16 08:45:14,Gensim LDA Coherence Score Nan,<python><machine-learning><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25081,60211920,2020-02-13 15:57:49,,"<p>I wrote the following code, 
But the following error shows, 
Please guide me.</p>

<pre><code>from gensim.models.wrappers import LdaMallet
import os
os.environ.update({'MALLET_HOME':r'C:/mallet'})
mallet_path = 'C:/mallet/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>Command 'C:/mallet/bin/mallet import-file --preserve-case
  --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\MHRADM~1\AppData\Local\Temp\316a7f_corpus.txt --output
  C:\Users\MHRADM~1\AppData\Local\Temp\316a7f_corpus.mallet' returned
  non-zero exit status 1.</p>
</blockquote>
",2020-02-13 16:31:31,2020-02-13 16:31:31,Topic modeling in LdaMallet,<gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
25087,60248118,2020-02-16 11:46:01,,"<p>I'm quite new to Python and coding in general, so I seem to have run into an issue.</p>

<p>I'm trying to run this code (credit to Matthew Mayo, whole thing can be found <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code># import warnings
# warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')
import sys
from gensim.corpora import WikiCorpus

def make_corpus (in_f, out_f):
    print(0)
    output = open(out_f, 'w', encoding = 'utf-8')
    print(1)
    wiki = WikiCorpus(in_f)
    print(2)
    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '/n')
        i += 1
        if i % 10000 == 0:
            print('Processed {} articles!'.format(i))
    print(3)
    output.close()
    print('Process complete!')



print('start')
if __name__ == '__main__':
    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
else:
    print(__name__)
</code></pre>

<p>However, the function branch seems to run partly, stopping at the <code>wiki = WikiCorpus(in_f)</code> - it never makes it to <code>print(2)</code> - and then exiting and repeating the beginning of the code, yielding no results. No error actually comes up, only a warning (<code>UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</code>).</p>

<p>The output is this:</p>

<pre><code>start
0
1
C:\Users\name\Anaconda3\lib\site-packages\gensim\utils.py:1254: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
start
__mp_main__
start
__mp_main__
start
__mp_main__
</code></pre>

<p>I've tried uninstalling all required packages (numpy, smart_open), as well as gensim itself (in an active conda environment), but nothing has changed.
Also, what is the difference between the <strong>main</strong> and the multiprocessing one?</p>

<p>-- Specifications: win64, py 3.7.3</p>

<p>Edit: after running logging at the DEBUG level, logging file</p>

<pre><code>2020-02-16 22:49:00,061:start: :13396 
2020-02-16 22:49:00,061:0 :13396 
2020-02-16 22:49:00,061:1 :13396 
2020-02-16 22:49:01,493:start: :22356 
2020-02-16 22:49:01,493:3 :22356 
2020-02-16 22:49:01,496:start: :25332 
2020-02-16 22:49:01,497:3 :25332 
2020-02-16 22:49:01,530:start: :7120 
2020-02-16 22:49:01,530:3 :7120 
2020-02-16 22:49:01,541:adding document #0 to Dictionary(0 unique tokens: []):13396
</code></pre>

<p>(also, the '3' was added in the <code>else</code> branch:)</p>

<pre><code>else:
    logging.debug('3 ')
</code></pre>
",2020-02-17 12:59:56,2020-02-18 21:13:46,Problems with gensim WikiCorpus - aliasing chunkize to chunkize_serial; (__mp_main__ instead of __main__?),<python><python-3.x><windows><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25113,60281456,2020-02-18 12:58:26,,"<p>I ma trying to learn NLP in python.</p>

<p>For that, i had this data called ""abstracts"", consisting of several rows with different setnences. After tokenizing them, we get a series called abstracts['tokenized']. This series has around 1,817 items.</p>

<p>Iam trying doc2bow function on one of its elements but am unable to understand the result. </p>

<p>Ideally, we should get the index of the word and its count of occurences in that element of the list, from doc2bow. </p>

<p>So for example, we have this string ""i have a cute dog with a cute tail""
doc2bow should return [(0,1),(1,1),(2,2),(3,2),(4,1),(5,1)]</p>

<p>But when running on this data (attached), I am not getting the same result. </p>

<p><strong>I am getting wrong frequencies for every word.</strong></p>

<p><a href=""https://docs.google.com/spreadsheets/d/1K8lpda32Tmp7ZikpCxY1HTloivgsW0DLq5qXHCreg_g/edit?usp=sharing"" rel=""nofollow noreferrer"">This file has 2 columns, one has raw data and other has doc2bow results.</a></p>

<pre><code>  type(abstracts['tokenized'])
  dictionary.doc2bow(abstracts['tokenized'][0])
  dictionary.doc2bow(abstracts['tokenized'][1])
  abstracts['tokenized'][1].index(""implementation"")
  corpus = [dictionary.doc2bow(abstract) for abstract in abstracts['tokenized']]
</code></pre>
",,2020-02-18 12:58:26,Interpreting gensim library's doc2bow function run over a series of words,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25118,60166663,2020-02-11 10:22:14,,"<p>While working on tasks like text classification, QA, the original vocabulary generated from the corpus is usually too large, containing a lot of 'unimportant' words. The most popular ways I've seen to reduce the vocabulary size are discarding stop words and words with low frequencies.</p>

<p>For example, in <code>gensim</code></p>

<pre><code>gensim.utils.prune_vocab(vocab, min_reduce, trim_rule=None):
    Remove all entries from the vocab dictionary with count smaller than min_reduce.
    Modifies vocab in place, returns the sum of all counts that were pruned.
</code></pre>

<p>But in practice, setting the minimum count is empirical and does not seems quite exact. I notice that the term frequency of each word in the vocabulary often follows long-tail distribution, is it a good way if I only keep the top-K words that occupies X% (95%, 90%, 85%, ...) of the total term frequency? Or are there any sensible ways to reduce the vocabulary, without seriously influencing the NLP task? </p>
",,2020-02-11 19:46:53,Are there good ways to reduce the size of a vocabulary in natural language processing?,<machine-learning><deep-learning><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25125,60185968,2020-02-12 10:17:04,,"<p>I am using the the Mallet LDA with gensims implemented wrapper.</p>

<p>Now I want to get the Topic distribution of several unseen documents, store it in a nested list and then print it out.</p>

<p>This is my code:</p>

<pre><code>other_texts = [
        ['wlan', 'usb', 'router'],
        ['auto', 'auto', 'auto'],
        ['human', 'system', 'computer']
 ]

corpus1 = [id2word.doc2bow(text) for text in other_texts]

to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc] # get topic probability distribution for a document
    to_pro.append(vector)
</code></pre>

<p>If I try to print the list <code>vector</code> it yields this result:</p>

<pre><code>[&lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC940&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC320&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC6A0&gt;]

</code></pre>

<p>I tried this code to print them out properly, but the probabilities of the topic distributuion are wrong:</p>

<pre><code>topic_dist = []
for line in to_pro:
    topic_dist += lda.get_document_topics(line)
td=[]
for topic in topic_dist:
    td.append(topic)
</code></pre>

<p>And I get this result:</p>

<pre><code>[[(0, 0.05458162849743133), (1, 0.05510823556400538), (2, 0.05603786367505091), (3, 0.05472256432318962), (4, 0.05471966342417517), (5, 0.05454446883678316), (6, 0.060267211268385176), (7, 0.05590590303517797), (8, 0.054558298009463865), (9, 0.0570497751708577), (10, 0.05586054626708894), (11, 0.05611284070096096), (12, 0.05483861615903838), (13, 0.054548627713420714), (14, 0.0548708631793431), (15, 0.055097199555668705), (16, 0.05572779508710042), (17, 0.05544789953285848)], [(0, 0.05457482739088479), (1, 0.05509130205455064), (2, 0.05599364448566309), (3, 0.05479472333893934), (4, 0.05489998490024729), (5, 0.054542940465732534), (6, 0.06014649090195501), (7, 0.0558787316629024), (8, 0.05455634249554292), (9, 0.05651159582517287), (10, 0.0558343047708517), (11, 0.05605027364084813), (12, 0.05483134591787102), (13, 0.054546952683828316), (14, 0.05488058477867337), (15, 0.0550725066190555), (16, 0.055951974201133244), (17, 0.055841473866147906)], [(0, 0.05457665942453363), (1, 0.055255130626316235), (2, 0.05616834056392741), (3, 0.05472749675259328), (4, 0.0547199851837743), (5, 0.054544546873748226), (6, 0.06037007389117332), (7, 0.05593838115178327), (8, 0.05456190582329174), (9, 0.056409168851414615), (10, 0.0559404965748031), (11, 0.05614914322415512), (12, 0.054842094317369555), (13, 0.054550171326841215), (14, 0.054870520851845996), (15, 0.05511732934346291), (16, 0.05579100118297473), (17, 0.05546755403599123)], [(0, 0.054581620307290336), (1, 0.05510823907508528), (2, 0.056037876384335425), (3, 0.05472256410518629), (4, 0.05471967034475046), (5, 0.05454446871605657), (6, 0.06026693118061518), (7, 0.05590622478877356), (8, 0.054558295773128575), (9, 0.05704995161755483), (10, 0.05586057502348091), (11, 0.056112803329985396), (12, 0.05483861481767718), (13, 0.05454862663175604), (14, 0.054870865577993026), (15, 0.055097113943380405), (16, 0.05572773919917307), (17, 0.055447819183777246)], [(0, 0.05457482815837349), (1, 0.05509132071436994), (2, 0.05599364089981504), (3, 0.05479471920764724), (4, 0.05489999995707833), (5, 0.05454293700828862), (6, 0.06014645177706313), (7, 0.05587868116251209), (8, 0.05455634846240247), (9, 0.056511585085478364), (10, 0.055834295810939794), (11, 0.056050296895854265), (12, 0.054831353686471636), (13, 0.05454695325610574), (14, 0.05488059866846103), (15, 0.055072528844072065), (16, 0.05595218064057245), (17, 0.05584127976449436)], [(0, 0.054576657976703774), (1, 0.05525504608539575), (2, 0.05616829811928526), (3, 0.05472749878845379), (4, 0.05471997497183866), (5, 0.054544547686709126), (6, 0.06037016659013718), (7, 0.05593821008515276), (8, 0.05456190840675052), (9, 0.05640917964821885), (10, 0.05594054039873076), (11, 0.05614912143569156), (12, 0.0548420823035294), (13, 0.054550172872614225), (14, 0.054870521717331436), (15, 0.055117319561282), (16, 0.05579110737872705), (17, 0.055467645973447846)], [(0, 0.054581639915369816), (1, 0.055108252268374285), (2, 0.056037916094392765), (3, 0.05472256597071497), (4, 0.05471966744573819), (5, 0.0545444687939403), (6, 0.06026693966026536), (7, 0.055906213964449725), (8, 0.05455829555351338), (9, 0.05704968653857304), (10, 0.0558606261827436), (11, 0.05611290790292455), (12, 0.05483860593828801), (13, 0.05454862649308445), (14, 0.05487085805236639), (15, 0.05509715099521129), (16, 0.05572773695595529), (17, 0.05544784127409454)], [(0, 0.05457482754746605), (1, 0.05509132328696252), (2, 0.055993666140583764), (3, 0.05479472184721206), (4, 0.05489996963702654), (5, 0.05454294168997213), (6, 0.060146365105445465), (7, 0.05587886571230439), (8, 0.05455633757025994), (9, 0.056511632004648656), (10, 0.055834239764847755), (11, 0.05605028881626678), (12, 0.054831347261978546), (13, 0.05454695137813789), (14, 0.05488060185684171), (15, 0.05507250450434276), (16, 0.055951827151308337), (17, 0.05584158872439472)], [(0, 0.05457665857245025), (1, 0.05525503335748317), (2, 0.05616811411295409), (3, 0.054727501563580076), (4, 0.054719978109952404), (5, 0.05454454660618627), (6, 0.060370135879343034), (7, 0.05593823717454384), (8, 0.05456190762146366), (9, 0.056409205316000424), (10, 0.05594060935464846), (11, 0.056149148701409454), (12, 0.05484207733245972), (13, 0.054550172010398135), (14, 0.05487051175914863), (15, 0.05511731933953272), (16, 0.055791267296383236), (17, 0.055467575892062346)]]
</code></pre>

<p>However printing one element from the list yields the correcr results:</p>

<pre><code>to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc[1]] # specifying document at index 1
    to_pro.append(vector)
</code></pre>

<pre><code>[[(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240289), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)]]
</code></pre>

<p>Another problem is, that for one document, the same distribution is printed 3 times. </p>

<p>I also looked into this answer: <a href=""https://stackoverflow.com/questions/45317151/gensim-interfaces-transformedcorpus-how-use"">gensim.interfaces.TransformedCorpus - How use?</a>, but it didnt help.</p>

<p>What am I doin wrong here? </p>
",,2020-02-12 10:44:59,Transforming a gensim.interfaces.TransformedCorpus to a readable result,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25127,60318511,2020-02-20 11:05:14,,"<p>i'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command <strong>!pip install gensim</strong> before i import it, but it still can not be imported, and the error said <strong>ModuleNotFoundError: No module named 'gensim'</strong></p>

<pre><code>!pip install gensim
import gensim
from gensim.models import KeyedVectors
model = KeyedVectors.load('model_fasttext2.vec')
model.vector_size
------------------------------------------------------------------------
Requirement already satisfied: gensim in c:\users\ip-03\anaconda3\lib\site-packages (3.8.1)
Requirement already satisfied: scipy&gt;=0.18.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.4.1)
Requirement already satisfied: six&gt;=1.5.0 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.14.0)
Requirement already satisfied: smart-open&gt;=1.8.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.9.0)
Requirement already satisfied: numpy&gt;=1.11.3 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.18.1)
Requirement already satisfied: boto&gt;=2.32 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.49.0)
Requirement already satisfied: boto3 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.12.3)
Requirement already satisfied: bz2file in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (0.98)
Requirement already satisfied: requests in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.22.0)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.3.3)
Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.3 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.15.3)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.9.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2019.11.28)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.25.8)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8.1)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.15.2)
</code></pre>

<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-10-ee4a48d372cd&gt; in &lt;module&gt;
      1 get_ipython().system('pip install gensim')
----&gt; 2 import gensim
      3 from gensim.models import KeyedVectors
      4 model = KeyedVectors.load('model_fasttext2.vec')
      5 model.vector_size

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention </p>
",,2020-02-20 11:21:27,No module named 'gensim' but already installed it,<python><machine-learning><jupyter-notebook><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
25132,60286735,2020-02-18 17:47:38,,"<p>I don't have large corpus of data to train word similarities e.g. 'hot' is more similar to 'warm' than to 'cold'. However, I like to train doc2vec on a relatively small corpus ~100 docs so that it can classify my domain specific documents. </p>

<p><strong>To elaborate let me use this toy example.</strong> Assume I've only 4 training docs given  by 4 sentences - ""I love hot chocolate."", ""I hate hot chocolate."", ""I love hot tea."", and ""I love hot cake."".
Given a test document ""I adore hot chocolate"", I would expect, doc2vec will invariably return ""I love hot chocolate."" as the closest document. This expectation will be true if word2vec already supplies the knowledge that ""adore"" is very similar to ""love"". However, I'm getting most similar document as ""I hate hot chocolate"" -- which is a bizarre!!</p>

<p>Any suggestion on how to circumvent this, i.e. be able to use pre-trained word embeddings so that I don't need to venture into training ""adore"" is close to ""love"", ""hate"" is close to ""detest"", and so on.</p>

<p><strong>Code (Jupyter Nodebook. Python 3.7. Jensim 3.8.1)</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [""I love hot chocolate."",
        ""I hate hot chocolate"",
       ""I love hot tea."",
       ""I love hot cake.""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
print(tagged_data)
#Train and save
max_epochs = 10
vec_size = 5
alpha = 0.025


model = Doc2Vec(vector_size=vec_size, #it was size earlier
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    if epoch % 10 == 0:
        print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.epochs) #It was model.iter earlier
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

print(""Model Ready"")

test_sentence=""I adore hot chocolate""
test_data = word_tokenize(test_sentence.lower())
v1 = model.infer_vector(test_data)
#print(""V1_infer"", v1)

# to find most similar doc using tags
sims = model.docvecs.most_similar([v1])
print(""\nTest: %s\n"" %(test_sentence))
for indx, score in sims:
    print(""\t(score: %.4f) %s"" %(score, data[int(indx)]))
</code></pre>
",2020-02-18 18:21:27,2020-02-18 20:49:40,Gensim's Doc2Vec - How to use pre-trained word2vec (word similarities),<python><nlp><gensim><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
25142,60301796,2020-02-19 13:45:52,,"<p>I have a large number of sentences in a database and I want to find the most similar of those sentences to a single sentence that the user types in.  </p>

<p>It looks like I may be able to do this with <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"" rel=""nofollow noreferrer"">annoy and gensim</a>, but all the examples I can see are using word2vec which I believe is good for finding single similar words, but not for sentences.  However, I note that the AnnoyIndexer() can take a word2vec OR a doc2vec model.</p>

<p>Am I correct that the process is the same, but swapping the word2vec model with a doc2vec model and using a doc2vec vector of the search sentence?</p>

<p>Do I need to use pre-trained word embeddings in any way, or do I literally just train the doc2vec model with the corpus of sentences that I have in my database?</p>

<p>Thank you!</p>
",,2020-02-19 21:40:11,Gensim and Annoy for finding similar sentences,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25158,60254825,2020-02-17 01:20:24,,"<p>Below is the output that I get using Gensim Mallet wrapper. From this SO <a href=""https://stackoverflow.com/questions/8447393/how-to-understand-the-output-of-topic-model-class-in-mallet"">link</a> I understood that LL/token means ""<strong>model's log-liklihood divided by the total number of tokens</strong>"". 1) However, for few topics like (1,8,11 etc.) I do not see any terms at all. 2) I tried to run the code for a range of topics from (10,20,2) (step of 2 starting from 10-20). But the output shows 17 as the last topic generated. I am missing something here..</p>

<pre><code>0       2.77778 watch 
1       2.77778 
2       2.77778 receive tape hope purchase 
3       2.77778 dvds wildlife pass yr interested 
4       2.77778 dvd version walk bored 
5       2.77778 volume courtyard trilogy 
6       2.77778 crazy picture minute 
7       2.77778 neighbor 
8       2.77778 
9       2.77778 buy mice trouble stay versus feeder 
10      2.77778 inside stir tv mine life bird wonderful year fascinated 
11      2.77778 
12      2.77778 
13      2.77778 recommend test real prefer greenery 
14      2.77778 age 
15      2.77778 funny triliogy play friend full minute 
16      2.77778 
17      2.77778 time tree 

&lt;950&gt; LL/token: -22.17456
&lt;960&gt; LL/token: -22.22132
&lt;970&gt; LL/token: -22.24897
&lt;980&gt; LL/token: -22.11585
&lt;990&gt; LL/token: -22.38062
</code></pre>
",,2020-02-17 15:19:52,Gensim Mallet: Output does not have terms for few topics,<nlp><gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
25162,60305405,2020-02-19 16:51:58,,"<p>I am building a Document Vector in Gensim.</p>

<p>I have a custom list of tags (called url) I want to use for the TaggedDocument:</p>

<pre><code>documents = [[TaggedDocument(doc, str(ur)) for ur in url] for doc in space]
</code></pre>

<p>Every custom url is a sequence of 10 numbers.</p>

<p>The result looks like this:</p>

<pre><code>[TaggedDocument(words=['googl', 'ibm', 'quantenrechnpentium', 'fdiv', 'bug', 'vierteljahrhundert', 
'intel', 'geld', 'technikquantencomput', 'computibm', 'googl', 'beweis', 
'quantum', 'supremacyforsch', 'natursycamor', 'qubit', 'groessenordn', 'supercomput', 'bauartibm', 
'algorithmus', 'rechn', 'quant', 'quantenrechn', 'werkzeug', 'rechenfehl', 'intel', 'erfahr', 
'professor', 'drthomas', 'fdiv', 'bug', 'intel', 'pentium', 'intel', 'fehl', 'austauschprogramm', 
'intel', 'sorgdesktop', 'pc', 'prozessor', 'ghz', 'cor', 'fertigungstechn', 'stromauflag', 'pc', 
'bauvorschlaeg', 'amd', 'ryzdesktop', 'schwaech', 'intel', 'geldquartal', 'rekordumsatz', 
'milliard', 'dollarmilliard', 'preis', 'geld', 'fertigungsanlag', 'servermarkt', 'ausblick', 'amd', 
'umsatz', 'jahresvergleich', 'anleg', 'ryz', 'epycs', 'ausblick', 'amd', 'erfolgmicrosoft', 'ryz', 
'kund', 'surfac', 'notebook', 'gruend', 'journalist', 'surfac', 'testgeraet', 'ryz', 'ryz', 
'vergleich', 'intel', 'cor', 'processor', 'conferenc', 'intel', 'atomkern', 'tremont', 
'mikroarchitektur', 'atom', 'prozessor', 'nanomet', 'fertigelkhart', 'lak', 'embedded', 'systemiot', 
'devic', 'skyhawk', 'lak', 'chips', 'nasatom', 'celeron', 'pentium', 'silv', 'tremont', 'goldmont', 
'plusceleron', 'tremont', 'cor', 'kern', 'lakefield', 'microsoft', 'surfac', 'neo', 'kombination', 
'cor', 'tremont', 'intel', 'atom', 'kern', 'cach', 'serv', 'tremont', 'risc', 'spezialist', 'sifiv', 
'kern', 'aussichtnanomet', 'chipsarm', 'cortex', 'raspberry', 'pi', 'rechenpowspannend', 'erweiter', 
'sichsogenannt', 'world', 'ids', 'trennung', 'welt', 'risc', 'socsprozess', 'cpu', 'kerncach', 
'zeil', 'ram', 'adressbereich', 'world', 'id', 'sicherheitslueck', 'spectr', 'art', 'schreck', 
'prozessor', 'markt', 'microsoft', 'sich', 'business', 'notebook', 'sogenannt', 'secured', 'cor', 
'pcs', 'massnahmstart', 'betriebssystem', 'notebook', 'bios', 'bios', 'windows', 'updat', 'uefi', 
'capsul', 'updattechnik', 'linuxsecured', 'cor', 'pcs', 'microsoft', 'angab', 'mobilrechn', 
'business', 'version', 'surfacuefi', 'bios', 'kern', 'microsoft', 'rahm', 'project', 'artikel'], tags='4577911')
</code></pre>

<p>As you can see at end is the field <code>tags</code> with my custom number. </p>

<p>After training the model with:</p>

<pre><code>model = Doc2Vec(docu, vector_size=5, window=2, min_count=2, dm =1)
</code></pre>

<p>I am runnning a similarity query with unseen Test Documents:</p>

<pre><code>rank = []
for line in test:
    tokens = line.split()

    new_vector = model.infer_vector(tokens)
    sims = model.docvecs.most_similar([new_vector])
    rank.append(sims)
</code></pre>

<p>Which yields this for the first document:</p>

<pre><code>[('8', 0.9214882850646973),('9', 0.919198751449585), 
('0', 0.9049716591835022), ('1', 0.9047936797142029), 
('6', 0.9028873443603516), ('2', 0.8913612365722656), 
('3', 0.8857095837593079), ('7', 0.8747860789299011), 
('5', 0.8512719869613647), ('4', 0.8370641469955444)]
</code></pre>

<p>As you can see the tags at index 0 of every sublist are not custom tags but generic numbers.</p>

<p>What am I doing wrong here?</p>
",2020-02-19 16:58:09,2020-02-19 21:43:33,Using custom Tags for Tagged Documents in Gensim,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25169,60324422,2020-02-20 16:23:37,,"<p>I am trying to implement Gensim LDA model on a dataset that contains approximately 4000 reviews that I got online. </p>

<p>Currently, I am able to achieve a <strong>perplexity of 6.1012985653792</strong> and a <strong>coherence score of 0.5448743855536398</strong> using the default parameters. However, the corresponding topic-keywords and reviews seem to be quite far off and I cannot see any relationship between them. I am having a hard time fine-tuning and understanding the parameters.</p>

<p>What parameters are the most important in the Gensim LDA model and what can I do to find out which is the best parameters for my dataset?</p>

<p>I have just learned LDA and I would really appreciate any help. Thank You!</p>
",,2020-02-20 16:23:37,Fine tune parameters in Gensim LDA model,<gensim><lda><topic-modeling>,2020-02-25 15:17:40,,CC BY-SA 4.0,False,False,True,False,False
25170,60324473,2020-02-20 16:25:37,,"<p>I am trying to save HTML files generated from <code>pyldavis</code> as multiple files every time a range is selected but getting an error while doing it. I want files to be generated as <code>malletresults1</code>, <code>malletresults2</code>, <code>malletresults3</code>. But this is giving me an error, also the file is not getting saved as an HTML file. 
'''</p>

<pre><code>def create_lda_optimize(self,texts,topics_range,corpus,dictionary,repetitions=4,visualize = True):
        coherence_values = []
        model_list = []
        j = 0
        #topics_range = list(range(10,20,1))
        mallet_path = '/home/ResearchAndDevelopment/RD003-TopicModelling/'
        for i in range(repetitions):
            ccv = []
            for num_topics in topics_range:
                model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, 
                                                         num_topics=num_topics, id2word=dictionary)
                model_list.append(model)
                lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)
                vis = pyLDAvis.gensim.prepare(lda_model,corpus,dictionary,mds = 'mmds') 
                name = str(i)
                filename = ""malletresults""+name
                if not os.path.exists(os.path.dirname(filename)):
                    try:
                        os.makedirs(os.path.dirname(filename))
                    except OSError as exc:                         
                        with open(filename, ""w"") as f:                                              
                            pyLDAvis.save_html(vis,filename+"".html"")
                            name+1
                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
                ccv.append(coherencemodel.get_coherence())
            coherence_values.append(ccv)
            print(model_list, coherence_values)
        def _mean(a):
            return sum(a) / len(a)
        avg_cv = list(map(_mean, zip(*coherence_values)))
        numberOfTopics = topics_range[avg_cv.index(max(avg_cv))]
</code></pre>

<p>'''</p>
",2020-04-21 10:39:01,2020-04-21 10:39:01,Saving pyldavis file iteratively,<python><file><lda><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,False
25175,60290296,2020-02-18 22:22:37,,"<p>I have trained several word2vec models using gensim for different languages, but the <code>size</code> is different for each of them.</p>

<p>vectors are obtained like this:</p>

<pre><code>vec_sp = word_vectors_sp.get_vector(""uno"")
</code></pre>

<p>How to use <code>vec_sp</code> as input for different model with different vector size:</p>

<pre><code>word_vectors_en.most_similar(positive=[vec_sp], topn=1)
</code></pre>

<p>to obtain the corresponding word in the second model</p>
",,2020-02-19 21:31:06,Word2Vec compare vectors from different models with different sizes,<python><nlp><artificial-intelligence><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25179,60292744,2020-02-19 03:51:08,,"<p>I want to extract and print bigrams using Gensim. For this purpose I used that code in GoogleColab:</p>

<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec
from gensim.corpora import WikiCorpus, Dictionary
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from collections import Counter

data = api.load(""text8"") # wikipedia corpus
bigram = Phrases(data, min_count=3, threshold=10)


cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split('_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>But there's an error: </p>

<p><a href=""https://i.stack.imgur.com/eIjwF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIjwF.png"" alt=""TypeError""></a></p>

<p>Then I tried this:</p>

<pre><code>cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split(b'_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>And then:</p>

<p><a href=""https://i.stack.imgur.com/ir6eB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ir6eB.png"" alt=""again""></a></p>

<p>What is wrong?</p>
",,2020-02-19 05:04:05,TypeError during extracting bigrams with Gensim(Python),<python><machine-learning><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25188,60343072,2020-02-21 16:49:29,,"<p>I have several thousand documents that I'd like to use in a gensim doc2vec model, but I only have 5grams for each of the documents, not the full texts in their original word order. In the doc2vec tutorial on the gensim website (<a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>), a corpus is created with full texts and then the model is trained on that corpus. It looks something like this:</p>

<pre><code>[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern',...], tags=[1]), TaggedDocument(words=[.....], tags=[2]),...]
</code></pre>

<p>Is it possible create a training corpus where each document consists of a list of 5grams rather than a list of words in their original order?</p>
",,2020-02-23 21:01:13,Gensim doc2vec training on ngrams,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25191,60344269,2020-02-21 18:17:58,,"<p>I`ve faced with trouble to load model using gensim.model.FastText.load().</p>

<p>Here is some code and error which I get:</p>

<pre><code>from gensim.models import FastText

class FastTextModel:
    def __init__(self, model_path, dim=300):
        self.dim = dim
        self.model = FastText.load(model_path).wv

...

class GeneralModel:
    def __init__(self, config):
        if config[""type""] == ""fasttext"":
            # path - path to model
            # dim -  dimension, here 300
            self.model = FastTextModel(config[""path""], config[""dim""])
</code></pre>

<pre><code>  File ""/project/preprocessing/pipeline.py"", line 15, in __init__
    self.model_ru = GeneralModel(config[""models""][""ru""])
  File ""/project/models/nlp_models.py"", line 101, in __init__
    self.model = FastTextModel(config[""path""], config[""dim""])
  File ""/project/models/nlp_models.py"", line 16, in __init__
    self.model = FastText.load(model_path).wv
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/fasttext.py"", line 936, in load
    model = super(FastText, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 423, in load
    obj._load_specials(fname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 453, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 464, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/npyio.py"", line 447, in load
    pickle_kwargs=pickle_kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/format.py"", line 738, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 67239904 into shape (445446,300)
</code></pre>

<p>I've downloaded models from Google Drive folder, and though that it can somehow damage .npy files (as they are quite big), so I've downloaded each file (there 7 files for that model) separately, but this didn`t help me.</p>

<p>Also, I read that sometimes it can be caused because of bad unzipping in the 'load' method, but I'm passing already unzipped files into it, so this also don`t work for me.</p>

<p>Will be grateful for the help!</p>
",,2020-02-23 21:12:55,Cannot load model with gensim FastText,<python><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25222,60396376,2020-02-25 14:01:05,,"<p>Currently doing an LDA analysis using Python and the Gensim Mallet wrapper. 
After training the model and getting the topics, I want to see how the topics are distributed over the various document. In the normal Gensim LDA analysis, it is possible to use the get_document_topics function, which I could have used to iterate over every document in my file. However, Mallet wrapper does not have this function. I can retrieve the distribution of topics over one specific document, but can't find a solution to collect and store this over every document (for instance into a list or dataframe). </p>

<p>I can use the following code to acquire the topic distribution over one document:</p>

<p><code>print (ldamallet[mm[6000]])</code></p>

<p>which would return the following output:</p>

<p><code>[(0, 0.3055555555555555), (1, 0.3253968253968254), (2, 0.36904761904761907)]</code></p>

<p>However, I can't get it to iterate over the more or less 9000 documents in my dataset. </p>

<p>Additional code that could be relevant:</p>

<pre><code>id2word = corpora.Dictionary(wordsFiltered)
id2word.filter_extremes(no_below=167, keep_tokens=None)
mm=[id2word.doc2bow(wordsFilter) for wordsFilter in wordsFiltered]
mallet_path = 'path'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=mm, num_topics=3, id2word=id2word) 

</code></pre>

<p>Anyone some suggestions? 
Thanks in advance! </p>
",2020-02-26 08:48:54,2020-02-26 08:48:54,LDA Mallet alternative for get_document_topics - Measuring topics per document,<python><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25242,60271345,2020-02-17 22:39:35,,"<p>I am trying to understand how to use LDA in my case. I have a corpus of many documents and I want to see how a very specific set of words and ngrams are distributed across topics. Is there a way to specify a list of specific words as a vocabulary for topic modeling? </p>

<p>I have been working with the gensim implementation and I believe the argument <code>id2word</code> handles this, but the documentation is not clear to me. Is my understanding correct?</p>
",,2020-02-20 15:56:27,Specifying vocabulary input in LDA,<python><nlp><cluster-analysis><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25245,60307249,2020-02-19 18:46:02,,"<p>I am working with a steadily growing corpus. I train my Document Vector with Doc2Vec which is implemented in Python. </p>

<p>Is it possible to update a Document Vector?</p>

<p>I want to use the Document Vector for Document recommendations.</p>
",,2020-02-19 21:47:40,Is it possible to update a Doc2Vec Vector?,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25260,60383368,2020-02-24 20:22:17,,"<p>I am going through this Notebook about LDA and Document Similarity:</p>

<p><a href=""https://www.kaggle.com/ktattan/lda-and-document-similarity"" rel=""nofollow noreferrer"">https://www.kaggle.com/ktattan/lda-and-document-similarity</a></p>

<p>In this Notebook the Document similarity for a small set of documents gets computed however I want to compute the similarity for the whole corpus.</p>

<p>Instead of using test_df like in the Notebook:</p>

<pre><code>new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])
new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])
</code></pre>

<p>I want to use train_df:</p>

<pre><code>new_bow= [id2word.doc2bow(doc) for doc in train_df['tokenized']]
new_doc_distribution = np.array([[tup[1] for tup in lst] for lst in model.get_document_topics(bow=new_bow)])
</code></pre>

<p>However this is does not work. My asumption is that its not possible because the lists that are used to create the numpy array (tup[1] in this case) are not of the same length. So its not possible to create a proper array which is needed to compute the Jensen Divergence.</p>

<p>Can somebody more experienced than me tell me if what I am trying is possible?</p>
",,2020-03-01 16:34:15,Is it possible to compute Document similarity for every document in an LDA corpus?,<python><numpy><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25263,60316242,2020-02-20 09:06:25,,"<p>I am following this <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a> in which i have a following Dataset from Quora:</p>

<p><a href=""https://i.stack.imgur.com/dBzId.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dBzId.jpg"" alt=""enter image description here""></a></p>

<p>Here i have already cleaned and tokenize the data in column <strong>q1_clean</strong> &amp; <strong>q1_clean</strong>.</p>

<p>Now i have trained the <strong>W2vModel</strong> by using GoogleNews pretrained model with the following code. </p>

<pre><code># We are concating the two columns of Question1 and Question2

nData = pd.Series(pd.concat([data['q1_clean'], data['q2_clean']]))
model_w2v = Word2Vec(nData, size=300) 

# step 2: intersect the initialized word2vec model with the pre-trained fasttext model
model_w2v.intersect_word2vec_format('GoogleNews-vectors-negative300.bin',lockf=1.0,binary=True)

# step 3: improve model with transfer-learning using the training data
model_w2v.train(nData, total_examples=model_w2v.corpus_count, epochs= 10)

</code></pre>

<p>Now i have to do the feature analysis, for that i have following function to get the average computed distance.</p>

<pre><code>def get_pairwise_distance(word1, word2, weight1, weight2, method = 'euclidean'):
    if(word1.size==0 or word2.size==0):
        return np.nan
    dist_matrix = pairwise_distances(word1, word2, metric=method)
    return np.average(dist_matrix, weights=np.matmul(weight1.reshape(-1,1),weight2.reshape(-1,1).T))
</code></pre>

<p>Here i have computed the tfidf to use as a weights: </p>

<pre><code>X_train_tokens = get_tokenized_questions(data=X_train)

from sklearn.feature_extraction.text import TfidfVectorizer
pass_through = lambda x:x
tfidf = TfidfVectorizer(analyzer=pass_through)
# compute tf-idf weights for the words in the training set questions
X_tfidf = tfidf.fit_transform(X_train_tokens)

# split into two
# X1_tfidf -&gt; tf-idf weights of first question in question pair and 
# X2_tfidf -&gt; tf-idf weights of second question in question pair
X1_tfidf = X_tfidf[:len(X_train)]
X2_tfidf = X_tfidf[len(X_train):]
</code></pre>

<p>and i am calling this <strong>get_pairwise_distance</strong> function like in the <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<pre><code>#cosine similarities
# here X1 and X2 are the embedded versions of the first and second questions in the question-pair data
# and X1_tfidf and X2_tfidf are the tf-idf weights of the first and second questions in the question-pair data

cosine = compute_pairwise_dist(X1, X2, X1_tfidf, X2_tfidf)
</code></pre>

<p><strong>For this function i need to pass the embedded version of <em>q1_clean</em> and <em>q2_clean</em> as X1 and X2 where weights are already computed using TFIDF. and i am getting no clue how to embed these two columns into vectors using pretrained model and pass it to the given function?</strong> </p>
",2020-02-20 10:15:34,2020-02-21 14:10:42,How to Embed your Dataframe using already trained model with Gensim (GoogleNews-vectors-negative300.bin),<machine-learning><scikit-learn><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,True
25302,60420718,2020-02-26 18:57:43,,"<p>I am training a LDA Model using Gensim:</p>

<pre><code>dictionary = corpora.Dictionary(section_2_sentence_df['Tokenized_Sentence'].tolist())
dictionary.filter_extremes(no_below=20, no_above=0.7)
corpus = [dictionary.doc2bow(text) for text in (section_2_sentence_df['Tokenized_Sentence'].tolist())]

num_topics = 15
passes = 200
chunksize = 100
lda_sentence_model = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=num_topics, 
                                                              id2word=dictionary, 
                                                              passes=passes, 
                                                              chunksize=chunksize,
                                                              random_state=100,
                                                              workers = 3)
</code></pre>

<p>After training i need the topics for further analysis. Unfortunately the show_topics function only returns <strong>10 topics</strong>. I expected the defined number of <strong>15 topics</strong>. Does anyone know if that is on purpose or an error that can be solved?</p>

<pre><code>print(len(lda_sentence_model.show_topics(formatted=False)))
</code></pre>
",,2020-02-26 22:21:20,Python Gensim LDA Model show_topics funciton,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25305,60451614,2020-02-28 12:21:32,,"<p>I see a code which uses Wikicorpus on an Arabic Wikipedia dump, and I know that the process will take a long time to execute, I also searched around about the warning that I get when executing it which says: </p>

<blockquote>
  <p>(UserWarning: detected Windows; aliasing chunkize to chunkize_serial<br>
  warnings.warn(""detected Windows; aliasing chunkize to
  chunkize_serial""))</p>
</blockquote>

<p>and answers said that it's ok, nothing serious, it's just a warning. 
But after waiting about 3 days without any response! I start wondering whether is it truly work on the Arabic dump file, or I have to do certain kind of pre-processing before passing the Arabic dump file to the Wikicorpus object?
the data size is about 989.6 MB.
and I surround the WikiCorpus code line with two print commands, to know when it started and when it finished executing, like this:</p>

<pre><code>print('start WikiCorpus')
wiki = WikiCorpus(self.in_f)
print('finish WikiCorpus')
</code></pre>

<p>where the self.in_f is the Arabic Wikipedia dump like this: (/the path where the file located/arwiki-20200201-pages-articles.xml.bz2), but never reached the second print command during the runtime.</p>
",,2020-02-28 22:16:33,Does WikiCorpus from gensim library works on Arabic Wikipedia dump?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25335,60454355,2020-02-28 15:05:28,,"<p>I have trained my model using Gensim. I draw a 2D plot using PCA but it is not clear too much. I wanna change it to 3D  with capable of zooming .my result is so dense.</p>

<pre><code>from sklearn.decomposition import PCA
from matplotlib import pyplot
X=model[model.wv.vocab]
pca=PCA(n_components=2)
result=pca.fit_transform(X)
pyplot.scatter(result[:,0],result[:,1])
word=list(model.wv.most_similar('eden_lake'))
for i, word in enumerate(words):
  pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
</code></pre>

<p>And the result:
<a href=""https://i.stack.imgur.com/eUFwI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eUFwI.jpg"" alt=""enter image description here""></a></p>

<p>it possible to do that?</p>
",2020-02-28 19:18:35,2020-03-01 07:13:30,Draw 3D Plot for Gensim model,<python-3.x><pca><gensim>,,,CC BY-SA 4.0,False,False,True,False,True
25345,60459403,2020-02-28 21:22:00,,"<p>My coworker and I have the exact same code, using the same libraries, but yet his code works and mine doesn't. We've gotten stuck trying to figure out what is wrong. Any help would be greatly appreciated. The code and error are below. </p>

<p><strong>Code:</strong></p>

<pre><code>import os
os.environ.update({'MALLET_HOME':r'C:...\\mallet-2.0.8/'})
mallet_path = 'C:...\\mallet-2.0.8\\bin\\mallet' 
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)
</code></pre>

<p><strong>Output and Error:</strong></p>

<pre><code>---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-79-6122457c60e1&gt; in &lt;module&gt;
----&gt; 1 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in __init__(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)
    129         self.random_seed = random_seed
    130         if corpus is not None:
--&gt; 131             self.train(corpus)
    132 
    133     def finferencer(self):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in train(self, corpus)
    270 
    271         """"""
--&gt; 272         self.convert_input(corpus, infer=False)
    273         cmd = self.mallet_path + ' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\
    274             '--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in convert_input(self, corpus, infer, serialize_corpus)
    259             cmd = cmd % (self.fcorpustxt(), self.fcorpusmallet())
    260         logger.info(""converting temporary corpus to MALLET format with %s"", cmd)
--&gt; 261         check_output(args=cmd, shell=True)
    262 
    263     def train(self, corpus):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py in check_output(stdout, *popenargs, **kwargs)
   1916             error = subprocess.CalledProcessError(retcode, cmd)
   1917             error.output = output
-&gt; 1918             raise error
   1919         return output
   1920     except KeyboardInterrupt:

CalledProcessError: Command 'C:\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\CST~1.JEO\AppData\Local\Temp\84f7e0_corpus.txt --output C:\Users\CST~1.JEO\AppData\Local\Temp\84f7e0_corpus.mallet' returned non-zero exit status 1.
</code></pre>
",,2020-02-28 21:22:00,Return nonzero for LdaMallet,<gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
25348,60524589,2020-03-04 11:02:45,,"<p>Just curiosity, but I was debugging gensim's FastText code for replicating the implementation of Out-of-Vocabulary (OOV) words, and I'm not being able to accomplish it.
So, the process i'm following is training a tiny model with a toy corpus, and then comparing the resulting vectors of a word in the vocabulary. That means if the whole process is OK, the output arrays should be the same.</p>

<p>Here is the code I've used for the test:</p>

<pre><code>from gensim.models import FastText
import numpy as np
# Default gensim's function for hashing ngrams
from gensim.models._utils_any2vec import ft_hash_bytes

# Toy corpus
sentences = [['hello', 'test', 'hello', 'greeting'],
             ['hey', 'hello', 'another', 'test']]

# Instatiate FastText gensim's class
ft = FastText(sg=1, size=5, min_count=1, \
window=2, hs=0, negative=20, \
seed=0, workers=1, bucket=100, \
min_n=3, max_n=4)

# Build vocab
ft.build_vocab(sentences)

# Fit model weights (vectors_ngram)
ft.train(sentences=sentences, total_examples=ft.corpus_count, epochs=5)

# Save model
ft.save('./ft.model')
del ft

# Load model
ft = FastText.load('./ft.model')

# Generate ngrams for test-word given min_n=3 and max_n=4
encoded_ngrams = [b""&lt;he"", b""&lt;hel"", b""hel"", b""hell"", b""ell"", b""ello"", b""llo"", b""llo&gt;"", b""lo&gt;""]
# Hash ngrams to its corresponding index, just as Gensim does
ngram_hashes = [ft_hash_bytes(n) % 100 for n in encoded_ngrams]
word_vec = np.zeros(5, dtype=np.float32)
for nh in ngram_hashes:
    word_vec += ft.wv.vectors_ngrams[nh]

# Compare both arrays
print(np.isclose(ft.wv['hello'], word_vec))

</code></pre>

<p>The output of this script is False for every dimension of the compared arrays.</p>

<p>It would be nice if someone could point me out if i'm missing something or doing something wrong. Thanks in advance!</p>
",,2020-03-04 18:02:19,Cannot reproduce pre-trained word vectors from its vector_ngrams,<python-3.x><gensim><fasttext><oov>,,,CC BY-SA 4.0,False,False,True,False,False
25353,60491035,2020-03-02 14:35:01,,"<p>When attempting to load a word2vec model trained by Gensim on a Windows machine, I receive the following error:</p>

<p><code>AttributeError: Can't get attribute 'EpochProgress' on &lt;module '__main__'&gt;</code></p>

<p>I've successfully trained numerous models with Gensim in the past on this system. The only variation being this time I split the <code>model.build_vocab()</code> and <code>model.train()</code> phases, adding in saves &amp; time hacks for each epoch. I also used a different iterator for the vocab build and the training phrases, but on the same dataset with the same tokenization pipeline.</p>

<p>Here is how I did the epoch progress tracking/saving:</p>

<pre class=""lang-py prettyprint-override""><code>class EpochProgress(CallbackAny2Vec):
    '''saves the model after each epoch'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0
        self.start_time = time.time()

    def on_epoch_begin(self, model):
        print(""epoch #{} started"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""epoch #{} completed"".format(self.epoch))
        passed = (time.time() - self.start_time)/60/60 # elapsed time since start in HOURS
        print(""{} hours have passed"".format(str(passed)))
        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
        model.save(output_path)
        print(""model saved at: {}"".format(output_path))
        self.epoch +=1
</code></pre>

<p><code>epoch_progress = EpochProgress('E:/jade_prism/embeddings/phrase-embed-over- time/mega_WOS_word2vec/w2v_models/in_progress/')</code></p>

<p>I then load the baseline model with the vocab build and set a few parameters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(baseline_models_directory+chosen_name)
model.window = window
model.size = size
model.workers = workers 
model.callbacks = [epoch_progress]
</code></pre>

<p>Then I do the training like this:</p>

<p><code>model.train(corpus, total_examples=model.corpus_count, epochs=epochs)</code></p>

<p>And finally, save the end product like this: </p>

<p><code>model.save('E:/w2v_models/trained/{}'.format(new_model_filename))</code></p>

<p>Training appeared to work properly, and model saved as expected- unfortunately now I can't load it.</p>

<p>Here is the full Debug readout:</p>

<pre><code>&gt; AttributeError                            Traceback (most recent call
&gt; last)
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1329         try:
&gt; -&gt; 1330             model = super(Word2Vec, cls).load(*args, **kwargs)    1331 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, *args, **kwargs)    1243         """"""
&gt; -&gt; 1244         model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)    1245         if not hasattr(model,
&gt; 'ns_exponent'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, fname_or_handle, **kwargs)
&gt;     602         """"""
&gt; --&gt; 603         return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
&gt;     604 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; load(cls, fname, mmap)
&gt;     425 
&gt; --&gt; 426         obj = unpickle(fname)
&gt;     427         obj._load_specials(fname, mmap, compress, subname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; unpickle(fname)    1383         if sys.version_info &gt; (3, 0):
&gt; -&gt; 1384             return _pickle.load(f, encoding='latin1')    1385         else:
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on &lt;module
&gt; '__main__'&gt;
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; AttributeError                            Traceback (most recent call
&gt; last) &lt;ipython-input-4-0206f9f8f3ad&gt; in &lt;module&gt;
&gt;       3 
&gt;       4 # Load the model based onthe model name
&gt; ----&gt; 5 model = gensim.models.Word2Vec.load(model_name)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1339             logger.info('Model
&gt; saved using code from earlier Gensim Version. Re-loading old model in
&gt; a compatible way.')    1340             from
&gt; gensim.models.deprecated.word2vec import load_old_word2vec
&gt; -&gt; 1341             return load_old_word2vec(*args, **kwargs)    1342     1343 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load_old_word2vec(*args, **kwargs)
&gt;     170 
&gt;     171 def load_old_word2vec(*args, **kwargs):
&gt; --&gt; 172     old_model = Word2Vec.load(*args, **kwargs)
&gt;     173     vector_size = getattr(old_model, 'vector_size', old_model.layer1_size)
&gt;     174     params = {
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load(cls, *args, **kwargs)    1639     @classmethod    1640     def
&gt; load(cls, *args, **kwargs):
&gt; -&gt; 1641         model = super(Word2Vec, cls).load(*args, **kwargs)    1642         # update older models    1643         if hasattr(model,
&gt; 'table'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in load(cls, fname, mmap)
&gt;      85         compress, subname = SaveLoad._adapt_by_suffix(fname)
&gt;      86 
&gt; ---&gt; 87         obj = unpickle(fname)
&gt;      88         obj._load_specials(fname, mmap, compress, subname)
&gt;      89         logger.info(""loaded %s"", fname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in unpickle(fname)
&gt;     377             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')
&gt;     378         if sys.version_info &gt; (3, 0):
&gt; --&gt; 379             return _pickle.loads(file_bytes, encoding='latin1')
&gt;     380         else:
&gt;     381             return _pickle.loads(file_bytes)
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on module '__main__'\&gt;
</code></pre>
",2020-03-03 00:54:42,2020-03-03 01:02:42,Unable to Load Model Trained in Gensim- pickle-related error,<python-3.x><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25378,60494795,2020-03-02 18:41:22,,"<p>I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. <strong>I am using the same corpus for get both matrix, I don't know why each matrix have a different number of terms
.</strong></p>

<p>My problem is that I have a matrix TFIDF with the shape <code>(56096, 15500)</code> (corresponding to: number of terms, number of documents) and matrix Word2vec with the shape <code>(300, 56184)</code> (corresponding to : number of word-embeddings, number of terms).<br>
And I need the same numbers of terms in both matrix. </p>

<p>I use this code for get the matrix of word-embeddings Word2vec: </p>

<pre><code>def w2vec_gensim(norm_corpus):
    wpt = nltk.WordPunctTokenizer()
    tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]
    # Set values for various parameters
    feature_size = 300
    # Word vector dimensionality
    window_context = 10
    # Context window size
    min_word_count = 1
    # Minimum word count
    sample = 1e-3
    # Downsample setting for frequent words
    w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=window_context, min_count =  min_word_count, sample=sample, iter=100)
    words = list(w2v_model.wv.vocab)
    vectors=[]
    for w in words:
        vectors.append(w2v_model[w].tolist())
    embedding_matrix= np.array(vectors)
    embedding_matrix= embedding_matrix.T
    print(embedding_matrix.shape)

    return embedding_matrix
</code></pre>

<p>And this code for get the TFIDF matrix: </p>

<pre><code>tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)


def matriz_tf_idf(datos, tv):
    tv_matrix = tv.fit_transform(datos)
    tv_matrix = tv_matrix.toarray()
    tv_matrix = tv_matrix.T
    return tv_matrix
</code></pre>

<p>And I need the same number of terms in each matrix. For example, if I have 56096 terms in TFIDF, I need the same number in embeddings matrix, I mean matrix TFIDF with the shape <code>(56096, 1550)</code> and matrix of embeddings Word2vec with the shape <code>(300, 56096)</code>. How I can get the same number of terms in both matrix? 
Because I can't delete without more data, due to I need the multiplication to make sense because my goal is to get the embeddings from the documents. </p>

<p>Thank you very much in advance.</p>
",2020-03-02 21:19:01,2020-03-12 18:48:14,Why I have a different number of terms in word2vec and TFIDF? How I can fix it?,<python><matrix><word2vec><embedding>,,,CC BY-SA 4.0,True,False,True,False,False
25385,60561960,2020-03-06 10:17:09,,"<p>Suppose I have a dataframe shown below:</p>

<p>|Text</p>

<p>|Storm in RI worse than last hurricane</p>

<p>|Green Line derailment in Chicago</p>

<p>|MEG issues Hazardous Weather Outlook </p>

<p>I created word2vec model using below code:</p>

<pre><code>def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

text_data = sent_to_words(df['Text'])
w2v_model = gensim.models.Word2Vec(text_data, size=100, min_count=1, window=5, iter=50)
</code></pre>

<p>now how I will convert the text present in the 'Text' column to vectors using this word2vec model?</p>
",,2020-03-06 10:42:31,How to convert the text into vector using word2vec embedding?,<python-3.x><machine-learning><nlp><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
25396,60445602,2020-02-28 05:11:29,,"<p>I have a use case where I want to find top n nearest words from a given set of words, to a vector. </p>

<p>Its like <code>similar_by_vector</code> where I want to restrict my vocab to a given set of words.</p>

<pre><code>similar_by_vector(vector, topn, vocab=[x,y,z...])
</code></pre>

<p>I want to create a low latency api using this, where vocab can be different for each request.
Any suggestions to how I can achieve this optimally?</p>
",,2020-02-28 05:11:29,How to get top n similar words from given list of words to a vector using gensim?,<java><python><gensim><word2vec><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
25402,60515074,2020-03-03 20:33:16,,"<p>Is there a way to save a gensim LDA model to ONNX format?  We need to be able to train using Python/gensim and then operationalize it into an Onnx model to publish and use.</p>
",,2020-03-03 20:43:46,Saving gensim LDA model to ONNX,<gensim><lda><onnx>,,,CC BY-SA 4.0,False,False,True,False,False
25405,60482161,2020-03-02 03:25:28,,"<p>I am attempting to do a semi supervised LDA model based on <a href=""https://gist.github.com/scign/2dda76c292ef76943e0cd9ff8d5a174a"" rel=""nofollow noreferrer"">this</a>. </p>

<p>However, the result I obtained from the LDA model is very different from the set of topics that I predefined.</p>

<p>These are the set of codes I took from the link.</p>

<pre><code>def viz_model(model, modeldict):
    ntopics = model.num_topics
    # top words associated with the resulting topics
    topics = ['Topic {}: {}'.format(t,modeldict[w]) for t in range(ntopics) for w,p in model.get_topic_terms(t, topn=1)]
    terms = [modeldict[w] for w in modeldict.keys()]
    fig,ax=plt.subplots()
    ax.imshow(model.get_topics())  # plot the numpy matrix
    ax.set_xticks(modeldict.keys())  # set up the x-axis
    ax.set_xticklabels(terms, rotation=90)
    ax.set_yticks(np.arange(ntopics))  # set up the y-axis
    ax.set_yticklabels(topics)
    plt.show()

def test_eta(eta, dictionary, ntopics, print_topics=True, print_dist=True):
    np.random.seed(42) # set the random seed for repeatability
    bow = [dictionary.doc2bow(line) for line in corp] # get the bow-format lines with the set dictionary
    with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings
        model = gensim.models.ldamodel.LdaModel(
            corpus=bow, id2word=dictionary, num_topics=ntopics,
            random_state=42, chunksize=100, eta=eta,
            eval_every=-1, update_every=1,
            passes=150, alpha='auto', per_word_topics=True)
    # visuzlize the model term topics
    viz_model(model, dictionary)
    print('Perplexity: {:.2f}'.format(model.log_perplexity(bow)))
    if print_topics:
        # display the top terms for each topic
        for topic in range(ntopics):
            print('Topic {}: {}'.format(topic, [dictionary[w] for w,p in model.get_topic_terms(topic, topn=3)]))
    if print_dist:
        # display the topic probabilities for each document
        for line,bag in zip(txt,bow):
            doc_topics = ['({}, {:.1%})'.format(topic, prob) for topic,prob in model.get_document_topics(bag)]
            print('{} {}'.format(line, doc_topics))
    return model

def create_eta(priors, etadict, ntopics):
    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1
    for word, topic in priors.items(): # for each word in the list of priors
        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary
        if (len(keyindex)&gt;0): # if it's in the dictionary
            eta[topic,keyindex[0]] = 1e7  # put a large number in there
    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics
    return eta
</code></pre>

<p>I have a set of reviews that I scraped online and a set of predefined topics.</p>

<pre><code>seed = {
    'delivery':0, 'fast':0, 'arrive':0, 'slow':0,'condition':0, 'day':0,'come':0, 'receive':0, 'prompt':0, 'pack':0, 'wrap':0, 'package':0, 'arrive':0,
    'colour':1, 'color':1, 'shade':1, 'pigment':1,
    'stick':2, 'oil':2, 'lightweight':2, 'gentle':2,'fresh':2, 'sensitive':2, 'sticky':2, 'oily':2, 'skin':2, 'apply':2,'last':2,""lasting"":2,
    'manufacture':3,'expiry':3, 'date':3, 'year':3,
    'promotion':4,'deal':4,'sale':4,'free':4,'freebie':4,'sample':4,'worth':4,'price':4, 'discount':4
}
</code></pre>

<p>The result that I obtained from the model is very different from my set of topics I defined earlier. </p>

<pre><code>Perplexity: -4.08
Topic 0: ['feel', 'use', 'dry', 'apply', 'easy', 'last', 'would', 'smooth', 'try', 'make']
Topic 1: ['good', 'receive', 'try', 'fast', 'buy', 'order', 'love', 'use', 'look', 'first']
Topic 2: ['buy', 'get', 'free', 'deliver', 'worth', 'take', 'come', 'try', 'happy', 'different']
Topic 3: ['fast', 'receive', 'free', 'come', 'thank', 'pack', 'cheap', 'great', 'nice', 'arrive']
Topic 4: ['remove', 'use', 'make', 'micellar', 'garni', 'sensitive', 'gentle', 'try', 'clean', 'feel']
</code></pre>

<p>What am I doing wrong here and what can I do to increase the probability of my predefine keywords appearing in topics generated by the model?</p>
",2020-03-04 06:29:57,2020-03-04 06:29:57,Semi Supervised Guided LDA - predefined topics different from topics generated,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25422,60464982,2020-02-29 12:05:54,,"<pre><code>from gensim.test.utils import datapath
from gensim import utils

class MyCorpus(object):
    """"""An interator that yields sentences (lists of str).""""""

    def __iter__(self):
        corpus_path = datapath('lee_background.cor')
        i = 1
        print(str(i))
        for line in open(corpus_path):
            # assume there's one document per line, tokens separated by whitespace

            yield utils.simple_preprocess(line)

import gensim.models

sentences = MyCorpus()
model = gensim.models.Word2Vec(sentences=sentences, iter=1)
</code></pre>

<p>This is the genism's documentation code from <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html</a>.</p>

<p>I have 2 questions regarding the iter parameter:</p>

<p>1) when it is set to 1, why is the print(str(i)) executed twice?</p>

<p>2) when the ""iter=10"", the 'simple_preprocess' is executed 11 times. If my own customized 'preprocess' is very heavy, is this going to be very slow? How to avoid this preprocessing repetitions in using genism word2vec?</p>
",,2020-02-29 19:24:06,How to understand the gensim's iter parameter and its implication on preprocessing?,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25444,60599396,2020-03-09 11:14:47,,"<p>I saved an LDAWallet model:</p>

<p>First I did the train : </p>

<pre><code> mallet_path = 'mallet-2.0.8/bin/mallet'
 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=id2word, 
 num_topics=14)
</code></pre>

<p>And then I saved the model using the save method:</p>

<pre><code>ldamallet.save('lda_v0.model')
</code></pre>

<p>I forgot the set the prefix to a certain file when I trained the mode, as a consequence I lost all the temporary files created by gensim when training (doctopics etc...).
And I think that because of that, when I load the model and want to predict topics  :</p>

<pre><code>model_lda = gensim.models.ldamodel.LdaModel.load('lda_v0.model')
###stuff
###stuff
###stuff
model_lda[input]
</code></pre>

<p>I get an error :</p>

<p><strong>[Errno 2] No such file or directory: '/var/folders/_f/ttl3hvqn75g4rb5cdg02qg1c0000gn/T/2e13a7_doctopics.txt.infer'</strong></p>

<p>I tried unsuccessfully to reproduce the same model with the data (and setting the prefix so that I don't lose the temporary files).
I'm wondering if it is possible to use the method print_topics (I forgot to say that loading the model is working and I can get all the topics and their words) and for each topics , retrieve the weight of the words related to the topics and compute the probability but I don't know how the lda model predict the topic for each document, so I'm not sure if my idea can work.</p>

<p>Do you have any idea how to fix this issue ?
I only want to predict for a document the probabibity of each topic.</p>

<p>Thank you</p>
",,2020-04-01 16:47:47,Python Mallet LDA Errno 2 No such file or directory,<nlp><gensim><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
25445,60600004,2020-03-09 11:50:33,,"<p>I followed the steps in gensim Python <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> to train wikipedia on LDA model, but i have problem : 
No such file or directory: '-f'</p>
",,2020-03-09 11:50:33,Training LDA on Wikipedia corpus to tag arbitary aritcle?,<gensim><wikipedia><training-data><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25446,60600199,2020-03-09 12:03:16,,"<pre><code>import pandas as pd
from ast import literal_eval
from nltk.corpus import stopwords
import string


stops = stopwords.words('english')
stops.extend(['none'])

import nltk

import re
import numpy as np
from pprint import pprint

import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess # for tokenizing
from gensim.models import CoherenceModel
from sklearn.feature_extraction import stop_words

import spacy
</code></pre>

<p>Error is :</p>

<hr>

<blockquote>
  <p>AttributeError                            Traceback (most recent call last)
   in 
       20 from sklearn.feature_extraction import stop_words
       21 
  ---> 22 import spacy
       23 
       24 import pyLDAvis</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy__init__.py in 
       10 from thinc.neural.util import prefer_gpu, require_gpu
       11 
  ---> 12 from . import pipeline
       13 from .cli.info import info as cli_info
       14 from .glossary import explain</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\pipeline__init__.py in 
        2 from __future__ import unicode_literals
        3 
  ----> 4 from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
        5 from .pipes import TextCategorizer, Tensorizer, Pipe, Sentencizer
        6 from .morphologizer import Morphologizer</p>
  
  <p>pipes.pyx in init spacy.pipeline.pipes()</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\syntax\nn_parser.cp37-win_amd64.pyd in init spacy.syntax.nn_parser()</p>
  
  <p>AttributeError: type object 'spacy.syntax.nn_parser.array' has no attribute '__reduce_cython__'</p>
</blockquote>
",2020-03-09 13:14:17,2020-03-09 13:14:17,python throwing some error while trying to load spacy,<python><nlp><installation><package><spacy>,,,CC BY-SA 4.0,True,True,True,False,True
25461,60602768,2020-03-09 14:36:19,,"<p>This is the code for creating the model :</p>

<pre><code>import gensim
NUM_TOPICS = 4
ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics = 
NUM_TOPICS,id2word=dictionary,passes=100)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=4)
print(topics)
</code></pre>

<p>This is the code for GridSearchCV :</p>

<pre><code>search_params = {'n_components': [4, 6, 8, 10, 20], 'learning_decay': [.5, .7, .9]}


# Init Grid Search Class
model = GridSearchCV(ldamodel, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)
</code></pre>

<p>This is the output : </p>

<pre><code>*---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-108-1a35c49ac19e&gt; in &lt;module&gt;
      9 
     10 # Do the Grid Search
---&gt; 11 model.fit(data_vectorized)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    627 
    628         scorers, self.multimetric_ = _check_multimetric_scoring(
--&gt; 629             self.estimator, scoring=self.scoring)
    630 
    631         if self.multimetric_:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in _check_multimetric_scoring(estimator, scoring)
    471     if callable(scoring) or scoring is None or isinstance(scoring,
    472                                                           str):
--&gt; 473         scorers = {""score"": check_scoring(estimator, scoring=scoring)}
    474         return scorers, False
    475     else:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in check_scoring(estimator, scoring, allow_none)
    399     if not hasattr(estimator, 'fit'):
    400         raise TypeError(""estimator should be an estimator implementing ""
--&gt; 401                         ""'fit' method, %r was passed"" % estimator)
    402     if isinstance(scoring, str):
    403         return get_scorer(scoring)
TypeError: estimator should be an estimator implementing 'fit' method, &lt;gensim.models.ldamodel.LdaModel object at 0x000002121E55D3C8&gt; was passed*
</code></pre>
",2020-03-10 05:42:35,2020-03-10 05:53:58,Scikit-Learn GridSearchCV failing on on a gensim LDA model,<python><scikit-learn><gensim><lda><gridsearchcv>,,,CC BY-SA 4.0,False,False,True,False,True
25465,60501793,2020-03-03 07:07:40,,"<p>I am dealing with tons of PDF documents (petetions data) filled with text data having numbers, tabular data etc. The objective of client is to summarize any such given document to reduce man-force in reading the entire document. I have tried conventional methods like lSA,Gensim-summarizer, BERT extractive summarizer, Pysummarizer. </p>

<p>The results are not at all good, Please suggest me any way where i can find a industry level summarizer(extrative/abstractive) that would give me a good start to solve this issue .</p>
",,2020-03-03 07:16:54,"Is there any way to summarize text data which has numbers and tables in python, either extractive way or abstarctive way?",<machine-learning><text><deep-learning><nlp><summarization>,,,CC BY-SA 4.0,False,False,True,False,False
25471,60584750,2020-03-08 05:30:34,,"<p>I trained a Word2Vec model using Gensim, and I have two sets of words:</p>

<pre><code>S1 = {'','','' ...}
S2 = {'','','' ...}
</code></pre>

<p>for each word w1 in S1, I want to find top 5 words that are most similar to w1. I am currently doing this way:</p>

<pre><code>model = w2v_model
 word_similarities = {}
 for w1 in S1:
    similarities = {}
    for w2 in S2:
       if w1 in model.wv and w2 in model.wv:
           similarity = model.similarity(w1, w2)
           similarities[w2] = similarity
    word_similarties[w1] = similarities
</code></pre>

<p>Then for each word in word_similarities, I can get the top N from its dict values. If S1 and S2 are large, this becomes very slow.</p>

<p>Is there a quicker way to compute large pairs of words in Word2Vec, either in genism or tensorflow?</p>
",,2020-03-10 21:47:57,How to speed up word2vec similarity calculation?,<tensorflow><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25472,60584927,2020-03-08 06:08:08,,"<p>I wanna calculate recall for query""terminator_2"" or any movie on google pre-trained model (GoogleNews-vectors-negative300.bin).but i don't know about the relevant movie.is  there any solution for this problem or not?</p>
",2020-03-11 15:56:54,2020-03-11 15:56:54,how to calculate recall when you don't know about relevant results,<gensim><precision-recall><pre-trained-model>,,,CC BY-SA 4.0,False,False,True,False,False
25491,60587089,2020-03-08 11:40:00,,"<p>I am going through the gensim LDA implementation and it says it needs a corpus and a dictionary of the corpus?</p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>

<p>What is the reason for this?</p>
",2020-03-08 20:43:44,2020-03-08 20:43:44,Why does the LDA gensim implemantion need the corpus and a dictionary?,<python><nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25492,60588557,2020-03-08 14:35:36,,"<p>I am using 'Gensim' to generate summary of different rows I have. Here is what the original dataframe looks like:</p>

<pre><code>df.head()

                                   Example Content
0   Not happy they have just reduced rates for Und...
1   One of the worst banks. I had a very bad exper...
2   Some one in lloyds has signed a form in My nam...
3   Card blocked due to ordering a takeaway from m...
4   There are plenty of better banks than Lloyds.\...
</code></pre>

<p>I am able to apply summarization to every row using gensim. Problem is, I want every rows summary to appear against its original, and this is not happening. Here is what my code looks like:</p>

<pre><code>a = []

for i in df['Example Content']:

    i= i + str("". This is second sentence. This is third"")             # this is to add two more sentences so that gensim summarizes it. These sentence add no value to summary.
    a = summarize(i, ratio=0.4, split = True)

df['Summary'] = a
</code></pre>

<p>And here is the ouput to the above code:</p>

<pre><code>                                     Example Content                                 Summary
0   Not happy they have just reduced rates for Und...       Today I got a new phone and switched my sim an...
1   One of the worst banks. I had a very bad exper...       Today I got a new phone and switched my sim an...
2   Some one in lloyds has signed a form in My nam...       Today I got a new phone and switched my sim an...
3   Card blocked due to ordering a takeaway from m...       Today I got a new phone and switched my sim an...
4   There are plenty of better banks than Lloyds.\...       Today I got a new phone and switched my sim an...
</code></pre>

<p>Below shown are all the individual summaries, generated by gensim, of each row:</p>

<pre><code>The 2nd address was a shopping centre and they didnt even give me the name of the business.
I wasn't to know as I through Gallarias Novas was the shop name but that was just the place.
They said that they had issued a new card that I hadn't received and even though they new I was abroad using my card they stopped it anyway.
When my new card did arrive after getting home I now know the reason was that they were making me have a con tactless card whcih I did nto request.

 Today I got a new phone and switched my sim and set up my banking apps inc Halifax and Lloyds.
Halifax worked fine, usual 4 digit code and confirmation call came through and all set up in mins.
</code></pre>

<p>How should i grab individual summaries corresponding to the original content and place them in the dataframe? </p>
",,2020-03-08 15:25:22,Gensim row wise dataframe summary,<python><pandas><for-loop><gensim><summarization>,,,CC BY-SA 4.0,False,False,True,False,False
25493,60588590,2020-03-08 14:39:23,,"<p>I am using gensim LDA topic modelling and working on fine tuning the model to improve the coherence score. I thought the bigger the dictionary the better would be the topic modelling but it doesn't looks like. So I am keen to understand the relationship between the Dictionary and the Coherence score</p>
",,2020-03-08 14:39:23,How Dictionary size impacts coherence score in gensim LDA,<python-3.x><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25504,60606705,2020-03-09 18:54:32,,"<p>I am trying to find sentence similarity through word emebeddings and then applying cosine similarity score. Tried CBOW/Skip Gram methods for embedding but did not solve the problem.</p>

<p>I am doing this for product review data. I have two columns:</p>

<pre><code>SNo         Product_Title                                Customer_Review   
 1       101.x battery works well                    I have an Apple phone and it's not that
          with Samsung smart phone                     that great.

 2       112.x battery works well                     I have samsung smart tv and I tell that it's
         with Samsung smart phone                     not wort buying.

 3      112.x battery works well                      This charger works very well with samsung 
        with Samsung smart phone.                      phone. It is fast charging.
</code></pre>

<p>The first two reviews are <code>irrelevant</code> as semantic meaning of <code>Product_Title</code> and <code>Customer_Review</code>  are completely different.</p>

<p>How can an algorithm find this semantic meaning of sentences and score them.</p>

<p>My Approach:</p>

<ol>
<li><p>Text pre-processing</p></li>
<li><p>Train CBOW/Skip gram using Gensim on my data-set</p></li>
<li><p>Do Sentence level encoding via averaging all word vectors in that sentence </p></li>
<li><p>Take cosine similarity of <code>product_title</code> and <code>reviews</code>.</p></li>
</ol>

<p>Problem: It was not able to find the context from the sentence and hence the result was very poor.</p>

<p>Approch 2:</p>

<p>Used pre-trained BERT without pre-processing sentences. The result was not improving either.</p>

<p>1.Any other approach that would capture the context/semantics of sentences.</p>

<p>2.How can we train BERT on our data-set from scratch without using pre-trained model?</p>
",,2020-03-30 23:41:10,How to find Sentence Similarity using deep learning?,<python><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
25514,60637654,2020-03-11 13:51:50,,"<p>I have been trying to apply LDA on my dataset to do topic modelling. But everytime I run the LDA model, the code just runs indefinitely before I have to shutdown the kernel. There is no error. 
Size of the dataset - 2GB</p>

<p>I have tried the LDAmodel from both gensim as well as pySpark.mllib.clustering libraries. I faced the same issue. All the other pre-processing steps work fine!
I tried running on GCP data proc but no luck.</p>

<pre><code>    '''
     from pyspark.mllib.clustering import LDA, LDAModel
     from pyspark.mllib.linalg import Vector, Vectors
     num_topics = 10
     max_iterations = 100
     lda_model = LDA.train(result_tfidf_rdd.mapValues(Vectors.fromML).map(list), k=num_topics, 
     maxIterations=max_iterations)
    '''
</code></pre>

<p>image of dataset</p>

<p><a href=""https://i.stack.imgur.com/uUjsj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uUjsj.jpg"" alt=""image of dataset""></a></p>
",2020-03-21 03:06:13,2020-03-21 03:06:13,LDA using PySpark,<out-of-memory><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25516,60638629,2020-03-11 14:42:06,,"<p>I have a data frame like this</p>

<pre><code>import pandas as pd
from gensim.corpora import Dictionary

tmp = pd.DataFrame({""word"":  [1, 0, 0, 0, 0, 0],
                    ""house"": [0, 1, 0, 0, 0, 0],
                    ""tree"":  [0, 0, 1, 0, 0, 1], # occurred twice
                    ""car"":   [0, 0, 0, 1, 0, 0],
                    ""food"":  [0, 0, 0, 0, 1, 0],
                    ""train"": [0, 0, 0, 0, 0, 1]})
mydict = gensim.corpora.Dictionary()
</code></pre>

<p>from this, I want to create a <code>gensim</code> corpus.</p>

<p>I have tried <code>mycorp = [mydict.doc2bow(col, allow_update=True) for col in tmp.columns]</code> but the resulting corpus seems to not have been properly created:</p>

<blockquote>
  <p>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</p>
</blockquote>

<p>Can someone help me with this? I would like the resulting dictionary to represent the fact that word ""tree"" occurred twice in this data frame (i.e. the sum of the column).</p>
",2020-03-11 15:15:50,2020-03-11 15:15:50,gensim corpus from sparse matrix,<python><python-3.x><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25528,60627294,2020-03-10 23:29:42,,"<p>I am using Python version 3.5 in a virtual environment and when trying to import the below command i am getting ""ImportError: cannot import name 'Type'""</p>

<p>from gensim.models.phrases import Phraser</p>

<p>I have uninstalled all other packages and just installed gensim and still it fails. Any suggestions would be of great help</p>

<p>----> 1 from gensim.models.phrases import Phraser
      2 from gensim.models.word2vec import Word2Vec
      3 import pickle
      4 from botocore.client import Config</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/<strong>init</strong>.py in 
      3 """"""
      4 
----> 5 from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
      6 import logging
      7 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/parsing/<strong>init</strong>.py in 
      2 
      3 from .porter import PorterStemmer  # noqa:F401
----> 4 from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
      5                             strip_tags, strip_short, strip_numeric,
      6                             strip_non_alphanum, strip_multiple_whitespaces,</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/parsing/preprocessing.py in 
     40 import glob
     41 
---> 42 from gensim import utils
     43 from gensim.parsing.porter import PorterStemmer
     44 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/utils.py in 
     38 import numpy as np
     39 import numbers
---> 40 import scipy.sparse
     41 
     42 from six import iterkeys, iteritems, itervalues, u, string_types, unichr</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/<strong>init</strong>.py in 
    154     # This makes ""from scipy import fft"" return scipy.fft, not np.fft
    155     del fft
--> 156     from . import fft</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/fft/<strong>init</strong>.py in 
     74 from <strong>future</strong> import division, print_function, absolute_import
     75 
---> 76 from ._basic import (
     77     fft, ifft, fft2, ifft2, fftn, ifftn,
     78     rfft, irfft, rfft2, irfft2, rfftn, irfftn,</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/fft/_basic.py in 
----> 1 from scipy._lib.uarray import generate_multimethod, Dispatchable
      2 import numpy as np
      3 
      4 
      5 def _x_replacer(args, kwargs, dispatchables):</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/uarray.py in 
     25     from uarray import _Function
     26 else:
---> 27     from ._uarray import *
     28     from ._uarray import _Function
     29 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/_uarray/<strong>init</strong>.py in 
    112 """"""
    113 
--> 114 from ._backend import *
    115 
    116 <strong>version</strong> = '0.5.1+5.ga864a57.scipy'</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/_uarray/_backend.py in 
----> 1 from typing import (
      2     Callable,
      3     Iterable,enter code here
      4     Dict,
      5     Tuple,</p>

<p>ImportError: cannot import name 'Type'</p>
",,2020-03-11 03:19:04,"Importing Gensim gensim.models.phrases import phraser fails with ""ImportError: cannot import name 'Type'""",<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25533,60672361,2020-03-13 14:34:59,,"<p>I have used gensims word embeddings to find vectors of each word. Then I used K-means to find clusters of word. There are close to <code>10,000</code> tokens/words and I want to plot them.</p>

<p>I want to plot the result in the following way:</p>

<ul>
<li>Annotate points with name of <code>words</code></li>
<li>Different color for clusters</li>
</ul>

<p>Here is what I have done. </p>

<pre><code>tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)#, random_state=13)


def tsne_plot(data):
    ""Creates and TSNE model and plots it""

    data=data.sample(n = 500).reset_index()
    word=data[""word""]
    cluster=data[""clusters""]
    data=data.drop([""clusters"",""word""],axis=1)

    X = tsne.fit_transform(data)

    plt.figure(figsize=(48, 48)) 
    for i in range(len(X)):
        plt.scatter(X[:,0][i],X[:,1][i],c=cluster[i])
        plt.annotate(word[i],
                     xy=(X[:,0][i],X[:,1][i]),
                     xytext=(3, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

tsne_plot(data)
</code></pre>

<p>Though it's annotating the <code>words</code> but failing to color different groups/clusters?</p>

<p>Anyother other approach which annoates with word anmes and colors different clusters?</p>
",,2020-03-14 12:46:30,How to plot the output of k-means clustering of word embedding using python?,<python-3.x><matplotlib><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25546,60629671,2020-03-11 05:19:59,,"<p>I have a .csv term-document matrix, and I wanna perform some latent dirichlet allocation using gensim in python. However, I'm not particularly familiar with Python <em>or</em> LDA.</p>

<p>I posted in the gensim...forum? I dunno if that's what it's called. The guy that wrote the package responded and had this to say:</p>

<blockquote>
  <p>how big is your term-document CSV matrix?</p>
  
  <p>If it's small enough = fits in RAM, you could: </p>
  
  <p>1) use numpy.loadtxt()
  to load your CSV into an in-memory matrix </p>
  
  <p>2) convert the matrix to a corpus with gensim.matutils.Dense2Corpus() . Check out its documents_columns flag, it lets you switch between document-term and term-document transposition easily. </p>
  
  <p>3) use that corpus to train your LDA model.</p>
</blockquote>

<p>So that leads me to believe that the answer to <a href=""https://stackoverflow.com/questions/27220927/passing-term-document-matrix-to-gensim-lda-model"">this question</a> isn't correct. </p>

<p>It seems like a dictionary is a necessary input to a LDA model; is this not correct? Here's what I have that I think successfully sticks the .csv into a corpus. </p>

<pre><code>file = np.genfromtxt(fname=fPathName, dtype=""int"", delimiter="","", skip_header=True, missing_values="""", filling_values=0)


corpus = gensim.matutils.Dense2Corpus(file, documents_columns=False)
</code></pre>

<p>Any help would be appreciated.</p>

<p>Edit: turns out that a Gensim dictionary and a Python dictionary are not exactly the same things.</p>
",2020-03-14 00:37:59,2020-03-14 00:37:59,"Trying to make use of a library to conduct some topic modeling, but it's not going well",<python><gensim><lda><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
25564,60613532,2020-03-10 08:03:07,,"<p>Here, best_model_lda is an sklearn based LDA model and we are trying to find a coherence score for this model..</p>

<pre><code>coherence_model_lda = CoherenceModel(model = best_lda_model,texts=data_vectorized, dictionary=dictionary,coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\n Coherence Score :',coherence_lda)
</code></pre>

<p>Output : This error pops up because i'm trying to find the coherence score of an  sklearn LDA topic model, is there a way around it. Also , what metric is the sklearn LDA using to group these words together ?</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics_from_model(model, topn)
   490                 matutils.argsort(topic, topn=topn, reverse=True) for topic in
--&gt; 491                 model.get_topics()
   492             ]

AttributeError: 'LatentDirichletAllocation' object has no attribute 'get_topics'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-106-ce8558d82330&gt; in &lt;module&gt;
----&gt; 1 coherence_model_lda = CoherenceModel(model = best_lda_model,texts=data_vectorized, dictionary=dictionary,coherence='c_v')
     2 coherence_lda = coherence_model_lda.get_coherence()
     3 print('\n Coherence Score :',coherence_lda)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in __init__(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)
   210         self._accumulator = None
   211         self._topics = None
--&gt; 212         self.topics = topics
   213 
   214         self.processes = processes if processes &gt;= 1 else max(1, mp.cpu_count() - 1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in topics(self, topics)
   433                     self.model)
   434         elif self.model is not None:
--&gt; 435             new_topics = self._get_topics()
   436             logger.debug(""Setting topics to those of the model: %s"", self.model)
   437         else:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics(self)
   467     def _get_topics(self):
   468         """"""Internal helper function to return topics from a trained topic model.""""""
--&gt; 469         return self._get_topics_from_model(self.model, self.topn)
   470 
   471     @staticmethod

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics_from_model(model, topn)
   493         except AttributeError:
   494             raise ValueError(
--&gt; 495                 ""This topic model is not currently supported. Supported topic models""
   496                 "" should implement the `get_topics` method."")
   497 

ValueError: This topic model is not currently supported. Supported topic models should implement the `get_topics` method.```
</code></pre>
",,2020-06-02 12:46:44,How do I calculate the coherence score of an sklearn LDA model?,<scikit-learn><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,True
25576,60727025,2020-03-17 17:05:46,,"<p>Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.</p>

<p>I'm using the function <code>Word2Vec</code> from the python library <code>gensim</code>.</p>

<p>My problem is that I <strong>can't run my model on every word of my corpus as long as I set the parameter <code>min_count</code> greater than one</strong>. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an <strong>error saying <em>word 'blabla' is not in the vocabulary</em></strong>, whereas this is exactly what I want ( I want this word to be out of the vocabulary).</p>

<p>I can imagine this is not very clear, then find below a reproducible example:</p>

<pre><code>import gensim
from gensim.models import Word2Vec

# My corpus
corpus=[[""paris"",""not"",""great"",""city""],
       [""praha"",""better"",""great"",""than"",""paris""],
       [""praha"",""not"",""country""]]

# Load a pre-trained model - The orignal one based on google news 
model_google = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)

# Initializing our model and upgrading it with Google's 
my_model = Word2Vec(size=300, min_count=2)#with min_count=1, everything works fine
my_model.build_vocab(corpus)
total_examples = my_model.corpus_count
my_model.build_vocab([list(model_google.vocab.keys())], update=True)
my_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)
my_model.train(corpus, total_examples=total_examples, epochs=my_model.iter)

# Show examples
print(my_model['paris'][0:10])#works cause 'paris' is present twice
print(my_model['country'][0:10])#does not work cause 'country' appears only once
</code></pre>

<p>You can find Google's model <a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">there</a> for example, but feel free to use any model or just do without, this is not the point of my post.</p>

<p>As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter <code>min_count</code> to 1, everything works fine.</p>

<p>I hope it is clear enough.</p>

<p>Thanks.</p>
",,2020-03-19 00:10:24,[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
25580,60665666,2020-03-13 06:13:41,,"<p>My objective is to retrieve the question from the answer given as input.
I have used sequence2sequence model by implementing LSTM and softmax activation function.
I am randomly generating a question from the database and then retrieving the question based on the answer given as the input, the resulting question i receive from the neural network is matched with the randomly generated question to check for similarity using SequenceMatcher.
The issue is my answer is not getting mapped properly with the given answer and the randomly generated question is also partial.
Following is my code written in spyder IDE :-</p>

<pre><code>import numpy as np
import tensorflow as tf
import pickle
import json
from tensorflow.keras import layers , activations , models , preprocessing
from difflib import SequenceMatcher
import random

#print( tf.VERSION )

from tensorflow.keras import preprocessing , utils

#====================== Preparing Data ========================================

with open('data_1.json', encoding='utf-8') as f:
    d = json.load(f)

questions_p1 = list()
questions_p2 = list()
questions_p3 = list()
questions_p4 = list()

questions = list()
answers = list()


for rep in d[""intents""]:
    for rep1 in rep[""proficiency""]:
        if(rep1[""level""] == ""P1""):
            for rep2 in rep1[""questions""]:
                questions_p1.append(rep2[""question""])

        if(rep1[""level""] == ""P2""):
            for rep2 in rep1[""questions""]:
                questions_p2.append(rep2[""question""])

        if(rep1[""level""] == ""P3""):
            for rep2 in rep1[""questions""]:
                questions_p3.append(rep2[""question""])

        if(rep1[""level""] == ""P4""):
            for rep2 in rep1[""questions""]:
                questions_p4.append(rep2[""question""])


for rep in d[""intents""]:
    for rep1 in rep[""proficiency""]:
        for rep2 in rep1[""questions""]:
            questions.append(rep2[""question""])
        for rep2 in rep1[""questions""]:
            answers.append(rep2[""responses""])

#=========================================================================
# answers_with_tags = list()
# for i in range( len( answers ) ):
#     answers_with_tags.append( answers[i] )


# answers = list()
# for i in answers_with_tags:
#     for j in i:
#         answers.append( '&lt;START&gt; ' + j + ' &lt;END&gt;' )

#print(answers)
tokenizer = preprocessing.text.Tokenizer()
tokenizer.fit_on_texts( questions + answers )

VOCAB_SIZE = len( tokenizer.word_index )+1
print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))

#======================= Neural Network =======================================

from gensim.models import Word2Vec


vocab = []
for word in tokenizer.word_index:
    vocab.append( word )

print(vocab)
# def tokenize( sentences ):
#     tokens_list = []
#     vocabulary = []
#     for sentence in sentences:
#         sentence = sentence.lower()
#         sentence = re.sub( '[^a-zA-Z]', ' ', sentence )
#         tokens = sentence.split()
#         vocabulary += tokens
#         tokens_list.append( tokens )
#     return tokens_list , vocabulary

a = list()
for i in tokenizer.word_index.keys():
    a.append(i)

# p = tokenize( questions + answers )
model = Word2Vec( [a] , min_count = 1) 

embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )
for i in range( len( tokenizer.word_index ) ):
    embedding_matrix[ i ] = model[ vocab[i] ]

#========================== LSTM Prep =========================================

#encoder_input__1
tokenized_answers = tokenizer.texts_to_sequences( answers )
maxlen_answers = max( [ len(x) for x in tokenized_answers ] )
padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers, padding='post' )
encoder_input_data = np.array( padded_answers )
print( encoder_input_data.shape , maxlen_answers )

#decoder_input_data
tokenized_questions = tokenizer.texts_to_sequences( questions )
maxlen_questions = max( [ len(x) for x in tokenized_questions ] )
padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )
decoder_input_data = np.array( padded_questions )
print( decoder_input_data.shape , maxlen_answers )

#decoder_output_data
tokenized_questions = tokenizer.texts_to_sequences( questions )
# for i in range(len(tokenized_questions)) :
#     tokenized_questions[i] = tokenized_questions[i][1:]
padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )
onehot_questions = utils.to_categorical( padded_questions , VOCAB_SIZE )
decoder_output_data = np.array( onehot_questions )
print( decoder_output_data.shape )

#============================ Implementing LSTM ===============================

encoder_inputs = tf.keras.layers.Input(shape=( None , ))
encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True, weights=[embedding_matrix], trainable = ""false"" ) (encoder_inputs)
encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 100 , return_state=True )( encoder_embedding )
encoder_states = [ state_h , state_c ]

decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))
decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True, weights=[embedding_matrix], trainable = ""false"") (decoder_inputs)
decoder_lstm = tf.keras.layers.LSTM( 100 , return_state=True , return_sequences=True )
decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )
decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) 
output = decoder_dense ( decoder_outputs )

model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )
model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['mae', 'acc'])
#model = tf.keras.models.load_model('model.h5')
model.summary()

model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=8 , epochs = 58) 
model.save( 'model2.h5' )

#==============================================================================

def make_inference_models():

    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)

    decoder_state_input_h = tf.keras.layers.Input(shape=( 100 ,))
    decoder_state_input_c = tf.keras.layers.Input(shape=( 100 ,))

    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

    decoder_outputs, state_h, state_c = decoder_lstm(
        decoder_embedding , initial_state=decoder_states_inputs)
    decoder_states = [state_h, state_c]
    decoder_outputs = decoder_dense(decoder_outputs)
    decoder_model = tf.keras.models.Model(
        [decoder_inputs] + decoder_states_inputs,
        [decoder_outputs] + decoder_states)

    return encoder_model , decoder_model

def str_to_tokens( sentence : str ):
    words = sentence.lower().split()
    tokens_list = list()
    for word in words:
        tokens_list.append( tokenizer.word_index[ word ] ) 
    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')

enc_model , dec_model = make_inference_models()

for _ in range(10):
    question = random.sample(questions,1)
    question = question[0][8:]
    question = question[:-6]
    print(question)
    states_values = enc_model.predict( str_to_tokens( input( 'Enter answer : ' ) ) )
    empty_target_seq = np.zeros( ( 1 , 1 ) )
    empty_target_seq[0, 0] = tokenizer.word_index['start']
    stop_condition = False
    decoded_translation = ''
    while not stop_condition :
        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )
        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )
        sampled_word = None
        for word , index in tokenizer.word_index.items() :
            if sampled_word_index == index :
                decoded_translation += ' {}'.format( word )
                sampled_word = word

        if sampled_word == 'end' or len(decoded_translation.split()) &gt; maxlen_answers:
            stop_condition = True

        empty_target_seq = np.zeros( ( 1 , 1 ) )  
        empty_target_seq[ 0 , 0 ] = sampled_word_index
        states_values = [ h , c ] 

    answer = decoded_translation
    ratio = SequenceMatcher(None, question, answer).ratio()
    print(decoded_translation)
    print(ratio)
    if(ratio &gt; 0.5):
        print(""Correct"")
</code></pre>

<p>The output i am getting :- </p>

<p><a href=""https://i.stack.imgur.com/VpXC2.png"" rel=""nofollow noreferrer"">Output Image</a></p>
",,2020-03-13 06:13:41,An inverse chatbot - Getting questions from an answer given as input,<python><deep-learning><neural-network><nlp><chatbot>,,,CC BY-SA 4.0,False,False,True,False,False
25590,60644150,2020-03-11 20:40:14,,"<p>I am trying to train a word2vec model with the <a href=""http://www.english-corpora.org/coha/"" rel=""nofollow noreferrer"">COHA corpus</a> by using pre-computed bigram counts co-occurrence counts that the corpus' author makes available <a href=""https://www.ngrams.info/download_coha.asp"" rel=""nofollow noreferrer"">here</a>.</p>

<p>How can I achieve that using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">gensim</a>?</p>
",,2020-03-12 20:06:59,How can you train a word2vec in gensim from a list of co-occurrence (bigram) counts?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25598,60680146,2020-03-14 05:56:44,,"<p>I am trying to use Gensim LDA modelling to topic model of dataset of food recipes. I wish to have topics based the key ingredients in the recipe. But the recipe text contains more words that are generic English and are not ingredient names. Hence my topic outcome is not as good as expected. I am trying to understand the impact of word frequency in the LDA topic outcome. Thanks.</p>
",,2020-09-06 15:54:08,What is the impact of word frequency on Gensim LDA Topic modelling,<python-3.x><gensim><lda><topic-modeling><word-frequency>,,,CC BY-SA 4.0,False,False,True,False,False
25606,60669506,2020-03-13 11:17:40,,"<p>Now I am trying to use <code>gensim Phrases</code> in order to learn the phrase/special meaning base on my own corpus.</p>

<p>Suppose I have the corpus related to the car brand, by removing the <strong>punctuation</strong> and <strong>stopwords</strong>, <strong>tokenizing the sentence</strong>, eg:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
...
</code></pre>

<p>In this way, I would like to use <code>gensim Phrases</code> to learn so that output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston_martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
...
</code></pre>

<p>However, if a lot of sentences that have a lot of punctuation:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
sent4 = 'jaguar, aston martin, mini cooper are british car brand'
sent5 = 'In all brand, I love jaguar, aston martin and mini cooper'
...

</code></pre>

<p>Then the output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, sent4, sent5, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston', 'martin_mini', 'cooper', 'british', 'car', 'brand']
['all', 'brand', 'love', 'jaguar', 'aston', 'martin_mini', 'cooper']
...
</code></pre>

<p>In this case, how should I handle the sentence with lot of punctuation to prevent <code>martin_mini</code> case and make the output looks like:</p>

<pre><code>['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston_martin', 'mini_cooper', 'british', 'car', 'brand'] # Change
['all', 'brand', 'love', 'jaguar', 'aston_martin', 'mini_cooper'] # Change
...
</code></pre>

<p>Thanks so much for helping!</p>
",,2020-03-14 01:56:47,Gensim phrase handling sentence with a lot of punctuation,<python><nlp><gensim><phrase>,,,CC BY-SA 4.0,False,False,True,False,False
25610,60729841,2020-03-17 20:35:14,,"<p>This is my input (sample*)</p>

<pre><code>data = [""['human', 'interface', 'computer']"",
 ""['survey', 'user', 'computer', 'system', 'response', 'time']"",
 ""['eps', 'user', 'interface', 'system']"",
 ""['system', 'human', 'system', 'eps']"",
 ""['user', 'response', 'time']"",
 ""['trees']""]
</code></pre>

<p>And I have tried implementing</p>

<p>dictionary = corpora.Dictionary(text_data)</p>

<p>But this is the error I get,</p>

<p>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</p>

<p>Please help if you see why that wouldn't work</p>
",,2020-03-17 20:42:37,Python Gensim Dictionary,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25616,60682634,2020-03-14 12:18:28,,"<p>I am trying to load a trained fasttext model using gensim. The model has been trained on some data. Earlier, I have used <code>model.save()</code> with a extension of <code>.bin</code> to use it later. After the training process and saving the model using <code>model.save</code> in <code>.bin</code> format, generates 3 files respectively. They are:</p>

<p>1) .bin  </p>

<p>2) bin.trainable vectors_ngrams_lockf</p>

<p>3) bin.wv.vectors_ngrams </p>

<p>Now I am unable to load the trained binary file (.bin).  </p>

<p>But I don't understand why I am getting a error named:</p>

<blockquote>
  <p>raise NotImplementedError(""Supervised fastText models are not supported"")
  NotImplementedError: Supervised fastText models are not supported</p>
</blockquote>

<p>After going through many blogs, peoples have suggested that <code>gensim</code> does not supports supervised training. It's fine. My question is how can I be able to load the trained binary model. Shall I need to train the model differently.</p>

<p>Any help is appreciated.</p>

<p>What I have tried after the training process: </p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim.models import FastText, fasttext
model = FastText.load_fasttext_format('m1.bin')
print(model)
</code></pre>
",2020-03-14 13:48:18,2020-03-14 20:00:51,Issues while loading a trained fasttext model using gensim,<python><python-3.x><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
25622,60685536,2020-03-14 17:41:13,,"<p>I'm trying to load a word2vec vector that i've trained myself using gensim word to vec this way:</p>

<pre><code>embedding_model = gensim.models.Word2Vec(sentences = X_train, 
                                         size = 300, 
                                         window = 10, 
                                         min_count = 1)
</code></pre>

<p>Into a CNN using an Embedding layer using the following setting:</p>

<pre><code>model = Sequential()
model.add(Embedding(4998, 300, weights=[emb_weights], input_length=100, trainable = False))
(...)
</code></pre>

<p>My problem is that I don't know how to extract the vector of the embedding weights correctly. <strong>I have tryed with:</strong></p>

<pre><code>emb_weights = embedding_model.wv.vector
</code></pre>

<p>And the CNN actually accepts the input, but my accuracy stalls immediatly at a 50% rate (for two classes):</p>

<pre><code>Epoch 5/5

25000/25000 [==============================]  

9s 378us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000
</code></pre>

<p><strong>Do you think I am giving the wrong input to the CNN?</strong></p>
",,2020-03-14 17:41:13,Plug self trained embedding vectors into a CNN,<keras><gensim><word2vec><embedding><cnn>,,,CC BY-SA 4.0,False,False,True,False,False
25645,60785538,2020-03-21 07:42:28,,"<p>I have calculated distances between two sentences using wmdistance() funtion of gensim with pre-trained model</p>
<p>Now, I want to similarity between them and tried with  n_similarity() funnction, but keyerror occured</p>
<p>keyerror : word not in vacabulary</p>
<p>This shows  screenshoot of error example
<a href=""https://i.stack.imgur.com/r1h9F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r1h9F.png"" alt=""screenshoot of error example"" /></a></p>
<p>Anyone have got idea on this, please?</p>
",2020-06-20 09:12:55,2020-03-22 05:51:19,"In gensim with pretrained model, wmdistance is working well, but n_similarity is not",<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25660,60786940,2020-03-21 10:56:43,,"<p>I have an runtime error: </p>

<pre><code>RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
  0%|          | 0/29 [00:48&lt;?, ?it/s]
</code></pre>

<p>When I try run this code: </p>

<pre><code>def topic_model_coherence_generator (corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):
    models=[]
    coherence_scores = []
    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):
        lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto',
                                   random_state=42, iterations=500, num_topics=topic_nums, passes=20, eval_every=None)

        cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,
                                                      texts=norm_corpus_bigrams, dictionary=dictionary,
                                                      coherence='c_v')

        coherence_score= cv_coherence_model_lda.get_coherence()
        coherence_scores.append(coherence_score)
        models.append(lda_model)
    return models, coherence_scores

lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus,
                                                               texts=norm_corpus_bigrams,
                                                               dictionary= dictionary,
                                                               start_topic_count=2,
                                                               end_topic_count=30,
                                                               step=1, cpus=16)
</code></pre>

<p>That I want is obtain the optimal number of topics of my corpus for obtain then the topics and interpreting topic model results. 
I'm biologist so I don't know how can I fix it. 
Thanks for your help</p>
",,2020-03-22 05:56:29,"Runtime Error when running a LDA model of gensim, how can I fix it?",<model><runtime-error><runtime><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25665,60738355,2020-03-18 11:14:39,,"<p>i'm working on an Multi-Label Emotion Classification problem to be solved by word2vec. this is my code that i've learned from a couple of tutorials. now the accuracy is very low. about 0.02 which is telling me something is wrong in my code. but i cannot find it. i tried this code for TF-IDF and BOW (obviously except word2vec part) and i got much better accuracy scores such as 0.28, but it seems this one is somehow wrong:</p>
<pre><code>np.set_printoptions(threshold=sys.maxsize)
wv = gensim.models.KeyedVectors.load_word2vec_format(&quot;E:\\GoogleNews-vectors-negative300.bin&quot;, binary=True)
wv.init_sims(replace=True)

#Pre-Processor Function
pre_processor = TextPreProcessor(
    omit=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
    
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
     
    segmenter=&quot;twitter&quot;, 
    
    corrector=&quot;twitter&quot;, 
    
    unpack_hashtags=True,
    unpack_contractions=True,
    
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    
    dicts=[emoticons]
)

#Averaging Words Vectors to Create Sentence Embedding
def word_averaging(wv, words):
    all_words, mean = set(), []
    
    for word in words:
        if isinstance(word, np.ndarray):
            mean.append(word)
        elif word in wv.vocab:
            mean.append(wv.syn0norm[wv.vocab[word].index])
            all_words.add(wv.vocab[word].index)

    if not mean:
        logging.warning(&quot;cannot compute similarity with no input %s&quot;, words)
        # FIXME: remove these examples in pre-processing
        return np.zeros(wv.vector_size,)

    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
    return mean

def  word_averaging_list(wv, text_list):
    return np.vstack([word_averaging(wv, post) for post in text_list ])

#Secondary Word-Averaging Method
def get_mean_vector(word2vec_model, words):
# remove out-of-vocabulary words
words = [word for word in words if word in word2vec_model.vocab]
if len(words) &gt;= 1:
    return np.mean(word2vec_model[words], axis=0)
else:
    return []

#Loading data
raw_train_tweets = pandas.read_excel('E:\\train.xlsx').iloc[:,1] #Loading all train tweets
train_labels = np.array(pandas.read_excel('E:\\train.xlsx').iloc[:,2:13]) #Loading corresponding train labels (11 emotions)

raw_test_tweets = pandas.read_excel('E:\\test.xlsx').iloc[:,1] #Loading 300 test tweets
test_gold_labels = np.array(pandas.read_excel('E:\\test.xlsx').iloc[:,2:13]) #Loading corresponding test labels (11 emotions)
print(&quot;please wait&quot;)

#Pre-Processing
train_tweets=[]
test_tweets=[]
for tweets in raw_train_tweets:
    train_tweets.append(pre_processor.pre_process_doc(tweets))

for tweets in raw_test_tweets:
    test_tweets.append(pre_processor.pre_process_doc(tweets))

#Vectorizing 
train_array = word_averaging_list(wv,train_tweets)
test_array = word_averaging_list(wv,test_tweets)

#Predicting and Evaluating    
clf = LabelPowerset(LogisticRegression(solver='lbfgs', C=1, class_weight=None))
clf.fit(train_array,train_labels)
predicted= clf.predict(test_array)
intersect=0
union=0
accuracy=[]
for i in range(0,3250): #i have 3250 test tweets.
    for j in range(0,11): #11 emotions
        if predicted[i,j]&amp;test_gold_labels[i,j]==1:
            intersect+=1
        if predicted[i,j]|test_gold_labels[i,j]==1:
            union+=1
    
    accuracy.append(intersect/union) if union !=0 else accuracy.append(0.0)
    intersect=0
    union=0
print(np.mean(accuracy))
</code></pre>
<p>The Result:</p>
<pre><code>0.4674498168498169
</code></pre>
<p>And i printed predicted variable (for tweet 0 to 10) to see how it looks like:</p>
<pre><code>  (0, 0)    1
  (0, 2)    1
  (2, 0)    1
  (2, 2)    1
  (3, 4)    1
  (3, 6)    1
  (4, 0)    1
  (4, 2)    1
  (5, 0)    1
  (5, 2)    1
  (6, 0)    1
  (6, 2)    1
  (7, 0)    1
  (7, 2)    1
  (8, 4)    1
  (8, 6)    1
  (9, 3)    1
  (9, 8)    1
</code></pre>
<p>as you can see, it only show 1's. for example (6,2) means in tweet number 6, emotion number 2 is 1. (9,8) means in tweet number 9, emotion number 8 is 1. the other emotions considered as 0. but you can imagine it like this to better understand what i've done in Accuracy method:</p>
<pre><code>gold emotion for tweet 0:      [1 1 0 0 0 0 1 0 0 0 1]
predicted emotion for tweet 0: [1 0 1 0 0 0 0 0 0 0 0]
</code></pre>
<p>i've used union and intersect for the indexes one by one. 1 to 1. 1 to 1. 0 to 1, until gold emotion 11 to predicted emotion 11. i did this for all tweets in two for loops.</p>
<h1>Creating Word2Vec vectors on my tweets:</h1>
<p>now i want to use gensim to create Word2Vec vectors on my tweet dataset. i changed some parts of the code above as below:</p>
<pre><code>#Averaging Words Vectors to Create Sentence Embedding
def word_averaging(wv, words):
    all_words, mean = set(), []

    for word in words:
        if isinstance(word, np.ndarray):
            mean.append(word)
        elif word in wv.vocab:
            mean.append(wv.syn0norm[wv.vocab[word].index])
            all_words.add(wv.vocab[word].index)

    if not mean:
        logging.warning(&quot;cannot compute similarity with no input %s&quot;, words)
        # FIXME: remove these examples in pre-processing
        return np.zeros(wv.vector_size,)

    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
    return mean

def  word_averaging_list(wv, text_list):
    return np.vstack([word_averaging(wv, post) for post in text_list ])

#Loading data
raw_aggregate_tweets = pandas.read_excel('E:\\aggregate.xlsx').iloc[:,0] #Loading all train tweets

raw_train_tweets = pandas.read_excel('E:\\train.xlsx').iloc[:,1] #Loading all train tweets
train_labels = np.array(pandas.read_excel('E:\\train.xlsx').iloc[:,2:13]) #Loading corresponding train labels (11 emotions)

raw_test_tweets = pandas.read_excel('E:\\test.xlsx').iloc[:,1] #Loading 300 test tweets
test_gold_labels = np.array(pandas.read_excel('E:\\test.xlsx').iloc[:,2:13]) #Loading corresponding test labels (11 emotions)
print(&quot;please wait&quot;)

#Pre-Processing
aggregate_tweets=[]
train_tweets=[]
test_tweets=[]
for tweets in raw_aggregate_tweets:
    aggregate_tweets.append(pre_processor.pre_process_doc(tweets))

for tweets in raw_train_tweets:
    train_tweets.append(pre_processor.pre_process_doc(tweets))

for tweets in raw_test_tweets:
    test_tweets.append(pre_processor.pre_process_doc(tweets))
    
print(len(aggregate_tweets))
#Vectorizing 
w2v_model = gensim.models.Word2Vec(aggregate_tweets, min_count = 10, size = 300, window = 8)

print(w2v_model.wv.vectors.shape)

train_array = word_averaging_list(w2v_model.wv,train_tweets)
test_array = word_averaging_list(w2v_model.wv,test_tweets)
</code></pre>
<p>but i get this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-8a5fe4dbf144&gt; in &lt;module&gt;
    110 print(w2v_model.wv.vectors.shape)
    111 
--&gt; 112 train_array = word_averaging_list(w2v_model.wv,train_tweets)
    113 test_array = word_averaging_list(w2v_model.wv,test_tweets)
    114 

&lt;ipython-input-1-8a5fe4dbf144&gt; in word_averaging_list(wv, text_list)
     70 
     71 def  word_averaging_list(wv, text_list):
---&gt; 72     return np.vstack([word_averaging(wv, post) for post in text_list ])
     73 
     74 #Averaging Words Vectors to Create Sentence Embedding

&lt;ipython-input-1-8a5fe4dbf144&gt; in &lt;listcomp&gt;(.0)
     70 
     71 def  word_averaging_list(wv, text_list):
---&gt; 72     return np.vstack([word_averaging(wv, post) for post in text_list ])
     73 
     74 #Averaging Words Vectors to Create Sentence Embedding

&lt;ipython-input-1-8a5fe4dbf144&gt; in word_averaging(wv, words)
     58             mean.append(word)
     59         elif word in wv.vocab:
---&gt; 60             mean.append(wv.syn0norm[wv.vocab[word].index])
     61             all_words.add(wv.vocab[word].index)
     62 

TypeError: 'NoneType' object is not subscriptable
</code></pre>
",2020-06-20 09:12:55,2020-04-08 14:17:36,Classification accuracy is too low (Word2Vec),<python><nlp><classification><word2vec><emotion>,,,CC BY-SA 4.0,False,False,True,False,False
25676,60788579,2020-03-21 11:39:37,,"<p>I have an runtime error:</p>

<pre><code>RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
  0%|          | 0/29 [00:48&lt;?, ?it/s]
</code></pre>

<p>When I try run this code (from the book ""Text Analytics with Python: A Practitioner's Guide to Natural Language Processing"", 2nd edition, by Dipanjan Sarkar):</p>

<pre><code>def topic_model_coherence_generator (corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):
    models=[]
    coherence_scores = []
    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):
        lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto',
                                   random_state=42, iterations=500, num_topics=topic_nums, passes=20, eval_every=None)

        cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,
                                                      texts=norm_corpus_bigrams, dictionary=dictionary,
                                                      coherence='c_v')

        coherence_score= cv_coherence_model_lda.get_coherence()
        coherence_scores.append(coherence_score)
        models.append(lda_model)
    return models, coherence_scores

lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus,
                                                               texts=norm_corpus_bigrams,
                                                               dictionary= dictionary,
                                                               start_topic_count=2,
                                                               end_topic_count=30,
                                                               step=1, cpus=16)
</code></pre>

<p>That I want is obtain the optimal number of topics of my corpus for obtain then the topics and interpreting topic model results. I'm biologist so I don't know how can I fix it. Thanks for your help</p>
",2020-03-22 03:15:42,2020-03-22 03:15:42,"I try to get the optimal number of topics in my corpus with LDA model but return to me a runtime error, how can I fix it?",<machine-learning><nlp><python><model>,,,CC BY-SA 4.0,False,False,True,False,False
25685,60723228,2020-03-17 13:17:19,,"<p>I have a database with supermarket product items(it contains name, descriptions, price, stock, etc). </p>

<p>I want to make a price comparison between those supermarkets, but, for that i need to know if supermarket A and B refers to the same product. </p>

<p>For example I found out that supermarket <strong>A</strong> has a product called <code>Leche Evaporada GLORIA Azul Paquete 6un Lata 400g</code> and supermarket <strong>B</strong> has a product named <code>Leche Evaporada Gloria Azul Pack 6 Unid x 400 g</code> and those refers to the same product.</p>

<p>I pointed out that I will need to have semantic comparison for those cases. I'm new in this problems so I don't really know what is the best solution to not underestimate the problem or overkill it.</p>

<p>What I'm doing right now with not so great results:</p>

<ol>
<li>I'm only using product names.</li>
<li>Remove stop words from those product names.</li>
<li>Convert the sentence in an array of words.</li>
<li>Get frequency for every word.</li>
<li>If a word has frequency &lt;= 1, then delete it.</li>
<li>With that words I create a dictionary(bag of words) that i will use to map an array of words(a sentence converted) to a feature vector.</li>
<li>Then I ""train"" a TFIDF model with all feature vectors.</li>
<li>Make comparisons(with no great results).</li>
</ol>

<p>I'm using python as LP and <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> to create models, dictionaries(bag of word) and to make comparisons.</p>

<p>EDIT:
Another examples:</p>

<pre><code>Leche Fresca UHT GLORIA Entera Bolsa 946ml == Leche Entera UHT Gloria Bolsa 946 ml
Yogurt Griego Gloria con Miel y Granola Vaso 115 g == Yogurt Griego GLORIA Batido con Miel Vaso 115g
Leche sin Lactosa GLORIA Mocaccino Botella 330ml == Shake Mocaccino UHT Gloria Frasco 330 ml.
</code></pre>
",2020-03-17 15:44:00,2020-03-17 15:44:00,What kind of model/technique should I use to compare supermarket product names,<python><machine-learning><nlp><artificial-intelligence><gensim>,2020-03-19 17:27:18,,CC BY-SA 4.0,False,False,True,False,False
25707,60823095,2020-03-23 23:45:46,,"<p>I am training a custom NER model from scratch using the spacy.blank(""en"") model. I add custom word vectors to it. The vectors are loaded as follows: </p>

<pre><code>from gensim.models.word2vec import Word2Vec
from gensim.models import KeyedVectors
med_vec = KeyedVectors.load_word2vec_format('./wikipedia-pubmed-and-PMC-w2v.bin', binary=True, limit = 300000)
</code></pre>

<p>and I add it to the blank model in this code snippet here: </p>

<pre><code>def main(model=None, n_iter=3, output_dir=None):
    """"""Set up the pipeline and entity recognizer, and train the new entity.""""""
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model) # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        nlp.vocab.reset_vectors(width=200)
        for idx in range(len(med_vec.index2word)):
            word = med_vec.index2word[idx]
            vector = med_vec.vectors[idx]
            nlp.vocab.set_vector(word, vector)
        for key, vector in nlp.vocab.vectors.items():
            nlp.vocab.strings.add(nlp.vocab.strings[key])
        nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
        print(""Created blank 'en' model"")
......Code for training the ner
</code></pre>

<p>I then save this model.</p>

<p>When I try to load the model,
<code>nlp = spacy.load(""./NDLA/vectorModel0"")</code></p>

<p>I get the following error: </p>

<pre><code>
`~\AppData\Local\Continuum\anaconda3\lib\site-packages\thinc\neural\_classes\static_vectors.py in __init__(self, lang, nO, drop_factor, column)
     47         if self.nM == 0:
     48             raise ValueError(
---&gt; 49                 ""Cannot create vectors table with dimension 0.\n""
     50                 ""If you're using pre-trained vectors, are the vectors loaded?""
     51             )

ValueError: Cannot create vectors table with dimension 0.
If you're using pre-trained vectors, are the vectors loaded?
</code></pre>

<p>I also get this warning: </p>

<pre><code> UserWarning: [W019] Changing vectors name from spacy_pretrained_vectors to spacy_pretrained_vectors_336876, to avoid clash with previously loaded vectors. See Issue #3853.
  ""__main__"", mod_spec)
</code></pre>

<p>The vocab directory in the model has a vectors file of size 270 MB. So I know it is not empty... What is causing this error? </p>
",2020-03-24 01:18:39,2020-03-24 01:18:39,pretrained vectors not loading in spacy,<spacy><ner>,,,CC BY-SA 4.0,False,True,True,False,False
25718,60792362,2020-03-21 19:29:03,,"<p>I need a suggestion in unsupervised training of Doc2Vec for the 2 options I have. The scenario is I have N documents each of size greater than 3000 tokens. So now for training which alternative is better:</p>

<ol>
<li>Training with whole document as such.</li>
<li>Breaking the documents into chunks of 1000 tokens and then training it.</li>
</ol>
",,2020-03-22 05:49:29,Doc2Vec Unsupervised training,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25720,60793449,2020-03-21 21:24:52,,"<p>I'm trying to understand how word2vec predicts a word, given a list of words.
Specifically, I trained my skip-gram model on twitter data of 500k tweets with the following parameters:</p>

<pre><code>model = gensim.models.Word2Vec(data, window=5, workers=7, sg=1, min_count=10, size=200)
</code></pre>

<p>Given the words <code>discrimination</code> and <code>uberx</code>, I get the following output:</p>

<pre><code>model.wv.most_similar(positive=[PorterStemmer().stem(WordNetLemmatizer().lemmatize(""discrimination"", pos='v')), WordNetLemmatizer().lemmatize(""uberx"", pos='v')], topn=30)
[('discret', 0.7425585985183716),
 ('fold_wheelchair', 0.7286415696144104),
 ('illeg_deni', 0.7280288338661194),
 ('tradit_cab', 0.7262350916862488),
 ('mobil_aid', 0.7252357602119446),
 ('accommod_disabl', 0.724936842918396),
 ('uberwav', 0.720955491065979),
 ('discrimin_disabl', 0.7206833958625793),
 ('deni_access', 0.7202375531196594),...]
</code></pre>

<p>However, when I search the dataset <code>data</code> which I dumped on my hard drive, for the words ""discrimination"", ""uberx"", and any other word from the output list, I never find a single instance of a datapoint which contained all 3 words. So my question is, how does the model know that, say, word ""accommodation disabled"" is the right word for the context ""discrimination"" and ""uberx"" if it has never seen those 3 words together in a single tweet?</p>
",,2020-03-21 23:00:22,How does word2vec predicts the word correctly but the actual dataset does not contain it?,<python><nlp><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25739,60873334,2020-03-26 17:54:33,,"<p>I've trained a model:</p>

<pre><code>from gensim.models import Word2Vec    

model = Word2Vec(master_sent_list,
                     min_count=5,   
                     size=300,      
                     workers=5,    
                     window=5,      
                     iter=30)  
</code></pre>

<p>Saved it according to <a href=""https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim"">this</a> post:</p>

<pre><code>model.wv.save_word2vec_format(""../moj_word2vec.txt"")
!gzip ../moj_word2vec.txt
!python -m spacy init-model en ../moj_word2vec.model --vectors-loc ../moj_word2vec.txt.gz
</code></pre>

<p>Everything looks fine:</p>

<pre><code> Successfully created model
22470it [00:02, 8397.55it/s]j_word2vec.txt.gz
 Loaded vectors from ../moj_word2vec.txt.gz
 Sucessfully compiled vocab
22835 entries, 22470 vectors
</code></pre>

<p>I then load the model <strong>under a different name</strong>:</p>

<pre><code>nlp = spacy.load('../moj_word2vec.model/')
</code></pre>

<p>Something goes wrong however, because I can't use common commands on <code>nlp</code>; that I can on <code>model</code>.</p>

<p>For example, these work:</p>

<pre><code>model.wv.most_similar('police')
model.vector_size
</code></pre>

<p>But these don't:</p>

<pre><code>nlp.wv.most_similar('police')
AttributeError: 'English' object has no attribute 'wv'

nlp.most_similar('police')
AttributeError: 'English' object has no attribute 'most_similar'

nlp.vector_size
AttributeError: 'English' object has no attribute 'vector_size'
</code></pre>

<p>So something seems to have broken in the loading, or perhaps the saving, could someone help please?</p>
",,2020-03-27 00:23:15,"Having trouble loading custom trained word vectors created in Gensim, into Spacy",<python-3.x><spacy><gensim>,,,CC BY-SA 4.0,False,True,True,False,False
25744,60839302,2020-03-24 21:03:32,,"<p>I have data like this which was the output of gensim LDA model</p>

<pre><code>date       id   score   
1/1/2019    11  [(5,0.8), (11,0.2)] 
1/2/2019    21  [(4,0.7), (10,0.1)] 
1/3/2019    35  [(3,0.4)]   
1/4/2019    44  [(5,0.8),(3,0.5), (11,0.2)] 
</code></pre>

<p>The results should like this. Can anyone help?</p>

<pre><code>date        id  score   new_score
1/1/2019    11  5       0.8
1/1/2019    11  11      0.2
1/2/2019    21  4       0.7
1/2/2019    21  10      0.1
1/3/2019    35  3       0.4
1/4/2019    44  5       0.8
1/4/2019    44  3       0.5
1/4/2019    44  11      0.2
</code></pre>
",2020-03-24 21:43:14,2020-03-24 22:06:47,how to turn list of tuples to data frame with different scores,<python><list><dataframe><tuples><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25750,60702160,2020-03-16 07:55:02,,"<pre><code>data1=[tokens.doc2bow(text) for text in texts]
ldamodel=gensim.models.ldamodel.LdaModel(corpus=data1,id2word=tokens,num_topics=10,random_state=100,update_every=1,chunksize=10,passes=10,alpha='auto',per_word_topics=True)
print(*ldamodel.print_topics(),sep=""\n"")
lda=ldamodel[data1]
l=[ldamodel.get_document_topics(item) for item in data1]
print(l)
</code></pre>

<p>While executing <code>get_document_topics()</code>, it is giving an output of hundreds of lines (as shown in picture). I don't know what does it mean. I actually want the probabilities of topics. Which method should I use to get the topic probabilities?</p>

<p><a href=""https://i.stack.imgur.com/uvDlR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uvDlR.png"" alt=""output of the get_document_topics()""></a></p>
",2020-03-20 04:15:46,2020-03-20 04:15:46,how to get topic probability from the ldamodel by using gensim?,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25771,60840809,2020-03-24 23:18:27,,"<p>When I saved my LdaModel <code>lda_model.save('model')</code>, it saved 4 files:</p>

<ol>
<li><code>model</code> </li>
<li><code>model.expElogbeta.npy</code> </li>
<li><code>model.id2word</code> </li>
<li><code>model.state</code></li>
</ol>

<p>I want to use <code>pyLDAvis.gensim</code> to visualize the topics, which seems to need the model, corpus and dictionary. I was able to load the model and dictionary with:</p>

<pre><code>lda_model = LdaModel.load('model')
dict = corpora.Dictionary.load('model.id2word')
</code></pre>

<p>Is it possible to load the corpus? How?</p>
",,2020-09-03 17:16:07,Gensim: How to load corpus from saved lda model?,<gensim><lda><corpus>,,,CC BY-SA 4.0,False,False,True,False,False
25778,60778921,2020-03-20 17:31:17,,"<p>While trying to load chines fasttext model(cc.zh.300.bin) with gensim, I stucked with following error</p>

<blockquote>
  <p>UnicodeDecodeError:'utf-8' codec can't decode byte 0xba in position 0:
  invalid start byte</p>
</blockquote>

<p>Anyone can help me, please? Detailed error below :</p>

<p><a href=""https://i.stack.imgur.com/73L4H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/73L4H.png"" alt=""enter image description here""></a></p>
",2020-03-20 18:52:26,2020-03-20 21:15:33,How can I load chinese fasttext model with gensim?,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
25796,60903484,2020-03-28 16:25:37,,"<pre><code>def generate_w2vModel(decTokenFlawPath, w2vModelPath):
    print(""training..."")
    model = Word2Vec(sentences= DirofCorpus(decTokenFlawPath), size=30, alpha=0.01, window=5, min_count=0, max_vocab_size=None, sample=0.001, seed=1, workers=1, min_alpha=0.0001, sg=1, hs=0, negative=10, iter=5)
    model.save(w2vModelPath)

def evaluate_w2vModel(w2vModelPath):
    print(""\nevaluating..."")
    model = Word2Vec.load(w2vModelPath)
    for sign in ['(', '+', '-', '*', 'main']:
        print(sign, "":"")
        print(model.most_similar_cosmul(positive=[sign], topn=10))

def main():
    dec_tokenFlaw_path = ['./data/cdg_ddg/corpus/']
    w2v_model_path = ""./w2v_model/wordmodel3"" 
    generate_w2vModel(dec_tokenFlaw_path, w2v_model_path)
    evaluate_w2vModel(w2v_model_path)
    print(""success!"")
</code></pre>

<p>this the python that i was running. This file is used to train word2vec model. The inputs are corpus files, and the output is the word2vec model. i got the following error :</p>

<pre><code>Traceback (most recent call last):
  File ""create_w2vmodel.py"", line 67, in &lt;module&gt;
    main()
  File ""create_w2vmodel.py"", line 62, in main
    generate_w2vModel(dec_tokenFlaw_path, w2v_model_path)
  File ""create_w2vmodel.py"", line 50, in generate_w2vModel
    model.save(w2vModelPath)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/models/word2vec.py"", line 930, in save
    super(Word2Vec, self).save(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/models/base_any2vec.py"", line 281, in save
    super(BaseAny2VecModel, self).save(fname_or_handle, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 691, in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol=pickle_protocol)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 550, in _smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 1311, in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 89, in smart_open
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 301, in file_smart_open
IOError: [Errno 21] Is a directory: './w2v_model/wordmodel3'
</code></pre>

<p>please help me to change this particular error. i think there is no folder like this, but i had already created the w2v_model/wordmodel3 in my folder. I had tried it in many ways. I will provide smart_open_lib.py program file below :</p>

<pre><code>def file_smart_open(fname, mode='rb'):
    """"""
    Stream from/to local filesystem, transparently (de)compressing gzip and bz2
    files if necessary.

    """"""
    _, ext = os.path.splitext(fname)

    if ext == '.bz2':
        PY2 = sys.version_info[0] == 2
        if PY2:
            from bz2file import BZ2File
        else:
            from bz2 import BZ2File
        return make_closing(BZ2File)(fname, mode)

    if ext == '.gz':
        from gzip import GzipFile
        return make_closing(GzipFile)(fname, mode)

    return open(fname, mode)
</code></pre>

<p>this is the trace back code which they tell. kindly request to help me fr change this error!!!</p>
",,2020-03-28 21:44:51,IOError : [Error no: 21] is a directory : './w2v-model/wordmodel3',<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25799,60832982,2020-03-24 14:22:18,,"<p>I'm using LDA for topic modelling using Gensim package on a set of documents and after training it I'm getting top terms for a topic which are not present in the tagged documents itself. Is it a coding error that I'm making?
Thanks in Advance</p>
",,2020-03-24 14:22:18,Topic Modelling using LDA - Top terms not present in Documents,<lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25806,60862484,2020-03-26 07:16:46,,"<p>I am having some confusion over the use of coherence score in evaluating LDA model.</p>

<p>I had run an LDA model on a dataset and have obtained coherence score ranging from 0.32 to 0.37 and perplexity score ranging from -6.75 to -6.77 for a various number of topics.</p>

<p>I am using the LDA model in the gensim package and this is the code which I use to calculate the coherence score.</p>

<pre><code>coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, 
coherence='c_v')
coherenceScore = coherencemodel.get_coherence()
</code></pre>

<p>I had always understood that the use of coherence score is to find the optimal number of topics used in the LDA model. But I was also told that a coherence score of 0.3 is bad.</p>

<p>Can someone kindly explain what is coherence score used for and does a score 0.3 signifies a bad model? 
And when we are comparing between different LDA models, which is the better evaluation method, perplexity or coherence score?</p>
",2020-04-01 03:09:21,2020-04-01 03:09:21,Coherence Value of 0.3 in LDA Model,<lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
25847,60868665,2020-03-26 13:41:34,,"<p>How to pass a <code>.txt</code> document given by user dynamically in LDA model?
I have tried the below code, but it's not working to give proper topic of the doc. The topic of my <code>.txt</code> is related to <strong>Sports</strong> so it should give the topic name as Sports. It's is giving the output as:</p>

<pre><code>Score: 0.5569453835487366   - Topic: 0.008*""bike"" + 0.005*""game"" + 0.005*""team"" + 0.004*""run"" + 0.004*""virginia""
Score: 0.370819091796875    - Topic: 0.016*""game"" + 0.014*""team"" + 0.011*""play"" + 0.008*""hockey"" + 0.008*""player""
Score: 0.061239391565322876  -Topic: 0.010*""card"" + 0.010*""window"" + 0.008*""driver"" + 0.007*""sale"" + 0.006*""price""*
</code></pre>

<pre><code>data = df.content.values.tolist()
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
data = [re.sub('\s+', ' ', sent) for sent in data]
data = [re.sub(""\'"", """", sent) for sent in data]

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):

    texts_out = []
    for sent in texts:
        doc = nlp("" "".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)
# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = gensim.corpora.Dictionary(data_lemmatized)

texts = data_lemmatized

corpus = [id2word.doc2bow(text) for text in texts]
# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

#f = io.open(""text.txt"", mode=""r"", encoding=""utf-8"")

p=open(""text.txt"", ""r"") #document by the user which is related to sports

if p.mode == 'r':
    content = p.read()

bow_vector = id2word.doc2bow(lemmatization(p))

for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
    print(""Score: {}\t Topic: {}"".format(score, lda_model.print_topic(index, 5)))


</code></pre>
",2020-03-27 12:37:12,2020-03-31 11:32:34,How to get topic of new document in LDA model,<python><lda><topic-modeling><document-classification><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,False
25849,60704127,2020-03-16 10:21:34,,"<p>I have a dataset of 1.2mil single sentence descriptions (5-50 words) and I want to cluster these into n clusters. For vector conversion, I want to use doc2vec to get 1.2mil equal size vectors. However, I'm not sure what should be the size parameter. I've read, it should be between 100-300 however since each document, in this case, has fewer tokens (words) should the vector be small?</p>
",2020-03-16 10:44:19,2020-03-16 16:47:29,tuning size parameter for doc2vec,<python><cluster-analysis><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25867,60850956,2020-03-25 14:31:31,,"<p>I'm trying to evaluate a home-made topic model. For this, I'm using the list of topics (represented by keywords), and want to use a <code>gensim.models.coherencemodel.CoherenceModel</code>, and call it on a corpus, which is a list of strings (each one being a document).
The <code>CoherenceModel</code> requires a <code>Dictionary</code>, but I don't understand what this corresponds to, and how I can get it.
I'm using the <code>TfidfVectorizer</code> from <code>sklearn</code> to vectorize the text, and <code>glove</code> embeddings from <code>gensim</code> to compute similarities within my model. </p>
",,2020-03-25 16:23:17,Topic Coherence with Dictionary from Glove (gensim),<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25869,60852962,2020-03-25 16:22:38,,"<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

class Corpus():
    """"""Iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = Corpus()

model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")

</code></pre>

<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>

<p><a href=""https://i.stack.imgur.com/QrJRM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrJRM.png"" alt=""enter image description here""></a></p>

<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. 
Should I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>
",,2020-05-20 11:53:36,Training time of gensim word2vec,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25875,60935894,2020-03-30 17:16:39,,"<p>In general, I have the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim
import gensim.downloader as api

model = api.load('word2vec-google-news-300')
...
matrix = model.similarity_matrix(dictionary, tfidf=None, threshold=0.0,
    exponent=2.0, nonzero_limit=100)
...
</code></pre>

<p>I am using the following dictionary for the variable dictioanry:</p>

<pre><code>4
0   apple   2
1   eaten   1
10  google  1
2   half    1
3   iphone  2
4   is  2
7   lost    2
11  missing 1
5   model   1
8   my  2
9   phone   2
12  pixel   1
6   plus    1
13  somewhere   1
</code></pre>

<p>So when I compute the similarity between 'phone' and 'iphone', I get a nonzero value.</p>

<pre class=""lang-py prettyprint-override""><code>sim_match.model.similarity('phone','iphone')
&gt;&gt; 0.33964303
</code></pre>

<p>But at the index 3,9 of matrix (the similarity of 'phone' and 'iphone' in the similarity matrix), I get a zero entry.</p>

<pre class=""lang-py prettyprint-override""><code>matrix[3,9]
&gt;&gt; 0.0
</code></pre>

<p>As a matter of fact, almost all entries are zero.</p>

<pre class=""lang-py prettyprint-override""><code>print(matrix)
&gt;&gt;  (0, 0)        1.0
&gt;&gt;  (1, 1)        1.0
&gt;&gt;  (2, 2)        1.0
&gt;&gt;  (3, 3)        1.0
&gt;&gt;  (4, 4)        1.0
&gt;&gt;  (5, 5)        1.0
&gt;&gt;  (6, 6)        1.0
&gt;&gt;  (7, 7)        1.0
&gt;&gt;  (11, 7)       0.21569395
&gt;&gt;  (8, 8)        1.0
&gt;&gt;  (9, 9)        1.0
&gt;&gt;  (10, 10)      1.0
&gt;&gt;  (11, 11)      1.0
&gt;&gt;  (7, 11)       0.21569395
&gt;&gt;  (12, 12)      1.0
&gt;&gt;  (13, 13)      1.0
</code></pre>

<p>Only the words 'lost' and 'missing' have a nonzero similarity and aren't equivalent.</p>

<p>I do know that the matrix is a numpy sparse matrix. Maybe its the parameters I'm passing, but I am confused as to why most of the elements in the matrix are zero. The parameters I passed, based on the documentation, seem that they shouldn't be causing so many entries to be 0. I do note that the nonzero_limit is 100, but shouldn't there be a hundred nonzero entries?</p>

<p>Is there anyway I could fix or get around this?</p>
",,2020-03-30 17:16:39,Why would the similarity matrix of a gensim Word2VecKeyedVectors be very sparse compared to to the model itself?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25880,60913104,2020-03-29 10:49:34,,"<p>I have trained two word2vec models suing gensim on separate corpora.
Both corpora are in English.
In one corpus there are some different words which are not present in the other. </p>

<p>I want to map the common words of one model to the other.
Then I want to get the vectors of the unknown words (after the mapping). </p>

<p>I tried the following code:</p>

<pre><code>from gensim.models import KeyedVectors
from gensim.models import translation_matrix

w2v_bin_path_old    = 'model_from_corpus1_30d.bin'
w2v_bin_path_new    = 'model_from_corpus2_30d.bin'
wv_old              = KeyedVectors.load_word2vec_format(w2v_bin_path_old, binary=True)
wv_new              = KeyedVectors.load_word2vec_format(w2v_bin_path_new, binary=True)

common_tokens       = set(wv_old.vocab.keys()).intersection(set(wv_new.vocab.keys()))
common_tokens       = [(tok, tok) for tok in common_tokens]

transmat            = translation_matrix.TranslationMatrix(wv_new, wv_old, common_tokens)
transmat.train(common_tokens)

transmat.translate('whatever')
</code></pre>

<p>However, this code returns only the top 5 words from the destination w2v model. 
I only want to get the vector of the proposed token. 
Is there a way to do it? </p>

<p>Thank you in advance!</p>
",,2020-03-30 16:48:24,How do i get a vector from gensim's translation_matrix,<mapping><translation><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25888,60938299,2020-03-30 19:42:38,,"<p>Does <code>sample= 0</code> in Gensim word2vec mean that no downsampling is being used during my training? The documentation says just that </p>

<blockquote>
  <p>""useful range is (0, 1e-5)""</p>
</blockquote>

<p>However putting the threshold to 0 would cause P(wi) to be equal to 1, meaning that no word would be discarded, am I understanding it right or not? </p>

<p>I'm working on a relatively small dataset of 7597 Facebook posts (18945 words) and my embeddings perform far better using <code>sample= 0</code>rather than anything else within the recommended range. Is there any particular reason? Text size? </p>
",,2020-03-30 23:00:28,Gensim word2vec downsampling sample=0,<python><math><gensim><word-embedding><subsampling>,,,CC BY-SA 4.0,False,False,True,False,False
25892,60872405,2020-03-26 17:02:41,,"<p>I have the following columns in a dataframe.</p>

<pre><code>Unnamed: 0, title, publication, author, year, month, title.1, content, len_article, gensim_summary, split_words, first_100_words
</code></pre>

<p>I am trying to run this small piece of code.</p>

<pre><code>import nltk
nltk.download('punkt')
# TOKENIZE
df.first_100_words = df.first_100_words.str.lower()
df['tokenized_first_100'] = df.first_100_words.apply(lambda x: word_tokenize(x, language = 'en'))
</code></pre>

<p>The last line of code throws an error.  I'm getting this error message.</p>

<pre><code>df.first_100_words = df.first_100_words.str.lower()
df['tokenized_first_100'] = df.first_100_words.apply(lambda x: word_tokenize(x, language = 'en'))
Traceback (most recent call last):

  File ""&lt;ipython-input-129-42381e657774&gt;"", line 2, in &lt;module&gt;
    df['tokenized_first_100'] = df.first_100_words.apply(lambda x: word_tokenize(x, language = 'en'))

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\pandas\core\series.py"", line 3848, in apply
    mapped = lib.map_infer(values, f, convert=convert_dtype)

  File ""pandas\_libs\lib.pyx"", line 2329, in pandas._libs.lib.map_infer

  File ""&lt;ipython-input-129-42381e657774&gt;"", line 2, in &lt;lambda&gt;
    df['tokenized_first_100'] = df.first_100_words.apply(lambda x: word_tokenize(x, language = 'en'))

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\nltk\tokenize\__init__.py"", line 144, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\nltk\tokenize\__init__.py"", line 105, in sent_tokenize
    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\nltk\data.py"", line 868, in load
    opened_resource = _open(resource_url)

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\nltk\data.py"", line 993, in _open
    return find(path_, path + ['']).open()

  File ""C:\Users\ryans\Anaconda3\lib\site-packages\nltk\data.py"", line 701, in find
    raise LookupError(resource_not_found)

LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

import nltk
nltk.download('punkt')

  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt/en.pickle

  Searched in:
    - 'C:\\Users\\ryans/nltk_data'
    - 'C:\\Users\\ryans\\Anaconda3\\nltk_data'
    - 'C:\\Users\\ryans\\Anaconda3\\share\\nltk_data'
    - 'C:\\Users\\ryans\\Anaconda3\\lib\\nltk_data'
    - 'C:\\Users\\ryans\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
    - ''
**********************************************************************
</code></pre>

<p>I'm pretty new to all the tokenization stuff.</p>

<p>The sample code is from this site.</p>

<p><a href=""https://github.com/AustinKrause/Mod_5_Text_Summarizer/blob/master/Notebooks/Text_Cleaning_and_KMeans.ipynb"" rel=""nofollow noreferrer"">https://github.com/AustinKrause/Mod_5_Text_Summarizer/blob/master/Notebooks/Text_Cleaning_and_KMeans.ipynb</a></p>
",2020-04-12 20:51:51,2020-04-15 11:38:56,"Resource punkt not found. But, it is downloaded and installed",<python><python-3.x><dataframe><nltk><tokenize>,,,CC BY-SA 4.0,True,False,True,False,False
25924,60894446,2020-03-27 21:37:59,,"<p>Is it possible to access a fasttext model (gensim) using multithreading?<br>
Currently, I'm trying to load a model once (due to size and loading time), so it stays in memory and access its similarity functions multiple thousands times in a row. I want to do that in parallel and my current approach uses a wrapper class that loads the model and is then passed to the workers. But it looks like it does not return any results.    </p>

<p>The wrapper class. Initiated once.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.fasttext import load_facebook_model

class FastTextLocalModel:
    def __init__(self):
        self.model_name = ""cc.de.300.bin""
        self.model_path = path.join(""data"", ""models"", self.model_name)
        self.fast_text = None

    def load_model(self):
        self.fast_text = load_facebook_model(self.model_path)

    def similarity(self, word1: str = None, word2: str = None):
        return self.fast_text.wv.similarity(word1, word2)
</code></pre>

<p>And the Processor class makes use of the <code>FastTextLocalModel</code> methods above:</p>

<pre class=""lang-py prettyprint-override""><code>fast_text_instance = FastTextLocalModel()
fast_text_instance.load_model()

with concurrent.futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
        docs = corpus.get_documents()  # docs is iterable
        processor = ProcessorClass(model=fast_text_instance)
        executor.map(processor.process, docs)
</code></pre>

<p>Using <code>max_workers=1</code> seems to work.<br>
I have to mention that I have no expertise in python multithreading.</p>
",,2020-03-28 21:51:18,Use fasttext model (gensim) with threading,<python><multithreading><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
25938,60942672,2020-03-31 02:46:07,,"<p>I am performing sentiment analysis using the gensim library's word2vec (doc2vec) models. I have successfully completed Word2vec, DBOW (Distributed Bag of Words), DMC (Distributed Memory Concatenated), DMM (Distributed Memory Mean). Now I want to do the combination of DBOW + DMC and DBOW + DMM. While doing (DBOW +DMC) I run into the error.</p>

<p>Below is the function for (DBOW + DMC) combination.</p>

<pre><code>def get_concat_vectors(model1,model2, corpus, size):
    vecs = np.zeros((len(corpus), size))
    n = 0
    for i in corpus.index:
        prefix = 'all_' + str(i)
        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])
        n += 1
    return vecs

train_vecs_dbow_dmc = get_concat_vectors(model_dbow,model_dmc, x_train, 200)
val_vecs_dbow_dmc = get_concat_vectors(model_dbow,model_dmc, x_validation, 200)

model = Sequential()
model.add(Dense(256, activation='relu', input_dim=200))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit(train_vecs_dbow_dmc, y_train, epochs=100, batch_size=32, verbose=2)
score = model.evaluate(val_vecs_dbow_dmc, y_validation, batch_size=128, verbose=2)

print(score[1])
</code></pre>

<p>The error is in this line (4th from above)</p>

<pre><code>for i in corpus.index: 
</code></pre>

<p>And the error is :</p>

<blockquote>
  <p>typeerror 'builtin_function_or_method' object is not iterable</p>
</blockquote>

<p>I am using tensorflow backend, juypter notebook 6.0.3, python 3.7, anaconda.</p>

<p>Please suggest me how to resolve this error.</p>

<p>Thanks in Advance.</p>
",,2020-03-31 02:46:07,How to solve typeerror 'builtin_function_or_method' object is not iterable during sentiment analysis using Gensim?,<tensorflow><jupyter-notebook><typeerror><python-3.7><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25951,60988425,2020-04-02 09:34:03,,"<p>I am trying to apply LDA for topic modeling using the Mallet wrapper of Gensim on Python.
The code that I am running is as follows:</p>

<pre><code>MALLET_PATH = 'C:/mallet-2.0.8/bin/mallet'
lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus, 
                                              num_topics=TOTAL_TOPICS, id2word=dictionary,
                                              iterations=500, workers=16)
</code></pre>

<p>Mallet is installed in C-drive and is running on the Command Prompt (C:\mallet-2.0.8\bin\mallet).
The help command is also working (import-dir --). Java is also installed. The environment variable
and the path have also been set for both Mallet and Java.Yet the output shows the following error.</p>

<pre><code>CalledProcessError: Command 'mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\imibh\AppData\Local\Temp\a8b7e6_corpus.txt --output C:\Users\imibh\AppData\Local\Temp\a8b7e6_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>Have already tried all the responses to past such queries on stack overflow without any improvement.</p>

<p>Would greatly appreciate any help.</p>

<p>Manit</p>
",2020-06-07 10:14:15,2020-06-07 10:21:01,Python Gensim Mallet,<python-3.x><jupyter-notebook><gensim><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
25961,60969839,2020-04-01 11:29:35,,"<p>I'm trying to generate a Latent Dirichlet Allocation model using 500 different txts. 
One part of my code is as follows:</p>

<pre><code>from gensim.models import Phrases
from gensim import corpora, models

bigram = Phrases(docs, min_count=10)
trigram = Phrases(bigram[docs])
for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
    for token in trigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
</code></pre>

<p>And it gives me the following error:</p>

<pre><code>File "".../scriptLDA.py"", line 74, in &lt;module&gt;
    docs[idx].append(token)
AttributeError: 'str' object has no attribute 'append'
</code></pre>

<p>Can someone fix it for me?
Thanks!</p>
",2020-04-01 11:40:28,2020-04-01 11:53:48,"Why am I getting ""AttributeError: 'str' object has no attribute 'append'"" in Python?",<python><model><lda>,,,CC BY-SA 4.0,False,False,True,False,False
25969,61021821,2020-04-03 23:21:08,,"<p>I am currently implementing a natural text generator for a school project. I have a dataset of sentences of predetermined lenght and key words, I convert them in vectors thanks to gensim and GoogleNews-vectors-negative300.bin.gz. I train a recurrent neural network to create a list of vectors that I compare to the list of vectors of the real sentence. So I try to get as close as possible to the ""real"" vectors.</p>

<p>My problem happens when I have to convert back vectors into words: my vectors aren't necessarily in the google set. So I would like to know if there is an efficient solution to get the closest vector in the Google set to an outpout vector.</p>

<p>I work with python 3 and Tensorflow </p>

<p>Thanks a lot, feel free to ask any questions about the project</p>

<p>Charles </p>
",,2020-04-04 01:08:20,get closest vector from unknown vector with gensim,<python-3.x><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
25973,60973681,2020-04-01 14:49:09,,"<p>I'm trying to generate a Latent Dirichlet Allocation model using 4999 different txts. One part of my code is as follows:</p>

<pre><code>txtsPath = ""/.../txt""

docs = []

for filename in os.listdir(txtsPath):
    file = os.path.join(txtsPath,filename)
    body = open(file, encoding=""utf8"").read()
    docs.append(body)

#print(docs)

import nltk.data
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer

def docs_preprocessor(docs):
    tokenizer = RegexpTokenizer(r'\w+')
    #tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
    for idx in range(len(docs)):
        docs[idx] = str(docs[idx]).lower()  # Convert to lowercase.
        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.

    # Remove numbers, but not words that contain numbers.
    docs = [[token for token in doc if not token.isdigit()] for doc in docs]

    # Remove words that are only one character.
    docs = [[token for token in doc if len(token) &gt; 3] for doc in docs]

    # Lemmatize all words in documents.
    lemmatizer = WordNetLemmatizer()
    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]

    return docs

docs = docs_preprocessor(docs)
#print(docs)


# Create Biagram &amp; Trigram Models
from gensim.models import Phrases
from gensim import corpora, models

bigram = Phrases(docs, min_count=10)
trigram = Phrases(bigram[docs])

for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
    for token in trigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)

dictionary = Dictionary(docs)
dictionary.filter_extremes(no_below=10, no_above=0.2)

# Create dictionary and corpus required for Topic Modeling
corpus = [dictionary.doc2bow(doc) for doc in docs]
print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))
#print(corpus[:1])

#temp = dictionary[0]
id2word = dictionary.id2token
model = models.ldamodel.LdaModel(corpus,num_topics=15,id2word=id2word)

print(model.print_topics(num_topics=15))

topics = (model.print_topics(num_topics=15))

with open('/.../topics.txt',""w+"") as f:
    f.write(str(topics))
    f.close()
</code></pre>

<p>And it gives me the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/.../scriptLDA.py"", line 87, in &lt;module&gt;
    model = models.ldamodel.LdaModel(corpus,num_topics=15,id2word=id2word)
  File ""/.../.local/lib/python3.6/site-packages/gensim/models/ldamodel.py"", line 441, in __init__
    raise ValueError(""cannot compute LDA over an empty collection (no terms)"")
ValueError: cannot compute LDA over an empty collection (no terms)
</code></pre>

<p>Can someone fix it for me?</p>

<p>And one last question, if the model to be generated has texts in two different languages, is there an easy way to remove the stop words from both languages?</p>

<p>Thanks!</p>
",,2020-04-01 14:49:09,Why am I getting ValueError: cannot compute LDA over an empty collection (no terms)' in Python?,<python><lda>,,,CC BY-SA 4.0,True,False,True,False,False
25983,60960326,2020-03-31 21:38:12,,"<p>In order to fine-tune <code>word2vec</code> embeddings in <code>gensim</code>, the following piece of code worked with previous versions:</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors- 
negative300.bin.gz', binary=True)
</code></pre>

<p>However, I get the error message that <code>Word2Vec.load_word2vec</code> is depracated : 
<code>DeprecationWarning: Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.</code>
When I use </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews- 
vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>and then try to fine tune the model with train method as below:</p>

<pre><code>model.train((corpus, total_examples=len(corpus2),epochs=10) )
</code></pre>

<p>I get the following error:</p>

<p>""AttributeError: 'Word2VecKeyedVectors' object has no attribute 'train'""</p>

<p>Is there still any solution to load the existing <code>Googlenews W2V</code> into <code>gensim</code> and fine-tune it with additional corpus?</p>

<p>In response to user:10473854: ignoring warning does not work as the module is already depracated. Also, running Word2Vec with the path for downloaded embedding will make Word2Vec fails. Check this:</p>

<pre><code>model = Word2Vec('GoogleNews-vectorsnegative300.bin.gz')
model.wv.vocab

{'/': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c3940&gt;,
'a': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c39e8&gt;,
'e': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c3278&gt;}
</code></pre>
",2020-04-01 06:39:45,2020-04-08 20:54:32,Is there still any solution to load the existing Googlenews W2v into gensim and finetune it with additional corpus?,<python><nlp><gensim><word2vec><embedding>,,,CC BY-SA 4.0,False,False,True,False,False
25987,60990456,2020-04-02 11:25:31,,"<p>I have my own corpus of plain text. I want to train a Bert model in TensorFlow, similar to gensim's word2vec to get the embedding vectors for each word.</p>

<p>What I have found is that all the examples are related to any downstream NLP tasks like classification. But, I want to train a Bert model with my custom corpus after which I can get the embedding vectors for a given word.</p>

<p>Any lead will be helpful.</p>
",,2020-04-02 17:46:23,Training a Bert word embedding model in tensorflow,<python><tensorflow><nlp><bert-language-model>,,,CC BY-SA 4.0,False,False,True,False,False
25992,60993778,2020-04-02 14:19:57,,"<p>Following the documentation of <code>?gensim.models.ldamodel</code>, I want to train an <code>ldamodel</code> and (from <a href=""https://stackoverflow.com/a/40282878/8839068"">this</a> SO answer create a worcloud from it). I am using the following code from both sources:</p>

<pre><code>from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
import gensim
import matplotlib.pyplot as plt
from wordcloud import WordCloud

common_dictionary = Dictionary(common_texts) # create corpus
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

lda = gensim.models.LdaModel(common_corpus, num_topics=10) # train model on corpus
for t in range(lda.num_topics):
    plt.figure()
    plt.imshow(WordCloud().fit_words(lda.show_topic(t, 200)))
    plt.axis(""off"")
    plt.title(""Topic #"" + str(t))
    plt.show()
</code></pre>

<p>However, I get an <code>AttributeError: 'list' object has no attribute 'items'</code> on the line <code>plt.imshow(...)</code></p>

<p>Can someone help me out here? (Answers to similar questions have not been working for me and I am trying to compile a minimal pipeline with this.)</p>
",,2020-04-02 14:46:24,How to create wordcloud from LDA model?,<python-3.x><lda><word-cloud>,,,CC BY-SA 4.0,False,False,True,False,False
25997,61055072,2020-04-06 07:42:56,,"<pre><code>list(gensim.utils.simple_preprocess(""i you he she I it we you they"", deacc=True))
</code></pre>

<p>gives as result:</p>

<pre><code>['you', 'he', 'she', 'it', 'we', 'you', 'they']
</code></pre>

<p>Is it normal? Are there any words that it skips? Should I use another tokenizer?</p>

<p>BONUS QUESTION:
What does the ""deacc=True"" paramater mean?</p>
",,2020-04-06 17:26:32,"Why does gensim's simple_preprocess Python tokenizer seem to skip the ""i"" token?",<python><nlp><tokenize><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
25998,61055625,2020-04-06 08:21:06,,"<p>I have a set of trigrams (see <a href=""https://drive.switch.ch/index.php/s/PvHTvjk327WsOKE"" rel=""nofollow noreferrer"">pickle file</a>). The column name is the trigram; each cell represents a document; the cell entries denominate the occurrence (binary).</p>

<p>I then preprocess the trigrams and train an LDA model using the below code. However, being new to LDA Mallet, I am doing something wrong -- and the ""words"" printed from a wordcloud are just numbers. I am lost and cannot figure out where the connection of words to number representation is lost/ how to recover it.</p>

<pre><code>with open('small_trigrams.pkl', 'rb') as file:
    small_trigrams = pickle.load(file)

small_mydict = gensim.corpora.Dictionary()    
small_trigrams_collection = []

for col in small_trigrams.columns:
    trigram = col.replace(""("", """").replace(""'"", """").replace("" "", """").replace("")"", """").strip().split("","", 3)
    value = small_trigrams[col].sum() # trigram occurrences
    for i in range(int(value)):
        small_trigrams_collection.append(trigram)            
small_mycorp = [small_mydict.doc2bow(trigram, allow_update=True) for trigram in small_trigrams_collection] # create corpus


# Train LDA on the trigrams features, assess topic coherence
small_topics_coherence = {} # dict with topics: coherence score
small_models = {} # collection of models

# train LDA on trigrams features
model = LdaMallet(path_to_mallet_binary,corpus=small_mycorp, num_topics=i, id2word=small_mydict) # train model

for t in range(model.num_topics)[:6]:
    plt.figure()
    plt.imshow(WordCloud().fit_words(dict(lda.show_topic(t, 200))))
    plt.axis(""off"")
    plt.title(""Topic #"" + str(t))
    plt.show()
</code></pre>

<p>Can someone point me to my mistake?</p>
",,2020-04-06 08:21:06,"LDA model: why are topic ""words"" numbers?",<python-3.x><lda><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
26009,61041080,2020-04-05 10:19:12,,"<p>Doubt - 1</p>

<p>I am training Doc2Vec with 150000 documents. Since these documents are from legal domain they are really hard to clean and get it ready for further training. Hence I decided to remove all the periods from a document. Having said that, I am confused on how the parameter of <code>Window_size</code> in doc2vec recognize the sentences now. There are two views presented in the question :<a href=""https://stackoverflow.com/questions/42242521/doc2vec-differentiate-sentence-and-document"">Doc2Vec: Differentiate Sentence and Document</a> </p>

<ol>
<li>The algorithm just works on chunks of text, without any idea of what a sentence/paragraph/document etc might be.</li>
<li>It's even common for the tokenization to retain punctuation, such as the periods between sentences, as standalone tokens.</li>
</ol>

<p>Therefore I am in confusion if my adopted approach of eliminating the punctuation (periods) is right. Kindly provide me with some supportive answers.</p>

<p>Doubt-2</p>

<p>The documents that I scrapped range from 500 - 5500 tokens hence my approach to have a pretty even sized documents for training doc2vec and even to reduce the vocabulary is :
Consider a document of size greater than 1500 tokens in this case I make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens. The motivation for this kind of approach is from a paper related to Classification of documents using BERT where the sequence of 512 tokens were generated like this.</p>

<p>So I want to know if this idea is somewhat good to proceed or it's not recommended to do this?</p>

<p><strong>Update</strong> - I just saw the common_text corpus used by gensim in the tutorial link <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> and found that the documents in that corpus are simply tokens of words and do not contain any punctuation.
 eg: </p>

<p><code>from gensim.test.utils import common_texts, common_dictionary, common_corpus</code></p>

<p><code>print(common_texts[0:10])</code></p>

<p>Output:</p>

<p><code>[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]</code></p>

<p>Same has been followed in the tutorial <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>.
So is my approach of removing periods in the document valid, if so then how will the window parameter work because in the documentation it is defined as follows:
window (int, optional)  The maximum distance between the current and predicted word within a sentence.</p>
",2020-04-05 17:55:09,2020-04-05 18:55:12,significance of periods in sentences while training documents with Doc2Vec,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26010,61041099,2020-04-05 10:20:49,,"<p>I (complete noob in machine learning and natural language processing) am using doc2vec approach (gensim python library) to find most similar document to a random string. Problem is that whenever I'd like to add a new document to trained model I need to retrain model from scratch. </p>

<p>Is there an approach that stands out in ability to add a new document/vocabulary to a trained model without the need to train from scratch or an approach that would train faster?</p>

<p>I'm overwhelmed by all the approaches to nlp and only started with what I found as the most popular (word2vec/doc2vec) and now I'm looking for direction where to study now. Thanks for any recommendations.</p>
",,2020-04-05 10:20:49,Which nlp approach is best suited for updating model with new words without the need to train from scratch?,<python><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
26036,61079896,2020-04-07 12:19:14,,"<p>I'm trying to execute below command</p>

<pre><code>    import os
os.environ.update({'MALLET_HOME':r'C:/Users/N-Workstation/Desktop/mallet-2.0.8/'})

mallet_path = 'C:\\Users\\N-Workstation\\Desktop\\mallet-2.0.8\\bin\\mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, id2word=id2word)
</code></pre>

<p>I have tried various way of changing the path but still getting following error:</p>

<pre><code>    CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-66-25177da4755d&gt; in &lt;module&gt;
      5 
      6 mallet_path = 'C:\\Users\\N-Workstation\\Desktop\\mallet-2.0.8\\bin\\mallet' # update this path
----&gt; 7 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, id2word=id2word)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in __init__(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)
    129         self.random_seed = random_seed
    130         if corpus is not None:
--&gt; 131             self.train(corpus)
    132 
    133     def finferencer(self):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in train(self, corpus)
    270 
    271         """"""
--&gt; 272         self.convert_input(corpus, infer=False)
    273         cmd = self.mallet_path + ' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\
    274             '--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in convert_input(self, corpus, infer, serialize_corpus)
    259             cmd = cmd % (self.fcorpustxt(), self.fcorpusmallet())
    260         logger.info(""converting temporary corpus to MALLET format with %s"", cmd)
--&gt; 261         check_output(args=cmd, shell=True)
    262 
    263     def train(self, corpus):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py in check_output(stdout, *popenargs, **kwargs)
   1916             error = subprocess.CalledProcessError(retcode, cmd)
   1917             error.output = output
-&gt; 1918             raise error
   1919         return output
   1920     except KeyboardInterrupt:

CalledProcessError: Command 'C:\Users\N-Workstation\Desktop\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\N-WORK~1\AppData\Local\Temp\f5956b_corpus.txt --output C:\Users\N-WORK~1\AppData\Local\Temp\f5956b_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>Can somebody please help me with this error?</p>

<p>Thanks,
Naseer</p>
",2020-04-08 07:32:28,2020-04-08 07:32:28,Getting CalledProcesserror when trying to run ldamallet function,<python><gensim><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
26065,61029524,2020-04-04 14:23:43,,"<p>I have a dataset of reviews for different Hotels. 
I'm trying to find out similar hotels using the reviews of hotels. So, I'm using a <code>Doc2vec</code> algorithm to achieve this.</p>

<p>Is there any way to measure the accuracy of a <code>Doc2Vec</code> model using <code>Gensim</code>, rather than evaluating the results using <code>most_similar()</code> function of <code>Gensim</code>?</p>
",2020-04-06 20:06:45,2020-04-06 20:06:45,How to measure the accuracy of a Doc2vec model?,<gensim><unsupervised-learning><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26071,61080872,2020-04-07 13:14:53,,"<p>I'm currently trying to process a large amount of very big (>10k words) text files. In my data pipeline, I identified the gensim tokenize function as my bottleneck, the relevant part is provided in my MWE below:</p>

<pre><code>import re
import urllib.request

url='https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/genesis/english-web.txt'
doc=urllib.request.urlopen(url).read().decode('utf-8')

PAT_ALPHABETIC = re.compile('(((?![\d])\w)+)', re.UNICODE)

def tokenize(text):
    text.strip()
    for match in PAT_ALPHABETIC.finditer(text):
        yield match.group()

def preprocessing(doc):
    tokens = [token for token in tokenize(doc)]
    return tokens

foo=preprocessing(doc)
</code></pre>

<p>Calling the <code>preprocessing</code> function for the given example takes roughly <code>66ms</code> and I would like to improve this number. Is there anything I can still optimize in my code? Or is my hardware (Mid 2010s Consumer Notebook) the issue? I would be interested in the runtimes from people with some more recent hardware as well.</p>

<p>Thank you in advance</p>
",,2020-04-07 14:13:00,Improve performance of large document text tokenization through Python + RegEx,<regex><python-3.x><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
26118,61085836,2020-04-07 17:26:27,,"<p>I've installed gensim by <code>pip install</code>, then I tried <code>import gensim</code> or <code>from gensim import utils</code>.<br>
I always got error:
AttributeError: module 'smart_open' has no attribute 's3'""<br>
Can anybody help to fix it? Thanks!!!</p>
",2020-04-07 20:40:02,2020-04-07 20:40:02,Gensim Installation Questions,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26120,61087427,2020-04-07 18:58:56,,"<p>I tried to save word2vec vector as text, but it didnt work out, I got an error, that I dont really understand, what duplicates appear here and what is this ""wv"", that is proposed. Maybe somone can explain is to me. Thank you in advance </p>

<pre><code>model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
model.save_word2vec_format('test_w2v.txt', binary=False)
</code></pre>

<pre><code>WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
Word2Vec(vocab=20, size=300, alpha=0.025)
Traceback (most recent call last):
  File ""/word2vec.py"", line 26, in &lt;module&gt;
    model.save_word2vec_format('test_w2v.txt', binary=False)
  File ""/word2vec.py"", line 1307, in save_word2vec_format
    raise DeprecationWarning(""Deprecated. Use model.wv.save_word2vec_format instead."")
DeprecationWarning: Deprecated. Use model.wv.save_word2vec_format instead.

</code></pre>
",,2020-04-08 06:49:44,saving word2vec in text format,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26129,61105818,2020-04-08 16:49:39,,"<p>When I calculate a topic coherence score for top 20 words using gensim, it returns NaN. I find that if I compute C_v score for the top 2 words, it returns 0.85; if I compute the C_v score for the top 3 words or any number larger than 3, it return NaN. This only happens to one topic.</p>

<p>My code is as follows</p>

<pre><code>i = 19
cm = CoherenceModel(topics=[tpc_gensim[i]], corpus=X_gensim,
                    texts=df_article.stem_lt.tolist(),
                    dictionary=d, coherence='c_v', topn=3)
coherence = cm.get_coherence()
print(i, coherence)
</code></pre>

<p>The top 3 words are:  hour session extended.</p>

<p>The error message is </p>

<pre><code>anaconda3/lib/python3.7/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars
  m_lr_i = np.log(numerator / denominator)
/anaconda3/lib/python3.7/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars
  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))
</code></pre>

<p>From the message, it seems the error is caused by dividing 0. What should I do in this case? How can I get a coherence score for the top 20 words?</p>
",,2020-04-08 16:49:39,"Gensim coherence score return NaN if using top 3 words, a number if using top 2 words",<python><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26138,61134211,2020-04-10 04:49:18,,"<p>I trained and test the 'IMDb movie reviews dataset' using the Gensim word2vec model and I want to predict the sentiments of my own unlabelled data. I tried but got an error. I am reusing an open-source code. Below is the full code:</p>

<pre><code>import pandas as pd
import numpy as np
import text_normalizer as tn
import model_evaluation_utils as meu
np.set_printoptions(precision=2, linewidth=80)
import gensim
import keras
from keras.models import Sequential
from keras.layers import Dropout, Activation, Dense
from sklearn.preprocessing import LabelEncoder

dataset = pd.read_csv(r'imdb_reviews.csv')
new_data = pd.read_csv('abcd.csv', header=0)
# take a peek at the data
print(dataset.head())
reviews = np.array(dataset['reviews'])
sentiments = np.array(dataset['Sentiments'])

# build train and test datasets
train_reviews = reviews[:35000]
train_sentiments = sentiments[:35000]
test_reviews = reviews[35000:]
test_sentiments = sentiments[35000:]

# normalize datasets
norm_train_reviews = tn.normalize_corpus(train_reviews)
norm_test_reviews = tn.normalize_corpus(test_reviews)
le = LabelEncoder()
num_classes=2 
# tokenize train reviews &amp; encode train labels
tokenized_train = [tn.tokenizer.tokenize(text)
                   for text in norm_train_reviews]
y_tr = le.fit_transform(train_sentiments)
y_train = keras.utils.to_categorical(y_tr, num_classes)
# tokenize test reviews &amp; encode test labels
tokenized_test = [tn.tokenizer.tokenize(text)
                   for text in norm_test_reviews]
y_ts = le.fit_transform(test_sentiments)
y_test = keras.utils.to_categorical(y_ts, num_classes)
# print class label encoding map and encoded labels
print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))
print('Sample test label transformation:\n'+'-'*35,
      '\nActual Labels:', test_sentiments[:3], '\nEncoded Labels:', y_ts[:3], 
      '\nOne hot encoded Labels:\n', y_test[:3])
# build word2vec model
w2v_num_features = 500
w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,
                                   min_count=10, sample=1e-3)
def averaged_word2vec_vectorizer(corpus, model, num_features):
    vocabulary = set(model.wv.index2word)

    def average_word_vectors(words, model, vocabulary, num_features):
        feature_vector = np.zeros((num_features,), dtype=""float64"")
        nwords = 0.

        for word in words:
            if word in vocabulary: 
                nwords = nwords + 1.
                feature_vector = np.add(feature_vector, model[word])
        if nwords:
            feature_vector = np.divide(feature_vector, nwords)

        return feature_vector

    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)
                    for tokenized_sentence in corpus]
    return np.array(features)
# generate averaged word vector features from word2vec model
avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,
                                                     num_features=500)
avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,
                                                    num_features=500)
print('Word2Vec model:&gt; Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features
def construct_deepnn_architecture(num_input_features):
    dnn_model = Sequential()
    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(512, activation='relu'))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(512, activation='relu'))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(2))
    dnn_model.add(Activation('softmax'))

    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 
                      metrics=['accuracy'])
    return dnn_model
w2v_dnn = construct_deepnn_architecture(num_input_features=500)
batch_size = 100
w2v_dnn.fit(avg_wv_train_features, y_train, epochs=15, batch_size=batch_size, 
            shuffle=True, validation_split=0.1, verbose=1)
y_pred = w2v_dnn.predict_classes(avg_wv_test_features)
predictions = le.inverse_transform(y_pred)
meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, 
                                      classes=['positive', 'negative'])
# This I added to predict and save the results of my own data
pred_y2 = w2v_dnn.predict_classes(new_data['Articles'])
print(pred_y2)
pd.DataFrame(pred_y2, columns=['Sentiments']).to_csv('abcd_sentiments.csv')
</code></pre>

<p>When I run this code I got the below error:</p>

<blockquote>
  <p>ValueError                                Traceback (most recent call
  last)  in 
  ----> 1 pred_y2 = w2v_dnn.predict_classes(new_data['Articles'])
        2 print(pred_y2)
        3 pd.DataFrame(pred_y2, columns=['Sentiments']).to_csv('abcd_sentiments.csv')</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/sequential.py
  in predict_classes(self, x, batch_size, verbose)
      266             A numpy array of class predictions.
      267         """"""
  --> 268         proba = self.predict(x, batch_size=batch_size, verbose=verbose)
      269         if proba.shape[-1] > 1:
      270             return proba.argmax(axis=-1)</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in predict(self, x, batch_size, verbose, steps, callbacks,
  max_queue_size, workers, use_multiprocessing)    1439     1440<br>
   # Case 2: Symbolic tensors or Numpy array-like.
  -> 1441         x, _, _ = self._standardize_user_data(x)    1442         if self.stateful:    1443             if x[0].shape[0] > batch_size
  and x[0].shape[0] % batch_size != 0:</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in _standardize_user_data(self, x, y, sample_weight, class_weight,
  check_array_lengths, batch_size)
      577             feed_input_shapes,
      578             check_batch_axis=False,  # Don't enforce the batch size.
  --> 579             exception_prefix='input')
      580 
      581         if y is not None:</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training_utils.py
  in standardize_input_data(data, names, shapes, check_batch_axis,
  exception_prefix)
      143                             ': expected ' + names[i] + ' to have shape ' +
      144                             str(shape) + ' but got array with shape ' +
  --> 145                             str(data_shape))
      146     return data
      147 </p>
  
  <p>ValueError: Error when checking input: expected dense_1_input to have
  shape (500,) but got array with shape (1,)</p>
</blockquote>

<p>Can somebody suggest me how to solve this error and predict sentiments of my unlabeled data?
I am using python 3.7 and jupyter notebook from Pycharm IDE.</p>

<p>Thanks in Advance.</p>
",2020-04-10 09:01:45,2020-04-10 09:01:45,How to predict unlabelled data's sentiment using Gensim word2vec model?,<python-3.7><gensim><word2vec><sentiment-analysis><valueerror>,,,CC BY-SA 4.0,False,False,True,False,True
26142,61136557,2020-04-10 08:26:58,,"<p>i want to use gensim to create Word2Vec vectors on my tweet dataset. the code is for multi-label emotion classification based on tweets. i have aggregated tweets file which contains 107k tweets. i used this for creating Word2Vec vectors based on. my code:</p>
<pre><code>np.set_printoptions(threshold=sys.maxsize)

#Pre-Processor Function
pre_processor = TextPreProcessor(
    omit=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
    
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
     
    segmenter=&quot;twitter&quot;, 
    
    corrector=&quot;twitter&quot;, 
    
    unpack_hashtags=True,
    unpack_contractions=True,
    
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    
    dicts=[emoticons]
)

#Averaging Words Vectors to Create Sentence Embedding
def word_averaging(wv, words):
    all_words, mean = set(), []
    
    for word in words:
        if isinstance(word, np.ndarray):
            mean.append(word)
        elif word in wv.vocab:
            mean.append(wv.syn0norm[wv.vocab[word].index])
            all_words.add(wv.vocab[word].index)

    if not mean:
        logging.warning(&quot;cannot compute similarity with no input %s&quot;, words)
        # FIXME: remove these examples in pre-processing
        return np.zeros(wv.vector_size,)

    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
    return mean

def  word_averaging_list(wv, text_list):
    return np.vstack([word_averaging(wv, post) for post in text_list ])

#Loading data
raw_aggregate_tweets = pandas.read_excel('E:\\aggregate.xlsx').iloc[:,0] #Loading all tweets to have a bigger word2vec corpus

raw_train_tweets = pandas.read_excel('E:\\train.xlsx').iloc[:,1] #Loading all train tweets
train_labels = np.array(pandas.read_excel('E:\\train.xlsx').iloc[:,2:13]) #Loading corresponding train labels (11 emotions)

raw_test_tweets = pandas.read_excel('E:\\test.xlsx').iloc[:,1] #Loading all test tweets
test_gold_labels = np.array(pandas.read_excel('E:\\test.xlsx').iloc[:,2:13]) #Loading corresponding test labels (11 emotions)
print(&quot;please wait&quot;)

#Pre-Processing
aggregate_tweets=[]
train_tweets=[]
test_tweets=[]
for tweets in raw_aggregate_tweets:
    aggregate_tweets.append(pre_processor.pre_process_doc(tweets))

for tweets in raw_train_tweets:
    train_tweets.append(pre_processor.pre_process_doc(tweets))

for tweets in raw_test_tweets:
    test_tweets.append(pre_processor.pre_process_doc(tweets))


#Vectorizing 
w2v_model = gensim.models.Word2Vec(aggregate_tweets, min_count = 10, size = 300, window = 8)


train_array = word_averaging_list(w2v_model.wv,train_tweets)
test_array = word_averaging_list(w2v_model.wv,test_tweets)
</code></pre>
<p>but i get this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-8a5fe4dbf144&gt; in &lt;module&gt;
    110 print(w2v_model.wv.vectors.shape)
    111 
--&gt; 112 train_array = word_averaging_list(w2v_model.wv,train_tweets)
    113 test_array = word_averaging_list(w2v_model.wv,test_tweets)
    114 

&lt;ipython-input-1-8a5fe4dbf144&gt; in word_averaging_list(wv, text_list)
     70 
     71 def  word_averaging_list(wv, text_list):
---&gt; 72     return np.vstack([word_averaging(wv, post) for post in text_list ])
     73 
     74 #Averaging Words Vectors to Create Sentence Embedding

&lt;ipython-input-1-8a5fe4dbf144&gt; in &lt;listcomp&gt;(.0)
     70 
     71 def  word_averaging_list(wv, text_list):
---&gt; 72     return np.vstack([word_averaging(wv, post) for post in text_list ])
     73 
     74 #Averaging Words Vectors to Create Sentence Embedding

&lt;ipython-input-1-8a5fe4dbf144&gt; in word_averaging(wv, words)
     58             mean.append(word)
     59         elif word in wv.vocab:
---&gt; 60             mean.append(wv.syn0norm[wv.vocab[word].index])
     61             all_words.add(wv.vocab[word].index)
     62 

TypeError: 'NoneType' object is not subscriptable
</code></pre>
<p>It looks like your post is mostly code; please add some more details. what is this error of site? my god. i don't have any more details. sorry i have to do this to bypass the error.</p>
<h1>Second averaging method</h1>
<pre><code>#Averaging Words Vectors to Create Sentence Embedding
def get_mean_vector(word2vec_model, words):
    # remove out-of-vocabulary words
    words = [word for word in words if word in word2vec_model.vocab]
    if len(words) &gt;= 1:
        return np.mean(word2vec_model[words], axis=0)
    else:
        return np.zeros(word2vec_model.vector_size)

#Vectorizing 
w2v_model = gensim.models.Word2Vec(aggregate_tweets, min_count = 11, size = 400, window = 18, sg=1)

train_array=[]
test_array=[]
for tweet in train_tweets:
    vec = get_mean_vector(w2v_model.wv, tweet)
    if len(vec) &gt; 0:
        train_array.append(vec)
        
for tweet in test_tweets:
    vec = get_mean_vector(w2v_model.wv, tweet)
    if len(vec) &gt; 0:
        test_array.append(vec)
</code></pre>
",2020-06-20 09:12:55,2020-04-10 10:40:42,Getting error on averaging Word2Vec crerated vectors,<python><nlp><classification><word2vec><emotion>,,,CC BY-SA 4.0,False,False,True,False,False
26146,61185290,2020-04-13 09:52:56,,"<p>I have some text data for which I need to do sentiment classification. I don't have positive or negative labels on this data (unlabelled). I want to use the Gensim word2vec model for sentiment classification.<br>
Is it possible to do this? Because till now I couldn't find anything which does that?
Every blog and article are using some kind of labelled dataset (such as imdb dataset)to train and test the word2vec model. No one going further and predicting their own unlabelled data.</p>

<p>Can someone tell me the possibility of this (at least theoretically)?</p>

<p>Thanks in Advance!</p>
",,2020-07-29 14:27:01,Is it possible to do sentiment analysis of unlabelled text using word2vec model?,<python-3.7><gensim><word2vec><sentiment-analysis>,,,CC BY-SA 4.0,False,False,True,False,False
26161,61119374,2020-04-09 10:46:29,,"<p>I am doing project about LDA topic modelling, i used gensim (python) to do that. I read some references and it said that to get the best model topic thera are two parameters we need to determine, the number of passes and the number of topic. Is that true? for the number of passes we will see at which point the passes are stable, for the number of topic we will see which topic that has the lowest value.</p>

<pre><code>num_topics = 10
chunksize = 2000
passes = 20
iterations = 400
eval_every = None 
</code></pre>

<p>And is it necessary to use all the parameters in gensim library?</p>
",,2020-04-10 07:06:43,Latent Dirichlet Allocation Implementation with Gensim,<machine-learning><lda><topic-modeling><unsupervised-learning><perplexity>,,,CC BY-SA 4.0,False,False,True,False,False
26166,61202090,2020-04-14 06:47:14,,"<p>Im a newbie to nltk and I have a question.
I have a text in a var and it's preprocessed and clean.</p>

<p><code>text = ""since the history of writing predates the concept of the text most texts were not written with this concept in mind most written works fall within a narrow range of the types described by text theory...""</code></p>

<p>I have a loaded model.</p>

<pre><code>from gensim.models import Word2Vec,  KeyedVectors
model = KeyedVectors.load_word2vec_format('model_example')
</code></pre>

<p>Now I have to use the text with the model to search for a term and its similarity.</p>

<pre><code>term = 'android'
most_similars = model.wv.most_similar(positive=term)
</code></pre>

<p>But I don't know how to pass the text to the model and then search for n target candidates. 
Thank you very much.</p>
",2020-04-14 07:40:41,2020-04-14 07:40:41,How to use a text to calculate target from its similarity in a word2vec model?,<model><nltk><word2vec><target><sentence-similarity>,,,CC BY-SA 4.0,True,False,True,False,False
26178,61062237,2020-04-06 14:35:23,,"<p>I need to train my own model with word2vec and fasttext. By readind different sourcs I found different information. 
So I did the model and trained it like this:</p>

<pre><code>model = FastText(all_words, size=300, min_count= 3,sg=1)
model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
</code></pre>

<p>So I read that that should be enough to creat and train the model. But then I saw, that some people do it seperatly:</p>

<pre><code>model = FastText(size=4, window=3, min_count=1)  # instantiate
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train
</code></pre>

<p>Now I am confused and dont know if what I did is correct. Can sombody help me to make it clear? 
Thank you</p>
",,2020-04-06 17:21:02,Gensim train word2vec and Fasttext,<python><machine-learning><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26185,61123616,2020-04-09 14:34:02,,"<p>I'm trying to estimate the cosine similarity between <em>each</em> document <code>i</code> in a Corpus <code>A</code> and <em>all</em> documents in a Corpus <code>B</code>.</p>

<p>Any idea how I can do this efficiently? I'm working with pretty large datasets.</p>

<p>Essentially, I want to get the document(s) in Corpus <code>B</code> which is (are) <em>most similar</em> for each document within Corpus <code>A</code>.</p>
",,2020-04-10 14:33:33,How to compute cosine similarity between 2 different CORPUSES?,<python><nlp><nltk><spacy><gensim>,,,CC BY-SA 4.0,True,True,True,False,False
26198,61125887,2020-04-09 16:32:46,,"<p>I am working with Word2vec (Gensim, in python) to understand meaning of sentences (by each word in them).
My goal is to be able to realize if the sentence indicates about the feeling of the speaker. 
Where can I find this kind of a dictionary of words?
For example one dictionary for words that indicate happiness and other for sadness.
Thanks </p>
",,2020-04-11 19:03:50,build WORD2VEC words dictionary to indicate feelings,<python><gensim><word2vec><glove>,,,CC BY-SA 4.0,False,False,True,False,False
26210,61066227,2020-04-06 18:13:15,,"<p>I am trying to run the following so that I can do topic modeling.  I can get everything up until the last part line I have run the original part in command prompt and set the environmental variable MALLET_HOME (so technically I can skip the part of the code that says import os and os.environ) and it works in command prompt.  Yes I have coded corpus earlier in the code I just didn't show that part as this code is long.</p>

<p>My friend can run my code and has no problem.  Any idea why I can't get the last part to work? </p>

<pre><code>#install_requires=['google-auth', 'pyasn1'],
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel```

```import os
os.environ['MALLET_HOME'] = 'C:/mallet-2.0.8'
mallet_path = 'C:/mallet-2.0.8/bin/mallet.bat' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)```

```CalledProcessError: Command 'C:/mallet-2.0.8/bin/mallet.bat import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\user~1\AppData\Local\Temp\1\6376f9_corpus.txt --output C:\Users\user~1\AppData\Local\Temp\1\6376f9_corpus.mallet' returned non-zero exit status 1.111```
</code></pre>
",,2020-04-06 18:13:15,Python - Mallet Error Return Non-Zero Status 1,<python><python-3.x><error-handling><gensim><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
26215,61126512,2020-04-09 17:09:16,,"<p>I'm using the Gensim HDP (Hierarchical Dirichlet Process) model, and I was wondering if there is a parameter which defines the acceptable closeness of topics. I want to have minimal overlap in the final topic distributions.</p>

<p>Also, does using the as best lda method the best way to visualize using pyLDAvis?</p>
",,2020-04-09 17:09:16,Gensim HDP Parameter Separation of Topics,<python><nlp><gensim><dirichlet>,,,CC BY-SA 4.0,False,False,True,False,False
26249,61182206,2020-04-13 05:49:23,,"<p>I have written this code</p>

<pre><code>import gensim
from gensim import corpora
</code></pre>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-29-a29f5eabd8f4&gt; in &lt;module&gt;
     85 # use the following command in anaconda prompt with the admionistrator privileges to install gensim
     86 # conda install -c conda-forge gensim
---&gt; 87 import gensim
     88 from gensim import corpora
     89 

~\Anaconda3\lib\site-packages\gensim\__init__.py in &lt;module&gt;
      3 """"""
      4 
----&gt; 5 from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
      6 import logging
      7 

~\Anaconda3\lib\site-packages\gensim\parsing\__init__.py in &lt;module&gt;
      2 
      3 from .porter import PorterStemmer  # noqa:F401
----&gt; 4 from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
      5                             strip_tags, strip_short, strip_numeric,
      6                             strip_non_alphanum, strip_multiple_whitespaces,

~\Anaconda3\lib\site-packages\gensim\parsing\preprocessing.py in &lt;module&gt;
     40 import glob
     41 
---&gt; 42 from gensim import utils
     43 from gensim.parsing.porter import PorterStemmer
     44 

~\Anaconda3\lib\site-packages\gensim\utils.py in &lt;module&gt;
     43 from six.moves import range
       44 
---&gt; 45 from smart_open import open
     46 
     47 from multiprocessing import cpu_count

~\Anaconda3\lib\site-packages\smart_open\__init__.py in &lt;module&gt;
     26 from smart_open import version
     27 
---&gt; 28 from .smart_open_lib import open, parse_uri, smart_open, register_compressor
     29 from .s3 import iter_bucket as s3_iter_bucket
     30 

~\Anaconda3\lib\site-packages\smart_open\smart_open_lib.py in &lt;module&gt;
     35 
     36 from smart_open import compression
---&gt; 37 from smart_open import doctools
     38 from smart_open import transport
     39 from smart_open import utils

~\Anaconda3\lib\site-packages\smart_open\doctools.py in &lt;module&gt;
     19 
     20 from . import compression
---&gt; 21 from . import transport
     22 
     23 PLACEHOLDER = '    smart_open/doctools.py magic goes here'

~\Anaconda3\lib\site-packages\smart_open\transport.py in &lt;module&gt;
     20 NO_SCHEME = ''
     21 
---&gt; 22 _REGISTRY = {NO_SCHEME: smart_open.local_file}
     23 
     24 

AttributeError: module 'smart_open' has no attribute 'local_file'
</code></pre>

<p>I have received the error : module 'smart_open' has no attribute 'local_file'
how can I solve it?</p>
",2020-04-13 18:36:47,2020-04-29 10:01:49,module 'smart_open' has no attribute 'local_file',<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26256,61198009,2020-04-13 22:47:13,,"<p>For reference, I already looked at the following questions: </p>

<ol>
<li><a href=""https://stackoverflow.com/questions/31742630/gensim-lda-for-text-classification"">Gensim LDA for text classification</a></li>
<li><a href=""https://stackoverflow.com/questions/60420718/python-gensim-lda-model-show-topics-funciton"">Python Gensim LDA Model show_topics funciton</a></li>
</ol>

<p>I am looking to have my LDA model trained from Gensim classify a sentence under one of the topics that the model creates. 
Something long the lines of </p>

<pre><code>lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=7, passes=20)
lda.print_topics()
for line in document: # where each line in the document is its own sentence for simplicity
    print('Sentence: ', line)
    topic = lda.parse(line) # where the classification would occur
    print('Topic: ', topic)
</code></pre>

<p>I know gensim does not have a <code>parse</code> function, but how would one go about accomplishing this? Here is the documentation that I've been following but I haven't gotten anywhere with it: </p>

<p><a href=""https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py</a></p>

<p>Thanks in advance. </p>

<p>edit: More documentation-  <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>
",2020-04-13 23:03:05,2020-06-29 06:58:35,Classify Text with Gensim LDA Model,<python-3.x><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
26259,61244205,2020-04-16 06:42:59,,"<p>I have only one document and use LDA to find just one topic. Here is what I got:</p>

<pre><code>[(0,
  '0.032*""women"" + 0.032*""gender"" + 0.032*""tech"" + 0.032*""amazons"" + 0.021*""peers"" + 0.021*""positions"" + 0.021*""policymakers"" + 0.021*""practices"" + 0.021*""markets"" + 0.021*""number""')]
</code></pre>

<p>How should I understand the number before each keyword? </p>
",2020-04-17 14:05:10,2020-04-17 14:05:10,What does the number before each keyword in a LDA topic mean?,<gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
26269,61198042,2020-04-13 22:50:06,,"<p>I received error : too many values to unpack (expected 2) , when running the below code. anyone can help me? I added more details.</p>

<pre class=""lang-py prettyprint-override""><code>
import gensim
import gensim.corpora as corpora

dictionary = corpora.Dictionary(doc_clean)

doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]


Lda = gensim.models.ldamodel.LdaModel

ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50, per_word_topics = True, eval_every = 1)


print(ldamodel.print_topics(num_topics=3, num_words=20)) 

for i in range (0,46):
    for index, score in sorted(ldamodel[doc_term_matrix[i]], key=lambda tup: -1*tup[1]):
        print(""subject"", i)
        print(""\n"")
        print(""Score: {}\t \nTopic: {}"".format(score, ldamodel.print_topic(index, 6)))

</code></pre>
",2020-04-14 03:04:54,2020-04-14 03:46:40,too many values to unpack (expected 2) lda,<lda>,,,CC BY-SA 4.0,False,False,True,False,False
26274,61070763,2020-04-07 00:01:04,,"<p>What is the appropriate way to save a <code>gensim</code> doc2vec model that has been transformed by T-SNE (from <code>sklearn.manifold</code>), e.g.</p>

<pre><code>x_full = model[doc_tags]
pca_full = PCA(n_components=50)
pca_result_full = pca_full.fit_transform(x_full)
tsne = TSNE(n_components=2, verbose=1)
x_tsne_full = tsne.fit_transform(pca_result_full)
</code></pre>

<p>where <code>doc_tags</code> is the tagged documents that the model was trained on. Reducing this model using T-SNE takes hours, so it would be good to save this for future use. </p>

<p>I have been putting it in a pandas dataframe, such as <code>df = pd.DataFrame(x_tsne_full, index=doc_tags, columns=['x', 'y'])</code> then saving this dataframe to CSV for future use. Is this the best way, or is there a T-SNE-appropriate way to save that is not apparent in its docs?</p>
",2020-04-07 00:07:00,2020-04-07 00:09:08,Save T-SNE results for future use,<python><gensim><dimensionality-reduction><saving-data>,,,CC BY-SA 4.0,False,False,True,False,True
26289,61218334,2020-04-14 22:30:06,,"<p>I'm trying to process text using gensim (specifically gensim.corpora.dictionary), but I keep getting a ""<code>ModuleNotFoundError: No module named 'gensim.corpora'; 'gensim' is not a package</code>"" error. The sample code is below. I used <code>pip install gensim</code> in my command prompt in install <code>gensim</code>. I verified that numPy and sciPy were installed and up-to-date. I checked the file path of gensim and confirmed that gensim is installed on the machine. There is also a corpora folder in gensim with no obvious issues. I'm running Python 3.6.8 through the command prompt. I can call other modules, like Pandas,nltk, and NumPy, that are in the same folder location as gensim so I'm not sure why I am getting issues when I try to import gensim. I don't know how to fix this issue. Has anyone come across this issue before? I will be grateful for any help on this. Thanks.</p>

<pre><code>from gensim.corpora.dictionary import Dictionary 
from nltk.tokenize import word_tokenize

my_documents=[  'The movie was about a spaceship and aliens',
                'I really liked the movie!',
                'Awesome action scenes, but boring characters.',
                'The movie was awful! I hate alien films.',
                'Space is cool! I liked the movie.',
                'More space films, please!']

tokenized_docs=[word_tokenize(doc.lower()) for doc in my_documents]
dic= corpora.Dictionary(tokenized_docs)
print(dic.token2id)
corpus=[dic.doc2bow(doc) for doc in tokenized_docs]
print(corpus)
</code></pre>

<p>The output generated after running <code>dir /s /b ""python""</code> and <code>dir /s /b ""pip""</code> in the command prompt can be found below. </p>

<p>C:\Users\Owner>dir /S /b ""python""</p>

<p>C:\Users\Owner.vscode\extensions\ms-python.python-2020.3.71659\pythonFiles\lib\python</p>

<p>C:\Users\Owner.vscode\extensions\ms-python.python-2020.3.71659\pythonFiles\lib\python\parso\python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Microsoft VS Code_\resources\app\extensions\python</p>

<p>C:\Users\Owner\AppData\Roaming\Python</p>

<p>C:\Users\Owner>dir /S /b ""pip""</p>

<p>C:\Users\Owner\AppData\Local\pip</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python\Python36\Lib\site-packages\pip</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python\Python37-32\Lib\site-packages\pip</p>
",2020-04-15 05:24:28,2020-04-16 07:16:10,ModuleNotFoundError: No module named 'gensim.corpora'; 'gensim' is not a package,<python-3.x><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
26303,61280748,2020-04-17 21:10:55,,"<p>I am running word2vec models in gensim. I don't understand 2 metrics (in_qsize/out_qsize) reported by the log file. I've spent a bit of time searching and can't find an explanation. Here is a sample from my log files:</p>

<pre><code>2020-04-17 21:04:09,032 : INFO : EPOCH 5 - PROGRESS: at 68.67% examples, 657466 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:10,038 : INFO : EPOCH 5 - PROGRESS: at 68.92% examples, 657527 words/s, in_qsize 20, out_qsize 0
2020-04-17 21:04:11,078 : INFO : EPOCH 5 - PROGRESS: at 69.14% examples, 657513 words/s, in_qsize 20, out_qsize 1
2020-04-17 21:04:12,136 : INFO : EPOCH 5 - PROGRESS: at 69.39% examples, 657458 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:13,139 : INFO : EPOCH 5 - PROGRESS: at 69.68% examples, 657687 words/s, in_qsize 17, out_qsize 4
</code></pre>
",2020-04-18 19:17:17,2020-04-18 19:17:17,meaning of in_qsize and out_qsize in gensim word2vec log files,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26310,61235170,2020-04-15 17:43:48,,"<p>I am trying to figure out solution for requirement where in I am required to map long text to unigrams or bigrams. 
For example
""Ability to motivate and manage team. You should be able to track the progress of the team and intervene to improve the progress"". This long text should be mapped to ""Team management"". Basically I am trying to figure out communication/analytical skills from the long text seen in document like Job descriptions. I am struggling to figure out a solution for this. I do not want to hard code as the long text keep changing. Thanks for any help.</p>
",,2020-04-15 21:39:32,How to map detailed text to a unigram or a bigram,<python-3.x><nlp><cluster-analysis><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26334,61300170,2020-04-19 05:58:37,,"<p>I am new in topic modelling area and trying to learn, I am following the steps from below site</p>

<p><a href=""https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/"" rel=""nofollow noreferrer"">https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/</a></p>

<p>However when I want to visualise my topics using LDA I am getting an error?!
This is the code copied from the site:</p>

<pre><code># Import required packages
import numpy as np
import logging
import pyLDAvis.gensim
import json
import warnings
warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity

from gensim.models.coherencemodel import CoherenceModel
from gensim.models.ldamodel import LdaModel
from gensim.corpora.dictionary import Dictionary
from numpy import array

# Import dataset
p_df = pd.read_csv('C:/Users/kamal/Desktop/R project/Reviews.csv')
# Create sample of 10,000 reviews
p_df = p_df.sample(n = 10000)
# Convert to array
docs =array(p_df['Text'])
# Define function for tokenize and lemmatizing
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer

def docs_preprocessor(docs):
    tokenizer = RegexpTokenizer(r'\w+')
    for idx in range(len(docs)):
        docs[idx] = docs[idx].lower()  # Convert to lowercase.
        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.

    # Remove numbers, but not words that contain numbers.
    docs = [[token for token in doc if not token.isdigit()] for doc in docs]

    # Remove words that are only one character.
    docs = [[token for token in doc if len(token) &gt; 3] for doc in docs]

    # Lemmatize all words in documents.
    lemmatizer = WordNetLemmatizer()
    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]

    return docs
# Perform function on our document
docs = docs_preprocessor(docs)
#Create Biagram &amp; Trigram Models 
from gensim.models import Phrases
# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.
bigram = Phrases(docs, min_count=10)
trigram = Phrases(bigram[docs])

for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
    for token in trigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
#Remove rare &amp; common tokens 
# Create a dictionary representation of the documents.
dictionary = Dictionary(docs)
dictionary.filter_extremes(no_below=10, no_above=0.2)
#Create dictionary and corpus required for Topic Modeling
corpus = [dictionary.doc2bow(doc) for doc in docs]
print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))
print(corpus[:1])

# Set parameters.
num_topics = 5
chunksize = 500 
passes = 20 
iterations = 400
eval_every = 1  

# Make a index to word dictionary.
temp = dictionary[0]  # only to ""load"" the dictionary.
id2word = dictionary.id2token

lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       alpha='auto', eta='auto', \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
# Print the Keyword in the 5 topics
print(lda_model.print_topics())
</code></pre>

<p>Here is my way I using LDA to see the topics. </p>

<pre><code>import pyLDAvis.gensim
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
LDAvis_prepared
</code></pre>

<p>And Error I am getting : </p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-22-8a46a8151430&gt; in &lt;module&gt;
      4 # Visualize the topics
      5 pyLDAvis.enable_notebook()
----&gt; 6 LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
      7 LDAvis_prepared

C:\ProgramData\Anaconda3\lib\site-packages\pyLDAvis\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)
    116     See `pyLDAvis.prepare` for **kwargs.
    117     """"""
--&gt; 118     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)
    119     return vis_prepare(**opts)

C:\ProgramData\Anaconda3\lib\site-packages\pyLDAvis\gensim.py in _extract_data(topic_model, corpus, dictionary, doc_topic_dists)
     24       corpus = gensim.matutils.Sparse2Corpus(corpus_csc)
     25 
---&gt; 26    vocab = list(dictionary.token2id.keys())
     27    # TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..
     28    # for now, I'll just make sure we don't ever get zeros...

AttributeError: 'dict' object has no attribute 'token2id'
</code></pre>
",,2020-04-19 05:58:37,Topic Modelling(Denisim) Visualisation using LDA,<python><lda><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
26341,61367839,2020-04-22 14:37:26,,"<p>I have big gensim Doc2vec model, I only need to infer vectors while i am loading the training documents vectors from other source.
Is it possible to load it as is without the big npy file</p>

<p>I did </p>

<p><strong>Edit:</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model_path = r'C:\model/model'
model = Doc2Vec.load(model_path)
model.delete_temporary_training_data(keep_doctags_vectors=False, keep_inference=True)
model.save(model_path)
</code></pre>

<p>remove the files <code>(model.trainables.syn1neg.npy,model.wv.vectors.npy)</code> <strong>manually</strong></p>

<pre><code>model = Doc2Vec.load(model_path)
</code></pre>

<p>but it ask for </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-5-7f868a7dbe0c&gt;"", line 1, in &lt;module&gt;
    model = Doc2Vec.load(model_path)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\doc2vec.py"", line 1113, in load
    return super(Doc2Vec, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 427, in load
    obj._load_specials(fname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\numpy\lib\npyio.py"", line 428, in load
    fid = open(os_fspath(file), ""rb"")

FileNotFoundError: [Errno 2] No such file or directory: 'C:\\model/model.trainables.syn1neg.npy'
</code></pre>

<p>Note:
Those files not exists in the directory,
The model run on a server and download the model file from the storage
My question is, Do the model must have those files for inference?
I want to run it as low memory consumption as possible.
Thanks.</p>

<p><strong>Edit:</strong>
Is the file model.trainables.syn1neg.npy is the model weights?
Is the file model.wv.vectors.npy is necessary for running an inference? </p>
",2020-04-22 21:30:03,2020-04-23 03:58:59,Load Doc2Vec without the docs vectors only for infer_vector,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26357,61303579,2020-04-19 11:28:03,,"<p>I am implementing a paper to compare our performance. In the paper, the uathor says </p>

<blockquote>
  <p>300-dimensional pre-trained word2vec vectors (Mikolov et al., 2013)</p>
</blockquote>

<p>I am wondering whether the pretrained word2vec Gensim model <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"" rel=""nofollow noreferrer"">here</a> is same as the pretrained embeddings on the official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google site</a> (the GoogleNews-vectors-negative300.bin.gz file)</p>

<p><br>
My source of doubt arises from this line in Gensim documentation (in Word2Vec Demo section) </p>

<blockquote>
  <p>We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases</p>
</blockquote>

<p>Does this mean the model on gensim is not fully trained? Is it different from the official embeddings by Mikolov? </p>
",2020-04-19 12:41:10,2020-04-20 19:56:12,Is the Gensim word2vec model same as the standard model by Mikolov?,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26364,61253758,2020-04-16 15:13:45,,"<p>I have a Doc2Vec model created with Gensim and want to use <code>scikit-learn</code> DBSCAN to look for clustering of sentences within the model.</p>

<p>I'm struggling to work out how to best transform the model vectors to work with DBSCAN and plot clusters and am not finding many directly applicable examples on the web.</p>

<p>Here is what I have so far:</p>

<pre class=""lang-python prettyprint-override""><code>import gensim
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

fnIn = 'NLPModels/doc2VecModel_vector_size{0}_epochs{1}.bin'

def doCluster(vector_size, epochs):
    model = gensim.models.doc2vec.Doc2Vec.load(fnIn.format(vector_size, epochs))

    Y = model.docvecs.index2entity # tags

    X = [] # Document vectors
    for tag in Y:
        X.append(model.docvecs[tag])

    db = DBSCAN(eps=.1, min_samples=5, metric='cosine').fit_predict(X)
    labels = set(db)
    print(labels)


doCluster(100, 10)
</code></pre>

<p>Output: <code>{0, 1, -1}</code></p>

<p>Which I believe to be two clusters (0 and 1) and outliers (-1).</p>

<p>Am I going about this in the right way?</p>

<p>How would I plot this on a chart to visualise the clusters?</p>

<p>Thanks.</p>
",2020-04-20 15:53:14,2020-04-20 15:58:59,Plotting DBSCAN Clustering of Doc2Vec model,<python><machine-learning><scikit-learn><gensim><dbscan>,,,CC BY-SA 4.0,False,False,True,False,True
26368,61337315,2020-04-21 06:34:44,,"<p>When we visualize the LDA using pyLDAvis, we can see topic overlap. I want know the word that is causing this topic overlap. Or I want to know the words that are at the intersection of the topic bubbles. Any guidance is appreciated.</p>
",,2020-04-21 13:02:09,Common words that cause topic overlap in gensim LDA,<python-3.x><gensim><lda><topic-modeling><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,False
26369,61337725,2020-04-21 07:03:49,,"<p>I have some volunteer essay writings in the format of:</p>

<pre><code>volunteer_names, essay
[""emi"", ""jenne"", ""john""], [[""lets"", ""protect"", ""nature""], [""what"", ""is"", ""nature""], [""nature"", ""humans"", ""earth""]]
[""jenne"", ""li""], [[""lets"", ""manage"", ""waste""]]
[""emi"", ""li"", ""jim""], [[""python"", ""is"", ""cool""]]
...
...
...
</code></pre>

<p>I want to identify the similar users based on their essay writings. I feel like word2vec is more suitable in problems like this. However, since I want to embed user names too in the model I am not sure how to do it. The examples I found in the internet only uses the words (See example code).</p>

<pre><code>import gensim 
sentences = [['first', 'sentence'], ['second', 'sentence']]
# train word2vec on the two sentences
model = gensim.models.Word2Vec(sentences, min_count=1)
</code></pre>

<p>In that case, I am wondering if there is special way of doing this in word2vec or can I simply consider user names as just words to input to the model. please let me know your thoughts on this.</p>

<p>I am happy to provide more details if needed.</p>
",2020-04-21 08:16:26,2020-04-21 08:16:26,How to embed user names in word2vec model in gensim,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26371,61371059,2020-04-22 17:13:02,,"<p>When I am trying to import gensim, I am getting no error. But when I try to read a word embedding model, I am getting the following error trace. Kindly help me.</p>

<p>I am using Windows 7, python 3.7.6 and anaconda 2020.2.</p>

<p>I have tried everything but unable to resolve this. Please help me.</p>

<p>Trace:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""C:\Users\user\Desktop\Work\6\PLOT_SCATTER.py"", line 10, in 
      from gensim.models import FastText</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\gensim__init__.py"", line 5, in 
      from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\gensim\parsing__init__.py"", line 4, in 
      from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\gensim\parsing\preprocessing.py"", line 42, in  from gensim import utils</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\gensim\utils.py"", line 45, in 
      from smart_open import open</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\smart_open__init__.py"", line 28, in 
      from .smart_open_lib import open, parse_uri, smart_open, register_compressor</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\smart_open\smart_open_lib.py"", line 28, in  import boto3</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\boto3__init__.py"", line 16, in  from boto3.session import Session</p>
  
  <p>File ""C:\Users\user\anaconda3\lib\site-packages\boto3\session.py"", line 17, in  import botocore.session</p>
  
  <p>File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\botocore\session.py"", line 28, in  import botocore.configloader</p>
  
  <p>File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\botocore\configloader.py"", line 19, in  from botocore.compat import six</p>
  
  <p>File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\botocore\compat.py"", line 24, in  from botocore.vendored import six</p>
  
  <p>ModuleNotFoundError: No module named 'botocore.vendored'</p>
</blockquote>
",,2020-04-22 17:13:02,error botocore.vendored module not found after installing gensim,<python><python-3.x><anaconda><gensim><botocore>,,,CC BY-SA 4.0,False,False,True,False,False
26392,61403580,2020-04-24 07:49:32,,"<p>I am trying to see what pre-trained model has included common phrases in news and I thought GoogleNews-vectors-negative300.bin should be a comprehensive one but it turned out that it does not even include deep_learning, machine_learning, social_network, social_responsibility. What pre-trained model could include those words that often occur in news, public reports?</p>

<pre><code>import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)

model.similarity('deep_learning', 'machine_learning')
</code></pre>
",2020-04-24 10:17:19,2020-04-24 21:00:40,Word not in vocabulary of GoogleNews-vectors-negative300.bin,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26394,61405111,2020-04-24 09:24:06,,"<p>I am currently training a Gensim FastText model with a document from a certain domain with the unsupervised training method from Gensim. </p>

<p>After this training of the word representations i would like to train a set of sentence+label lines and ultimately test the model and return a precision and recall value like it is possible in facebooks fastText implementation via train_supervised + test. Does GenSims implementation support the supervised training and testing? I couldnt get it to work / find the required methods.</p>

<p>Any help is much appreciated.</p>
",,2020-04-24 20:54:30,Supervised training and testing in GenSims FastText implementation,<machine-learning><text><classification><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
26407,61310229,2020-04-19 19:17:45,,"<p>I want to use the HDP model from <code>gensim</code> to get the number of topics for my corpus, I already used this corpus and dictionary to train a regular LDA model from <code>gensim</code> and it works fine. But now when I do</p>

<pre><code>hdp = models.HdpModel(bow_corpus, dictionary)
</code></pre>

<p>I get</p>

<pre><code>Traceback (most recent call last):
  File ""models.py"", line 185, in &lt;module&gt;
    hdp = models.HdpModel(bow_corpus, dictionary)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 391, in __init__
    self.update(corpus)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 467, in update
    start_time = time.clock()
AttributeError: module 'time' has no attribute 'clock'
</code></pre>

<p>Is this a bug?</p>

<pre><code>$ python --version
Python 3.8.2 (default, Feb 26 2020, 22:21:03) 
</code></pre>

<p>Edit to add more system information</p>

<pre><code>&gt;&gt;&gt; print(gensim.__version__)
3.8.1

uname -a
Linux ** 5.5.9-arch1-2 #1 SMP PREEMPT Thu, 12 Mar 2020 23:01:33 +0000 x86_64 GNU/Linux
</code></pre>
",2020-04-22 11:34:06,2020-04-22 11:34:06,Is this a bug on gensim hdp model for python 3.8?,<python-3.x><time><gensim><python-3.8>,,,CC BY-SA 4.0,False,False,True,False,False
26424,61407835,2020-04-24 11:59:17,,"<p>When I tried to run below code, I get keyerror: </p>

<pre><code>KeyError: word fransz not in vocabulary. 
</code></pre>

<p>What is the issue?</p>

<pre><code>import numpy as np
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize,word_tokenize
import string
text=""Victor Marie Hugo, Romantik akma bal Fransz air, romanc ve oyun yazar. En byk ve nl Fransz yazarlardan biri kabul edilir. Hugo'nun Fransa'daki edebi n ilk olarak iirlerinden sonra da romanlarndan ve tiyatro oyunlarndan gelir. Pek ok iirinin iinde zellikle Les Contemplations ve La Lgende des sicles byk sayg grr. Fransa dnda en ok Sefiller ve Notre Dame'n Kamburu romanlaryla tannr.Genliinde iddetli bir kral yanls olsa da, gr yllar iinde deiti ve tutkulu bir cumhuriyet destekisi oldu. Eserleri zamannn politik ve sosyal sorunlarna ve de sanatsal akmlarna deinir. Hugo'nun cenazesi 1885'te Panthon'da gmld. Hugo hakknda en ok eser yazlan ilk 100 kii listesinde yer almaktadr. Victor Hugo, Joseph Lopold Sigisbert Hugo (17731828) ve Sophie Trbuchet (17721821) iftinin nc oluydu; Abel Joseph Hugo (17981855) ve Eugne Hugo (18001837) isminde iki aabeyi vard. 1802'de Besanon'da dodu. Napolyon'un bir kahraman olduunu dnen serbest fikirli bir cumhuriyetiydi. Annesi 1812'de Napolyon'a kar komplo kurduu iin idam edilen General Victor Lahorie ile sevgili olduu dnlen Katolik bir Kralcyd.Hugo'nun ocukluu lkede siyasi karmakln olduu bir dnemde geti. Doumundan iki yl sonra Napolyon mparator ilan edilmi, 18 yandayken de Bourbon Monarisi yeniden tahta geirilmiti. Hugo'nun ailesinin ters dini ve politik grleri Fransa'da egemenlik mcadelesi veren kuvvetleri yanstyordu. Hugo'nun babas spanya'da yenilene kadar orduda yksek rtbeli bir subayd.Babas subay olduu srece aile sk sk tand ve bu yolculuklar srasnda Hugo pek ok ey rendi. ocukluunda Napoli'ye giderken geni Alpler'deki geitleri ve karl zirveleri, muhteem Akdeniz mavisini ve enlikler yaplan Roma'y grd. 5 yanda olmasna ramen bu 6 aylk geziyi her zaman aklnda tuttu. Aile Napoli'de birka ay kalp doruca Paris'e dnd.Hugo'nun annesi Sophie evliliinin banda kocasna talya (Leopold Napoli'ye yakn bir vilayette valiydi) ve spanya'ya ( vilayette grev almt) kadar elik etti. Askeri hayatn getirdii yorucu yolculuklar ve kocasnn inancnn zayfl nedeniyle ters dmelerinden dolay Sophie 1803'te Leopold'dan bir sreliine ayrlp  ocuuyla Paris'e yerleti. Bundan sonra Hugo'nun eitimi ve yetimesi zerine eildi. Bu yzden Hugo'nun kariyerinin ilk dnemindeki iir ve kurgu almalar annesinin inancnn ve krala ballnn yansmasyd. Ama ban Fransa'daki 1848 Devrimi'nin ektii olaylar srasnda Katolik Kralc yanls eitime bakaldrp Cumhuriyetilii ve zgr dnceyi desteklemeye balad.Genliinde ak oldu ve annesinin isteklerine kar gelip ocukluk arkada Adle Foucher (18031868) ile gizlice nianland. Annesi ile yakn ilikisinden dolay Adle ile evlenmek iin annesinin lmne (1821) kadar bekledi ve 1822'de evlendi.Adle ve Victor Hugo'nun ilk ocuu Leopold 1823'te dodu ama doduktan ksa sre sonra ld. Sonraki sene kzlar 28 Austos 1824'te Lopoldine dodu. Onu 4 Kasm 1826'da doan Charles, 28 Ekim 1828'de doan Franois-Victor, ve 24 Austos 1830'da doan Adle takip etti.Hugo'nun en byk ve en sevdii kz Lopoldine, Charles Vacquerie ile evliliinden ksa sre sonra 19 yandayken 1843'te ld. 4 Eyll 1843'te Seine nehrinde bouldu. Gemi alabaro olduundan ar etei tarafndan dibe doru ekildi ve kocas Charles Vacquerie de onu kurtarmaya alrken ld. O zaman metresi ile Fransa'nn gneyinde seyahat etmekte olan Hugo kznn lmn oturduu cafede okuduu bir gazeteden rendi. Kznn lm Hugo'yu olduka harap etti.III. Napolyon'un 1851 ylnn sonundaki askeri darbesi sebebiyle srgne kt. Fransa'dan ayrldktan sonra, Channel Adalar'na gitmeden nce ksa bir sre Brksel'de yaad. 1852'den 1855'e kadar Jersey'de yaad. 1855'te 15 yl yaayaca Guernsey'e tand. III. Napolyon 1859'da genel af ilan ettiinde lkesine dnme frsat elde ettiyse de srgnde kalmay tercih etti. Kaybedilen Fransa-Prusya Sava'nn sonucu olarak III. Napolyon iktidardan ekilmek zorunda kalnca lkesine dnd. Paris Kuatmas'ndan sonra hayatnn geri kalann Fransa'da geirmek iin geri dnmeden nce tekrar Guernsey'e tanp 1872 ve 1873 aras orada kald. Hugo ilk romann (Han d'Islande, 1823) evliliinden bir yl sonra yaymlad.  yl sonra da ikinci roman (Bug-Jargal, 1826) basld. 1829 ve 1840 arasnda zamannn en iyi airlerinden biri olarak nn pekitiren be iir kitab (Les Orientales, 1829; Les Feuilles d'automne, 1831; Les Chants du crpuscule, 1835; Les Voix intrieures, 1837; ve Les Rayons et les ombres, 1840) yaynlad.""
punctuations = "",;:()[]/{}''""
sentence=""!.?""
no_punct = """"
for char in text:
   if char not in punctuations:
       no_punct = no_punct + char
t_sen = """"
for char in no_punct:
   if char in sentence:
       t_sen = no_punct.split(char)

corpus=[]
for cumle in t_sen:
    corpus.append(cumle.split())

model=Word2Vec(corpus,size=30,window=5,min_count=5,sg=1)
model.wv.most_similar('fransz')
</code></pre>
",2020-04-24 17:09:32,2020-04-25 04:56:41,KeyError: word fransz not in vocabulary,<python><nlp><data-science><gensim><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
26441,61427634,2020-04-25 14:57:53,,"<p>My code is to build a Word2Vec model on DataBricks using python and sparkSQL. I got this code to work on another data set but when I tried it on a diffrent data set I get the error. I get the feeling it didnt work well on the other dataset, I just didn't get an error. My goal is to build the model.</p>

<pre><code>import gensim
import tensorflow as tf
import pandas as pd
import sqlalchemy

print('gensim version: \t%s' % gensim.__version__)
print('TensorFlow version: \t%s' % tf.__version__)
print('Pandas version: \t%s' % pd.__version__)
print('SQLalchemy version: \t%s' % sqlalchemy.__version__)

df = spark.sql(""select WORK.CASE_SMRY from somePlace AS CASE_EVENTS_FLAT inner join edl_views.casemanagement AS Case_CMS ON (EVENTS_FLAT.CASE_ID = CMS.CASE_ID) where DTAC_CMS.PRD_PTRM_NM = 'A&amp;T HGT' AND DTAC_CMS.LANG_CD = 'en_US' "")

data_file_name = ""df""
raw_df = df.toPandas( ) #convert spark.sql to a pandas data frame
print(""Data loaded"")


raw_corpus = raw_df.join((str(raw_df['CASE_SMRY']+"" "")))  # PROBLEM AREA
print(""Raw Corpus contains {0:,} characters"".format(len(raw_corpus)))```

#Code is incomplete, but true up to the problem area. I know my code is crude and there are probably better ways to get a DB table into one long string but this was what I found on the net that worked...well it did on another dataset.

#Whole Code: https://github.com/WalczRobert/Recipes/blob/master/Databricks_Gensim_Word2Vec.ipynb
</code></pre>
",,2020-04-25 14:57:53,AttributeError: 'builtin_function_or_method' object has no attribute 'is_unique' on dataset,<python><databricks><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26447,61443023,2020-04-26 15:06:25,,"<p>I am working on a website which performs topic modelling on a text input by the user. Currently my code is reading the text from text file. </p>

<p>This is the code i have right now:</p>

<pre><code>@app.route('/topicmodelling', methods=['POST', 'GET'])

def topicmodelling():
    st = RSLPStemmer()
    texts = []

    file2 = open('C:/Users/anjal/Desktop/Final Year Project ITX3999/extracted datasets/test1.txt', 'r', encoding=""utf8"")

    for i in file2:
        tokens = word_tokenize(i.lower())
        stopped_tokens = [w for w in tokens if not w in stopwords.words('english')]
        stemmed_tokens = [st.stem(i) for i in stopped_tokens]
        texts.append(stemmed_tokens)

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]

    # generate LDA model using gensim
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=20)
    displayresult=ldamodel.print_topics(num_topics=5, num_words=4)

    return render_template('topicmodelling.html',displayresult=displayresult)

</code></pre>

<p>I want to convert this code to make it read the text which is inside my database and perform topic modelling on it.</p>

<p>This is the schema of my SQLite table from which I want to take the text to be topic modelled.</p>

<blockquote>
  <p>CREATE TABLE text(tID INTEGER PRIMARY KEY AUTOINCREMENT, inptext varchar(10000));</p>
</blockquote>

<p>I have already coded the functions which allow the users to enter the text using a textbox. The code is shown below: </p>

<pre><code>def inputtext(inptext):
    params = {'inptext':inptext}
    connection = s.connect(DB_FILE)
    cursor = connection.cursor()
    cursor.execute(""insert into text(inptext) VALUES(:inptext)"", params)
    connection.commit()
    cursor.close()

def getText(inptext):
    return inptext

@app.route('/inputtext', methods=['POST','GET'])
def input():
    if (request.method == 'POST'):
        inptext = request.form['inptext']
        getText(request.form['inptext'])
        inputtext(request.form['inptext'])
        return render_template('inputtext.html', msg2=""Topic Modelling completed. SEE RESULTS"")
    else:
        return render_template('inputtext.html')
</code></pre>

<p>All i want to do is change the file part in the topicmodelling function so that it takes values from the table in my database.</p>

<p>Please tell me how the code will look. </p>
",,2020-04-26 15:06:25,How to perform topic modelling on text within an SQLite database,<python><database><sqlite><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26459,61354352,2020-04-21 22:33:30,,"<p>I have the text and I want to filter the model with respect to the text. It is OK?</p>

<pre><code>import pandas as pd
import gensim
import nltk
from nltk import word_tokenize
from nltk.collocations import *
from nltk.stem.wordnet import WordNetLemmatizer
import re

text = ""though quite simple room solid choice allocated room already used summer holiday apartment bel endroit nice place place winter""
from gensim.models import Word2Vec,  KeyedVectors

model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz')
model_filter = [w for w in list(model.wv.vocab) if w in text]
</code></pre>

<p>If it is OK how to filter in the results (model_filter ) of the most similar function (modelo_filtrado.most_similar_cosmul????), those that belong to the text?
Thx.</p>
",,2020-04-22 00:11:01,How to filter a model with respect to text and then use most_similar?,<python><model><nltk><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
26464,61394114,2020-04-23 18:08:53,,"<p>I am trying to perform <strong><em>text classification on Roman Urdu Dataset using word2vec word embedding and deep learning model</em></strong>. My approach is based on first of all </p>

<ul>
<li><p>loading the corpus and cleaning the data, tokenize the sentences and
later sentences to words.</p></li>
<li><p>The tokenized data are being trained on word2vec model using gensim
library having vector size 300 and window size 3. </p></li>
<li><p>Later these word    embedding are  used to get the feature vector for
each document by    getting mean of word vector.</p></li>
<li><p>After that the acquired doc vectors are being split into training and
testing data and finally sent to deep learning model to text
classification (Positive,Negative, Neutral). </p></li>
</ul>

<p><strong><em>I am getting an accuracy of not more than 44% can anyone suggest what is wrong with my approach.</em></strong></p>

<p>Below is my code for reference.</p>

<pre><code>model_w2v = Word2Vec(sentences = sentences, size = 500, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)
model_w2v.init_sims(replace = True)
</code></pre>

<p>Creating doc vectors (by taking mean of word vectors):</p>

<pre><code>docs_vectors = pd.DataFrame()
#stopwords = nltk.corpus.stopwords.words('english') # removing stop words
stopwords = ['hai','nhi','main','tum']
for doc in yelp['Reviews'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it
    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc &amp; for 2nd doc remove the details of 1st &amp; proced through 2nd and so on..)
    dx = cleaner(word_tokenize(doc))
    for word in dx: # looping through each word of a single document and spliting through space
        #if word not in stopwords: # if word is not present in stopwords then (try)
        try:
          word_vec = model_w2v[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed
          temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe
        except:
          pass
    doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)
    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe

docs_vectors.shape
</code></pre>

<p><strong>My deep learning model:</strong></p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import LeakyReLU
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
numpy.random.seed(7)
# load the dataset but only keep the top n words, zero the rest
top_words = 32401
##(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)
# truncate and pad input sequences
max_review_length = 500
#X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
#X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)
# create the model
embedding_vecor_length = 32
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(LSTM(120))
#model.add(LeakyReLU(alpha=0.1))
model.add(Dense(1, activation='relu'))
#model.add(LeakyReLU(alpha=0.1))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(train_x, train_y, epochs=3, batch_size=64)
# Final evaluation of the model
scores = model.evaluate(test_x, test_y, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))
</code></pre>
",,2020-04-23 18:08:53,How to get better text classification accuracy using deep learning and word2vec features>,<python><tensorflow><deep-learning><word2vec><text-classification>,,,CC BY-SA 4.0,True,False,True,False,False
26483,61448908,2020-04-26 22:25:59,,"<p>I have chat interaction [Utterances] between Customer and Advisor and would want to know if the advisor interactions contains particular sentences or similar sentences in the below list:</p>

<p>Example sentences i am looking for in the Advisor interactions  </p>

<pre><code>[""I would be more than happy to help you with this"",
""I would be happy to look over the account to see how I can help get this sorted out for you"",
""Id be more than happy to look into this for you!"",
""Oh, I see, let me assist you with this concern."",
""I am more than happy to do everything I can to resolve this matter for you."",
""I would be happy to look over the account to see how I can help get this sorted out for you."",
""I am happy to have a look.""]


I have a dataset which contains the list of interaction_id and Utterances(Sample below)

```Example Chat interaction between Advisor and CLient : 
Client : Hello I would like to place an order for replacement battery
Agent: Hi Welcome to Battery service department. I would be happy to help you with your battery replacement Order.
</code></pre>

<p>How do get/Extract the sentences with similar intent or meaning.
I am newbie to NLP and i believe I have a sentences classification/Extraction problem in hand and would like to know is there any way i can achieve what i need</p>

<p>Basically I am trying to achieve the below:  </p>

<pre><code>ID    Utt                                               Help_Stmt_Present

IRJST   Hi Welcome to Battery service department. 
        I would be happy to help you with your battery
        replacement Order.                                     Yes 


</code></pre>
",2020-04-26 22:33:57,2020-04-26 22:58:23,How to extract sentences which has similar meaning/intent compared against a example list of sentences,<python-3.x><nlp><gensim><doc2vec><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
26484,61396498,2020-04-23 20:34:51,,"<p>I installed gensim module using (pip install gensim) and it installed successfully</p>

<pre><code>Successfully installed boto-2.49.0 boto3-1.12.45 botocore-1.15.45 docutils-0.15.2 gensim-3.8.2 jmespath-0.9.5 s3transfer-0.3.3 smart-open-1.11.1
</code></pre>

<p>but while importing it on my jupyter notebook, its showing:</p>

<pre><code>unable to import 'smart_open.gcs', disabling that module
</code></pre>
",,2020-05-13 06:37:34,Unable to import gensim module,<python-3.x><jupyter-notebook><gensim><word2vec><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26494,61460683,2020-04-27 14:03:20,,"<p>I would like to train a <code>word2vec</code> model using what is an unordered list of keywords and categories for each document. Therefore my vocabulary is quite small around 2.5k tokens.</p>

<p>Would the performance be improved if at the training step, I used actual sentences from the document?</p>

<p>From example:</p>

<pre><code>doc_keywords = ['beach', 'holiday', 'warm']
doc_body = 'Going on a beach holiday it can be very warm'
</code></pre>

<p>If there is a benefit to using the full documents, could someone also explain why this is the case?</p>

<p>Since the model predicts the next word in a document, what would be the benefit to it learning <code>very -&gt; warm</code> as two words which often come together, given that <code>very</code> is not in my vocabulary.</p>
",,2020-04-27 18:18:40,"word2vec, using document body or keywords as training corpus",<machine-learning><nlp><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26503,61343809,2020-04-21 12:41:06,,"<p>I have a list of 10 sentences from a text file.
I want to use an existing topics model to get the topics of every sentence.</p>

<p>In all the tutorials I found - they trained the topic model on their corpus. I want to use one that was trained in existing corpus and just apply it on my sentences.</p>

<p>Is this possible?</p>
",,2020-04-21 17:10:35,get topic of sentence from pre-trained model,<nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26515,61357566,2020-04-22 04:56:09,,"<p>I'm working with around 8k documents and all of them are based on a single topic. However, the documents cover various different events that happened across the world, related to that single topic. I want to find these subtopics (or events) from the documents. Now to achieve this, I'm using the gensim LDA model:</p>

<pre><code>corpus = [dictionary.doc2bow(doc) for doc in docTrain]

model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=17, chunksize=10000, id2word=dictionary,random_state=123, alpha = 0.01, eta = 0.9, passes = 10 )

coherencemodel = gensim.models.CoherenceModel(model=model, texts=data, dictionary=dictionary, coherence='c_v')
</code></pre>

<p>Since I was unaware of the number of topics in this case, I used elbow method to determine the optimal number of topics in this case, which comes out to be 17 or 18. Also, the coherence score is not increasing beyond 0.4.</p>

<p>I want to know what is going wrong and if there is any other approach that would help me solve this problem in a better way. Please let me know if any other information regarding my approach is required. </p>
",,2020-04-22 04:56:09,Topic modeling - How to get different sub-topics from a single topic,<machine-learning><data-science><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
26527,61436912,2020-04-26 06:56:06,,"<p>I wish to achieve a <code>text vectorization</code> of variable length input text based upon it's <code>KMeans Clustering</code> labels and <code>encode</code> it in someway to use it later for a <code>text-classification</code> purpose.</p>

<p><strong>Questions</strong></p>

<ol>
<li>How to effectively vectorize it keeping in mind there would be new words in test cases ?</li>
<li>Once represeted in KMeans Cluster Represetation, how to encode (possibly OneHotEncoder) to use it for fitting on models.</li>
<li>Could you help me to select which one of the one-hot-encoding approaches I've tried would work well with text-classification models.</li>
</ol>

<p><strong>For example:</strong>
I wish to achieve this in first query...</p>

<pre><code>             Lines                         KMeans Cluster Representation
0   This is a good company ABC Co. Ltd  | [c1, c1, c1, c2, c2, c3, c3, c3]
1   DEF LLC lacks manpower              | [c3, c3, c4, c4]
2   Total Amount is $200                | [c4, c2, c1, c5]
3   GHI JKL LLC                         | [c3, c3, c3]
4   ABC                                 | [c3]
</code></pre>

<hr>

<p><strong>Approach so far:</strong></p>

<ol>
<li><p>I have used Gensim's <code>Word2Vec</code> to <code>vectorize</code> train data with a output dimension of <code>300</code>. Then passed <code>word2vec</code> scores of each unique word to a sklearn's <code>KMeans Clustering</code> model choosing a cluster of <code>5</code> which gives me an output <code>label</code> for each word.</p></li>
<li><p>For new words in test data, I've put a check if that word is not available in <code>word2vec's</code> vocabulary, instead of assigning a random score and passing it to fitted <code>KMeans</code> to get a random label, I've directly assigned it a cluster label (that I imagine contains those kind of words).</p></li>
<li><p>Now for Query 2, I have two possible ways I could think of... </p>

<p>A. I am deciding to first label-encode all cluster labels with a mapping (0,1,...5) and then pad all the sequences with -1 uptil a chosen max_length. Resulting data is then again mapped with 0: [1,0,0,0,0] in a One-Hot-Encoder style mapping. </p>

<p>B. Use sklearn's MultiLabelBinarizer to hot-encode it in more clean way.</p></li>
</ol>

<p><strong>Suppose this is my dataframe ..</strong></p>

<pre><code>from keras.preprocessing.sequence import pad_sequences

df = pd.DataFrame({""Lines"": [""this is a good company ABC Co. Ltd"", 
                             ""DEF Ltd lacks manpower"", 
                             ""Total Amount is $200"", 
                             ""GHI JKL LLC"", 
                             ""ABC""],
                   ""Cluster_Rep"": [[""c1"", ""c1"", ""c1"", ""c2"", ""c2"", ""c3"", ""c3"", ""c3""], 
                                   [""c3"", ""c3"", ""c4"", ""c4""],
                                   [""c4"", ""c2"", ""c1"", ""c5""],
                                   [""c3"", ""c3"", ""c3""], 
                                   [""c3""]]})
</code></pre>

<p><strong>Here's 3.A. what looks like</strong></p>

<p>I mapped cluster labels I got from word2vec -> Kmeans with 0,1,..4 and then padded..</p>

<pre><code>mapping_1 = {'c1':0, ""c2"":1, ""c3"":2, ""c4"":3, ""c5"":4}
mapped = [[mapping_1[b] for b in i] for i in df.Cluster_Rep.tolist()]
padded = pad_sequences(mapped, padding='post', value=-1)
print(padded)
[out]: 
         [[ 0  0  0  1  1  2  2  2]
          [ 2  2  3  3 -1 -1 -1 -1]
          [ 3  1  4 -1 -1 -1 -1 -1]
          [ 2  2  2 -1 -1 -1 -1 -1]
          [ 2 -1 -1 -1 -1 -1 -1 -1]]
</code></pre>

<p>Then I re-mapped it using a list in OneHotEncoder style ....</p>

<pre><code>mapping_2 = {0: [1,0,0,0,0], 
             1: [0,1,0,0,0], 
             2: [0,0,1,0,0], 
             3: [0,0,0,0,1], 
             4: [0,0,0,0,1], 
            -1: [0,0,0,0,0]}

re_mapped = [[mapping_2 [b] for b in i] for i in padded]
print(re_mapped)
[out]:
    [[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0],
      [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]],
     [[0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0],
      [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], ..... likewise
</code></pre>

<p><strong>Here's what 3.B. looks like:</strong></p>

<pre><code>from sklearn.preprocessaing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
mapped = pd.DataFrame(mlb.fit_transform(df['Cluster_Rep']),columns=mlb.classes_, index=df.index)
print(mapped)
[out]:
       c1  c2 c3 c4  c5
     0  1   1   1   0   0
     1  0   0   1   1   0
     2  0   1   0   1   1
     3  0   0   1   0   0
     4  0   0   1   0   0
</code></pre>

<p>I know it's a long post, but please bear with me on this and let me know about your valuable suggestions/examples or more options. </p>

<p>My end goal is represent these text lines for classification. These lines would often have names, places, locations etc which makes it difficult for me to consider easier options like BoW model, etc. </p>

<p>Let me know about the 3 questions I've asked and which approach you think would fit better ??</p>

<p><em>Thanks in advance!</em></p>
",2020-04-26 07:02:20,2020-04-26 07:02:20,How to implement KMeans Clustering with Word2Vec for a text-classification model?,<python><nlp><word2vec><text-classification><one-hot-encoding>,,,CC BY-SA 4.0,False,False,True,False,True
26550,61361996,2020-04-22 09:45:35,,"<p>I have a number of known themes and a dataset with phrases on these themes. 
As a result I need get the theme of a phrase which is not in the dataset.
I train gensim doc2vec model on this dataset using only phrases. Here TaggedDocument is a list of words of a phrase.</p>

<pre><code>train_data = list(create_tagged_document(data))
#&gt; [TaggedDocument(words=['anarchism', 'originated', ... 'social', 'or', 'emotional'], tags=[0])]
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=1000)
model.build_vocab(train_data)
model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>

<p>Then I find the closest phrases to a new phrase. </p>

<pre><code>test_vec = model.infer_vector(['lorem', 'ipsum', 'new', 'phrase', 'words'])
similars = model.docvecs.most_similar([test_vec], topn=len(model.docvecs))
</code></pre>

<p>After that I check the themes of the closest phrases (based on cosine similarity probably) and if the closest phrases are close enough then decide that new phrase has the same theme as the closest.
Does this approach make sense? And did doc2vec model used correctly?</p>
",,2020-04-22 09:45:35,"Python, gensim, doc2vec. Classify phrases by themes",<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26591,61569868,2020-05-03 05:06:15,,"<p>gensim's <code>wv.most_similar</code> returns phonologically close words (similar sounds) instead of semantically similar ones. Is this normal? Why might this happen? </p>

<p>Here's the documentation on <code>most_similar</code>: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar</a></p>

<pre><code>In [144]: len(vectors.vocab)
Out[144]: 32966

... 

In [140]: vectors.most_similar('fight')
Out[140]:
[('Night', 0.9940935373306274),
 ('knight', 0.9928507804870605),
 ('fright', 0.9925899505615234),
 ('light', 0.9919329285621643),
 ('bright', 0.9914385080337524),
 ('plight', 0.9912853240966797),
 ('Eight', 0.9912533760070801),
 ('sight', 0.9908033013343811),
 ('playwright', 0.9905624985694885),
 ('slight', 0.990411102771759)]

In [141]: vectors.most_similar('care')
Out[141]:
[('spare', 0.9710584878921509),
 ('scare', 0.9626247882843018),
 ('share', 0.9594929218292236),
 ('prepare', 0.9584596157073975),
 ('aware', 0.9551078081130981),
 ('negare', 0.9550014138221741),
 ('glassware', 0.9507938027381897),
 ('Welfare', 0.9489598274230957),
 ('warfare', 0.9487678408622742),
 ('square', 0.9473209381103516)]
</code></pre>

<p>The training data contains academic papers and this was my training script: </p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim
import gensim.models.keyedvectors as word2vec

dim_size = 300
epochs = 10
model = FT_gensim(size=dim_size, window=3, min_count=1)
model.build_vocab(sentences=corpus_reader, progress_per=1000)
model.train(sentences=corpus_reader, total_examples=total_examples, epochs=epochs)

# saving vectors to disk
path = ""/home/ubuntu/volume/my_vectors.vectors""
model.wv.save_word2vec_format(path, binary=True)

# loading vectors 
vectors = word2vec.KeyedVectors.load_word2vec_format(path)
</code></pre>
",2020-05-07 15:35:25,2020-10-12 21:27:14,Gensim's `model.wv.most_similar` returns phonologically similar words,<python><data-science><gensim><embedding><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
26606,61521498,2020-04-30 10:39:00,,"<p>I am trying to run Python's gensim package in R environment via reticulate. More specifically, I am trying to build a doc2vec model, for which a corpus of tokens and tags needs to be prepared.</p>

<p>The TaggedDocument function is where I am having problems. Here's an example in python of what I am trying to reproduce in R:</p>

<pre><code>import pandas as pd
import numpy as np
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

data = [""this is the first sentence"",
        ""running doc2vec via reticulate"",
        ""r and python bff forever"",
        ""this aint working""]

tags = [""a"",""b"",""a"",""c""]

corpus = pd.DataFrame({""sentences"": data, ""labels"": tags})

tagged_data = corpus.apply(
    lambda r: TaggedDocument(words=word_tokenize(r['sentences']), tags=[r.labels]), axis=1)

</code></pre>

<p>This results in an output of this kind:</p>

<pre><code>0       ([this, is, the, first, sentence], [a])
1    ([running, doc2vec, via, reticulate], [b])
2         ([r, and, python, bff, forever], [a])
3                  ([this, aint, working], [c])
dtype: object
</code></pre>

<p>which can be used to build a vocabulary and train a doc2vec model.</p>

<p>How can I get to the same result in R (possibly without loops)?</p>

<pre><code>library(reticulate)

gensim &lt;- import(""gensim"") 
Doc2Vec &lt;- gensim$models$Doc2Vec 
TaggedDocument &lt;- gensim$models$doc2vec$TaggedDocument


sentences &lt;- c(""this is the first sentence"",
        ""running doc2vec via reticulate"",
        ""r and python bff forever"",
        ""this aint working"")

labels &lt;- c(""a"",""b"",""c"",""a"")
</code></pre>

<p>Thanks in advance!</p>

<p><strong>* EDIT *</strong> </p>

<p>I have been trying an even simpler setting: </p>

<pre><code>library(reticulate)

gensim &lt;- import(""gensim"") 
Doc2Vec &lt;- gensim$models$Doc2Vec 
TaggedDocument &lt;- gensim$models$doc2vec$TaggedDocument


sentences &lt;- c(""this is the first sentence"")
tags &lt;- c(""a"")

df &lt;- data.frame (tokens= sentences, labels = tags)

tagged_docs &lt;- TaggedDocument(words = df$tokens, tags = df$labels)
</code></pre>

<p>but I keep receiving the same error message: </p>

<blockquote>
  <p>Blockquote
  Error in py_call_impl(callable, dots$args, dots$keywords) : 
    AttributeError: 'str' object has no attribute 'words' - Detailed traceback: 
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1184, in build_vocab
      progress_per=progress_per, trim_rule=trim_rule
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1381, in scan_vocab
      total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1310, in _scan_vocab
      if isinstance(document.words, string_types):</p>
</blockquote>

<p><strong>---</strong> </p>

<p>What am I doing wrong?</p>
",2020-05-05 15:32:44,2020-10-04 10:07:32,How can I use the TaggedDocument function (Gensim \ Doc2Vec) via Reticulate in R?,<r><gensim><doc2vec><reticulate>,,,CC BY-SA 4.0,True,False,True,False,False
26614,61572397,2020-05-03 09:41:08,,"<p>I copy a simple Python script by <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">Building a Wikipedia Text Corpus for Natural Language Processing</a> to build the corpus by stripping all Wikipedia markup from the articles, using gensim. This is the cose:</p>

<pre><code>""""""
Creates a corpus from Wikipedia dump file.
Inspired by:
https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py
""""""

import sys
from gensim.corpora import WikiCorpus

    def make_corpus(in_f, out_f):

    """"""Convert Wikipedia xml dump file to text corpus""""""

    output = open(out_f, 'w')
    wiki = WikiCorpus(in_f)

    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('Processed ' + str(i) + ' articles')
    output.close()
    print('Processing complete!')


if __name__ == '__main__':

    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
</code></pre>

<p>Anyway, I obtained the error:</p>

<pre><code>ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>although I have installed the <code>gensim</code> package:</p>

<pre><code>python3 -m pip install gensim
</code></pre>

<p><strong>EDIT</strong>. If I try with</p>

<pre><code>pip install -U gensim
</code></pre>

<p>I obtain the error</p>

<pre><code> ImportError: cannot import name 'SourceDistribution' from 
 'pip._internal.distributions.source' (C:\Users\Standard\Anaconda3\lib\site- 
 packages\pip\_internal\distributions\source\__init__.py)
</code></pre>
",2020-05-03 11:09:39,2020-05-03 15:49:29,Build the corpus by Wikipedia: ModuleNotFoundError: No module named 'gensim',<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26619,61472611,2020-04-28 04:26:29,,"<pre><code>from gensim.sklearn_api.phrases import PhrasesTransformer

# Create the model. Make sure no term is ignored and combinations seen 3+ times are captured.
m = PhrasesTransformer(min_count=1, threshold=3)
text = [['I', 'love', 'computer', 'science', 'computer', 'science', 'is', 'my', 'passion', 'I', 'studied', 'computer', 'science']]

# Use sklearn fit_transform to see the transformation.
# Since computer and science were seen together 3+ times they are considered a phrase.
m.fit_transform(text)
</code></pre>

<p>The above code does return computer_science as expected. But What is the right method to extract phrases pragmatically?</p>
",,2020-04-29 20:11:25,Phrase detection using PhrasesTransformer,<nlp><gensim><n-gram><phrase>,,,CC BY-SA 4.0,False,False,True,False,True
26654,61509408,2020-04-29 18:48:56,,"<p>I am currently getting a segmentation fault when I am loading a model with gensim. In order to create the model and save it, I do:</p>

<pre><code>glove_file = 'QGModels/embeddings/glove.6B.300d.txt'
tmp_file = 'QGModels/embeddings/word2vec-glove.6B.300d.txt'
glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file)
model.save('QGModels/embeddings/model.model')
</code></pre>

<p>However the problem starts when I load the model and use the most_similar method using:</p>

<pre><code>model = KeyedVectors.load('QGModels/embeddings/model.model')
closestWords = model.most_similar(positive=[answer], topn=count)
</code></pre>

<p>And then get a segmentation fault:</p>

<blockquote>
  <p>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</p>
</blockquote>

<p>Any and all help is appreciated! Thank You.</p>
",,2020-04-29 18:54:56,Segmentation Fault with Gensim,<python><segmentation-fault><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26658,61625273,2020-05-06 00:22:50,,"<p>I have tokenized my strings and made a Pandas column out of them and if I print the column <code>df['word_splits']</code> it looks like this.</p>

<pre><code>0    ['explanation', 'why', 'the', 'edits', 'made',...
1    [""d'aww"", '!', 'he', 'matches', 'this', 'backg...
2    ['hey', 'man', ',', ""i'm"", 'really', 'not', 't...
3    ['more', 'i', ""can't"", 'make', 'any', 'real', ...
4    ['you', ',', 'sir', ',', 'are', 'my', 'hero', ...
Name: word_splits, dtype: object
</code></pre>

<p>Next, I'm running Word2Vec</p>

<pre><code>model = gensim.models.Word2Vec(sentences=df[""word_splits""])
</code></pre>

<p>When I print out the vocabulary, using</p>

<pre><code>words = list(model.wv.vocab)
print(words)
</code></pre>

<p>I'm getting characters instead of a long list of words (vocabulary).</p>

<pre><code>['[', ""'"", 'e', 'x', 'p', 'l', 'a', 'n', 't', 'i', 'o', ',', ' ', 'w', 'h', 'y', 'd', 's', 'm', 'u', 'r', 'c', 'f', 'v', '?', '""', 'j', 'g', 'k', '.', ']', '!', 'b', '-', 'q', 'z']
</code></pre>

<p>Not sure what I'm doing wrong. </p>
",,2020-05-06 04:40:51,Word2Vec Giving Characters instead of Words,<python><pandas><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26693,61596101,2020-05-04 15:42:09,,"<p>I'm trying to calculate a between-topic cosine similarity score from a <code>Gensim</code> LDA topic model, but this proves more complicated than I first expected.</p>

<p><code>Gensim</code> has a method to calculate distances between topics <code>model.diff(model)</code>, but unfortunately cosine distance is not implemented; it has jaccard distance, but it is a bit too vector-length dependent (i.e., when comparing top 100 most important words per topic the distance is lower than comparing top 500, and the distance is 0 when full-length vectors are compared, as each topic includes all terms, but with different probabilities).</p>

<p>My problem is that the output from the model looks like this (only shown 4 top words):</p>

<pre><code>(30, '0.008*""tax"" + 0.004*""cut"" + 0.004*""bill"" + 0.004*""spending""')
(18, '0.009*""candidate"" + 0.009*""voter"" + 0.009*""vote"" + 0.009*""election""')
(42, '0.047*""shuttle"" + 0.034*""astronaut"" + 0.026*""launch"" + 0.025*""orbit""')
(22, '0.023*""boat"" + 0.020*""ship"" + 0.015*""migrant"" + 0.013*""vessel""')
</code></pre>

<p>So, in order to calculate the cosine sim/distance, I would have to parse the second element of the tuple (i.e., the  <code>'0.008*""tax"" +...'</code> part, which indicates term probabilities. </p>

<p>I was wondering whether there is an easier way to get cosine similarity out of the model? Or parsing each individual string of term/probabilities is really the only way to go?</p>

<p>Thanks for the help.</p>
",,2020-05-04 19:17:29,Calculating cosine similarity from a Gensim model,<python><gensim><topic-modeling><cosine-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
26702,61608275,2020-05-05 07:52:35,,"<p>I am training text data using gensim doc2vec model on google colab repository GPU runtime, and want to save trained model in test.d2v file. following is code snippet</p>

<pre><code>T = [TaggedDocument(doc, [i]) for i, doc in enumerate(data['info'])]
model = Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1)
model.save('test.d2v')
</code></pre>

<p>Following error is generated in colab notebook.</p>

<p>** /usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: <a href=""https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function</a>
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL</p>
",,2020-05-05 13:39:05,save gensim doc2vec trained model on google colab,<nlp><google-colaboratory><gensim><sentiment-analysis><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
26718,61511101,2020-04-29 20:29:46,,"<p>Let's say we train a model with more than 1 million words. In order to find the most similar words we need to calculate the distance between the embedding of the test word and embeddings of all the 1 million words words, and then find the nearest words. It seems that Gensim calculate the results very fast. Although when I want to calculate the most similar, my function is extremely slow:</p>

<pre><code>def euclidean_most_similars (model, word, topn = 10):
  distances = {}
  vec1 = model[word]
  for item in model.wv.vocab:
    if item!= node:
      vec2 = model[item]
      dist = np.linalg.norm(vec1 - vec2)
      distances[(node, item)] = dist
  sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))
</code></pre>

<p>I would like to know how Gensim manages to calculate the most nearest words so fast and what is an efficient way to calculate the most similares.</p>
",,2020-04-30 03:46:08,How does gensim manage to find the most similar words so fast?,<python><time-complexity><gensim><word2vec><similarity>,,,CC BY-SA 4.0,False,False,True,False,False
26794,61638940,2020-05-06 15:22:45,,"<p>I've got an LDA model through using gensim. I can save it locally:</p>

<pre><code>ldamodel.save('models/lda/lda.model')
</code></pre>

<p>This results in four files in the specified place:</p>

<pre><code>lda.model
lda.model.expElogbeta.npy
lda.model.id2word
lda.model.state
</code></pre>

<p>Loading them back is as simple as </p>

<pre><code>ldamodel =  models.LdaModel.load('models/lda/lda.model')
</code></pre>

<p>However, I want this model to be saved on s3. I can work out how to save individual bits, for example:</p>

<pre><code>s3.meta.client.upload_file('models/lda/lda.model', 'bucket-name', 'lda.model')
</code></pre>

<p>But I can't work out how to actually meaningfully read them back in so they will function as expected as a coherent model. So the idea being that somebody other than me could take the files from s3 and use them as a model in Python. </p>

<p>Can anybody help?</p>
",,2020-05-07 09:04:10,Save a gensim LDA model to s3,<python><amazon-s3><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
26839,61709811,2020-05-10 09:30:57,,"<p>Working on Text classification using Deep Neural Network in Python and
 getting the error that I mentioned in the title.</p>

<pre><code>import numpy as np # linear algebra
import pandas as pd
import array as arr 

from subprocess import check_output
train = pd.read_csv('karti.csv',encoding='latin1')
print(train.head())


import nltk as nl
train['tokens'] = [nl.word_tokenize(sentences) for sentences in train.review]
words = []
for item in train.tokens:
    words.extend(item)

stemmer = nl.stem.lancaster.LancasterStemmer()
words = [stemmer.stem(word) for word in words]


filtered_words = [word for word in words if word not in nl.corpus.stopwords.words('english')]


import numpy as np
import gensim
# let X be a list of tokenized texts (i.e. list of lists of tokens)
model = gensim.models.Word2Vec(filtered_words, size=100)
w2v = dict(zip(model.wv.index2word, model.wv.syn0))
#np.asarray(w2v, dtype=np.float32)
print(type(w2v))
print(w2v['h'])
print(len(w2v))


training = []
for index,item in train.iterrows():

  vec = np.zeros(100)
  token_words = [stemmer.stem(word) for word in item['tokens']]
  token_words = [word for word in token_words if word not in nl.corpus.stopwords.words('english')]
  for w in token_words:

    if w in w2v:

      vec += w2v[w]
  norm = np.linalg.norm(vec)
  if norm != 0:

      vec /= np.linalg.norm(vec)

  training.append(vec)


training_new = np.array(training)
from numpy import array

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(training_new[:,1])
print(len(integer_encoded))
# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

train_y = onehot_encoded
train_x = list(training_new[:,0])
print(train_y)
print(len(train_x))
import tensorflow as tf
import tflearn
# reset underlying graph data
tf.reset_default_graph()
# Build neural network
net = tflearn.input_data(shape=[None, len(train_x)])
#net = tflearn.input_data(shape=[None, len(other)])
#net = tflearn.input_data(len(train_x))
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, len(train_y[0]), activation='linear')
net = tflearn.regression(net)

# Define model and setup tensorboard
model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
# Start training (apply gradient descent algorithm)
model.fit(train_x, train_y, n_epoch=10,batch_size=8, show_metric=True)
</code></pre>

<p>MY CSV FILE CONTAINS 3 COLUMNS rating, review, and text all are in string datatype. CSV file contains 150 rows. Done in Google Colab Python. Using Deep Neural Network Text Classification is done</p>
",2020-05-10 11:29:55,2020-05-10 11:29:55,"Cannot feed value of shape (8,) for Tensor 'InputData/X:0', which has shape '(?, 150)",<python><csv><machine-learning><deep-learning><nltk>,,,CC BY-SA 4.0,True,False,True,False,True
26846,61693100,2020-05-09 07:12:38,,"<p>I'm using windows 10 and python 3.3. I tried to download fasttext_model300 to calculate soft cosine similarity between documents, but when I run my python file, it stops after arriving at this statement:</p>

<pre><code>fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')
</code></pre>

<p>There are no errors or not responding, It just stops without any reaction.</p>

<p>Does anybody know why it happens?</p>

<p>Thanks</p>
",2020-05-10 05:53:37,2020-05-10 05:53:37,how to fix the problem of downloading fasttext-model300?,<text-mining><gensim><similarity><cosine-similarity><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
26847,61693901,2020-05-09 08:33:13,,"<p>In my study, I am exploring if there is a statistically significant ideological bias in one set of media as compared to another. I was hoping to explore this using the word embeddings approach. </p>

<p>Let us take US and UK news media for example. If I build a corpora of all US media articles for a given time period and a separate corpora of all UK media articles for the same period, train them each using the same word embeddings algorithm (<code>gensim/word2vec/fasttext</code>) with the same set of parameters (e.g., window and vector size), is it possible to test if cosine similarity obtained between a pair of words in the US corpora is statistically significantly larger than cosine similarity obtained between the same pair of words in the UK corpora?</p>

<p>Many thanks for your help!</p>
",2020-05-09 08:53:22,2020-05-09 13:37:24,Is it possible to compare similarity scores across two word embeddings repository?,<nlp><stanford-nlp><gensim><word2vec><fasttext>,,,CC BY-SA 4.0,False,False,True,True,False
26851,61736874,2020-05-11 18:38:20,,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
",,2020-05-12 00:24:17,How to compare cosine similarities across three pretrained models?,<nlp><gensim><word2vec><word-embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
26918,61746512,2020-05-12 07:56:42,,"<p>I am working with Gensim FASTText modeling and have the following questions.</p>

<ul>
<li>The output of ""ft_model.save(BASE_PATH + MODEL_PATH + fname)"" saves the following 3 files. Is this correct? is there a way to combine all three files? </li>
</ul>

<blockquote>
<pre><code>ft_gensim-v3
ft_gensim-v3.trainables.vectors_ngrams_lockf.npy
ft_gensim-v3.wv.vectors_ngrams.npy
</code></pre>
</blockquote>

<p>When I attempt to load the training file and then use it, I get the following error from <code>if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:</code>  </p>

<blockquote>
  <p>'function' object has no attribute 'wv'</p>
</blockquote>

<p>Finally, both models, is there a way not to have to store the output of <code>def read_train(path,label_path)</code> and <code>def lemmetize(df_col)</code>so I do not have to run this part of the code every time I want to train the model or compare? </p>

<p>Thanks for the assistance. </p>

<p><strong>Here is my FastText Train Model</strong></p>

<pre><code>import os
import logging
from config import BASE_PATH, DATA_PATH, MODEL_PATH
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from pprint import pprint as print
from gensim.models.fasttext import FastText as FT_gensim
from gensim.test.utils import datapath

#Read Training data
import pandas as pd
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
        #print(d)
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)


#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)


word_tokenized_corpus = read_train('Train Data.xlsx','SMEQueryText.csv')


#Train fasttext model
import tempfile
import os

from gensim.models import FastText
from gensim.test.utils import get_tmpfile
fname = get_tmpfile(""ft_gensime-v3"")

def train_fastText(data, embedding_size = 60, window_size = 40, min_word = 5, down_sampling = 1e-2, iter=100):
    ft_model = FastText(word_tokenized_corpus,
                      size=embedding_size,
                      window=window_size,
                      min_count=min_word,
                      sample=down_sampling,
                      sg=1,
                      iter=100)

    #with tempfile.NamedTemporaryFile(prefix=BASE_PATH + MODEL_PATH + 'ft_gensim_v2-', delete=False) as tmp:
    #    ft_model.save(tmp.name, separately=[])
    ft_model.save(BASE_PATH + MODEL_PATH + fname)
    return ft_model


# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = train_fastText(train_data)

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','QueryText.csv')
</code></pre>

<p><strong>Here is my Usage Model</strong></p>

<pre><code>import pandas as pd
from gensim.models import FastText
import gensim
from config import BASE_PATH, DATA_PATH, MODEL_PATH

#Read Training data
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)

#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)

def load_training():
    return FT_gensim.load(BASE_PATH + MODEL_PATH +'ft_gensim-v3')

#compare similarity
def compare_similarity(model, real_data, labelled):
    maxWord = ''
    category = ''
    maxSimilaity = 0
    #print(""train data"",labelled[1])
    for i in range(len(labelled)):
        if model.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
            #print('labelled',labelled['QueryText'][i], 'i', i)
            maxWord = labelled['QueryText'][i]
            category = labelled['Subjectmatter'][i]
            maxSimilaity = model.similarity(real_data, labelled['QueryText'][i])

    return maxWord, category, maxSimilaity

# Output from Main to excel
from pandas import ExcelWriter
def export_Excel(data, aFile = 'FASTTEXTOutput.xlsx'):
    df = pd.DataFrame(data)
    writer = ExcelWriter(aFile)
    df.to_excel(writer,'Sheet1')
    writer.save()

# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = load_training
    for i in range(len(test_data)):
        output_df['test_query'][i] = test_data['query_text'][i]
        #&lt;first change&gt;
        maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
        output_df['Similar word'][i] = maxWord
        output_df['category'][i] = category
        output_df['similarity'][i] = maxSimilaity
    #&lt;second change&gt;    
    return output_df

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
    export_Excel(output)
</code></pre>

<p><strong>Here is the full tracible error message</strong></p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-22-57803b59c0b9&gt; in &lt;module&gt;
      1 # run main
      2 if __name__ == ""__main__"":
----&gt; 3     output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
      4     export_Excel(output)

&lt;ipython-input-21-17cb88ee0f79&gt; in main(test_path, train_path, labelled)
     13         output_df['test_query'][i] = test_data['query_text'][i]
     14         #&lt;first change&gt;
---&gt; 15         maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
     16         output_df['Similar word'][i] = maxWord
     17         output_df['category'][i] = category

&lt;ipython-input-19-84d7f268d669&gt; in compare_similarity(model, real_data, labelled)
      6     #print(""train data"",labelled[1])
      7     for i in range(len(labelled)):
----&gt; 8         if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
      9             #print('labelled',labelled['QueryText'][i], 'i', i)
     10             maxWord = labelled['QueryText'][i]

AttributeError: 'function' object has no attribute 'wv'
</code></pre>
",2020-05-12 17:31:40,2020-05-12 20:51:25,Python Gensim FastText Saving and Loading Model,<python><gensim><fasttext>,,,CC BY-SA 4.0,True,False,True,False,False
26922,61797610,2020-05-14 12:40:30,,"<p>I want to train GloVe embeddings based on my own corpus. However, I want the initial weights for the words in my vocabulary to be equal to the vectors from the pre-trained GloVe model.</p>

<p>1) Is there some function where I can specify the initial weights? I've been trying to find such function but I didn't succeed.</p>

<p>2) Also, I found e.g. the glove and gensim packages. Are there any reasons why I should use one of these packages and not the other?</p>

<p>3) I also have to train another similar model but with fastText. Is the function for specifying the initial weights in fastText the same as it is in GloVe?</p>

<p>I'm a complete beginner when it comes to Python so I would really appreciate any tips.</p>
",,2020-05-14 12:40:30,Initial weights for GloVe and fastText embeddings in Python,<python><gensim><fasttext><glove>,,,CC BY-SA 4.0,False,False,True,False,False
26942,61800216,2020-05-14 14:42:01,,"<p>i try to compute the similarity of two words using cosine distance (<a href=""https://stackoverflow.com/questions/29484529/cosine-similarity-between-two-words-in-a-list"">source</a>).
this is the code :</p>

<pre><code>def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the ""length"" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
</code></pre>

<p>the similarity is 0.1889822365046136 when i called:</p>

<pre><code>cosdis(word2vec('tahu') , word2vec('tempe'))
</code></pre>

<p>when try to compare with the result of similarity using library (gensim word2vec) the result is different (for example the difference is 0.2). why is that?
this is how i get the similarity using library:</p>

<pre><code>from gensim.models import Word2Vec
modelword2vec = Word2Vec.load(""/idwiki_word2vec_300.model"")
modelword2vec.similarity('tahu' , 'tempe')
</code></pre>

<p>the similarity is 0.21785985</p>
",2020-05-14 21:30:26,2020-05-15 00:16:00,why do i get different result of cosine similarity when compare to library result,<python><nlp><cosine-similarity><edit-distance>,,,CC BY-SA 4.0,False,False,True,False,False
26971,61622340,2020-05-05 20:20:44,,"<p>Do we have an option to save a trained Gensim Word2Vec model as a saved model using tf 2.0 <code>tf.saved_model.save</code>? In other words, how can I save a trained embedding vector as a saved model signature to work with tensorflow 2.0. The following steps are not correct normally:</p>

<pre><code>model = gensim.models.Word2Vec(...)

model.init_sims(..)

model.train(..)

model.save(..)

module = gensim.models.KeyedVectors.load_word2vec(...)

tf.saved_model.save(
    module, 
    export_dir
)
</code></pre>

<p>EDIT:</p>

<p>This example helped me about how to do it : <a href=""https://keras.io/examples/nlp/pretrained_word_embeddings/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/pretrained_word_embeddings/</a></p>
",2020-06-16 08:47:14,2020-06-16 08:47:14,Save trained gensim word2vec model as a tensorflow SavedModel,<tensorflow><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27001,61807128,2020-05-14 20:48:25,,"<p>I'm actually injecting 77 document in a gensim mode by reading them from a database with a first script and i save the document on file system.</p>

<p>I then load an other doc to check the similarity with a vector</p>

<pre><code>def read_corpus_bdd(cursor, tokens_only=False):
    for i, (url_id, url_label, contenu) in enumerate(cursor):
        tokens = gensim.utils.simple_preprocess(contenu)
        if tokens_only:
            yield tokens
        else:
            # For training data, add tags
            # yield gensim.models.doc2vec.TaggedDocument(tokens, dataLine[0])
            yield gensim.models.doc2vec.TaggedDocument(tokens, [int(str(url_id))])
            print (int(str(url_id)))
targetContentCorpus = list(read_corpus_bdd(cursor))

# Param of trainer corpus
model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=40)

# Build a vocabulary
model.build_vocab(targetContentCorpus)

###############################################################################

model.train(targetContentCorpus, total_examples=model.corpus_count, epochs=model.epochs)

##generate file model name for save
from datetime import date
pathModelSave=os.getenv(""MODEL_BASE_SAVE"") +'/projet_'+ str(projetId)
</code></pre>

<p>When i infer the vector :</p>

<pre><code>inferred_vector = model.infer_vector(test_corpus[0])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

len(sims) #output 335
</code></pre>

<p>So I don't understand where this 335 come from and also why </p>

<pre><code>sims[0][0]
</code></pre>

<p>return other id than the tagged one in the yield section
    enter code here</p>
",,2020-05-14 20:48:25,Anormal number of sims document in gensim,<gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27002,61849249,2020-05-17 08:52:35,,"<p>I have used doc2vec to find the similarities in multiple documents, but when i am checking the same document which i created my model, the score should be '1' right? as the used document and the to be predict document is same. Sadly, I am getting different score when trying to find the similarities. Below is the attached code. Please tell me how to make this right, I can't find what is wrong here. Pleas help me...<a href=""https://i.stack.imgur.com/97bk9.png"" rel=""nofollow noreferrer"">doc2vec - calculating document cosine similarity</a></p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
df['Tagged_data'] = df['sent_to_word_tokenize_text'].apply(lambda x: [TaggedDocument(d, [i]) for i, d in enumerate(x)])
sadguru_model = Doc2Vec(df['Tagged_data'][0], vector_size = 1000, window = 500, dm = 1, min_count = 1, workers = 2, epochs = 100) 
test_doc = word_tokenize(' '.join([word for word in df['Sentence_Tokenized_Text'][0]])) 
# Sadguru model document 
index0 = sadguru_model.docvecs.most_similar(positive=sadguru_model.infer_vector(test_doc)], topn =1) output: index0 = [(4014, 0.5270981788635254)] 
</code></pre>

<p>output: <code>index0 = [(4014, 0.5270981788635254)]</code></p>
",2020-05-17 21:11:30,2020-05-17 21:11:30,Getting less than 1 score while trying to check the cosine similarities of same document,<nlp><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27005,61852870,2020-05-17 13:50:29,,"<p>text='Alice is a student.She likes studying.Teachers are giving a lot of homewok.'</p>

<h1>I am trying to get topics from a simple text(like above) with coherance score.This is my LDA model:</h1>

<pre><code>id2word = corpora.Dictionary(data_lemmatized)
texts = data_lemmatized
corpus = [id2word.doc2bow(text) for text in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=5, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<h1>When i try to run this coherance model:</h1>

<pre><code>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, 
coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<h1>I am  supposed to get this king of output-> Coherence Score:  0.532947587081</h1>

<p>I get this error:
raise RuntimeError('''
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.</p>

<pre><code>    This probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:

        if __name__ == '__main__':
            freeze_support()
            ...

    The ""freeze_support()"" line can be omitted if the program
    is not going to be frozen to produce an executable.
</code></pre>

<p>What should i do to fix this?</p>
",2020-05-17 16:45:39,2020-05-23 02:35:59,How to fix LDA model coherence score runtime Error?,<python><nlp><runtime-error><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
27010,61789168,2020-05-14 04:09:52,,"<p>I am trying to parse the <a href=""https://www.kaggle.com/watts2/glove6b50dtxt"" rel=""nofollow noreferrer"">Glove6b50d data from Kaggle</a> in via Google Colab, then run it through the word2vec process (apologies for the huge URL - it's the fastest link I've found). However, I'm hitting a bug where '-' tokens are not parsed correctly, resulting in the above error. </p>

<p>I have attempted to handle this in a few ways. I've also looked into the load_word2vec_format method itself and tried to ignore errors, however it doesn't seem to make a difference. I've tried a map method on line two, following combinations of advice from these links: <a href=""https://stackoverflow.com/questions/45269652/python-convert-string-to-float-error-with-negative-numbers"">[a]</a> and <a href=""https://stackoverflow.com/questions/21385673/shortest-way-to-replace-parts-of-strings-in-numpy-array"">[b]</a>. This hasn't fixed or changed the error message received (i.e. removing it changes nothing in the text). </p>

<pre><code>gloveFile = pd.read_fwf(""https://storage.googleapis.com/kagglesdsdata/datasets/652874/1154868/glove.6B.50d.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1589683535&amp;Signature=kaS%2FTkSmvp7lhqwLJ%2B1lyuvP76PcDpwK1dnsCZEO0AiVXqQm7jsBc1r5g9af%2BuVkOSvMgqUDXYL4O%2BN43pnL5RLs7ns%2B3w%2BEtCYDTfJz6q1O0zfPz4%2BTcD3GV7UAGgVjVNIvncC9fHWcd2YuKwiZaTvKL%2BGRnMkf9b%2BYnOweYeXEeA1sX005krj%2FLMBbVTXmDTwOtN4HwVNb3%2BrbezkWkoEC6sxLPnGcsEKaBe%2Biv%2FuVSQG5FsQlwvRgsSU%2FMgk0c4bi%2FHxF04lrQW0E0s767TIXwHeodRHYpk5KQeKmyd91uKD2Zb8v8xQcf2%2BkmSNGQHbX0mDz8HBwYEmOdV7aMQ%3D%3D&amp;response-content-disposition=attachment%3B+filename%3Dglove.6B.50d.txt"",
                    delimiter=""\n\t\s+"", header=None)

map(lambda gloveFile: gloveFile.replace(r'[^\x00-\x7F]+' , '-'), gloveFile[0])

numpy.savetxt(r'/usr/local/lib/python3.6/dist-packages/gensim/test/test_data/glove6b50d.txt', gloveFile.values, fmt=""%s"")

from gensim.models import KeyedVectors
from gensim.test.utils import datapath, get_tmpfile
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('glove6b50d.txt')

glove2word2vec(glove_file, ""glove6b50d_word2vec.txt"")

model = KeyedVectors.load_word2vec_format(""glove6b50d_word2vec.txt"", binary=False)
</code></pre>

<p>Per the comment below, the exact error I'm getting is as follows:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-132-6ad5a51f4fb3&gt; in &lt;module&gt;()
      9 glove2word2vec(glove_file, ""glove6b50d_word2vec.txt"")
     10 
---&gt; 11 model = KeyedVectors.load_word2vec_format(""glove6b50d_word2vec.txt"", binary=False)
     12 

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;listcomp&gt;(.0)
    220                 if len(parts) != vector_size + 1:
    221                     raise ValueError(""invalid vector on line %s (is this really the text format?)"" % line_no)
--&gt; 222                 word, weights = parts[0], [datatype(x) for x in parts[1:]]
    223                 add_word(word, weights)
    224     if result.vectors.shape[0] != len(result.vocab):

ValueError: could not convert string to float: '-'
</code></pre>

<p>The system works fine using a text file containing only: ""test -1.0 1.526 -2.55"" or ""- -1.0 1.526 -2.55"". Additionally, searching the source text file (glove.6B.50d.txt) for occurrences of "" - "" comes up with no results. I'm on Windows, so I have done so by executing: </p>

<pre><code>findstr /C:"" - "" glove.6B.50d.txt
</code></pre>

<p>Calling <code>print(gloveFile)</code> both pre- and post-map call provide the following output. Note that I've kept the mapping call in for completeness of my efforts, not for its effect. </p>

<pre><code>0       the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.0...
1       , 0.013441 0.23682 -0.16899 0.40951 0.63812 0....
2       . 0.15164 0.30177 -0.16763 0.17684 0.31719 0.3...
3       of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.7...
4       to 0.68047 -0.039263 0.30186 -0.17792 0.42962 ...
...                                                   ...
399995  chanty 0.23204 0.025672 -0.70699 -0.045465 0.1...
399996  kronik -0.60921 -0.67218 0.23521 -0.11195 -0.4...
399997  rolonda -0.51181 0.058706 1.0913 -0.55163 -0.1...
399998  zsombor -0.75898 -0.47426 0.4737 0.7725 -0.780...
399999  andberger 0.072617 -0.51393 0.4728 -0.52202 -0...
</code></pre>

<p>If I print the first ten lines of the <code>glove6b50d_word2vec.txt</code> file, I get the following text, which matches the word2vec format. Additionally, if I count the occurrences of the string <code>"" - ""</code> in the document, I find none. </p>

<pre><code>['400000 50\n', 'the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n', ', 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n', '. 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n', 'of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n', 'to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n', 'and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n', 'in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n', 'a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n', '"" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n']
</code></pre>

<p>My search methods are evidently thusfar ineffective. Would really appreciate some help. </p>
",2020-05-15 03:08:57,2020-05-15 22:58:14,Glove6b50d parsing: could not convert string to float: '-',<python><text><gensim><word-embedding><glove>,,,CC BY-SA 4.0,False,False,True,False,False
27014,61853117,2020-05-17 14:07:44,,"<p>I have trained a word2vec model using gensim. In the models matrix some values' floating point looks like this: ""-7.18556e-05""</p>

<p>I need to use the values on the matrix as a string. Is there a way to remove those ""e-05"",""e-04"" etc.?</p>

<pre><code>import nltk
from gensim.models import Word2Vec
from nltk.corpus import stopwords

text = ""My text is here""
sentences = nltk.sent_tokenize(text)
for i in range(len(sentences)):
    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]

model = Word2Vec(sentences, min_count=1)

words = model.wv.vocab

for word in words:
    matrix = model.wv[words.keys()]
</code></pre>
",,2020-05-17 21:48:51,Gensim Word2Vec model floating point,<python><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
27017,61854776,2020-05-17 16:02:04,,"<p>I have got this error: ""KeyError: word 'restriction' not in vocabulary"", when I read a text file to generate word embedding vectors, while the word 'restrictions' is in the text file. I wonder if my code for reading a textfile (a simple paragraph) is erroneous?</p>

<p>MY CODE IS WRITTN BELOW:</p>

<pre><code>from gensim.models import Word2Vec
# define training data
with open('D:\\test.txt', 'r') as file:
sentences = """"
#read from textfile
for line in file:
    for word in line.split(' '):
        sentences += word + ' '
# train model
model = Word2Vec(sentences, min_count=1)
# summarize the loaded model
print(model)
# summarize vocabulary
words = list(model.wv.vocab)
# save model
model.save('model.bin')
# load model
new_model = Word2Vec.load('model.bin')
print(new_model)
print(str(model['restriction']))
</code></pre>

<p><strong>This error does not happen when I use pre-written sentences inside the code as follows:</strong></p>

<pre><code>from gensim.models import Word2Vec
# define training data
sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],  
                ['this', 'is', 'the', 'second', 'sentence'],  
                ['yet', 'another', 'sentence'],  
                ['one', 'more', 'sentence', 'with', 'restriction'],
                ['and', 'the', 'final', 'sentence']]
# train model
model = Word2Vec(sentences, min_count=1)
# summarize the loaded model
print(model)
# summarize vocabulary
words = list(model.wv.vocab)
print(words)
# access vector for one word
print(model['sentence'])
# save model
model.save('model.bin')
# load model
new_model = Word2Vec.load('model.bin')
print(new_model)
print('the model prints: ')
print(model['restriction'])
</code></pre>
",2020-05-17 19:59:56,2020-05-18 20:15:06,"KeyError: ""word 'restrictions' not in vocabulary"" while generating word embedding vectors for text, read from a textfile",<python><deep-learning><text-files><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27018,61854977,2020-05-17 16:16:51,,"<p>I'm a beginner in Python and actually face the following problem:
Initial point is a Dataframe like the following:</p>

<pre><code>    class       plaintext
    x           [agilent, dissolution, exchange, solve, product,...]
    y           [information, data, germany, laptop, berlin...]
    z           [login, system, desk, product, solve, usb, ...]           
    x           [motioncoat, actega, actega, germany, home,...]
    z           [agilent, dissolution, exchange, solve, product,...]
</code></pre>

<p>I want to do is buliding a dictionary, corpus, bow in gensim and using it for hdp and tfidf.
The problem I have i, that I want the topics and relevance of words after classes so what i did was:</p>

<p><code>df = df.groupby('class')['plaintext'].agg(list).reset_index()</code></p>

<p>Then I get something like this:</p>

<pre><code>    class       plaintext
    x           [[certificate, quality, ...][motioncoat, actega, actega, germany, home,...]]
    y           [information, data, germany, laptop, berlin...]
    z           [[login, system, desk, product,...][agilent, dissolution, exchange, solve,...]]           
</code></pre>

<p>But for the dictionary i need one list like in each row like <code>[agilent, dissolution, exchange, solve, product,...]</code>
I tried different approches like <code>df['plaintext'] = [sum(x, []) for x in df['plaintext']]</code> or loops but it always crashes.
As I have huge amount of data, I guess the list gets too long. Until now I used the following code, but its grouped by the domain of plaintext, not the class...</p>

<pre><code>#dic
from gensim.corpora import Dictionary
text_dictionary= Dictionary(df['plaintext'])
print(text_dictionary)

# Create Corpus of word_freq in Doc 
text_corpus = [text_dictionary.doc2bow(text) for text in df['plaintext']]
print(text_corpus[438])
</code></pre>

<p>Is there any solution, how I can get a corpus and bow grouped by <code>'class'</code> ?</p>
",,2020-05-17 16:16:51,groupby and agg command in Dataframe crashes because list/str of aggregation too long,<python><dataframe><dictionary><group-by><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27023,61842256,2020-05-16 19:14:14,,"<p>I want to a grid search to find the optimal model for topic modelling: </p>

<p>A minimum working example is: </p>

<h1>Load Libraries</h1>

<pre><code>#import sys
# !{sys.executable} -m spacy download en
import re, numpy as np, pandas as pd
from pprint import pprint

# Gensim
import gensim, spacy, logging, warnings
import gensim.corpora as corpora
from gensim.utils import lemmatize, simple_preprocess
from gensim.models import CoherenceModel


# Sklearn
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
</code></pre>

<h1>Prepare Data</h1>

<pre><code># Import Dataset
df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')
df = df.loc[df.target_names.isin(['soc.religion.christian', 'rec.sport.hockey', 'talk.politics.mideast', 'rec.motorcycles']) , :]



# stop words: words to be removed from sentences 
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would',
                   'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 
                   'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 
                   'think', 'see', 'rather', 'easy', 'easily', 'lot', 
                   'lack', 'make', 'want', 'seem', 'run', 'need', 'even',
                   'right', 'line', 'even', 'also', 'may', 'take', 'come'])
# Tokenization 
# =============

def sent_to_words(sentences):
    for sent in sentences:
        sent = re.sub('\S*@\S*\s?', '', sent)  # remove emails
        sent = re.sub('\s+', ' ', sent)  # remove newline chars
        sent = re.sub(""\'"", """", sent)  # remove single quotes
        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)  # split sentences into words 
        yield(sent)  

# Convert to list
data = df.content.values.tolist() # list of lists [ each list a document / email ]
data_words = list(sent_to_words(data))


def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    for sent in texts:
        doc = nlp("" "".join(sent)) 
        texts_out.append("" "".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))
    return texts_out

nlp = spacy.load('en', disable=['parser', 'ner'])


data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
</code></pre>

<h1>train model:</h1>

<pre><code>vectorizer = CountVectorizer(analyzer='word',       
                             min_df=10,                        # consider words that occured at least 10 times 
                             stop_words='english',             # remove built-in english stopwords
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',  # a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.
                             # max_features=50000,             # max number of uniq words
                            )
# Document-Word matrix
data_vectorized = vectorizer.fit_transform(data_lemmatized)
</code></pre>

<h1>Do Grid Search</h1>

<pre><code># Define Search Param
search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

# Init the Model
lda = LatentDirichletAllocation()

# Init Grid Search Class
model = GridSearchCV(lda, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)
</code></pre>

<h1>Question</h1>

<p>My intention is to get the log-likelihood for each combination of topics and learning decays. The different number of topics are: <code>[10, 15, 20, 25, 30]</code> while the learning decays are: <code>[.5, .7, .9]</code> </p>

<p>From the grid search above, I can get the following output from <code>model.cv_results_</code> </p>

<p><a href=""https://i.stack.imgur.com/vrqBl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vrqBl.png"" alt=""enter image description here""></a></p>

<p><code>model.cv_results_</code> is a dictionary object. But the structure is not clear in order to find the combination of results I want. For example, what is the log-likelihood ratio when the topic is 10 and learning decay 0.5 ??? </p>

<p>Can someone help with this so I can create a list that stores the results for what is a the <code>mean_test_score</code> - which I think is the log-likelihood test - for each number of topics <strong>at a given learning decay</strong>?</p>

<p>For example, I want: </p>

<p>if learning decay = 0.5 
I want to get the mean_test scores for all different number of topics. </p>

<p>I am very frustrated with the documentation in sklearn on this,  especially after the upgrade of the library. </p>
",,2020-05-16 19:14:14,Grid search for topic modelling,<python><scikit-learn><nlp><lda><topic-modeling>,2020-05-17 14:18:56,,CC BY-SA 4.0,False,True,True,False,True
27027,61921588,2020-05-20 19:43:49,,"<p>I am trying to load a pre-trained Doc2vec model using gensim and use it to map a paragraph to a vector. I am referring to <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> and the pre-trained model I downloaded is the English Wikipedia DBOW, which is also in the same link. However, when I load the Doc2vec model on wikipedia and infer vectors using the following code:</p>

<pre><code>import gensim.models as g
import codecs

model=""wiki_sg/word2vec.bin""
test_docs=""test_docs.txt""
output_file=""test_vectors.txt""

#inference hyper-parameters
start_alpha=0.01
infer_epoch=1000

#load model
test_docs = [x.strip().split() for x in codecs.open(test_docs, ""r"", ""utf-8"").readlines()]
m = g.Doc2Vec.load(model)

#infer test vectors
output = open(output_file, ""w"")
for d in test_docs:
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
output.flush()
output.close()
</code></pre>

<p>I get an error:</p>

<pre><code>/Users/zhangji/Desktop/CSE547/Project/NLP/venv/lib/python2.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
Traceback (most recent call last):
  File ""/Users/zhangji/Desktop/CSE547/Project/NLP/AbstractMapping.py"", line 19, in &lt;module&gt;
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>

<p>I know there are couple of threads regarding the infer_vector issue on stack overflow, but none of them resolved my problem. I downloaded the gensim package using</p>

<pre><code>pip install git+https://github.com/jhlau/gensim
</code></pre>

<p>In addition, after I looked at the source code in gensim package, I found that when I use Doc2vec.load(), the Doc2vec class doesn't really have a load() function by itself, but since it is a subclass of Word2vec, it calls the super method of load() in Word2vec and then make the model m a Word2vec object. However, the infer_vector() function is unique to Doc2vec and does not exist in Word2vec, and that's why it is causing the error. I also tried casting the model m to a Doc2vec, but I got this error:</p>

<pre><code>&gt;&gt;&gt; g.Doc2Vec(m)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 599, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 513, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 635, in scan_vocab
    for document_no, document in enumerate(documents):
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 1367, in __getitem__
    return vstack([self.syn0[self.vocab[word].index] for word in words])
TypeError: 'int' object is not iterable
</code></pre>

<p>In fact, all I want with gensim for now is to convert a paragraph to a vector using a pre-trained model that works well on academic articles. For some reasons I don't want to train the models on my own. I would be really grateful if someone can help me resolve the issue.</p>

<p>Btw, I am using python2.7, and the current gensim version is 0.12.4.</p>

<p>Thanks!</p>
",,2020-05-21 01:34:38,Cannot load Doc2vec object using gensim,<python><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27047,61765136,2020-05-13 02:05:07,,"<p>I tried to use the code here :
<a href=""https://stackoverflow.com/questions/45981305/convert-python-dictionary-to-word2vec-object"">Convert Python dictionary to Word2Vec object</a></p>

<p>The error does not make sense. I wrote the file in non-binary format and the first line is as it should be.<br>
Any idea what might be going wrong?
Or another way to achieve the same end result?</p>

<pre><code>/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
   1496         return _load_word2vec_format(
   1497             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,
-&gt; 1498             limit=limit, datatype=datatype)
   1499 
   1500     def get_keras_embedding(self, train_embeddings=False):

/usr/local/lib/python3.7/site-packages/gensim/models/utils_any2vec.py in _load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    392                 parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
    393                 if len(parts) != vector_size + 1:
--&gt; 394                     raise ValueError(""invalid vector on line %s (is this really the text format?)"" % line_no)
    395                 word, weights = parts[0], [datatype(x) for x in parts[1:]]
    396                 add_word(word, weights)

ValueError: invalid vector on line 1 (is this really the text format?)
</code></pre>
",2020-05-13 04:34:21,2020-05-13 04:34:21,Converting dictionary to KeyedVectorFormat,<word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27096,61944240,2020-05-21 21:36:07,,"<p>I was using Gensim 3.6.0 for loading a pre-trained Word2Vec and it showed the following error while calling <code>model.wv</code>.</p>

<pre><code>/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  """"""Entry point for launching an IPython kernel.
</code></pre>

<p>Here is my code sample</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('/path/to/file/my-vec-300d-v2', binary=False)
print(model.wv['hello'].shape)
print(model.wv['hello']) 
</code></pre>
",,2020-05-21 21:36:07,"Gensim v3.6.0 Word2Vec DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead)",<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27110,61977341,2020-05-23 19:11:08,,"<p>I used following code lemmatize texts that were already excluding stop words and kept words longer than 3. However, after using following code, it split existing words such as 'wheres' to ['where', 's']; 'youre' to ['-PRON-','be']. I didn't expect 's', '-PRON-', 'be' these results in my text, what caused this behaviour and what I can do?</p>

<pre><code>def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
""""""https://spacy.io/api/annotation""""""

texts_out = []
for sent in texts:
    doc = nlp("" "".join(sent)) 
    texts_out.append([token.lemma_ for token in doc]) # though rare, if only keep the tokens with given posttags, add 'if token.pos_ in allowed_postags'
return texts_out

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
nlp = spacy.load('en', disable=['parser', 'ner'])

data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
</code></pre>
",,2020-05-23 19:11:08,Unexpected lemmatize result from gensim,<nlp><nltk><gensim><lemmatization>,,,CC BY-SA 4.0,True,True,True,False,False
27133,61873864,2020-05-18 16:01:41,,"<p>My issue is the following. I have some pretrained vectors saved in txt format, I load them in a dict. But when I try to save them after training them again in gensim it gives me an error, like the following:</p>

<pre><code>UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>

<p>I'm using this code to create the gensim word2vec:</p>

<pre><code>w2vObject = gensim.models.Word2Vec(min_count=1, sample=threshold, sg=1,size=dimension, negative=15, iter=epochsNum, window=3) # create only the shell

print('Starting vocab build')
# t = time()
w2vObject.build_vocab(sentences, progress_per=10000) #here is the vocab being built as told in google groups gensim

print(w2vObject.wv['the'], 'before train')
</code></pre>

<p>Then I'm replacing the current untrained vectors with:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    # vector = ''.join(num for num in values[1:])
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')

    else:
        coefs = np.asarray(vector)

f.close()
</code></pre>

<p>This code replaces the untrained random vectors wit my own pretrained:</p>

<pre><code>i = 0
for elem in w2vObject.wv.vocab:
    if elem in embeddings_index.keys():
        w2vObject.wv[elem] = embeddings_index[elem]
        i += 1
        print('Found one', i)

print(i)
</code></pre>

<p>Next I train them again with gensim:</p>

<pre><code>w2vObject.train(sentences, total_examples=w2vObject.corpus_count, epochs=epochsNum)#w2vObject.iter)
</code></pre>

<p>Finally save them:</p>

<pre><code>print(w2vObject.wv, 'after train')
w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False)
print('saved')
</code></pre>

<p>If I don't replace the vectors with my own saving works, but I need to replace them and save them as txt, any help?</p>

<p>EDIT:</p>

<p>Here is my_split() function:</p>

<pre><code>def my_split(s):
    return list(re.split(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))[0] ,list(re.findall(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))
</code></pre>

<p>And here is a bit of data 300 dimensions for the embedding_index:</p>

<pre><code>'hood -0.013093032778433955 -0.004199660490964164 -0.013285915004532987 0.004154925177649314 -0.004331536946207293 -0.013220217973950956 -0.004774150107654365 0.004774714449991327 0.0040749706101727646...
's gravenhage 0.01400977963089465 -0.0047073654478706935 -0.004326147699308312 0.01323622314514233 -0.004702524319745591 0.004695915697719624 0.00497792763673179 -0.004391661500805715 0.0046651111592470...
'tween 0.008467020793348493 -0.008027116343722267 0.007882368315816719 0.00754852526967863 0.008563484027417608 0.00812782576892597 0.008192394872536986 0.0075759585496093206...
</code></pre>

<p>Added code here:
<a href=""https://pastebin.com/GKPnENxv"" rel=""nofollow noreferrer"">Python code runs fine without my vectors, crashes with them</a></p>

<p>Populate embedding_index I go through all the words and vectors in the txt and if for some reason a vector is not 300 dim, skip it:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt', encoding='utf-8')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')
    else:
        coefs = np.asarray(vector)
        embeddings_index[word] = coefs

f.close()
</code></pre>

<p>EDIT2:
Here is the stack trace of the full error:
Traceback (most recent call last):</p>

<pre><code>  File ""GensimTestSave.py"", line 136, in &lt;module&gt;
    w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False) #encoding='utf-8' )
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/keyedvectors.py"", line 1453, in save_word2vec_format
    fname, self.vocab, self.vectors, fvocab=fvocab, binary=binary, total_vec=total_vec)
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in _save_word2vec_format
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in &lt;genexpr&gt;
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>
",2020-05-19 16:56:55,2020-05-25 08:43:14,Gensim saving word vectors in txt format error,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27138,61949436,2020-05-22 06:57:42,,"<p>I'm trying to use <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">Gensim implementation of LDA</a> which suggests using automatic learning of hyperparameters <code>alpha</code> and <code>eta</code>:</p>

<blockquote>
  <p>We set <code>alpha = 'auto'</code> and <code>eta = 'auto'</code>. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly.</p>
</blockquote>

<p>However, after seeing <a href=""https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"" rel=""nofollow noreferrer"">this article</a> about LDA hyperparameter tuning, I can see that it is also possible to tune these parameters as black-box: train the model with different fixed values of parameters, and then select the best one:</p>

<blockquote>
  <p>Lets call the function, and iterate it over the range of topics, alpha, and beta parameter values</p>
</blockquote>

<p>Is there any essential difference between these two methods? Is there any special case when the second method is better than the first one?</p>
",,2020-05-22 06:57:42,What's the difference between automatic and manual LDA hyperparameter tuning?,<gensim><lda><topic-modeling><hyperparameters>,,,CC BY-SA 4.0,False,False,True,False,False
27148,61997229,2020-05-25 06:54:32,,"<p>I have a document in a text file in 2 lines as shown below. I wanted to apply tf-idf to it and I get the error as shown below, I am not sure where is int object in my file? why would it throw this error?</p>

<p>Env: </p>

<p><code>Jupter notebook, python 3.7</code></p>

<p>Error:</p>

<pre><code>AttributeError: 'int' object has no attribute 'lower'
</code></pre>

<p>file.txt:</p>

<pre><code>  Random person from the random hill came to a running mill and I have a count of the hill. This is my house. 

  A person is from a great hill and he loves to run a mill. 

  Sub-disciplines of biology are defined by the research methods employed and the kind of system studied: theoretical biology uses mathematical methods to formulate quantitative models while experimental biology performs empirical experiments.

  The objects of our research will be the different forms and manifestations of life, the conditions and laws under which these phenomena occur, and the causes through which they have been effected. The science that concerns itself with these objects we will indicate by the name biology.
</code></pre>

<p>Code: </p>

<pre><code>import pandas as pd
import spacy
import csv
import collections
import sys
import itertools
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
from nltk.tokenize import sent_tokenize
from gensim import corpora, models
from stop_words import get_stop_words
from nltk.stem import PorterStemmer

data = pd.read_csv('file.txt', sep=""\n"", header=None)

data.dtypes
0    object
dtype: object

data.shape()
4, 1

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(data)
print(X)
</code></pre>
",,2020-05-25 07:13:29,Unsatisfactory output from Tf-Idf,<python-3.x><tf-idf><tfidfvectorizer>,,,CC BY-SA 4.0,True,True,True,False,True
27152,61983014,2020-05-24 07:45:57,,"<p>I try import gensim library in python3. 
all of the libraries are last version.
in first import got an <code>TypeError: expected bytes, Descriptor found</code>
and in second import got an this error:</p>

<pre><code> import gensim

&gt; AttributeError                            Traceback (most recent call last)
&lt;ipython-input-7-e70e92d32c6e&gt; in &lt;module&gt;
----&gt; 1 import gensim

~\Anaconda3\lib\site-packages\gensim\__init__.py in &lt;module&gt;
      3 """"""
      4 

~\Anaconda3\lib\site-packages\smart_open\transport.py in &lt;module&gt;
     20 NO_SCHEME = ''
     21 
---&gt; 22 _REGISTRY = {NO_SCHEME: smart_open.local_file}
     23 
     24 

AttributeError: module 'smart_open' has no attribute 'local_file'
</code></pre>
",,2020-05-27 06:37:21,"import gensim and got an TypeError: expected bytes, Descriptor found",<python><import><typeerror><gensim><attributeerror>,,,CC BY-SA 4.0,False,False,True,False,False
27164,61952950,2020-05-22 10:29:01,,"<p>I am looking for the most similar words for out-of-vocab OOV words using gensim. Something like this:</p>

<pre><code>    def get_word_vec(self, model, word):
    try:
        if word not in model.wv.vocab:
            mostSimWord = model.wv.similar_by_word(word)
            print(mostSimWord)
        else:
            print( word )
    except Exception as ex:
        print(ex)
</code></pre>

<p>Is there are way to achieve this task? Options other than gensim also welcomed.</p>
",,2020-05-22 18:21:24,Find most similar words for OOV word,<python><nlp><gensim><similarity><oov>,,,CC BY-SA 4.0,False,False,True,False,False
27174,61918360,2020-05-20 16:40:53,,"<p>I have trained gensim LDA model with my corpus(list of list) having some 50k sentences(list), but when i try to see the topics distribution for some words it gives an empty list []..
I have also set min_probability=0.0 while training the model as one of its parameters to see if all the topics are shown irrespective of very low probability.</p>

<p>[EDIT]: I debugged it and find out what was the problem in my case it was the chunksize parameter which we set at the time of training. Earlier it was 2000(default) and now when i changed it to higher value >=50000(in my case), all the words/terms started giving their topic distributions.. but i don't know yet why the chunksize was causing the issue...</p>
",2020-05-21 07:35:30,2020-05-21 07:35:30,LDA get_term_topics gives empty list,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
27200,62032372,2020-05-26 22:22:47,,"<p>I read this question (<a href=""https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad"">Coherence score 0.4 is good or bad?</a>)  and found that the coherence score (u_mass) is from -14 to 14. But when I did my experiments, I got a score of -18 for u_mass and 0.67 for c_v. I wonder how is my u_mass score out of range (-14, 14)?</p>

<p>Update: I used gensim library and scanned   the numbers of topics from 2 to 50. For u_mass, it starts from 0 to the lowest negative point and turn back a bit, like an upsidedown version of c_v.</p>
",2020-05-27 07:10:37,2020-05-27 19:09:41,Coherence score (u_mass) -18 is good or bad?,<nlp><lda><topic-modeling><lsa><topicmodels>,,,CC BY-SA 4.0,False,False,True,False,False
27221,62085134,2020-05-29 11:39:24,,"<p>I wrote the code below, I used Used spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:</p>

<p>love_VERB old-fashioneds_NOUN</p>

<p>now I want to Train 4 more Word2vec models and average the resulting embedding matrices.
but I dont have any idea for it, can you help me please ?</p>

<pre><code># Tokenization of each document
from gensim.models.word2vec import FAST_VERSION
from gensim.models import Word2Vec
import spacy
import pandas as pd
from zipfile import ZipFile
import wget

url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')

with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()

# nrows , max amount of rows
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000)
documents = df.text.values.tolist()

nlp = spacy.load('en_core_web_sm')  # you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}


vocab = [s for s in new_sentences]

sentences = documents[:103]  # first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_ in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append(new_sentence)


# initialize model
w2v_model = Word2Vec(
                     size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5,
                     min_count=1,  # &lt;-- it seems your min_count was too high
                     workers=-1,
                     hs=0
                     )


new_sentences


w2v_model.build_vocab(vocab)

w2v_model.train(vocab, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)
w2v_model.wv['car_NOUN']
</code></pre>
",,2020-05-29 17:26:43,I want to Train 4 more Word2vec models and average the resulting embedding matrices,<python><pandas><nlp><spacy><gensim>,,,CC BY-SA 4.0,False,True,True,False,False
27223,61990540,2020-05-24 18:30:54,,"<p>I have a large dataframe (~4.7M rows) where one of the columns contains document text. I am trying unsuccessfully to run Gensim summarize on a specific column for the entire dataframe.</p>

<pre class=""lang-py prettyprint-override""><code>df['summary'] = df['variable_content'].apply(lambda x: summarize(x, word_count=200))
</code></pre>

<p>Extracting each row of <code>variable_content</code> into a variable and running summarize works well, but is slow and ugly. I also get the error:</p>

<pre><code>ValueError: input must have more than one sentence
</code></pre>

<p>but can't find a row with only one sentence (most are hundreds/thousands). Can anyone help?</p>
",2020-05-25 06:00:51,2020-05-25 06:00:51,Summarize Pandas dataframe column,<python><pandas><dataframe><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27237,62007088,2020-05-25 16:54:27,,"<p>My question is <strong>how I should interpret my situation?</strong></p>

<p>I trained a Doc2Vec model following this tutorial <a href=""https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/"" rel=""nofollow noreferrer"">https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/</a>. </p>

<p>For some reason, <code>doc_model.docvecs.doctags</code> returns <code>{}</code>. But <code>doc_model.docvecs.vectors_docs</code> seems to return a proper value.</p>

<p>Why the doc2vec object doesn't return any doctags but vectors_docs?</p>

<p>Thank you for any comments and answers in advance.</p>

<p>This is the code I used to train a Doc2Vec model.</p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, TaggedDocument, Doc2Vec
import timeit
import gensim

embeddings_dim = 200    # dimensionality of user representation

filename = f'models/customer2vec.{embeddings_dim}d.model'
if TRAIN_USER_MODEL:

    class TaggedDocumentIterator(object):
        def __init__(self, df):
           self.df = df
        def __iter__(self):
            for row in self.df.itertuples():
                yield TaggedDocument(words=dict(row._asdict())['all_orders'].split(),tags=[dict(row._asdict())['user_id']])

    it = TaggedDocumentIterator(combined_orders_by_user_id)

    doc_model = gensim.models.Doc2Vec(vector_size=embeddings_dim, 
                                      window=5, 
                                      min_count=10, 
                                      workers=mp.cpu_count()-1,
                                      alpha=0.055, 
                                      min_alpha=0.055,
                                      epochs=20)   # use fixed learning rate

    train_corpus = list(it)

    doc_model.build_vocab(train_corpus)

    for epoch in tqdm(range(10)):
        doc_model.alpha -= 0.005                    # decrease the learning rate
        doc_model.min_alpha = doc_model.alpha       # fix the learning rate, no decay
        doc_model.train(train_corpus, total_examples=doc_model.corpus_count, epochs=doc_model.iter)
        print('Iteration:', epoch)

    doc_model.save(filename)
    print(f'Model saved to [{filename}]')

else:
    doc_model = Doc2Vec.load(filename)
    print(f'Model loaded from [{filename}]')
</code></pre>

<p><code>doc_model.docvecs.vectors_docs</code> returns<a href=""https://i.stack.imgur.com/bN6r4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bN6r4.jpg"" alt=""enter image description here""></a></p>
",,2020-05-25 20:45:35,Why does a Gensim Doc2vec object return empty doctags?,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27247,62071147,2020-05-28 17:44:16,,"<p>I am trying to use doc2vec for text classification but after importing when i am trying to use it inside a function its says doc2vec in not defined. Please help me to identify which all libraries should I be installing to use doc2vec.</p>

<p>here I have 2 columns called Action(Text or sentences) and Category(actual tags)</p>

<pre><code>from gensim.models import Doc2Vec
import gensim
from gensim.models.doc2vec import TaggedDocument
def label_sentences(corpus, label_type):
    labeled = []
    for i, v in enumerate(corpus):
        label = label_type + '_' + str(i)
        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))
    return labeled
X_train, X_test, y_train, y_test = train_test_split(df.Action, df.Category, random_state=0, test_size=0.3)
X_train = label_sentences(X_train, 'Train')

Error
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-66-d20e5f21684d&gt; in &lt;module&gt;
----&gt; 1 X_train = label_sentences(X_train, 'Train')

&lt;ipython-input-53-bca3c695cfb6&gt; in label_sentences(corpus, label_type)
      8     for i, v in enumerate(corpus):
      9         label = label_type + '_' + str(i)
---&gt; 10         labeled.append(doc2vec.TaggedDocument(v.split(), [label]))
     11     return labeled

NameError: name 'doc2vec' is not defined
</code></pre>
",,2020-05-28 17:59:21,name 'doc2vec' is not defined,<python><word2vec><text-classification>,,,CC BY-SA 4.0,False,False,True,False,False
27252,62052038,2020-05-27 20:11:50,,"<p>I have trained a gensim Word2Vec model in Python3 using a custom dataset. Now, as I am deploying the model (as a chrome extension using Angular Typescript), I am willing to use the trained gensim model vocabulary to generate embeddings on the go. Due to memory constraints, I plan to use the KeyedVectors instead of deploying the entire model? I currently have a "".model"" file stored. </p>

<p>As I am really new to deploying gensim models, can someone please suggest a good method to use the trained gensim models (preferably, KeyedVectors) with Angular? </p>

<p>I can provide more information about the problem if needed.  </p>
",2020-05-27 22:50:59,2020-05-27 22:50:59,How to deploy gensim KeyedVectors in Angular?,<angular><machine-learning><google-chrome-extension><data-science><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27282,62086077,2020-05-29 12:32:13,,"<p>I want to train a neural net for sentiment analysis. I have followed the tutorials on the keras webpage but I had to adapt the code to my usecase in order to be able to use the net afterwards. </p>

<p>For this purpose I decode back the texts from the imdb dataset from keras from numbers to text, and then I stemmize the text because I need to use the text stemmized. After that, since I want to control the way I am doing the word embeddings rather than using text_to_sequences an pad_sequences I am training a doc2vec embeddings and I am using it on the training set, so that I can obtain the embeddings from the text I want to classify. </p>

<p>The problem is that, the net does not learn anything, the accuracy does not improve and I can not reduce the loss function. I have tried many many things, like the architecture of the net, all the hyperparameters and changing the last layer from 2 nets to 1 and from sparse_categorical_entropy to binary_crossentropy. Let's see if anybody can help and show some light to my problem. I plug the code here and thanks in advance.</p>

<pre><code>from keras.datasets import imdb
max_features = 40000
(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=max_features)

import numpy as np
data = np.concatenate((training_data, testing_data), axis=0)
targets = np.concatenate((training_targets, testing_targets), axis=0)


index = imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = "" "".join([reverse_index.get(i - 3, """") for i in data[0]])

import nltk
from nltk .stem import LancasterStemmer

toke_corpus = list()
lan = LancasterStemmer()

from tqdm import tqdm
lista_reviews = list()

for review in tqdm(data):
  lista_reviews.append(np.array([lan.stem(reverse_index.get(i - 3, '')) for i in review][1:]))

train_x, test_x = lista_reviews[10000:], lista_reviews[:10000]
train_y, test_y = targets[10000:], targets[:10000]

 from gensim.models.callbacks import CallbackAny2Vec

 class EpochLogger(CallbackAny2Vec):
     '''Callback to log information about training'''
     def __init__(self):
         self.epoch = 0
     def on_epoch_begin(self, model):
         print(""Epoch #{} start"".format(self.epoch))
     def on_epoch_end(self, model):
         print(""Epoch #{} end"".format(self.epoch))
         self.epoch += 1


from gensim.models.doc2vec import Doc2Vec, TaggedDocument

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(lista_reviews)]
print(""DOcuments already built"")
epoch_logger = EpochLogger()
model = Doc2Vec(documents, vector_size=512, window=5, min_count=3, workers=8, epochs = 7, callbacks=[epoch_logger])


encoded_x_train, encoded_x_test = list(), list()
from tqdm import tqdm
for i in tqdm(train_x):
    encoded_x_train.append(model.infer_vector(i))
for k in tqdm(test_x):
    encoded_x_test.append(model.infer_vector(k))

import keras

reduce_lr = keras.callbacks.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.50, patience=2, verbose=1, mode='auto', cooldown=0, min_lr=0.00001)

early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')

from keras import models
from keras.models import Sequential
from keras import layers
from keras.layers import Embedding, Bidirectional, Dense, LSTM, Conv1D, MaxPooling1D, Flatten

model1 = Sequential()
model1.add(Embedding(input_dim = max_features, input_length=512, output_dim=128, trainable=False))

model1.add(Conv1D(filters=64,
                 kernel_size=5,
                 padding='valid',
                 activation='linear',
                 strides=1))
model1.add(MaxPooling1D(pool_size=4))
model1.add(Dense(64, activation='linear'))
model1.add(LSTM(32, activation='tanh'))
# model1.add(Dense(32, activation='relu'))
# model1.add(Flatten())
# model1.add(Dense(1, activation='sigmoid'))
model1.add(Dense(2, activation='softmax'))
model1.summary()


from keras import optimizers
# sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)


model1.compile(loss='sparse_categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

history  = model1.fit( np.array(encoded_x_train), np.array(train_y),
 epochs= 20,
 batch_size = 500,
 validation_data = (np.array(encoded_x_test), np.array(test_y)), callbacks = [reduce_lr, early]
)
</code></pre>
",,2020-05-29 13:18:46,NLP Sentiment Analysis net is not learning,<python><tensorflow><keras><nlp>,,,CC BY-SA 4.0,True,False,True,False,False
27298,62076017,2020-05-28 23:09:42,,"<p>I am trying to make my code run on Raspberry Pi 4 and been stuck on this error for hours. This code segment throws an error on it but runs perfectly on windows with the same project</p>

<p>def create_lem_texts(data):  # as a list
    def sent_to_words(sentences):
        for sentence in sentences:
            yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations</p>

<pre><code>data_words = list(sent_to_words(data))
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)  # higher threshold fewer phrases.
bigram_mod = gensim.models.phrases.Phraser(bigram)

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    print(os.getcwd())
    for sent in texts:
        doc = nlp("" "".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

data_words_nostops = remove_stopwords(data_words)
data_words_bigrams = make_bigrams(data_words_nostops)
print(os.getcwd())
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

return data_lemmatized
</code></pre>

<p>This code is in turned called by this function:</p>

<pre><code>def assign_topics_tweet(tweets):
owd = os.getcwd()
print(owd)
os.chdir('/home/pi/Documents/pycharm_project_twitter/topic_model/')
print(os.getcwd())
lda = LdaModel.load(""LDA26"")
print(lda)
id2word = Dictionary.load('Id2Word')
print(id2word)
os.chdir(owd)
data = create_lem_texts(tweets)
corpus = [id2word.doc2bow(text) for text in data]
topics = []
for tweet in corpus:
    topics_dist = lda.get_document_topics(tweet)
    topics.append(topics_dist)
return topics
</code></pre>

<p>And here is the error message</p>

<pre><code>    Traceback (most recent call last):
  File ""/home/pi/Documents/pycharm_project_twitter/Twitter_Import.py"", line 193, in &lt;module&gt;
    main()
  File ""/home/pi/Documents/pycharm_project_twitter/Twitter_Import.py"", line 169, in main
    topics = assign_topics_tweet(data)
  File ""/home/pi/Documents/pycharm_project_twitter/TopicModel.py"", line 238, in assign_topics_tweet
    data = create_lem_texts(tweets)
  File ""/home/pi/Documents/pycharm_project_twitter/TopicModel.py"", line 76, in create_lem_texts
    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
  File ""/home/pi/Documents/pycharm_project_twitter/TopicModel.py"", line 67, in lemmatization
    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
  File ""/home/pi/Documents/pycharm_project_twitter/TopicModel.py"", line 67, in &lt;listcomp&gt;
    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
  File ""token.pyx"", line 871, in spacy.tokens.token.Token.lemma_.__get__
  File ""strings.pyx"", line 136, in spacy.strings.StringStore.__getitem__
KeyError: ""[E018] Can't retrieve string for hash '18446744073541552667'. This usually refers to an issue with the `Vocab` or `StringStore`.""

Process finished with exit code 1
</code></pre>

<p>I tried reinstalling spacy and the en model, running it directly on pi, spacy versions are the same on both my windows machine and on the Pi. And there is basically no information online on this error</p>
",,2020-05-31 21:36:43,"Python Spacy KeyError: ""[E018] Can't retrieve string for hash",<python><raspberry-pi><spacy><raspberry-pi4>,,,CC BY-SA 4.0,False,True,True,False,False
27309,62025647,2020-05-26 15:25:06,,"<p>I am currently composing topic models using gensim and MALLET. As you can see in the Code snippet, I am using the coherence model with the mallet model to get coherence scores.</p>

<pre><code># Create the model
ldamallet = LdaMallet(mallet_path, workers=4, optimize_interval=optimize_interval, corpus=self.corpus, num_topics=num_topics, id2word=self.id2word)

result_topics = ldamallet.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)
print(""Model built"")

# Compute Coherence Score
coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=self.data_lemmatized, dictionary=self.id2word, coherence=coherence_mode)
print(""Coherence Model built"")

coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print(""Coherence Score calculated"")
</code></pre>

<p>Now, to look at the performance (as I have to run a lot of models to optimize parameters), I timed one run of the program usind 100 topics and an optimize_interval of 40. It came to 2 mins 20 secs, where building the ldamallet model took 43 secs, building the Coherence Model &lt; 1 sec, but getting the coherence score took 1 min 36 secs... like double the time it took to build the original model. Does anyone know why this is the case or how to make coherence score calculation more effecient?</p>

<p>Best,
Nero</p>
",2020-05-26 15:30:22,2020-05-26 15:30:22,Extremely low performance on calculating coherence scores,<gensim><lda><topic-modeling><mallet><topicmodels>,,,CC BY-SA 4.0,False,False,True,False,False
27329,62140106,2020-06-01 20:01:45,,"<p>The <code>most_similar</code> method finds the top-N most similar words.</p>

<p>Is there a method or a way to find the N least similar words?</p>
",2020-06-14 13:25:08,2020-06-14 13:25:08,Least Similar with Gensim Doc2Vec,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27335,62059196,2020-05-28 07:27:12,,"<p>I've tried to load pre-trained FastText vectors from <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">fastext - wiki word vectors</a>.</p>

<p>My code is below, and it works well. </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_fasttext_format('./wiki.en/wiki.en.bin')
</code></pre>

<p>but, the warning message is a little annoying. </p>

<pre><code>gensim_fasttext_pretrained_vector.py:13: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings)
</code></pre>

<p>The message said, <code>load_fasttext_format</code> will be deprecated so, it will be better to use <code>load_facebook_vectors</code>. </p>

<p>So I decided to changed the code. and My changed code is like below.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
</code></pre>

<p><strong>But</strong>, the error occurred, the error message is like this. </p>

<pre><code>Traceback (most recent call last):
  File ""gensim_fasttext_pretrained_vector.py"", line 13, in &lt;module&gt;
    model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
AttributeError: type object 'FastText' has no attribute 'load_facebook_vectors'
</code></pre>

<p>I couldn't understand why these thing happen.
I just change what the messages said, but it doesn't work. 
If you know anything about this, please let me know. </p>

<p>Always, thanks for you guys help. </p>
",,2020-05-28 08:50:09,gensim - fasttext - Why `load_facebook_vectors` doesn't work?,<python><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
27348,62125400,2020-06-01 03:49:11,,"<p>i don't know how to train model in multiples batches with doc2vec . Since i load all my data in ram and it't can not be loaded </p>

<pre><code>#Import all the dependencies
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
#import ReadExeFileCapstone
import update-doc2vec 
mapData = ReadExeFileCapstone.readData()

# print ('mapData', mapData)

max_epochs = 10000
vec_size = 200
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm =1)
data = []
for key in mapData:
    listData = mapData[key]
    # print (""listData: "", len(listData), listData)

    for i in range(len(listData)):
        listToStr = ' '.join([str(elem) for elem in listData[i]]) #convert array to list string
        data.append(listToStr)

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]


model.build_vocab(tagged_data)
#build vocab
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha
# train model   
model.save(""d2v_ASM.model"")
print(""Model Saved"")
</code></pre>
",2020-06-01 16:42:23,2020-06-02 21:07:58,Is there anyway to train doc2vec model in multiples batches,<gensim><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
27354,62094471,2020-05-29 20:55:51,,"<p>I'm trying to find duplicate documents. I plan to do this incrementally. So every time I add a document it checks for duplicates against a big store of other documents. It works for the most part, but there is one weird thing.</p>

<p>I'm doing this:</p>

<pre><code>from gensim.similarities import Similarity

 if os.path.exists(myPath):
            index = Similarity.load(myPath)
            index.add_documents(corpus)
 else:
            index = Similarity(
                output_prefix='shardlocation'
                corpus=corpus,
                num_features=150000
            )
index.save(myPath)
return index
</code></pre>

<p>I'm setting <code>num_features</code> to 150,000. Because when I <code>index.add_documents</code>, <code>num_features</code> doesn't increase and I get this error <code>IndexError: index 5 is out of bounds for axis 0 with size 5</code>. </p>

<p>So I could set <code>num_features</code> to something like 1m or something and probably be safe, but that seems silly? I was looking for something that would change the <code>num_features</code>, but couldn't find it in the docs.</p>

<p>Googling around didn't help, which makes me think I'm doing this whole process wrong. I'm new to python/machine-learning, so it's possible my thought process here is completely off. Any help is appreciated!</p>
",,2020-05-29 20:55:51,Gensim Similarity Index updating num_features,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27363,62045818,2020-05-27 14:39:55,,"<p>I am doing a natural language process project, when I try to use gensim'API to get LSI similarity matrix, every time I run my code, the LSImodel gives me a different similarity matrix. they are not totally different, but slightly different, like last time one of the number is -0.42562, but it change to -0.42116 next time I run my code. It makes my rest analysis change totally.</p>

<pre><code>Lsi = gensim.models.LsiModel
lsimodel = Lsi(corpus_tfidf, id2word=dictionary, num_topics=20)
lsi_similarity = similarities.MatrixSimilarity(lsimodel[corpus_tfidf])
</code></pre>

<p>I have checked that my input corpus_tfidf and dictionary is the same every time. why would this happen? is there some solution for it?</p>
",,2020-05-27 14:39:55,why Lsimodel from Gensim show different output while taking the same input?,<python><nlp><gensim><lsa>,,,CC BY-SA 4.0,False,False,True,False,False
27370,62064270,2020-05-28 12:04:41,,"<p>I wrote the code below: to implement the word2vec on it, now im testing to get embedding for w2v_model.wv['car_NOUN']   but I get error as below : <strong>""word 'car_NOUN' not in vocabulary""</strong> but im sure that word car_NOUN is in the vocabulary , what is the problem ?
can someone help me?</p>

<p>about code : I used Use spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:love_VERB .then I wanted to implement word2vec on new list but I came up with that error</p>

<p>love_VERB old-fashioneds_NOUN</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-145-f6fb9c62175c&gt; in &lt;module&gt;()
----&gt; 1 w2v_model.wv['car_NOUN']

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

KeyError: ""word 'car_NOUN' not in vocabulary""
</code></pre>

<pre><code>! pip install wget
import wget
url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')


from zipfile import ZipFile
with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()


import pandas as pd
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000) # nrows , max amount of rows 
documents = df.text.values.tolist()
print(documents[:4])


import spacy

nlp = spacy.load('en_core_web_sm') #you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}
#document = [line.strip() for line in open('moby_dick.txt', encoding='utf8').readlines()]

sentences = documents[:103] #first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_  in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append("" "".join(new_sentence))

def convert(new_sentences): 
    return ' '.join(new_sentences).split() 

x=convert(new_sentences)


from gensim.models import Word2Vec
from gensim.models.word2vec import FAST_VERSION


# initialize model
w2v_model = Word2Vec(size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5, 
                     min_count=100,
                     workers=-1, 
                     hs=0
)

w2v_model.build_vocab(x)

w2v_model.train(x, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)


w2v_model.wv['car_NOUN']
</code></pre>
",,2020-05-28 15:23:31,"implement word2vec, but I got error, that word car_NOUN is in the vocabulary",<python><pandas><nlp><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
27413,62177737,2020-06-03 16:22:28,,"<p>I want to use this <a href=""https://github.com/cogniinsight/Word-embedding-model-for-Bangla"" rel=""nofollow noreferrer"">pre-trained Word2Vec model</a> but not sure how to use it</p>

<p>I tried to load the zip file and then use it </p>

<pre><code>!wget -P /root/input/ -c ""https://raw.githubusercontent.com/cogniinsight/Word-embedding-model-for-Bangla/master/scripts/DataProcessor.tar.gz""

model = gensim.models.Word2Vec.load('/root/input/DataProcessor.tar.gz')
</code></pre>

<p>but getting this error:<br>
<code>Not a gzipped file (b've')</code></p>
",2020-06-06 01:10:47,2020-06-06 01:10:47,I want to use pre-trained Word2Vec model but not sure how to use it,<model><nlp><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
27415,62179243,2020-06-03 17:40:03,,"<p>everyone!
I'm trying to import gensim on jupyter but I got the following error:</p>

<p><code>ImportError: cannot import name 'deprecated'</code></p>

<p>What can I do?</p>

<p>PS: I tried in gensim version 3.8.1 and 3.8.3</p>
",,2020-06-03 17:40:03,ImportError: cannot import name 'deprecated' when import gensim,<python-3.x><jupyter-notebook><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27457,62235365,2020-06-06 17:23:19,,"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn</code> and <code>gensim</code>.
As we set the <code>random_state</code> variable to a fixed integer, the results were always the same.</p>

<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state</code> the same.</p>

<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL</code> version was the same and the <code>MKL_CBWR</code> variable was set to <code>AUTO</code>.</p>

<p><a href=""https://stackoverflow.com/questions/46766714/t-sne-generates-different-results-on-different-machines"">t-SNE generates different results on different machines</a></p>

<p><a href=""https://stackoverflow.com/questions/38228088/same-python-code-same-data-different-results-on-different-machines"">Same Python code, same data, different results on different machines</a></p>

<p>Still, we are not able to get the same results.</p>

<p>What else should we check or why is this happening?</p>

<p><strong>Update</strong></p>

<p>If we generate a <code>pkl</code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).</p>

<p>Still, we are looking to get the same results (if possible) without importing the pkl file.</p>

<p><strong>Library versions</strong></p>

<pre><code>gensim 3.8.3.
sklearn 0.19.2.
matplotlib 2.2.3.
numpy 1.17.2.
scipy 1.1.0.
</code></pre>

<p><strong>Code</strong></p>

<p>Full code can be found <a href=""https://t.ly/YlCi"" rel=""nofollow noreferrer"">here</a>, sample data link inside.</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt

from gensim.models import KeyedVectors
%matplotlib inline

import time

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns

wordvectors_file_vec = '../libraries/embeddings-new_large-general_3B_fasttext.vec'
wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)

math_quests = # some transformations using wordvectors

df_subset = pd.DataFrame()

pca = PCA(n_components=3, random_state = 42)
pca_result = pca.fit_transform(mat_quests)
df_subset['pca-one'] = pca_result[:,0]
df_subset['pca-two'] = pca_result[:,1] 

time_start = time.time()
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)
tsne_results = tsne.fit_transform(mat_quests)

df_subset['tsne-2d-one'] = tsne_results[:,0]
df_subset['tsne-2d-two'] = tsne_results[:,1]

pca_50 = PCA(n_components=50, random_state = 42)
pca_result_50 = pca_50.fit_transform(mat_quests)
print('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))

time_start = time.time()
tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)
tsne_pca_results = tsne.fit_transform(pca_result_50)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))
</code></pre>
",2020-06-07 00:25:15,2020-06-07 01:37:03,Models generate different results when moving to Azure Machine Learning Studio,<python><scikit-learn><gensim><azure-machine-learning-studio>,,,CC BY-SA 4.0,False,False,True,False,True
27459,62181162,2020-06-03 19:29:13,,"<p>I was doing this and got this error :</p>

<pre><code>from gensim.models import Word2Vec

ImportError: cannot import name 'open' from 'smart_open' (C:\ProgramData\Anaconda3\lib\site-packages\smart_open\__init__.py)
</code></pre>

<p>Then I did this :</p>

<pre><code>import smart_open
dir(smart_open)

['BZ2File','BytesIO','DEFAULT_ERRORS','IS_PY2','P','PATHLIB_SUPPORT','SSLError','SYSTEM_ENCODING','Uri','__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','boto','codecs','collections','gzip','hdfs','http','importlib','io','logger','logging','os','pathlib','pathlib_module','requests','s3','s3_iter_bucket','six','smart_open','smart_open_hdfs','smart_open_http','smart_open_lib','smart_open_s3','smart_open_webhdfs','sys','urlparse','urlsplit','warnings','webhdfs']
</code></pre>

<p>As you can see there is no 'open' in it so how should I solve this. I tried to install different versions
and I upgraded all version too.</p>
",,2020-07-24 15:51:56,cannot import name 'open' from 'smart_open',<deep-learning><nlp><importerror><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27464,62200198,2020-06-04 17:00:29,,"<p>Is there a way to get the topic distribution of an unseen document using a pretrained LDA model without using the LDA_Model[unseenDoc] syntax? I am trying to implement my LDA model into a web application, and if there was a way to use matrix multiplication to get a similar result then I could use the model in javascript.</p>

<p>For example, I tried the following:</p>

<pre><code>import numpy as np
import gensim
from gensim.corpora import Dictionary
from gensim import models
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
nltk.download('wordnet')


def Preprocesser(text_list):

    smallestWordSize = 3
    processedList = []

    for token in gensim.utils.simple_preprocess(text_list):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; smallestWordSize:
            processedList.append(StemmAndLemmatize(token))

    return processedList

lda_model = models.LdaModel.load('LDAModel\GoldModel')  #Load pretrained LDA model
dictionary = Dictionary.load(""ModelTrain\ManDict"")      #Load dictionary model was trained on

#Sample Unseen Doc to Analyze
doc = ""I am going to write a string about how I can't get my task executor \
to travel properly. I am trying to use the \
AGV navigator, but it doesn't seem to be working network. I have been trying\
to use the AGV Process flow but that isn't working either speed\
trailer offset I am now going to change this so I can see how fast it runs""

termTopicMatrix = lda_model.get_topics()    #Get Term-topic Matrix from pretrained LDA model
cleanDoc = Preprocesser(doc)                #Tokenize, lemmatize, clean and stem words
bowDoc = dictionary.doc2bow(cleanDoc)       #Create bow using dictionary
dictSize = len(termTopicMatrix[0])          #Get length of terms in dictionary
fullDict = np.zeros(dictSize)               #Initialize array which is length of dictionary size
First = [first[0] for first in bowDoc]      #Get index of terms in bag of words
Second = [second[1] for second in bowDoc]   #Get frequency of term in bag of words
fullDict[First] = Second                    #Add word frequency to full dictionary


print('Matrix Multiplication: \n', np.dot(termTopicMatrix,fullDict))
print('Conventional Syntax: \n', lda_model[bowDoc])

Output:
Matrix Multiplication: 
 [0.0283254  0.01574513 0.03669142 0.01671816 0.03742738 0.01989461
 0.01558603 0.0370233  0.04648389 0.02887623 0.00776652 0.02147539
 0.10045133 0.01084273 0.01229849 0.00743788 0.03747379 0.00345913
 0.03086953 0.00628912 0.29406082 0.10656977 0.00618827 0.00406316
 0.08775404 0.00785408 0.02722744 0.09957815 0.01669402 0.00744392
 0.31177135 0.03063149 0.07211428 0.01192056 0.03228589]
Conventional Syntax: 
 [(0, 0.070313625), (2, 0.056414187), (18, 0.2016589), (20, 0.46500313), (24, 0.1589748)]
</code></pre>

<p>In the pretrained model there are 35 topics and 1155 words.</p>

<p>In the ""Conventional Syntax"" output, the first element of each tuple is the index of the topic and the second element is the probability of the topic. In the ""Matrix Multiplication"" version, the probability is the index and the value is the probability. Clearly the two don't match up.</p>

<p>For example, the lda_model[unseenDoc] shows that topic 0 has a 0.07 probability, but the matrix multiplication method says that topic has a 0.028 probability. Am I missing a step here?</p>
",,2020-06-05 01:36:53,Is there a way to infer topic distributions on unseen document from gensim LDA pre-trained model using matrix multiplication?,<gensim><lda><topic-modeling>,,,CC BY-SA 4.0,True,False,True,False,False
27496,62269854,2020-06-08 19:31:10,,"<p>I am trying to extract topics from some texts. I have a column called 'Texts' like</p>

<pre><code>34     fino a 2 giorni.  questo il tempo massimo di ...
36     il morbillo  contagioso anche fuori...
44     altra cattiva notizia: il Milan ha perso ...
45     tra persistenza della memoria e del cielo...
49     i ricercatori della university medici...
96     analizzando per 22 ricerche si  messo in luc...
259    ... il melone  molto rinfrescante ...
275    e non perch il governo abbia la capacit ...
287    da qualche ore fino a qualche mese: quanto s...
298    l'esperto ha specificato che l'affermazione ch...
</code></pre>

<p>and I would need to extract topics using lda. I have been using this code</p>

<pre><code>from gensim import corpora, models
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
import nltk

df['Texts'] = df.apply(lambda row: nltk.word_tokenize(row['Texts']), axis=1)

dictionary_LDA = corpora.Dictionary(df['Texts'].tolist())
dictionary_LDA.filter_extremes(no_below=3)
corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in df['Texts'].tolist()]

num_topics = 20
%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA.keys()))
</code></pre>

<p>However, there is a problem with <code>dictionary_LDA</code> as I am using <code>df['Texts'].tolist()</code> and this is not list of list of tokens like<code>[[""word1"",""word2"",""word3""], [""word4"",""word5"",""word6""],...]</code> (I think it should be required something like this). 
I will be also have probably some 'bad'results as the texts includes stopwords... but this is another story. </p>

<p>Could you please tell me how to get a list of list of tokens or something that can allow me to use the code above?
Thank you</p>
",,2020-06-08 21:39:10,Tokenisation for topics extraction with LDA,<python><nltk><lda>,,,CC BY-SA 4.0,True,False,True,False,False
27497,62222500,2020-06-05 19:04:11,,"<p>I have run some gensim code to create an LDA model from a set of documents.  From there it has created a list of 10 topics.  Is it possible to then go back to all of those documents, and assign them to one of those 10 topics (or none if applicable)?</p>

<p>I have not been able to find any code samples to accomplish this online.</p>

<p>Thank you</p>
",,2020-06-05 19:04:11,"After creating an LDA model from documents, how do I assign topics?",<python><machine-learning><nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
27520,62204536,2020-06-04 21:34:55,,"<p>I have an information retrieval scenario. </p>

<p>Basically, my purpose is to find textual similarity between two kind of documents. </p>

<p>From the first document, I extract some keywords using gensim summarization, and I retrieve the tags associated with the document (already given by the user). </p>

<p>Then, I retrieve 2 similar words with fasttext for each of the word found in the previous step. </p>

<p>After that I put all the words and the similar words together and I use this as a searching query for bm25. </p>

<p>The problem is that ofted the bm25 gives me very poor results.</p>

<p>The query often can be very long and I was thinking that the poor results could depend on this. </p>

<p>Does bm25 suffers with long query? 
Should I query the bm25  with a fewer words query instead?</p>

<p>Thank you</p>
",,2020-06-04 21:34:55,Long query search with Rank BM25,<nlp>,,,CC BY-SA 4.0,False,False,True,False,False
27542,62174945,2020-06-03 14:08:34,,"<p>I have several gensim models fit to ~5 million documents. I'd like to pull the top 100 most representative documents from each of models for each topic to help me pick the best model. </p>

<p>Let's say I have a model <code>lda</code> and corpus <code>corpus</code>, I can get the topic probabilities in the following form:</p>

<pre><code>topic_probs = lda[corpus]
</code></pre>

<p>Where <code>topic_probs</code> is a list of tuples: <code>(topic_num, topic_prob)</code>.</p>

<p>How can I sort this list of tuples by topic, and then probability, then retrieve the top 100 documents from the corpus? I'm guessing the answer looks something like the <a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi"">method for assigning topics here</a>, but I'm struggling with how to work with a list of tuples while maintaining the document indices.</p>

<p>(This is somewhat complicated by the fact that I didn't know about the <code>minimum_probability</code> argument to <code>gensim.LdaModel</code>, so topics with &lt; 0.01 probability are suppressed. These models take 2-3 days to run each, so I'd like to avoid re-running them if possible).</p>
",,2020-06-05 19:02:18,Gensim: extract 100 most representative documents for each topic,<python><tuples><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
27565,62227888,2020-06-06 05:48:00,,"<p>Can someone help me with the flow process using NLP and python? I'm totally new to this NLP. </p>
",,2020-06-06 05:48:00,How to get similarity % from 2 resumes of which one resume is the best for x job role and other fresher resume needs to compare with the best one?,<python-3.x><nlp><nltk><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
27589,62195045,2020-06-04 12:50:49,,"<p>I am currently building topic models with Gensim. Running the gensim.models.LDAMulticore() method seems to be doing something weird. It somehow seems to rerun my file or reload it or something, as the prints I have in my code are being printed to console a lot of times. </p>

<p>My current setup is iterating through multiple alpha and beta values to max out the coherence score. as you can see down in the code, I am calling the LDA function (which contains gensim.models.LDAMulticore()) within the loop. </p>

<pre><code>def main():
    alphas = list(np.arange(0.01, 1, 0.3))
    alphas.append('symmetric')
    alphas.append('asymmetric')
    # alphas = [0.01]

    # Beta parameter
    betas = list(np.arange(0.01, 1, 0.3))
    betas.append('symmetric')
    # betas = [0.01]

    for alpha in alphas:
        for beta in betas:
            print(""Alpha: "" + str(alpha) + "", beta: "" + str(beta))
            runLDAGensim(""posts_corona"", 20, 1, 20, alpha, beta, optimize_interval=10, minTopics=15)
</code></pre>

<p>This is what I get on console:</p>

<pre><code>Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.01
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.31
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.61
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: 0.9099999999999999
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
Alpha: 0.01, beta: symmetric
...
</code></pre>

<p>So even before I get a print which tells me that he is past the line in which gensim.models.LDAMulticore() is stated (I positioned one in the function), I get the notice that a) he ran the same print line in the loop multiple times without the loop moving forward, and b) he moved forward in the loop without finishing to execute the second line in it. If I wait for a while longer (it takes a few minutes...) and he ran through all loop variations, I get a result for the LDA. But only for the first iteration (alpha, beta = 0.01). Then it all starts again. </p>

<p>This behaviour is extremely performance heavy, as he somehow needs to run the whole loops before actually doing the LDA. I tryed a different scenario (you can see it commented out in the code), where I only gave the loops one value to go through, 0.01 for both. This version executed in a fraction of the runtime it took the programm to complete the first of first iteration before. It doesnt make any sense to me.</p>

<p>Hope someone out there can help me here,
Nero</p>
",,2020-06-04 12:50:49,Why is gensim LDA rerunning my python file all the time?,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
27591,62291303,2020-06-09 20:28:05,,"<p>I'm coming from Keras to PyTorch. <strong>I would like to create a PyTorch Embedding layer</strong> (a matrix of size <code>V x D</code>, where <code>V</code> is over vocabulary word indices and <code>D</code> is the embedding vector dimension) with GloVe vectors but am confused by the needed steps.</p>

<p>In Keras, <a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""nofollow noreferrer"">you can load the GloVe vectors</a> by having the Embedding layer constructor take a <code>weights</code> argument:</p>

<pre class=""lang-py prettyprint-override""><code># Keras code.
embedding_layer = Embedding(..., weights=[embedding_matrix])
</code></pre>

<p>When looking at PyTorch and the TorchText library, I see that the embeddings should be loaded <strong>twice</strong>, once in a <code>Field</code> and then again in an <code>Embedding</code> layer. Here is <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"" rel=""nofollow noreferrer"">sample code</a> that I found:</p>

<pre class=""lang-py prettyprint-override""><code># PyTorch code.

# Create a field for text and build a vocabulary with 'glove.6B.100d'
# pretrained embeddings.
TEXT = data.Field(tokenize = 'spacy', include_lengths = True)

TEXT.build_vocab(train_data, vectors='glove.6B.100d')


# Build an RNN model with an Embedding layer.
class RNN(nn.Module):
    def __init__(self, ...):

        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        ...

# Initialize the embedding layer with the Glove embeddings from the
# vocabulary. Why are two steps needed???
model = RNN(...)
pretrained_embeddings = TEXT.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)
</code></pre>

<p>Specifically:</p>

<ol>
<li>Why are the GloVe embeddings loaded in a <code>Field</code> in addition to the <code>Embedding</code>?</li>
<li>I thought the <code>Field</code> function <code>build_vocab()</code> just builds its vocabulary from the training data. How are the GloVe embeddings involved here during this step?</li>
</ol>

<p>Here are other StackOverflow questions that did <strong>not</strong> answer my questions:</p>

<p><a href=""https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings"">PyTorch / Gensim - How to load pre-trained word embeddings</a></p>

<p><a href=""https://stackoverflow.com/questions/50747947/embedding-in-pytorch"">Embedding in pytorch</a></p>

<p><a href=""https://stackoverflow.com/questions/50340016/pytorch-lstm-using-word-embeddings-instead-of-nn-embedding"">PyTorch LSTM - using word embeddings instead of nn.Embedding()</a></p>

<p>Thanks for any help.</p>
",,2020-06-10 00:21:50,PyTorch: Loading word vectors into Field vocabulary vs. Embedding layer,<python><machine-learning><pytorch><word-embedding>,,,CC BY-SA 4.0,False,True,True,False,False
27597,62244610,2020-06-07 11:28:17,,"<p>I try to use parallelism with word2vec implemented in the gensim library. I notice that, more I increase of threads, more the training is slow and I don't know why.
Are there any settings to make?
I use :
- debian
- python 3.6.9
- cython 
how can i benefit parallelism ?</p>

<p>Thanks for advance</p>
",,2020-06-08 16:45:22,parallelization not benefit for training word2vec model,<parallel-processing><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27609,62279337,2020-06-09 09:30:27,,"<p>I'm running into this error: ""ValueError: Cannot create a tensor proto whose content is larger than 2GB""</p>

<p>The reason for the error seems to be that I'm trying to create a tensor from a 13Gb big numpy array. The array is from BioSentVec (pretrained embeddings from PubMed, source: <a href=""https://github.com/ncbi-nlp/BioSentVec#biosentvec"" rel=""nofollow noreferrer"">https://github.com/ncbi-nlp/BioSentVec#biosentvec</a>). This is what the original code looks like (source: <a href=""https://github.com/kaggarwal/ClinicalNotesICU"" rel=""nofollow noreferrer"">https://github.com/kaggarwal/ClinicalNotesICU</a>):</p>

<pre><code>X = tf.placeholder(shape=(None, 48, 76), dtype=tf.float32, name='X')  # B*48*76
y = tf.placeholder(shape=(None), dtype=tf.float32, name='y')
text = tf.placeholder(shape=(None, None), dtype=tf.int32, name='text')  # B*L
dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='dropout_kp')
T = tf.placeholder(dtype=tf.int32)
N = tf.placeholder(dtype=tf.int32)

W = tf.get_variable(name=""W"", shape=vectors.shape,
                    initializer=tf.constant_initializer(vectors), trainable=False)
</code></pre>

<p>I found this thread which seems to fit to my problem: <a href=""https://stackoverflow.com/questions/51470991/create-a-tensor-proto-whose-content-is-larger-than-2gb/51473218"">create a tensor proto whose content is larger than 2GB</a>
 and implemented it like this:</p>

<pre><code>X = tf.placeholder(shape=(None, 48, 76), dtype=tf.float32, name='X')  # B*48*76
y = tf.placeholder(shape=(None), dtype=tf.float32, name='y')
text = tf.placeholder(shape=(None, None), dtype=tf.int32, name='text')  # B*L
dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='dropout_kp')
T = tf.placeholder(dtype=tf.int32)
N = tf.placeholder(dtype=tf.int32)
W = tf.get_variable(name=""W"", shape=vectors.shape)
size = vectors.shape[0]*vectors.shape[1] * 1.0
w_init = size
w_plhdr = tf.placeholder(dtype=tf.float32, shape=vectors.shape)

with tf.Session() as sess: 
    sess.run(tf.constant_initializer(vectors))
    sess.run(W.assign(w_plhdr), {w_plhdr: w_init})
</code></pre>

<p>and receive this error:</p>

<pre><code>TypeError: Fetch argument &lt;tensorflow.python.ops.init_ops.Constant object at 0x1ce0216a90&gt; has invalid type &lt;class 'tensorflow.python.ops.init_ops.Constant'&gt;, must be a string or Tensor. (Can not convert a Constant into a Tensor or Operation.)
</code></pre>

<p>Can anybody help me in understanding or even solving this problem? I also think that the reason for my issue could be origin else, as the original code from the source also has worked for the authors of the paper. I loaded the embeddings with gensim and saved the model as a pickle. Then I accessed the model and the vectors with the saved files from gensim.</p>
",,2020-06-09 09:30:27,Loading BioSentVec: Cannot create a tensor proto with content is larger than 2GB,<python><tensorflow><protocol-buffers><gensim><pubmed>,,,CC BY-SA 4.0,False,False,True,False,False
27616,62211396,2020-06-05 08:42:07,,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.stack.imgur.com/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
",2020-06-14 12:52:43,2020-06-14 12:52:43,How does gensim word2vec word embedding extract training word pair for 1 word sentence?,<nlp><text-mining><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
27629,62308418,2020-06-10 16:24:41,,"<p>I am trying to load the pretrained vec file of Facebook fasttext crawl-300d-2M.vec with the next code:</p>

<pre><code>from gensim.models.fasttext import load_facebook_model, load_facebook_vectors

model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')
</code></pre>

<p>But it fails with the next error:</p>

<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>

<p>It is not possible to load this vector?</p>

<p>If it is possible, afterwards can I train it with my own sentences?</p>

<p>Thanks in advance.</p>

<p>Whole error trace:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-181-f8262e0857b8&gt; in &lt;module&gt;
----&gt; 1 model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in load_facebook_vectors(path, encoding)
   1196 
   1197     """"""
-&gt; 1198     model_wrapper = _load_fasttext_format(path, encoding=encoding, full_model=False)
   1199     return model_wrapper.wv
   1200 

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in _load_fasttext_format(model_file, encoding, full_model)
   1220     """"""
   1221     with gensim.utils.open(model_file, 'rb') as fin:
-&gt; 1222         m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
   1223 
   1224     model = FastText(

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in load(fin, encoding, full_model)
    339         model.update(dim=magic, ws=version)
    340 
--&gt; 341     raw_vocab, vocab_size, nwords, ntokens = _load_vocab(fin, new_format, encoding=encoding)
    342     model.update(raw_vocab=raw_vocab, vocab_size=vocab_size, nwords=nwords, ntokens=ntokens)
    343 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _load_vocab(fin, new_format, encoding)
    192     # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
    193     if nlabels &gt; 0:
--&gt; 194         raise NotImplementedError(""Supervised fastText models are not supported"")
    195     logger.info(""loading %s words for fastText model from %s"", vocab_size, fin.name)
    196 

NotImplementedError: Supervised fastText models are not supported
</code></pre>
",2020-06-11 09:43:31,2020-06-11 09:43:31,"Word embedding with gensim and FastText, training on pretrained vectors",<python><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
27630,62310124,2020-06-10 18:01:13,,"<p>I am trying to save a custom FastText model trained with gensim. I want to save the binary files to have the possibility of training again the model, if it may.</p>

<p>The code to save the binary file is the next one:</p>

<pre><code>from gensim.models.fasttext import save_facebook_model

save_facebook_model(model,'own_fasttext_model.bin')
</code></pre>

<p>But I am obtaining the next error in that same line:</p>

<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-192-c9c2c41985af&gt; in &lt;module&gt;
      2 from gensim.models.fasttext import save_facebook_model
      3 
----&gt; 4 save_facebook_model(model,'own_fasttext_model.bin')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in save_facebook_model(model, path, encoding, lr_update_rate, word_ngrams)
   1334     """"""
   1335     fb_fasttext_parameters = {""lr_update_rate"": lr_update_rate, ""word_ngrams"": word_ngrams}
-&gt; 1336     gensim.models._fasttext_bin.save(model, path, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in save(model, fout, fb_fasttext_parameters, encoding)
    666     if isinstance(fout, str):
    667         with open(fout, ""wb"") as fout_stream:
--&gt; 668             _save_to_stream(model, fout_stream, fb_fasttext_parameters, encoding)
    669     else:
    670         _save_to_stream(model, fout, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _save_to_stream(model, fout, fb_fasttext_parameters, encoding)
    629 
    630     # Save words and ngrams vectors
--&gt; 631     _input_save(fout, model)
    632     fout.write(struct.pack('@?', False))  # Save 'quot_', which is False for unsupervised models
    633 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _input_save(fout, model)
    573 
    574     assert vocab_dim == ngrams_dim
--&gt; 575     assert vocab_n == len(model.wv.vocab)
    576     assert ngrams_n == model.wv.bucket
    577 

AssertionError: 
</code></pre>

<p>Any clue on what could be happening?</p>

<p>Thanks in advance.</p>
",,2020-06-24 18:53:06,Saving FastText custom model binary with Gensim,<save><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
27632,62343252,2020-06-12 11:22:16,,"<p>I am trying to use gensim phraser on a column in a df. The sample df is given below</p>

<pre><code>col1   col2
1      ""this is test1 and is used for test1""
2      ""this is content of row which is second row""
3      ""this is the third row""
</code></pre>

<p>I have wrote a method for bigrams</p>

<pre><code>def bigrams(text):
    bigram = Phrases(text, min_count=1)
    bigram_mod = Phraser(bigram)
    return [bigram_mod[doc] for doc in text]
</code></pre>

<p>And I tried</p>

<pre><code>df['col2'].apply(bigrams)
df['col2'].apply(lambda x: bigrams([x])) - so that the text is enclosed in list
</code></pre>

<p>but I get the characters as output and not the bigrams. What am I missing here.</p>
",,2020-06-13 11:27:23,Use gensim phraser on pandas column using apply method,<python><pandas><gensim><n-gram><phrase>,,,CC BY-SA 4.0,False,False,True,False,False
27645,62344470,2020-06-12 12:39:35,,"<p>i am doing synonym-detection based on word2vec models with gensim. What possibilities are for automatic calculate of recall and precision. I just want some metrics based on the trained Model whitout giving a list of correct synoynms. 
Cheers,
Marvin</p>
",,2020-06-12 12:39:35,Automatic calculate Recall/ Precision to Word2vec model,<python><gensim><word2vec><synonym>,,,CC BY-SA 4.0,False,False,True,False,False
27662,62298548,2020-06-10 07:59:18,,"<p>I want to use a pretrained model in Microsoft powerapps. As of now, powerapps provides some readymade models which we can train and use. However, take an example, if we want to call gensim for sematic match of 2 strings in powerapp, then how to use it?<br>
I dont want to run a model on AzureML or a server and call it via exposed API. But the requirement is the gensim or word2vec is hosted by powerapps and run the prediction on demand.</p>
",,2020-06-10 07:59:18,Pretrained models in microsoft powerapps,<gensim><powerapps>,,,CC BY-SA 4.0,False,False,True,False,False
27677,62299869,2020-06-10 09:12:50,,"<p>I used Word2Vec to get the vectors of a sample textual data.
Assuming for each word (W), <code>W1, W2, W3, ... , Wn</code> we have a vector. 
And also we have a similarity matrix that shows cosine similarity between words.</p>

<p>Now the question is for a new word (X) that is in the similarity matrix, can I get the vector?
Lets say the similarity between W1 and X is 0.33, ans similarity between W2 and X is 0.12. Can I compute the vector X by multiply the vector of each word(W) to the similarity and then sum the values?</p>

<p>I read about it and still I'm not sure if it's a correct approach.
For example the link below does't answer my question, since I don't have data related to the new word, only similarity matrix is available.
<a href=""https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"">https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words</a></p>
",,2020-06-10 09:12:50,Word embedding of new word using similarity,<vector><cosine-similarity><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
27684,62348760,2020-06-12 16:44:04,,"<p>I am getting different values of similarity measures when I swap the position of camera_final and baby_final. Though the similarity measures I am using are symmetrical in nature. I am using LDA model from gensim.</p>

<pre><code>
texts = [camera_final, baby_final]

from gensim import corpora 

dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]

import numpy
numpy.random.seed(1) # setting random seed to get the same results each time.

from gensim.models import ldamodel
model = ldamodel.LdaModel(corpus, id2word=dictionary,  minimum_probability=1e-8)

bow_baby = model.id2word.doc2bow(baby_final)

bow_camera = model.id2word.doc2bow(camera_final)



lda_bow_camera = model[bow_camera]
lda_bow_baby = model[bow_baby]




from gensim.matutils import kullback_leibler,jensen_shannon,jaccard,cossim,hellinger

print('hellinger camera')

print(hellinger(lda_bow_baby,lda_bow_camera))

print('jensen_shannon camera')

print(jensen_shannon(lda_bow_baby,lda_bow_camera))

print('jaccard camera')

print(jaccard(lda_bow_baby,lda_bow_camera))

print('Cosine camera')

print(cossim(lda_bow_baby,lda_bow_camera))

</code></pre>
",2020-06-12 16:44:58,2020-06-12 16:44:58,Why I am getting different values of similarity measures when I change the position of the input data?,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
27685,62348981,2020-06-12 16:58:33,,"<p>The code works absolutely fine for the data set containing 500000+ instances but whenever I reduce the data set to 5000/10000/15000 it throws a key error : word ""***"" not in vocabulary.Not for every data point but for most them it throws the error.The data set is in excel format.  [1]: <a href=""https://i.stack.imgur.com/YCBiQ.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/YCBiQ.png</a>
I don't know how to fix this problem since i have very little knowledge about it,,I am still learning.Please help me fix this problem! </p>

<pre><code>    purchases_train = []
    for i in tqdm(customers_train):
        temp = train_df[train_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_train.append(temp)

    purchases_val = []
    for i in tqdm(validation_df['CustomerID'].unique()):
        temp = validation_df[validation_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_val.append(temp)


    model = Word2Vec(window = 10, sg = 1, hs = 0,
                     negative = 10, # for negative sampling
                     alpha=0.03, min_alpha=0.0007,
                     seed = 14)

    model.build_vocab(purchases_train, progress_per=200)

    model.train(purchases_train, total_examples = model.corpus_count, 
                epochs=10, report_delay=1)


    model.save(""word2vec_2.model"")
    model.init_sims(replace=True)

    # extract all vectors
    X = model[model.wv.vocab]

    X.shape

    products = train_df[[""StockCode"", ""Description""]]

    products.drop_duplicates(inplace=True, subset='StockCode', keep=""last"")


 products_dict=products.groupby('StockCode'['Description'].apply(list).to_dict()

    def similar_products(v, n = 6):
        ms = model.similar_by_vector(v, topn= n+1)[1:]
        new_ms = []
        for j in ms:
            pair = (products_dict[j[0]][0], j[1])
            new_ms.append(pair)

        return new_ms

        similar_products(model['21883'])
</code></pre>
",2020-06-12 17:12:29,2020-06-12 17:26:42,"word2vec recommendation system KeyError: ""word '21883' not in vocabulary""",<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27690,62315518,2020-06-11 01:09:41,,"<p>I have a word array from a pickle file, and a corresponding vector array from an npy file, how do I combine them to make a Gensim W2V model?</p>
",,2020-06-11 16:39:08,Combining a word array and a vector array to make a Gensim W2V model,<gensim><word2vec><embedding><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
27739,62419353,2020-06-16 23:57:19,,"<p>Would it be possible to look for texts that are within a certain topic (determined by LDA)? </p>

<p>I have a list of 5 topics with 10 words each, found by using lda.</p>

<p>I have analysed the texts in a dataframes column. 
I would like to select/filter rows/texts that are in one specific topic. </p>

<p>If you need more information, I will provide you. </p>

<p>What I am referring to is the step that returns this output:</p>

<pre><code>[(0,
  '0.207*""house"" + 0.137*""apartment"" + 0.118*""sold"" + 0.092*""beach"" + '
  '0.057*""kitchen"" + 0.049*""rent"" + 0.033*""landlord"" + 0.026*""year"" + '
  '0.024*""bedroom"" + 0.023*""home""'),
 (1,
  '0.270*""school"" + 0.138*""homeworks"" + 0.117*""students"" + 0.084*""teacher"" + '
  '0.065*""pen"" + 0.038*""books"" + 0.022*""maths"" + 0.020*""exercise"" + '
  '0.020*""friends"" + 0.020*""college""'),
 ... ]
</code></pre>

<p>created by </p>

<pre><code># LDA Model

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', 
                                           # alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys()))
</code></pre>

<h1>Print the Keyword in the 10 topics</h1>

<pre><code>from pprint import pprint
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>The original column with texts that have been analysed is called <code>Texts</code> and it looks like: </p>

<pre><code>Texts 

""Children are happy to go to school...""
""The average price for buying a house is ... ""
""Our children love parks so we should consider to buy an apartment nearby""

etc etc...
</code></pre>

<p>My expected output would be </p>

<pre><code>Texts                                            Topic 
    ""Children are happy to go to school...""         2
    ""The average price for buying a house is ... ""  1
    ""Our children love parks so we should consider to buy an apartment nearby""                                   

      2
</code></pre>

<p>Thanks </p>
",2020-06-17 00:39:12,2020-06-27 15:42:09,Select texts by topic (LDA),<python><gensim><text-classification><lda>,,,CC BY-SA 4.0,False,False,True,False,False
27740,62387623,2020-06-15 11:51:59,,"<pre><code>df_clean['message'] = df_clean['message'].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x))
</code></pre>

<p>I tried this on a dataframe's column 'message' but I get the error: </p>

<pre><code>TypeError: decoding to str: need a bytes-like object, list found
</code></pre>
",,2020-06-15 12:14:26,How to remove stopwords in gensim?,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27755,62304813,2020-06-10 13:29:10,,"<p>I am working on a NLP problem with gensim that requires the use of multilingual embeddings. I have the already pretrained and aligned .txt embeddings that <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">FastText provides in their web</a>. Sadly, they don't provide the full model, but these vectors are missing some important vocabulary on my problem and the per-character embedding ability of a FastText model here comes very handy for these cases.</p>

<p>My question:</p>

<ol>
<li>Is there a way to recreate the entire model so I can infer new vocabulary that is also in the vector space of aligned embeddings?</li>
<li>If not, Is there still a way to obtain those terms in that aligned embedding space? Without having to retrain a new entire FastText and then align it the already pre-trained ones? </li>
</ol>
",,2020-06-10 13:29:10,Full FastText model from KeyedVectors to infer new words in aligned space,<nlp><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
27760,62451936,2020-06-18 14:09:49,,"<p>Unfortunately I do not know how to introduce the issue that I have been having with LDA, trying to assign topics to texts. 
I am getting this error when I try to determine the threshold for assigning the topics: </p>

<blockquote>
  <p>ValueError                                Traceback (most recent call
  last)  in 
       32 
       33 print(scores)
  ---> 34 threshold = sum(scores)/len(scores)
       35</p>
  
  <p>ValueError: operands could not be broadcast together with shapes (1,2)
  (0,)</p>
</blockquote>

<p>Investigating a bit, I found that the output from </p>

<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=3, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', 
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys()))

d_lda = lda_model[corpus]

scores = []
for doc in d_lda:
    for topic in doc:
        for topic_id, score in topic:
            scores.append(score)

print(scores)
threshold = sum(scores)/len(scores)
</code></pre>

<p><strong>Output:</strong> </p>

<pre><code>[0.17162731, 0.7584814, 0.069891326, [1], [1], [0], [1], [1], [1], [(1, 0.99998814)], [(1, 0.99999917)], [(0, 0.99994564)], [(1, 0.9999982)], [(1, 0.9999973)], [(1, 0.9999984)], 0.04412621, 0.8588418, 0.09703196, [1], [1], [], [], [1], [(1, 1.9999986)], [(1, 0.99999666)], [], [], [(1, 0.99988884)], 0.05475739, 0.102039, 0.84320354, [2], [2], [2], [(2, 0.9999983)], [(2, 0.99995077)], [(2, 0.999979)], 0.70694596, 0.13442636, 0.15862773, [], [0], [], [0], [], [(0, 0.99998266)], [], [(0, 0.99997467)], 0.07213888, 0.13442898, 0.7934322, [], [], [2], [2], []]
</code></pre>

<p>is causing the issue, as I should have a list of values and not a list of list. 
I am reporting below the steps to get the LDA model, just to see if I have set something wrong in my code. I hope you can help me to go through it. </p>

<pre><code>data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
</code></pre>

<p>Example of output from <code>data_lemmatized</code>: </p>

<pre><code>[['dogs', 'classic', 'pam', 'home', 'market', 'online']]
</code></pre>

<p><strong>Creation of Dictionary:</strong></p>

<pre><code>id2word = corpora.Dictionary(data_lemmatized)

texts = data_lemmatized
corpus = [id2word.doc2bow(text) for text in texts]

id2word[0]
[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]
</code></pre>

<p>Example of output: </p>

<pre><code>[[('dogs', 1),
  ('classic', 1),
  ('pam', 1),
  ('home', 1),
  ('market', 1),
  ('online', 1)]]
</code></pre>

<p>Then the code at the top of the question. 
Could you please tell me what that error means and how I can fix it?
Thank you</p>
",2020-06-18 14:15:21,2020-06-18 14:15:21,Problem with threshold in assigning topics to texts using LDA,<python><nltk><gensim><text-classification><lda>,,,CC BY-SA 4.0,True,False,True,False,False
27770,62356383,2020-06-13 06:36:31,,"<p>I am using Gensim's Mallet wrapper for topic modeling - </p>

<pre><code>LdaMallet(path_to_mallet_binary, corpus=corpus, num_topics=100, id2word=words, workers=6, random_seed=2)
</code></pre>

<p>While the above worked surprisingly fast, the step (see below) to obtain the topic distribution for each document (n=40,000) is taking a very long time.</p>

<pre><code>#Store topic distributuon for all documents
all_topics=[]
for x in tqdm(range(0, len(doc_list))):
    all_topics.append(lda_model[corpus[x]])
</code></pre>

<p>It has taken ~18 hours to complete 30,000 documents. Not sure what I am doing incorrectly. Is there a way to get topic distribution for all documents much faster?</p>
",2020-06-13 06:55:15,2020-06-18 07:20:48,Gensim Mallet Wrapper: How can I get all documents' topic weights?,<python><gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
27780,62339753,2020-06-12 07:42:45,,"<p>I've trained an lda model using GuidedLDA (<a href=""https://guidedlda.readthedocs.io/"" rel=""nofollow noreferrer"">https://guidedlda.readthedocs.io/</a>). Is it possible to transfer this lda model into the gensim framework to continue working with it?</p>
",,2020-06-12 07:42:45,Is it possible to load pre-trained lda model into gensim?,<python><nlp><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
27787,62358583,2020-06-13 10:38:34,,"<p>I am trying to train Gensim Doc2Vec model on tagged documents. I have around 4000000 documents. Following is my code:</p>

<pre><code>import pandas as pd
import multiprocessing
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from tqdm import tqdm
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import os
import re



def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'plurality', 'one', 'more', 'least', 'at', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))

    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

chunk_patent = pd.DataFrame()
chunksize = 10 ** 5
cores = multiprocessing.cpu_count()
directory = os.getcwd()
for root,dirs,files in os.walk(directory):
    for file in files:
       if file.startswith(""patent_cpc -""):
           print(file)
           #f=open(file, 'r')
           #f.close()
           for chunk_patent_temp in pd.read_csv(file, chunksize=chunksize):
                #chunk_patent.sort_values(by=['cpc'], inplace=True)
                #chunk_patent_temp = chunk_patent_temp[chunk_patent_temp['cpc'] == ""G06K7""]
                if chunk_patent.empty:
                    chunk_patent = chunk_patent_temp
                else:
                    chunk_patent = chunk_patent.append(chunk_patent_temp)
train_tagged = chunk_patent.apply(lambda r: TaggedDocument(words=text_process(r['text']), tags=[r.cpc]), axis=1)
print(train_tagged.values)

if os.path.exists(""cpcpredict_doc2vec.model""):
    doc2vec_model = Doc2Vec.load(""cpcpredict_doc2vec.model"")
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)), update=True)
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
else:
    doc2vec_model = Doc2Vec(dm=0, vector_size=300, min_count=100, workers=cores-1)
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)))
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
</code></pre>

<p>I have tried modifying the Doc2vec parameters but without any luck.</p>

<p>On the same data I have trained Word2vec model, which is much accurate in comparison to the doc2vec model. Further, ""most_similar"" results for word2vec model is very different from the doc2vec model. </p>

<p>Following is the code for searching most similar results:</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from gensim.models import Doc2Vec
import re

def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))
    #for index, row in df.iterrows():
    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

model = Word2Vec.load(""cpc.model"")
print(model.most_similar(positive=['barcode'], topn=30))

model1 = Doc2Vec.load(""cpcpredict_doc2vec.model"")

pred_tags = model1.most_similar('barcode',topn=10)
print(pred_tags)
</code></pre>

<p>Further, the output of the aforementioned is cited below:</p>

<pre><code>[('indicium', 0.36468246579170227), ('symbology', 0.31725651025772095), ('G06K17', 0.29797130823135376), ('dataform', 0.29535001516342163), ('rogue', 0.29372256994247437), ('certification', 0.29178398847579956), ('reading', 0.27675414085388184), ('indicia', 0.27346929907798767), ('Contra', 0.2700084149837494), ('redemption', 0.26682156324386597)]

[('searched', 0.4693435728549957), ('automated', 0.4469209909439087), ('production', 0.4364866018295288), ('hardcopy', 0.42193126678466797), ('UWB', 0.4197841286659241), ('technique', 0.4149003326892853), ('authorized', 0.4134449362754822), ('issued', 0.4129987359046936), ('installing', 0.4093806743621826), ('thin', 0.4016669690608978)]
</code></pre>
",,2020-06-14 15:57:14,Improving DOC2VEC Gensim efficiency,<python><nltk><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
27805,62454568,2020-06-18 16:16:19,,"<p>I need to train a Word2Vec model, which I have done, and then I need to use it to calculate the likelihood of previously unseen data. I'm stuck on how to do this though. I've seen WMD but that's for calculating similarity between 2 sentences, while I'm trying to calculate the likelihood of text with my model.</p>
",,2020-06-18 16:16:19,How to calculate the likelihood of a sentence using a Word2Vec model?,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27838,62492450,2020-06-20 22:31:59,,"<p>I am new to Gensim and I am learning Gensim and followed the example here: <a href=""https://www.machinelearningplus.com/nlp/gensim-tutorial/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/gensim-tutorial/</a></p>
<p>I am not to sure about the last line that creates the corpus from dictionary. When creating the dictionary, we already used simple_preprocess to process the &quot;documents&quot; line by line. I was thinking while creating the corpus using the dictionary, we needed to use simple_preprocess again to process &quot;documents&quot; line by line. Is that redundant?</p>
<pre><code>documents = [&quot;This is the first line&quot;,
         &quot;This is the second sentence&quot;,
         &quot;This third document&quot;]

# Create the Dictionary and Corpus
mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])
# Why need to use simple_preprocess and pass the documents again while
# the last call already created the dictionary using simple_preporcess on documents
corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]
</code></pre>
<p>Thanks,</p>
<p>Alex</p>
",2020-06-22 16:49:07,2020-06-22 16:49:07,Questions on Gensim create corpus from dictionary,<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27896,62529967,2020-06-23 07:55:10,,"<p>In order to use the <code>gensim.similarities.docsim.Similarity</code> class to compute similarities between words, one need to provide the corpus and the size of the dictionary.</p>
<p>In my case, the corpus are the word vectors computed using a word2vec model.</p>
<p>I wonder why <code>gensim</code> needs the size of the dictionary? And also, if it needs here the size of the dictionary used to create the word2vec model, or the size of the dictionary of the corpus, for which I want to compute the similarities.</p>
",2020-06-23 08:17:04,2020-06-23 17:14:06,Why computing similarity with gensim needs the size of the dictionary?,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27899,62534889,2020-06-23 12:36:29,,"<p>i have trouble with this error while using word2vec from gensim</p>
<pre><code>MemoryError: Unable to allocate 380. MiB for an array with shape (331792, 300) and data type float32
</code></pre>
<p>and this is my code</p>
<pre><code>from gensim.models import Word2Vec

@app.route('/admin/index')
def show_admin():

model_baru = Word2Vec.load('model/idwiki_word2vec_300.model')
tester = model_baru.most_similar(positive=['wanita'],topn=3)
hasil = print(tester)

return render_template('admin/index.html',hasil=hasil)
</code></pre>
<p>what happen with this error?
please help me</p>
<p>thanks for your attention</p>
",2020-06-23 13:25:16,2020-06-23 13:25:16,"MemoryError: Unable to allocate 380. MiB for an array with shape (331792, 300) and data type float32",<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27959,62581874,2020-06-25 18:19:05,,"<p>What is the difference between using <code>gensim.models.LdaMallet</code> and <code>gensim.models.LdaModel</code>? I noticed that the parameters are not all the same and would like to know when one should be used over the other?</p>
",,2020-06-25 18:19:05,(gensim) LdaMallet vs LdaModel?,<gensim><lda><topic-modeling><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
27966,62565772,2020-06-24 23:52:38,,"<p>I would like to know if it is possible to group together same words included in the LDA's output, i.e. words generated by</p>
<pre><code>doc_lda = lda_model[corpus]
</code></pre>
<p>for example</p>
<pre><code>[(0,
  '0.084*&quot;tourism&quot; + 0.013*&quot;touristic&quot; + 0.013*&quot;Madrid&quot; + '
  '0.010*&quot;travel&quot; + 0.008*&quot;half&quot; + 0.007*&quot;piare&quot; + '
  '0.007*&quot;turism&quot;')]
</code></pre>
<p>I would like to group <code>tourism, touristic</code> and <code>turism</code> (mispelled) together.
Would it be possible?</p>
<p>This is some relevant previous code:</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys())) 
</code></pre>
<p>Thank you</p>
",2020-06-29 00:52:37,2020-07-03 20:03:52,Grouping words with same meaning. in LDA,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
27972,62636828,2020-06-29 11:09:49,,"<p>While trying to import gensim, I run into the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\usr\Documents\hello\test.py&quot;, line 3, in &lt;module&gt;
    import gensim
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\__init__.py&quot;, line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\corpora\__init__.py&quot;, line 6, in &lt;module&gt;
    from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\corpora\indexedcorpus.py&quot;, line 15, in &lt;module&gt;
    from gensim import interfaces, utils
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\interfaces.py&quot;, line 21, in &lt;module&gt;
    from gensim import utils, matutils
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\matutils.py&quot;, line 21, in &lt;module&gt;
    from scipy.stats import entropy
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\stats\__init__.py&quot;, line 384, in &lt;module&gt;
    from .stats import *
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\stats\stats.py&quot;, line 179, in &lt;module&gt;
    from scipy.spatial.distance import cdist
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\spatial\__init__.py&quot;, line 99, in &lt;module&gt;
    from .qhull import *
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
<p>I have tried uninstalling numpy, scipy and gensim using <code>pip</code> in the command prompt and installing them again, but this does not resolve the issue.</p>
<p>I have also looked at the suggestions to a similar problem <a href=""https://stackoverflow.com/questions/34253458/installing-scipy-package-in-windows"">here</a>, and tried installing <a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#numpy"" rel=""nofollow noreferrer"">numpy1.19.0+mklcp37cp37mwin_amd64.whl</a>, but it resulted in a separate error <code>Importing the numpy c-extensions failed.</code> Thus, I have stuck to using numpy, scipy and gensim installed via <code>pip</code>.</p>
<p>Additionally, I installed scipy version 1.4.1 as the latest 1.5.0 version will give the following error as described in this link:
<a href=""https://stackoverflow.com/questions/62566691/error-when-loading-scipy-oserror-winerror-126-the-specified-module-could-not"">Error when loading scipy: OSError: [WinError 126] The specified module could not be found</a></p>
<p>Any help is greatly appreciated!</p>
<p>For additional information, I am using Python 3.7 and Windows 10.</p>
",2020-06-30 04:11:06,2020-06-30 07:09:11,"""ImportError: DLL load failed: The specified module could not be found"" when trying to import gensim",<numpy><scipy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
27978,62543491,2020-06-23 20:47:35,,"<p>I am trying to understand what is going wrong in the following example.</p>
<p>To train on the 'text8' dataset as described in the docs, one only has to do the following:</p>
<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)
</code></pre>
<p>doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.</p>
<p>However, when loading the same textfile which is used above manually, as in</p>
<pre><code>text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)
</code></pre>
<p>The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.</p>
<p>What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?</p>
",2020-06-23 20:56:06,2020-06-23 21:09:44,Inconsistent results when training gensim model with gensim.downloader vs manual loading,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
27980,62620509,2020-06-28 09:13:55,,"<p>I wrote LDA model in notebook.
I'm trying to wrap my gensim LDA model with mallet, getting the following error:</p>
<p>CalledProcessError: Command '../input/mymallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input /tmp/fbcc4b_corpus.txt --output /tmp/fbcc4b_corpus.mallet' returned non-zero exit status 126.</p>
<p>The error raised because of the second line:</p>
<pre><code>mallet_path = '../input/mymallet/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>
<p>The path is correct.</p>
<p>Tried this solution:  <a href=""https://stackoverflow.com/questions/55288724/gensim-mallet-calledprocesserror-returned-non-zero-exit-status"">Gensim mallet CalledProcessError: returned non-zero exit status</a></p>
<p>didn't work for me..</p>
",2020-06-28 10:24:23,2020-06-28 10:24:23,How to fix mallet on gensim,<gensim><lda><kaggle><mallet>,,,CC BY-SA 4.0,False,False,True,False,False
27994,62603978,2020-06-26 23:00:08,,"<p>I'm running an NLP algorithm that ultimately performs Topic Modelling with gensim's LDA model. I've run the code many times and it works properly when I run selection (even when selecting the whole program) but when I run the file I get [Errno 32] Broken pipe.</p>
<p>I've defined a user function that goes as follows:</p>
<pre><code>def compute_cv(DICT, CORPUS, DOCSLEM, startnum, lim, step, BESTCV):
    coherence_values = []
    model_list = []
    perplexity_values =[]
    for num_topics in range(startnum, lim, step):
        model = gensim.models.LdaModel(CORPUS, num_topics=num_topics, id2word=DICT, passes=2)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=DOCSLEM, dictionary=DICT)
        coherence_values.append(coherencemodel.get_coherence())
        perplexity = model.log_perplexity(CORPUS)
        perplexity_values.append(perplexity)
        if BESTCV &lt; coherencemodel.get_coherence():
            BESTCV = coherencemodel.get_coherence()
            BESTMD = num_topics - 2
    return model_list, coherence_values, perplexity_values, BESTCV, BESTMD
</code></pre>
<p>This function is run in a cycle that goes as follows:</p>
<pre><code>OPTCV=0
for i in range(1, 3):
    BESTCV = 0
    BESTMD = 0
    model_list, coherence_values, perplexity_values, BESTCV, BESTMD = compute_cv(DICT=DICT, CORPUS=CORPUS,
                                                                 DOCSLEM=DOCSLEM, startnum=2,
                                                                 lim=6, step=1, BESTCV=BESTCV)
    if BESTCV &gt; OPTCV:
        OPTCV = BESTCV
        OPTMODEL = model_list[BESTMD]
</code></pre>
<p>As I've said, the code runs perfectly with &quot;run selection&quot; option but crashes with &quot;run file&quot;. I use Spyder 4.1.2 in anaconda environment with python 3.7.6.</p>
<p>I'd appreciate any suggestions or if you know how to solve this, if you consider you need further information don't doubt to reply!</p>
<p>Thank you!</p>
",2020-06-26 23:37:52,2020-06-26 23:37:52,Broken Pipe Error: runing selection works properly but runing file crashes,<python><gensim><broken-pipe>,,,CC BY-SA 4.0,False,False,True,False,False
28004,62656022,2020-06-30 11:05:00,,"<p>I have trained a LDA model using the following parameters:</p>
<pre><code>&gt;&gt; model = gensim.models.ldamodel.LdaModel(corpus=corpus,
 id2word=id2word,
 num_topics=25,
 passes=10,
 minimum_probability=0)
</code></pre>
<p>Then, I applied this model to a given corpus:</p>
<pre><code>&gt;&gt; lda_corpus = model[corpus]
</code></pre>
<p>I was expecting lda_corpus to be a list of lists or 2D matrix, where the number of rows is the number of docs and the number of columns is the number of topics and each element matrix, a tuple of the form (topic_index, probability). However I am getting this weird result where some elements are again a list:</p>
<pre><code>
&gt;&gt; print(lda_model_1[corpus[0]])


&gt;&gt; ([(0, 0.012841966), (3, 0.073988825), (4, 0.05184835), (8, 0.38537887), (10, 0.022958927), (11, 0.24562633), (13, 0.05168812), (17, 0.06522224), (21, 0.024792604)], [(0, [11]), (1, [8, 3, 17, 13]), (2, [3, 17, 8, 13]), (3, [8, 3]), (4, [11]), (5, [8, 17, 3]), (6, [4]), (7, [4, 8]), (8, [8, 13, 3]), (9, [11]), (10, [8, 0]), (11, [8, 13, 0]), (12, [21]), (13, [11]), (14, [11]), (15, [8]), (16, [8, 11, 13, 0]), (17, [11]), (18, [11, 17]), (19, [8, 13, 17, 3]), (20, [17, 13, 8]), (21, [17, 11, 8]), (22, [11]), (23, [8]), (24, [8, 13]), (25, [8, 3, 13])], [(0, [(11, 1.0)]), (1, [(3, 0.15384258), (8, 0.71774876), (13, 0.011975089), (17, 0.11643356)]), (2, [(3, 0.45133045), (8, 0.21692151), (13, 0.09479065), (17, 0.23232804)]), (3, [(3, 0.24423833), (8, 0.75576156)]), (4, [(11, 1.0)]), (5, [(3, 0.02001735), (8, 1.6895359), (17, 0.2904468)]), (6, [(4, 1.0)]), (7, [(4, 1.2565874), (8, 0.7367453)]), (8, [(3, 0.05150538), (8, 0.8553984), (13, 0.07775658)]), (9, [(11, 2.0)]), (10, [(0, 0.13937186), (8, 0.8588695)]), (11, [(0, 0.023420962), (8, 0.7131521), (13, 0.263427)]), (12, [(21, 1.0)]), (13, [(11, 0.99124163)]), (14, [(11, 2.0)]), (15, [(8, 1.0)]), (16, [(0, 0.011193657), (8, 1.7189965), (11, 0.23104382), (13, 0.029387457)]), (17, [(11, 1.9989293)]), (18, [(11, 0.9135094), (17, 0.08400644)]), (19, [(3, 0.07146881), (8, 2.1837764), (13, 0.38799366), (17, 0.352704)]), (20, [(8, 0.22638415), (13, 0.24114841), (17, 0.52740365)]), (21, [(8, 0.02224951), (11, 0.24574266), (17, 0.7231928)]), (22, [(11, 1.0)]), (23, [(8, 1.0)]), (24, [(8, 0.972818), (13, 0.027181994)]), (25, [(3, 0.16742931), (8, 0.7671518), (13, 0.05224549)])])
</code></pre>
<p>I would appreciate any help.</p>
",,2020-07-08 10:12:52,Unexpected output when applying LDA trained model to given corpus,<python><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28034,62626282,2020-06-28 17:53:54,,"<p>This might seem like a trivial question to many but its not clear for me.</p>
<p>So i have a model that looks like this</p>
<pre><code>w2v_model = gensim.models.word2vec.Word2Vec(size=300, 
                                        window=7, 
                                        min_count=10, 
                                        workers=8)
</code></pre>
<p>I also tokenize my x_train and x_test like this</p>
<pre><code>x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=300)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=300)
</code></pre>
<p>My embedding layer looks like this</p>
<pre><code>embedding_matrix = np.zeros((vocab_size, 300), dtype='float32')
 for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]

embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, 
trainable=False)
</code></pre>
<p>So I get that the 300 in the model is the dimension size and the 300 in the pad_sequences is the max sequence length aka max sentence length</p>
<p>I also get that the 300 in the embedding_matrix definition is the w2v dimension size again and that the first 300 in the embedding_layer is referring to the w2v dimension size</p>
<p>and the second 300 would be the max input length aka the same as the max length in the pad_sequences but what I don't get is.</p>
<p>If my dimensional size of my w2v model equals to let's say 240, does this mean my pad_sequences maxlen should also be 240 and also the input_length in my embedding_layer?</p>
<p>For example, the w2v size is 300 but my sentence length that I want to predict is 360 and my pad_sequences maxlen would be 500. Would my neural net be able to correctly predict this sentence of length 360?</p>
",,2020-06-28 17:53:54,Is the size in a Word2Vec model equal to the max input size of a sentence?,<python><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28129,62730537,2020-07-04 14:46:03,,"<p>I've installed gensim, however I keep getting an error when I try to import it
<code>from gensim.models import Word2Vec</code>
ImportError: cannot import name 'open'</p>
<p>I'm using the updated version of gensim 3.8.0 and smart_open 2.1.0.</p>
<p>I have reinstalled several times but still can't get it to work.</p>
",,2020-07-04 14:46:03,Importing gensim,<gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28147,62654478,2020-06-30 09:36:47,,"<p>Trying to create Dictionary from set of big files (expected to be). I use os.walk inside ReadFiles to walk all files inside directory and use ReadTxtFile to read each file. Got an error while use ReadFiles, and OK for one file (comment in code):</p>
<pre><code>import string
import copy
from nltk.corpus import stopwords
import pymorphy2
import os
from gensim import corpora
from smart_open import smart_open
from pathlib import Path

#INIT
path_to_text_file_directory = 'd:\\1'


#
stop_words = stopwords.words(&quot;russian&quot;)
morph = pymorphy2.MorphAnalyzer()

#lemmanization
def preprocess_text(text):
    symbtoremove = (copy.copy(string.punctuation)).replace('-','') + '@0123456789'
    words = text.split()
    table = text.maketrans('', '', symbtoremove)
    words = [w.translate(table) for w in words]    
    words_parse = [morph.parse(w)[0] for w in words if len(w)&gt;3 and w not in stop_words]
    words = [w.normal_form for w in words_parse if w.tag.POS == 'NOUN']    
    return words


class ReadTxtFile(object):
    def __init__(self, fname):
        self.fname = fname
        
    def __iter__(self):
        for line in open(self.fname, encoding='utf-8'):   
            yield  preprocess_text(line)                    

                
class ReadFiles(object):
    
    def __init__(self, dirname):
        self.dirname = dirname
        
    def __iter__(self):        
        tree = os.walk(path_to_text_file_directory)
        for address, dirs, files in os.walk(path_to_text_file_directory):
            for fname in files:
                if Path(fname).suffix == '.txt':                    
                     yield ReadTxtFile(os.path.join(address, fname))                 

#mydict = corpora.Dictionary(ReadTxtFile('d:\\1\\11-3.txt')) -- THIS OK!!!
mydict = corpora.Dictionary(ReadFiles(path_to_text_file_directory)) #THIS GOT ERROR
</code></pre>
<p><strong>TypeError: decoding to str: need a bytes-like object, list found</strong></p>
",2020-06-30 12:31:56,2020-06-30 12:31:56,Iterator over iterator : Reading TXT-files. decoding to str: need a bytes-like object for TXT file,<python><dictionary><iterator><gensim><yield>,,,CC BY-SA 4.0,True,False,True,False,False
28155,62749877,2020-07-06 05:39:36,,"<p>I am trying to use word2vec in short text document clustering. However, I am not sure how the approach should be.</p>
<p>This is what I have currently done.</p>
<pre><code># Sample of dataset that has been preprocessed and tokenized
text = [['distinguish', 'external', 'internal', 'entity', 'swim_lane', 'actor'],
 ['collaboration', 'model'],
 ['understanding', 'difference', 'functional', 'process', 'orientation'],
 ['identifying', 'attribute', 'process'],
 ['business', 'process', 'model'],
 ['trying', 'think', 'term', 'people', 'process', 'technology'],
 ['putting', 'real', 'world', 'context', 'theory', 'imparted'],
 ['business', 'process', 'management']]

#train word2vec
from gensim.models import Word2Vec
model = Word2Vec(text, min_count = 1)

#word-vector dictionary
words = model.wv.vocab
word_vec = {}
for word in words:
    word_vec[word] = model[word]
word_vec
</code></pre>
<p>Because word2vec converts each word into a vector, how can I use the output of word2vec to represent a document which is then used as the input of the clustering algorithm?</p>
<p>Does taking the average of the list of vectors (of the words) in a document works?</p>
",,2020-07-06 05:39:36,Word2Vec in short text clustering,<cluster-analysis><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28164,62783726,2020-07-07 20:50:29,,"<p>I am new in NLU and I am doing a project on document embedding. I want to fine-tune the doc2vec model in gensim on my small dataset to see if it can help for document clustering. I read the tutorial on the website but they did not mention anything about fine-tuning. Where I can find doc2vec pertained on wikipedia or twitter on any big dataset.</p>
",,2020-07-07 20:50:29,Fine tune doc2vec on gensim,<nlp><gensim><nlu>,,,CC BY-SA 4.0,False,False,True,False,False
28168,62618064,2020-06-28 04:00:08,,"<p>I am trying to look up dictionary indices for thousands of strings and this process is very, very slow. There are package alternatives, like <code>KeyedVectors</code> from <code>gensim.models</code>, which does what I want to do in about a minute, but I want to do what the package does more manually and to have more control over what I am doing.</p>
<p>I have two objects: (1) a dictionary that contains key : values for word embeddings, and (2) my pandas dataframe with my strings that need to be transformed into the index value found for each word in object (1). Consider the code below -- is there any obvious improvement to speed or am I relegated to external packages?</p>
<p>I would have thought that key lookups in a dictionary would be blazing fast.</p>
<h1>Object 1</h1>
<pre><code>embeddings_dictionary = dict()
glove_file = open('glove.6B.200d.txt', encoding=&quot;utf8&quot;)
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
</code></pre>
<h1>Object 2 (The slowdown)</h1>
<pre><code>no_matches = []
glove_tokenized_data = []
for doc in df['body'][:5]:
    doc = doc.split()
    ints = []
    for word in doc:
        try:
    # the line below is the problem
            idx = list(embeddings_dictionary.keys()).index(word)
        except:
            idx = 400000  # unknown
            no_matches.append(word)
        ints.append(idx)
    glove_tokenized_data.append(ints)

</code></pre>
",,2020-06-28 04:52:59,Is there a faster way to lookup dictionary indices?,<python><dictionary><nlp>,,,CC BY-SA 4.0,False,False,True,False,False
28173,62766787,2020-07-07 01:30:18,,"<p>First, apologies for being long-winded.</p>
<p>I'm not a mathematician, so I'm hoping there's a &quot;dumbed down&quot; solution to this. In short, I'm attempting to compare two bodies of text to generate recommendations. What you'll see below is a novice attempt at measuring similarity using NLP. I'm open to all feedback. But my primary question: does the method described below serve as an accurate means of finding similarities (in wording, sentiment, etc) in two bodies of text? If not, how would you generate such a recommendation engine (new methods, new data, etc)?</p>
<p>I currently have two dictionaries one with personality data called <strong>personality_feature_dict</strong> that includes the personality type and associated descriptor words: <code>{'Type 1': ['able', 'accepting', 'according', 'accountable'...]}</code> and the other called <strong>book_feature_dict</strong> containing book titles and their own descriptor words, which were extracted using TF-IDF <code>{'Book Title': ['actually', 'administration', 'age', 'allow', 'anti'...]}</code></p>
<p>As it stands, I'm using the following code to calculate the similarity between dictionary values from each to identify % similarity. First, I create a larger corpus using all dictionary items.</p>
<pre><code>book_values = list(book_feature_dict.values())
personality_values = list(personality_feature_dict.values()) 

texts = book_values + personality_values

dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

import numpy as np
np.random.seed(1)
</code></pre>
<p>Then I create an LDA model to identify similarities. My knowledge of LDA modeling is limited, so if you spot an error here, I appreciate you flagging it!</p>
<pre><code>from gensim.models import ldamodel
model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=4, minimum_probability=1e-8)
</code></pre>
<p>Finally, I iterate through sets of values as bags of words and compare how the first personality type or <code>(personality_feature_dict.values())[personality_num]</code> compares to 99 book descriptions/values by finding the Hellinger distance.</p>
<pre><code>from gensim.matutils import hellinger
personality_num = 0
i = 0

while i &lt; 98:

    s_0 = list(book_feature_dict.values())[i]
    s_0_bow = model.id2word.doc2bow(s_0)
    s_0_lda_bow = model[s_0_bow]

    e_0 = list(personality_feature_dict.values())[personality_num]
    e_0_bow = model.id2word.doc2bow(e_0)
    e_0_lda_bow = model[e_0_bow]

    x = 100 - (hellinger(e_0_lda_bow, s_0_lda_bow)*100)
    i = i+1

</code></pre>
<p>Finally, I print all instances where the LDA model comes back with a high correlation as a percentage.</p>
<pre><code>    if x &gt; 50:
        print (list(personality_feature_dict.keys())[personality_num])
        print('similarity to ', (list(book_feature_dict.keys())[i]), 'is')
        print(x, '%', '\n\n')

</code></pre>
<p>The result looks something like this:</p>
<pre><code>Personality Type 
similarity to  Name of Book 1 is
84.6029228744518 % 


Personality Type 
similarity to  Name of Book 2 is
83.09513184950528 % 


Personality Type 
similarity to  Name of Book 3 is
85.44322295890642 % 

...
</code></pre>
",,2020-09-30 08:49:45,Is there a Python library or tool that analyzes two bodies of text for similarities in order to provide recommendations?,<python><python-3.x><nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28192,62802812,2020-07-08 20:05:57,,"<p>I am trying to remove stopwords during an NLP pre-processing step. I use the <code>remove_stopwords()</code> function from <code>gensim</code> but would also like to add my own stopwords</p>
<pre><code># under this method, these custom stopwords still show up after processing
custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;]
data_text['text'].apply(lambda x: [item for item in x if item not in custom_stops])
# remove stopwords with gensim
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
# split the sentences into a list
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
</code></pre>
",,2020-07-08 20:19:22,Remove custom stopwords,<python><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28193,62803770,2020-07-08 21:17:28,,"<p>I'm trying to find the most similar documents to a new document. The doc2vec model was trained first, and now I'm introducing a new document; I've inferred the vector for the new document, but I don't know the ins and outs of doc2vec well... If the new document has a lot of words (in a row) that the old model never encountered, how will it be handled?</p>
",2020-07-11 00:44:53,2020-07-11 00:44:53,How does Doc2Vec assess new words?,<new-operator><gensim><word2vec><word><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28201,62735947,2020-07-05 01:19:16,,"<p>I am new to the world of NLP. I am working on a word embedding program with gensim. It works with files of small size. If I try using a larger file that is several gigabytes, I get an error message saying that it cannot open the file after stalling for two hours.</p>
<p>I was looking into this, and it seems like I need to find a way to &quot;stream&quot; the file in order to read it into the program but I am very confused how to do this.  I can't find any examples of word embedding with streaming files.</p>
<p>Also is this process going to be slow for terabytes of data? Thanks for your help!</p>
<pre><code>import urllib.request
import nltk
from gensim.models import Word2Vec


sample = open(&quot;file.txt&quot;, &quot;r&quot;) 
s = sample.read() 
data = s.replace(&quot;\n&quot;, &quot; &quot;)     
all_sentences = nltk.sent_tokenize(data)
all_words = [nltk.word_tokenize(sent) for sent in all_sentences]
word2vec = Word2Vec(all_words, min_count=2)
vocabulary = word2vec.wv.vocab

simword0 = &quot;taco&quot;
try:
  sim_words = word2vec.wv.most_similar(simword0)

except KeyError:
  print(simword0, &quot;not in vocabulary&quot;)
</code></pre>
",2020-07-05 05:09:48,2020-07-05 05:09:48,Word embedding with large files,<nlp><inputstreamreader>,,,CC BY-SA 4.0,True,False,True,False,False
28211,62737231,2020-07-05 05:37:05,,"<p>I have the approximately 20gigabyte text data which is separated by \n.</p>
<p>I want to read that text as list to put it in the gensim library's Word2vec analysis(As far as I know, only the list form of input is allowed for the gensim Word2vec process).</p>
<p>I tried to read the text data line by line and append to the empty list. But it keep caused Memory error.</p>
<p>Since according to what I found in searching, read line by line does not really damage to the memory usage, I suspect it could be because of the process of appending results to the list. But I don't know how can I solve this problem.</p>
<p>My computer's RAM size is 32gb. and I use ubuntu 20.04 LTS and python 3 with pycharm.</p>
<p>Is it fundamentally impossible to load the 20gb of text file as a list in my computer..? which means that I better to add more RAM size to the computer.</p>
<p>Also, I wonder If it would be better to read as a csv not as a text because I found that pandas have good tools to read huge file. (I tried with this approach but I failed to convert the text column to the list because of the memory issue)</p>
<p>This is the code I used. Thanks for any advice.</p>
<pre><code>##tried with text file
corpus = []
with open(&quot;textfile.txt&quot;, &quot;r&quot;) as f:
    for line in f:
        corpus.append(line.splitlines()[0])


##tried with dataframe
iter_csv = pd.read_csv(&quot;textfile.csv&quot;, iterator=True, sep=',', chunksize=300, header=None
                       , usecols=[4], names=['text'],encoding='UTF-8')
df_forCount_tmp = pd.concat(iter_csv, axis=0, ignore_index=True) 
#memory error....
corpus = []
def convertTolist(text):
    return ast.literal_eval(text)

for dfi, df in df_forCount_tmp.iterrows():
    corpus.extend(convertTolist(df['text']))
</code></pre>
",,2020-07-05 05:37:05,Read 20gigabyte text as a list for the gensim Word2vec analysis,<python><list><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28221,62752484,2020-07-06 08:54:12,,"<p>I am really confused to see the results on the coherence scores using Python Gensim and R TexmineR packages.</p>
<p>I've trained both models for the same number of topics (from 5 to 15). Using the first one the average score is around 4.0, for the second one is not more than 2.0. So, for me the models coming from R are not to use as their coherence score is too low.</p>
<p>I would like to use R as I am more experienced in this language. But if I don't find a solution I will have to switch to Python.</p>
<p>Just wanted to know if you have any idea on how to solve this problem? Is it because gensim Python provide better quality algorithm?</p>
<p>Ps. I've tried training LDA in R with different parameters to make sure the source of the problem is not situated inthere.</p>
<p>Thank you in advance for your support and help.</p>
<p>All the best,
Evangelia</p>
",,2020-08-26 09:07:13,Why Python gensim package gives better coherence scores compared to TexmineR in R,<python><r><text-mining><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28227,62806942,2020-07-09 03:29:56,,"<p>Below is the code, I am executing. The error occurs on the 3rd line (<code>vectors.init_sims(True)</code>)</p>
<pre><code>fname = get_tmpfile(path_to_embedding_file)
vectors = KeyedVectors.load(fname, mmap='r')

vectors.init_sims(True)
</code></pre>
<p>This is the error stack-</p>
<pre><code>Traceback (most recent call last):
  File &quot;generate_pair_histograms.py&quot;, line 82, in &lt;module&gt;
    vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure
  File &quot;C:\users\janki\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 1354, in init_sims
    self.vectors_norm = _l2_norm(self.vectors, replace=replace)
  File &quot;C:\users\janki\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 2374, in _l2_norm
    m /= dist
ValueError: output array is read-only
</code></pre>
<p>Is this a known issue? Can someone provide a solution or workaround to this?</p>
",,2020-07-09 17:40:24,Setting gensim wordVectors init_sim property as True error - ValueError: output array is read-only,<nlp><gensim><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28241,62722755,2020-07-03 21:13:01,,"<p>I am new to topic recognition and I want to try it over questions in a dataframe using Latent Dirichelet Allocation following <a href=""https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc"" rel=""nofollow noreferrer"">this article</a>. The article asks to create a <code>corpus</code> over which it implements the LDA but whenever I try to create <code>corpus</code>, which seems to be a dataset of articles taken from BBCs website, I get empty lists. Indeed with</p>
<pre><code>from gensim import corpora, models

# list_of_list_of_tokens = [[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], [&quot;d&quot;,&quot;e&quot;,&quot;f&quot;]]
# [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;] are the tokens of document 1, [&quot;d&quot;,&quot;e&quot;,&quot;f&quot;] are the tokens of document 2...
dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)
dictionary_LDA.filter_extremes(no_below=3)
</code></pre>
<p>I get:</p>
<pre><code>&gt;&gt;&gt; corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]
&gt;&gt;&gt; corpus
[[], []]

&gt;&gt;&gt; list_of_list_of_tokens
[['a', 'b', 'c'], ['d', 'e', 'f']]
&gt;&gt;&gt; dictionary_LDA.doc2bow(['a', 'b', 'c'])
[]
</code></pre>
<p>So how can I create a the corpus to train my LDA on? How can I extract the topics on any given question (like this one for instance)?</p>
",,2020-07-03 21:13:01,How to create the corpus LDS will be trained on?,<python-3.x><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28260,62810179,2020-07-09 08:04:45,,"<p>I use Gensim to work with LDA models. My model works if I set the parameter <code>num_topics</code> to a small value (under 100 topics). If I increase the value to a larger (like 200 topics) number the model &quot;breaks&quot;, it is not returning proper results, only empty topics with all the same most important words and a 0.000 importance value.</p>
<p>Is someone having similar problems? What is happening?</p>
<p>For information: My dataset has around 7000 small text documents with a minimum length of 5 words and up to 500 words. I know that there are other options I should consider beside LDA, but I want to understand why the model isnt even delivering any results</p>
",,2020-07-09 08:04:45,Gensim LDA Model breaks,<nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28265,62742964,2020-07-05 15:49:26,,"<p>I am currently attempting to record and graph coherence scores for various topic number values in order to determine the number of topics that would be best for my corpus. After several trials using u_mass, the data proved to be inconclusive since the scores don't plateau around a specific topic number. I'm aware that CV ranges from -14 to 14 when using u_mass, however my values range from -2 to -1 and selecting an accurate topic number is not possible. Due to these issues, I attempted to use c_v instead of u_mass but I receive the following error:</p>
<pre><code>    An attempt has been made to start a new process before the
    current process has finished its bootstrapping phase.

    This probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:
</code></pre>
<p>This is my code for computing the coherence value</p>
<pre><code>cm = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary,coherence='c_v')
      print(&quot;THIS IS THE COHERENCE VALUE &quot;)
      coherence = cm.get_coherence()
      print(coherence)
</code></pre>
<p>If anyone could provide assistance in resolving my issues for either c_v or u_mass, it would be greatly appreciated! Thank you!</p>
",,2020-07-05 15:49:26,LDA: Coherence Values using u_mass v c_v,<machine-learning><gensim><lda><dirichlet>,,,CC BY-SA 4.0,False,False,True,False,False
28266,62743531,2020-07-05 16:39:07,,"<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
",,2020-07-06 06:45:22,Using Gensim Fasttext model with LSTM nn in keras,<keras><nlp><gensim><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
28287,62822914,2020-07-09 20:10:06,,"<p>I have completed a topic modeling LDA using gensim, and printed out each of the top words for the topics using  the code:</p>
<pre><code>LDA = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                       id2word=dictionary,
                                       num_topics=5, 
                                       random_state=100,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)
pprint(LDA.print_topics())
doc_lda = LDA[corpus]
</code></pre>
<p>This code was ran on an excel sheet I read in, specifically a column named &quot;text' (research['text]). Now I am curious how I could add a column in my excel sheet to put what topic number  the corresponding cell is apart of something like this:</p>
<pre><code>   Name     Text              Topic
   bob      walked the dog      2
   chris    with the cat        3
</code></pre>
",,2020-07-09 20:10:06,Topic Numbers (LDA-Gensim),<gensim><modeling><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28288,62823331,2020-07-09 20:38:52,,"<p>When using the pyLDAvis package as follows, inside my jupyter notebook,</p>
<pre><code>pyLDAvis.enable_notebook()

lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)
lda_tfidf.fit(dtm_tfidf)
pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)
</code></pre>
<p>The resulting plot autosizes the width of my jupyter notebook, making all of the other cells overlap with the boarder - I have tried:</p>
<pre><code>from IPython.core.display import display, HTML
display(HTML(&quot;&lt;style&gt;.container { width:95% !important; }&lt;/style&gt;&quot;))
</code></pre>
<p>with no luck, as well as</p>
<pre><code># Using gensim[![enter image description here][1]][1]
v = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)
pyLDAvis.display(v)
</code></pre>
<p>spending a while searching the docs.... There dosen't seem to be anything about this is the documentation, has anyone dug into the code before and can point me in the right direction?</p>
<p>There does not seem to be any other posts of this, but have checked versions etc... with no luck</p>
<p>An image of the overlap is shown below:</p>
<pre><code></code></pre>
<p>:<a href=""https://i.stack.imgur.com/3V0gD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3V0gD.png"" alt=""overlapimg"" /></a></p>
",2020-08-25 12:36:29,2020-09-25 12:50:14,LDA visualisation in Jupyter notebook,<python><nlp><jupyter-notebook><data-visualization><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,True
28310,62797525,2020-07-08 14:48:50,,"<p>I have a set of 20 small document which talks about a particular kind of issue (training data). Now i want to identify those docs out of 10K documents, which are talking about the same issue.</p>
<p>For the purpose i am using the doc2vec implementation:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
    
# Tokenize_and_stem is creating the tokens and stemming and returning the list
# documents_prb store the list of 20 docs
tagged_data = [TaggedDocument(words=tokenize_and_stem(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents_prb)]
max_epochs = 20
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
    
def doc2vec_score(s):
    s_list = tokenize_and_stem(s)
    v1 = model.infer_vector(s_list)
    similar_doc = model.docvecs.most_similar([v1])
    original_match = (X[int(similar_doc[0][0])])
    score = similar_doc[0][1]
    match = similar_doc[0][0]
    return score,match


final_data  = []

# df_ws is the list of 10K docs for which i want to find the similarity with above 20 docs
for index, row in df_ws.iterrows():
    print(row['processed_description'])
    data = (doc2vec_score(row['processed_description']))
    L1=list(data)
    L1.append(row['Number'])
    final_data.append(L1)
     
with open('file_cosine_d2v.csv','w',newline='') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['score','match','INC_NUMBER'])
    for row in final_data:
        csv_out.writerow(row)
</code></pre>
<p>But, I am facing the strange issue, the results are highly un-reliable (Score is 0.9 even if there is not a slightest match) and score is changing with great margin every time. I am running the <code>doc2vec_score</code> function. Can someone please help me what is wrong here ?</p>
",2020-07-09 00:53:04,2020-07-09 00:53:04,Why doc2vec is giving different and un-reliable results?,<machine-learning><nlp><gensim><similarity><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
28341,62841794,2020-07-10 20:25:10,,"<p>I have a list of text comments that are fed into a non-negative matrix factorization topic modelling program.</p>
<pre><code>import pandas as pd
import numpy as np
# load the data
import csv
with open('C:\\...\\comments.csv', newline='') as f:
    reader = csv.reader(f)
    next(reader) # skip header
    df = [tuple(row) for row in reader]

# set the number of topics 
total_topics = 3

# process the data
from nltk.tokenize import word_tokenize
from collections import defaultdict
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from gensim.parsing.preprocessing import remove_stopwords
from nltk.corpus import stopwords

data_text = pd.DataFrame(df,columns=['text'])
# remove stopwords and tokenize the text
custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;, &quot;stopword3&quot;]
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: [item for item in x if item.lower() not in custom_stops])
CORPUS = pd.DataFrame(data_text['filtered_text'])

# Remove empty strings
CORPUS.dropna(inplace=True)
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV
# lemmatize the text
for index,entry in enumerate(CORPUS['filtered_text']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    for word, tag in pos_tag(entry):
        # Below condition is to check for Stop words and consider only alphabets
        if word not in stopwords.words('english') and word.isalpha():
            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
    # The final processed set of words for each iteration will be stored in 'text_final'
    CORPUS.loc[index,'text_final'] = str(Final_words)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def build_feature_matrix(documents, feature_type='frequency'):
    feature_type = feature_type.lower().strip()  
    if feature_type == 'binary':
        vectorizer = CountVectorizer(binary=True, min_df=1,ngram_range=(1, 1))
    elif feature_type == 'frequency':
        vectorizer = CountVectorizer(binary=False, min_df=1,ngram_range=(1, 1))
    elif feature_type == 'tfidf':
        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 1))
    else:
        raise Exception(&quot;Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'&quot;)
    feature_matrix = vectorizer.fit_transform(documents).astype(float)
    return vectorizer, feature_matrix

# create a feature matrix
vectorizer, tfidf_matrix = build_feature_matrix(CORPUS['text_final'], feature_type='tfidf')
td_matrix = tfidf_matrix.transpose()
td_matrix = td_matrix.multiply(td_matrix &gt; 0)

from sklearn.decomposition import NMF
nmf = NMF(n_components=total_topics, random_state=42, alpha=.1, l1_ratio=.5)
nmf.fit(tfidf_matrix) 

def get_topics_terms_weights(weights, feature_names):
    feature_names = np.array(feature_names)
    sorted_indices = np.array([list(row[::-1]) 
                           for row 
                           in np.argsort(np.abs(weights))])
    sorted_weights = np.array([list(wt[index]) 
                               for wt, index 
                               in zip(weights,sorted_indices)])
    sorted_terms = np.array([list(feature_names[row]) 
                             for row 
                             in sorted_indices])
    
    topics = [np.vstack((terms.T, 
                     term_weights.T)).T 
              for terms, term_weights 
              in zip(sorted_terms, sorted_weights)]     
    return topics

def print_topics_udf(topics, total_topics=1,
                     weight_threshold=0.0001,
                     display_weights=False,
                     num_terms=None):
    
    for index in range(total_topics):
        topic = topics[index]
        topic = [(term, float(wt))
                 for term, wt in topic]
        topic = [(word, round(wt,2)) 
                 for word, wt in topic 
                 if abs(wt) &gt;= weight_threshold]
                     
        if display_weights:
            print( 'Topic #' +str(index+1)+' with weights')
            print (topic[:num_terms] if num_terms else topic)
        else:
            print ('Topic #'+str(index+1)+' without weights')
            tw = [term for term, wt in topic]
            print (tw[:num_terms] if num_terms else tw)
        print()

feature_names = vectorizer.get_feature_names()
weights = nmf.components_

topics = get_topics_terms_weights(weights, feature_names)
# print topics and weights
# print_topics_udf(topics=topics,total_topics=total_topics,num_terms=None,display_weights=False) 
# print topics with weights
# print_topics_udf(topics=topics,total_topics=total_topics,num_terms=None,display_weights=True) 

# display the topics
# this takes the top term from each group and assigns it as the topic theme
for index in range(0,total_topics):
        print(&quot;Topic&quot;,index+1,&quot;=&quot;,topics[index][0][0])
</code></pre>
<p>The example output may be something like:</p>
<pre><code>Topic 1 = problem
Topic 2 = software
Topic 3 = recommendation
</code></pre>
<p>How can I assign a specific comment from the file a specific topic? e.g., the comment &quot;My computer has an issue of turning off intermittently&quot; would be mapped to Topic 1 &quot;problem&quot;</p>
",,2020-07-16 12:19:22,Assign Topic from NNMF Topic Modelling,<python><nlp><nmf>,,,CC BY-SA 4.0,True,False,True,False,True
28345,62827849,2020-07-10 05:29:48,,"<p>I know the <code>most_similar</code> method works when entering a previously added string, but how do you <em>reverse</em> search a numpy array of some word?</p>
<pre class=""lang-py prettyprint-override""><code>modelw2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)
differenceArr = modelw2v[&quot;King&quot;] - modelw2v[&quot;Queen&quot;]


# This line does not work
modelw2v.most_similar(differenceArr) 
</code></pre>
",2020-07-10 07:25:10,2020-07-10 08:02:11,How to find most similar to an array in gensim,<python-3.x><nlp><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28350,62801052,2020-07-08 18:10:33,,"<p>I'm training a <code>Doc2Vec</code> model using the below code, where <code>tagged_data</code> is a list of <code>TaggedDocument</code> instances I set up before:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40

model = Doc2Vec(alpha=0.025, 
                min_alpha=0.001)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.001
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>When I later check the model results, they're not good. What might have gone wrong?</p>
",2020-07-10 19:16:59,2020-08-28 06:34:28,"My Doc2Vec code, after many loops of training, isn't giving good results. What might be wrong?",<gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28353,62686729,2020-07-01 23:26:02,,"<p>I have the same problem as <a href=""https://stackoverflow.com/questions/50214899/indexerror-when-trying-to-update-gensims-ldamodel"">this poster</a>, but unfortunately the accepted answer doesn't work. Is there a new solution to this problem?</p>
",,2020-07-01 23:26:02,Index error while updating gensim LDA model,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28370,62880636,2020-07-13 16:56:50,,"<p>While working on sentiment analysis of twitter data, I encountered a problem that I just can't solve. I wanted to train a RandomForest Classifier to detect hate speech. I, therefore, used a labeled dataset with tweets that are labeled as 1 for hate speech and 0 for normal tweets. For vectorization, I am using Word2Vec. I first performed a hyperparametrization to find good parameters for the classifier.
During hyperparametrization I used a repeated stratified KFold cross-validation (scoring = accuracy)
Mean accuracy is about 99.6% here. However, once I apply the model to a test dataset and plot a confusion matrix, the accuracy is merely above 50%, which is of course awful for a binary classifier.
I successfully use the exact same approach with Bag of Words and had no problems at all here.
Could someone maybe have a quick look at my code? That would be so helpful. I just cannot find what is wrong. Thank you so much!</p>
<p>(I also uploaded the code to google collab in case that is easier for you: <a href=""https://i.stack.imgur.com/UY5iT.png"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/15BzElijL3vwa_6DnLicxRvcs4SPDZbpe?usp=sharing</a> )</p>
<p>First I preprocessed my data:</p>
<pre><code>train_csv = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/MLDA_project/data2/train.csv')
train = train_csv     
#check for missing values (result shows that there are no missing values)
train.isna().sum()    
# remove the tweet IDs
train.drop(train.columns[0], axis = &quot;columns&quot;, inplace = True)    
# create a new column to save the cleansed tweets
train['training_tweet'] = np.nan

# remove special/unknown characters
train.replace('[^a-zA-Z#]', ' ', inplace = True, regex = True)    
# generate stopword list and add the twitter handles &quot;user&quot; to the stopword list
stopwords = sw.words('english')
stopwords.append('user')    
# convert to lowercase
train = train.applymap(lambda i:i.lower() if type(i) == str else i)    
# execute tokenization and lemmatization
lemmatizer = WordNetLemmatizer()

for i in range(len(train.index)):
    #tokenize the tweets from the column &quot;tweet&quot;
    words = nltk.word_tokenize(train.iloc[i, 1])
    #consider words with more than 3 characters
    words = [word for word in words if len(word) &gt; 3] 
    #exclude words in stopword list
    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)] 
    #Join words again
    train.iloc[i, 2]  = ' '.join(words)  
    words = nltk.word_tokenize(train.iloc[i, 2])
train.drop(train.columns[1], axis = &quot;columns&quot;, inplace = True)

majority = train[train.label == 0]
minority = train[train.label == 1]
# upsample minority class
minority_upsampled = resample(minority, replace = True, n_samples = len(majority))      
# combine majority class with upsampled minority class
train_upsampled = pd.concat([majority, minority_upsampled])
train = train_upsampled
np.random.seed(10)
train = train.sample(frac = 1)
train = train.reset_index(drop = True)
</code></pre>
<p>Now <code>train</code> has the labels in column 0 and the preprocessed tweets in column 1.</p>
<p>Next I defined the Word2Vec Vectorizer:</p>
<pre><code>def W2Vvectorize(X_train):
tokenize=X_train.apply(lambda x: x.split())
w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(X_train), epochs=20)
w2v_words = list(w2vec_model.wv.vocab)
vector=[]
from tqdm import tqdm
for sent in tqdm(tokenize):
    sent_vec=np.zeros(100)
    count =0
    for word in sent: 
        if word in w2v_words:
            vec = w2vec_model.wv[word]
            sent_vec += vec 
            count += 1
    if count != 0:
        sent_vec /= count #normalize
    vector.append(sent_vec)
return vector
</code></pre>
<p>I split the dataset into test and training set and vectorized both subsets using W2V as defined above:</p>
<pre><code>x = train[&quot;training_tweet&quot;]
y = train[&quot;label&quot;]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=train['label'])

print('X Train Shape = total * 0,8 =', X_train.shape)
print('y Train Shape = total * 0,8 =', y_train.shape)
print('X Test Shape = total * 0,2 =', X_test.shape)
print('y Test Shape = total * 0,2 =', y_test.shape) # change 0,4 &amp; 0,6

train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>Now I carry out the hyperparametrization:</p>
<pre><code># define models and parameters
model = RandomForestClassifier()
n_estimators = [10, 100, 1000]
max_features = ['sqrt', 'log2']
# define grid search
grid = dict(n_estimators=n_estimators,max_features=max_features)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf_w2v, y_train)
# summarize results
print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))
</code></pre>
<p>This results in the following output:</p>
<pre><code>Best: 0.996628 using {'max_features': 'log2', 'n_estimators': 1000}
0.995261 (0.000990) with: {'max_features': 'sqrt', 'n_estimators': 10}
0.996110 (0.000754) with: {'max_features': 'sqrt', 'n_estimators': 100}
0.996081 (0.000853) with: {'max_features': 'sqrt', 'n_estimators': 1000}
0.995885 (0.000872) with: {'max_features': 'log2', 'n_estimators': 10}
0.996481 (0.000691) with: {'max_features': 'log2', 'n_estimators': 100}
0.996628 (0.000782) with: {'max_features': 'log2', 'n_estimators': 1000}
</code></pre>
<p>Next, I wanted to draw a confusion matrix with the test data using the Model:</p>
<pre><code>clf = RandomForestClassifier(max_features = 'log2', n_estimators=1000) 
   
clf.fit(train_tf_w2v, y_train)
name = clf.__class__.__name__
        
expectation = y_test
test_prediction = clf.predict(test_tf_w2v)
acc = accuracy_score(expectation, test_prediction)   
pre = precision_score(expectation, test_prediction)
rec = recall_score(expectation, test_prediction)
f1 = f1_score(expectation, test_prediction)

fig, ax = plt.subplots(1,2, figsize=(14,4))
plt.suptitle(f'{name} \n', fontsize = 18)
plt.subplots_adjust(top = 0.8)
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, ax=ax[0])
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, normalize=True, ax = ax[1])
plt.show()
    
print(f&quot;for the {name} we receive the following values:&quot;)
print(&quot;Accuracy: {:.3%}&quot;.format(acc))
print('Precision score: {:.3%}'.format(pre))
print('Recall score: {:.3%}'.format(rec))
print('F1 score: {:.3%}'.format(f1))
</code></pre>
<p>This outputs:</p>
<p><a href=""https://i.stack.imgur.com/UY5iT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UY5iT.png"" alt=""confusion matrix"" /></a></p>
<p>for the RandomForestClassifier we receive the following values:
Accuracy: 57.974%
Precision score: 99.790%
Recall score: 15.983%
F1 score: 27.552%</p>
",2020-07-15 15:35:19,2020-07-15 15:35:19,Word2Vec - Model with high cross validation score performs incredibly bad for test data,<machine-learning><cross-validation><word2vec><sentiment-analysis>,,,CC BY-SA 4.0,True,False,True,False,False
28384,62861346,2020-07-12 12:54:23,,"<p>When I do the below:</p>
<pre><code>&gt;&gt;&gt; import gensim.downloader as api
&gt;&gt;&gt; model = api.load(&quot;glove-twitter-25&quot;)  # load glove vectors
</code></pre>
<p>the gensim.downloader API throws the below error:</p>
<blockquote>
<p>[Errno 2] No such file or directory:
'/Users/vtim/gensim-data/information.json'.</p>
</blockquote>
<p>What am I doing wrong?</p>
",2020-07-12 20:20:42,2020-08-11 16:13:40,Why can't I download a dataset with the Gensim download API,<python><download><dataset><gensim><glove>,,,CC BY-SA 4.0,False,False,True,False,False
28398,62831166,2020-07-10 09:22:47,,"<p>I'm working with HDPmodel of gensim, but it has some parameters but, please, can anybody explain me what exactly the value of &quot;K&quot; means? I know that T is the number of topics wanted but in that case what is &quot;K&quot;? I don't understand the &quot;second level truncation level&quot; phrase. If you can give me an example it could be awesome. Thanks</p>
",,2020-07-10 09:22:47,"What's the meaning of ""K"" in HDPmodel from gensim?",<python><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28432,62966641,2020-07-18 09:00:27,,"<p>I'm currently trying to develop a program in python that uses NLP to predict part types. In my organization, each part has a Part Number (12 digit number) and a Part description. I want my program to be able to predict what type of part it is based on these two fields (Part number and description). The 5th and 6th digit of part number indicates the design group, so I can narrow down the part type. For ex. if 5th and 6th digit is 42 I can estimate that the part belongs to Brakes group.
I have a list of part descriptions (~25000 parts) and part types (~350 nos.). I used NLTK in Python to match strings but it is isn't giving good results. Any ideas on how I can improve the matching?
Mine is a manufacturing company with several types of parts (brackets, bolts, nuts, hoses, brakes, etc.) and the part descriptions contain these strings (or their abbreviations e.g. SCR for screws, BKT for bracket) either at the start, end or somewhere in the middle of the whole description string.</p>
<p>Any ideas as to how I should proceed with this problem (approach, algo, etc.)? Any suggestions are welcome.</p>
<p>I have followed the logic described in this page (<a href=""https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp"" rel=""nofollow noreferrer"">https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp</a>)
code below:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from nltk.corpus import stopwords
import numpy as np

stopwords = set(stopwords.words(&quot;english&quot;))

def listtostring(clean_line):
    str1 = &quot; &quot;
    return(str1.join(clean_line))

#using Lemmatization
lemma = nltk.wordnet.WordNetLemmatizer()

file_docs = []
test_doc = []
test_line = []

with open('E:\JS\Py\Text_similarity\Idea_list.csv', 'r') as inp, open('E:\JS\Py\Text_similarity\Idea_List.txt', 'w') as out:
    for line in inp:
        out.write(line + '.\n')

with open ('E:\JS\Py\Text_similarity\Idea_List.txt') as f:
    tokens = sent_tokenize(f.read())
    #wtokens = word_tokenize(f.read())
    for line in tokens:
        file_docs.append(line)
        

# create a clean document after removing the stop-words and generate lemmas
clean_doc = []
for line in file_docs:
    clean_line = [word for word in line.split() if word not in stopwords]
    
    for word in line.split():
        test_line.append(lemma.lemmatize(word))
    test_doc.append(listtostring(test_line))
    test_line = []
    
    #clean_doc.append(listtostring(clean_line))
    clean_doc = test_doc

print(&quot;Number of documents:&quot;,len(clean_doc))

gen_docs = [[w.lower() for w in word_tokenize(text)] 
            for text in clean_doc]


dictionary = gensim.corpora.Dictionary(gen_docs)
corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)

# building the index
sims = gensim.similarities.Similarity('E:/JS/Py/Text_similarity/',tf_idf[corpus],
                                        num_features=len(dictionary))

file2_docs = []
#test_doc = []
#test_line = []

with open('E:\JS\Py\Text_similarity\Idea_query.csv', 'r') as inp, open('E:\JS\Py\Text_similarity\Idea_query.txt', 'w') as out:
    for line in inp:
        out.write(line + '.\n')

with open ('E:\JS\Py\Text_similarity\Idea_query.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)
        
test_doc = []
clean_doc2 = []
for line in file2_docs:
    clean_line = [word for word in line.split() if word not in stopwords]
    
    for word in line.split():
        test_line.append(lemma.lemmatize(word))
    test_doc.append(listtostring(test_line))
    test_line = []
    
    #clean_doc.append(listtostring(clean_line))
    clean_doc2 = test_doc

print(&quot;Number of documents:&quot;,len(clean_doc2))  
for line in clean_doc2:
    query_doc = [w.lower() for w in word_tokenize(line)]
    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and create bag of words
        
    # building the index
    sims = gensim.similarities.Similarity('E:/JS/Py/Text_similarity/',tf_idf[corpus],
                                            num_features=len(dictionary))

    # perform a similarity query against the corpus
    query_doc_tf_idf = tf_idf[query_doc_bow]

    #print(&quot;\n&quot;,clean_doc2,&quot;\n&quot;)

    filter_result = filter(lambda x: x[1]&gt;0.5, enumerate(sims[query_doc_tf_idf]))
    final_result = sorted(filter_result, key=lambda x: x[1], reverse=True)
  
    #print(final_result)
    #print matching entries with similarity scores
    for s in final_result:
        print(s[0], round(s[1]*100,2), &quot;: &quot; + file_docs[s[0]])
</code></pre>
<p>Hope this clarifies a bit.</p>
",2020-07-18 19:21:39,2020-07-18 19:21:39,How to predict the part type based on combination of a part number and part description using Python?,<python><python-3.x><nlp><nltk><prediction>,,,CC BY-SA 4.0,True,False,True,False,False
28439,62903275,2020-07-14 20:11:24,,"<p>I have a word2vec model trained on Tweets. I also have a list of words, and I need to get the embeddings from the words, compute the first two principal components, and plot each word on a 2 dimensional space.</p>
<p>I'm trying to follow tutorials such as this one: <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-embeddings-python-gensim/</a></p>
<p>However in all such tutorials, they create a model based on a random sentence they use and then calculate PCA on all the words in the model. I don't want to do that, I only want to calculate and plot specific words. How can I use the model that I already have, which has thousands of words, and compute the first two principal components for a set list of words I have (around 20)?</p>
<p>So like in the link above, they have &quot;model&quot; with only the words from the sentence they wrote. And then they do &quot;X = model[model.wv.vocab]&quot;, then &quot;pca.fit_transform(X)&quot;. If I were to copy this code, I would do a PCA on the huge model, which I don't want to do. I just want to extract the embeddings of some words from that model and then compute PCA on those few words. Hopefully this makes sense, thanks in advance. Please let me know if I need to clarify anything.</p>
",,2020-07-14 20:33:02,PCA on word2vec embeddings using pre existing model,<python><nlp><jupyter-notebook><pca><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28454,62939869,2020-07-16 17:02:29,,"<p>The word embeddings when training with Gensim are meaningless: the similarities on known relationships (King - Man + Woman -&gt; Queen) and others simply don't hold (not even close), they might as well be random.</p>
<p>Using the same training data and equivalent parameters, I get meaningful results with Facebook's FastText. In comparison, I've tried the Gensim FastText and Word2Vec classes, both returning meaningless embeddings.</p>
<p>To narrow things down, here are some of the settings:</p>
<ul>
<li>30GB training data text file. Space separated tokens, newline separated sentences.</li>
<li>Final vocab size after training: 12M+ word vectors</li>
<li>Dimension = 100</li>
<li>Disabled subwords</li>
<li>Disabled n-grams</li>
<li>Negative sampling = 8</li>
<li>Skip-gram</li>
<li>Min-count = 20</li>
</ul>
<p>Training code below:</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;Setting Model&quot;)
model = FastText(
# model = Word2Vec(
  sg=1,
  size=dim, 
  max_vocab_size=max_vocab_size,
  window=10, 
  negative=8,
  min_count=20,
  max_n=0,
  min_n=0,
  workers=16,
  alpha=0.1,
)
print(&quot;Building vocab&quot;)
model.build_vocab(
  corpus_file=corpus_file,
  progress_per=1_000_000
)
print(&quot;Training model&quot;)
model.train(
  corpus_file=corpus_file,
  total_words=model.corpus_total_words,
  total_examples=model.corpus_count,
  epochs=5,
  word_ngrams=0,
)
print(&quot;Saving model&quot;)
model.save(model_name)
</code></pre>
<p>I would rather avoid using Facebook's FastText as it would require me changing its source code to achieve this vocab size.</p>
<p>What could be going wrong, or how can I debug this? I have been able to import Facebook's FastText vectors into Gensim and it works fine.</p>
",2020-07-16 18:19:04,2020-07-16 18:19:04,Gensim training meaningless word embeddings,<nlp><gensim><embedding><word-embedding><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
28463,62970416,2020-07-18 15:35:12,,"<p>I am working on a project which groups jobs posted on various job portals into clusters based on the description of the jobs using K-means.</p>
<p>I found the work vector using Word2Vec, but i guess this will not serve the purpose as I will need a vector of the whole job description.</p>
<p>I know that I can average out the word vector of a sentence to get the sentence vector but worried about the accuracy as this will loose the ordering of the words.</p>
<p>Is there any other way I can get the vectors ?</p>
",,2020-07-20 00:57:27,Get sentence vector for a K-means clustering task,<machine-learning><nlp><vectorization><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28482,62941476,2020-07-16 18:39:05,,"<p>I'm trying to use Doc2Vec to go through the classic exercise of training on Wikipedia articles, using the article title as the tag.</p>
<p>Here's my code and the results, is there something that I'm missing that they would not give the matching results with most_similar? Following <a href=""https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html"" rel=""nofollow noreferrer"">this tutorial</a>, but I used the wiki-english-20171001 dataset that came with gensim.</p>
<pre><code>import gensim.downloader as api
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import re

def cleanText(text):
    text = re.sub(r'\|\|\|', r' ', text)
    text = re.sub(r'http\S+', r'&lt;URL&gt;', text)
    text = text.lower()
    text = re.sub(r'[^\w\s]','',text)
    return text


wiki = api.load(&quot;wiki-english-20171001&quot;)

data = [d for d in wiki]

for i in range(10):
    print(data[i])
def my_create_tagged_docs(data):
    for wikiidx in range(len(data)):
        yield TaggedDocument([i for i in data[wikiidx].get('section_texts') for i in cleanText(i).split()], [data[wikiidx].get('title')])


wiki_data = my_create_tagged_docs(data)
del data
del wiki


model = Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter =10, epochs=40)
model.build_vocab(wiki_data)

model.train(wiki_data, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<pre><code>model.docvecs.most_similar(positive=[&quot;Lady Gaga&quot;], topn=10)
</code></pre>
<pre><code>[('Chlorothrix', 0.35521823167800903),
 (&quot;A Child's Garden of Verses&quot;, 0.3533579707145691),
 ('Fish Mooney', 0.35129639506340027),
 ('2000 ParisRoubaix', 0.3463437855243683),
 ('Calvin C. Chaffee', 0.3439667224884033),
 ('Murders of Eve Stratford and Lynne Weedon', 0.3397218585014343),
 ('Black Air', 0.3396576941013336),
 ('Turzyn', 0.3312540054321289),
 ('Scott Baker', 0.33018186688423157),
 ('Amongst the Waves', 0.3297169804573059)]
</code></pre>
<pre><code>model.docvecs.most_similar(positive=[&quot;Machine learning&quot;], topn=10)
</code></pre>
<pre><code>[('Wolf Rock, Connecticut', 0.3855834901332855),
 ('Amlia Rodrigues', 0.3349645137786865),
 ('Victoria Park, Leicester', 0.33312514424324036),
 ('List of visual anthropology films', 0.3311382532119751),
 ('Sadqay Teri Mout Tun', 0.3287636637687683),
 ('T. Damodaran', 0.32876330614089966),
 ('Urqu Jawira (Aroma)', 0.32281631231307983),
 ('Tiggy Wiggy', 0.3226730227470398),
 ('Frdric Brun (cyclist, born 1988)', 0.32106447219848633),
 ('Unholy Crusade', 0.3200794756412506)]
</code></pre>
",,2020-07-16 19:04:06,Doc2Vec not providing adequate results in most_similar,<python><gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28506,62910645,2020-07-15 08:27:31,,"<p>Need assistance regarding the Principal components to be displayed in pyLDAvis. It shows PC1 and PC2 by default, however I am interested in exploring other components as well.</p>
<p>I referred this <a href=""https://pyldavis.readthedocs.io/en/latest/modules/API.html"" rel=""nofollow noreferrer"">documentation</a> but the specified option <code>plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}</code> only changes plot labels.</p>
<p>I am using pyLDAvis through gensim:</p>
<pre><code>pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
</code></pre>
",2020-07-17 15:14:38,2020-08-10 19:40:30,pyLDAvis arguments for specifying principal components,<python-3.x><gensim><lda><topic-modeling><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,False
28522,63025899,2020-07-22 02:30:55,,"<p>I am currently using Gensim LDA for topic modeling.</p>
<p>While Tuning hyper-parameters I found out that the model always gives negative log-perplexity</p>
<p>Is it normal for model to behave like this?? (is it even possible?)</p>
<p>if it is, is smaller perplexity better than bigger one? (-100 is better than -20??)</p>
",,2020-08-29 00:16:03,Gensim lda gives negative log-perplexity value - is it normal and how can i interpret it?,<gensim><lda><perplexity>,,,CC BY-SA 4.0,False,False,True,False,False
28525,62993607,2020-07-20 10:39:17,,"<p>i am trying to vectorize a text and classify it by using gensim and tensorflow.keras.</p>
<p>Before the train I have shapes as follows:</p>
<pre><code>
X_train, y_train (1019471, 100, 1) (1019471, 5) 
X_validate, y_validate (127419, 100, 1) (127419, 5) 
X_test, y_test (127476, 100, 1) (127476, 5)

</code></pre>
<pre><code>    for function in functionSource:
        func_list.append([str(preprocess_text(function))])
    return func_list
</code></pre>
<pre><code>def preprocess_text(sen):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r&quot;\s+[a-zA-Z]\s+&quot;, ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence
</code></pre>
<pre><code>    embedding_dim = 100
    model = Word2Vec(func_ist, min_count=1,size= embedding_dim, workers=6, window =3, sg = 1)
</code></pre>
<p>After the train I have the following shapes:</p>
<pre><code>X_train, y_train (1018222, 100, 1) (1019471, 5) 
X_validate, y_validate (127398, 100, 1) (127419, 5) 
X_test, y_test (127461, 100, 1) (127476, 5)
</code></pre>
<p><strong>logging for gensim</strong></p>
<pre><code>INFO: 2020-07-20 12:44:54,936 - word2vec.py:1596 - scan_vocab : collected 1018222 word types from a corpus of 1019471 raw words and 1019471 sentences
INFO: 2020-07-20 12:44:54,936 - word2vec.py:1647 - prepare_vocab : Loading a fresh vocabulary
INFO: 2020-07-20 12:44:56,790 - word2vec.py:1671 - prepare_vocab : effective_min_count=1 retains 1018222 unique words (100% of original 1018222, drops 0)
INFO: 2020-07-20 12:44:56,790 - word2vec.py:1677 - prepare_vocab : effective_min_count=1 leaves 1019471 word corpus (100% of original 1019471, drops 0)
INFO: 2020-07-20 12:44:58,912 - word2vec.py:1736 - prepare_vocab : deleting the raw counts dictionary of 1018222 items
INFO: 2020-07-20 12:44:58,930 - word2vec.py:1739 - prepare_vocab : sample=0.001 downsamples 0 most-common words
INFO: 2020-07-20 12:44:58,930 - word2vec.py:1742 - prepare_vocab : downsampling leaves estimated 1019471 word corpus (100.0% of prior 1019471)
INFO: 2020-07-20 12:45:00,543 - base_any2vec.py:1022 - estimate_memory : estimated required memory for 1018222 words and 100 dimensions: 1323688600 bytes
INFO: 2020-07-20 12:45:00,543 - word2vec.py:1888 - reset_weights : resetting layer weights
</code></pre>
<p>I couldn't understand the part that my dataset losing some of the text, can anyone explain to me the reason why the shape gets smaller?</p>
<p>i am trying to learn about the topic. i would be grateful if anyone can explain it</p>
",,2020-07-20 18:10:25,Gensim vector shape changing,<python><nlp><gensim><word2vec><shapes>,,,CC BY-SA 4.0,False,False,True,False,False
28567,63062583,2020-07-23 20:24:38,,"<p>Now, from the outputs, I am unable to understand the embeddings or how these embeddings are changing or will change with new data.</p>
<p>Is this way correct for solving the problem statement and if so how can I optimize this to find the best embeddings for my dataset. Can anyone provide any suggestions on the same</p>
",2020-07-29 02:45:24,2020-07-29 02:45:24,Building a recommendation system using word2vec,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28594,63079854,2020-07-24 18:55:28,,"<p>I have extracted about 40MB of the English wikipedia into plain text. I would to use it to build a word2vec model with gensim. To do this I need to split it into sentences first. How can I do this?  I tried:</p>
<pre><code>from __future__ import unicode_literals, print_function
import spacy
from spacy.lang.en import English 
nlp = spacy.load('en_core_web_sm')
nlp.max_length = 47084146
ftest = open(&quot;test_02&quot;, &quot;r&quot;)
raw_test = ftest.read().replace(&quot;\n&quot;, &quot; &quot;)
sentences = [i for i in nlp(raw_test).sents] 

f = open(&quot;sentences.txt&quot;, &quot;w&quot;)

for sent in sentences:
    f.write(str(sent)+&quot;\n&quot;)
f.write(&quot;\n&quot;)
f.close()
</code></pre>
<p>But this fails with: <code>MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32</code></p>
<p>I have no idea why it wants to use so much RAM!</p>
<p>How can I do this?</p>
<hr />
<pre><code>Traceback (most recent call last):
  File &quot;../../processwiki.py&quot;, line 8, in &lt;module&gt;
    sentences = [i for i in nlp(raw_test).sents] 
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/language.py&quot;, line 449, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File &quot;nn_parser.pyx&quot;, line 233, in spacy.syntax.nn_parser.Parser.__call__
  File &quot;nn_parser.pyx&quot;, line 274, in spacy.syntax.nn_parser.Parser.predict
  File &quot;nn_parser.pyx&quot;, line 287, in spacy.syntax.nn_parser.Parser.greedy_parse
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 167, in __call__
    return self.predict(x)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 131, in predict
    y, _ = self.begin_update(X, drop=None)
  File &quot;_parser_model.pyx&quot;, line 243, in spacy.syntax._parser_model.ParserModel.begin_update
  File &quot;_parser_model.pyx&quot;, line 300, in spacy.syntax._parser_model.ParserStepModel.__init__
  File &quot;_parser_model.pyx&quot;, line 425, in spacy.syntax._parser_model.precompute_hiddens.__init__
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 183, in begin_update
    Yf = self._add_padding(Yf)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 214, in _add_padding
    Yf_padded = self.ops.xp.vstack((self.pad, Yf))
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in vstack
  File &quot;/mnt/storage/software/languages/anaconda/Anaconda3-2020.02-tflow-2.2.0/lib/python3.7/site-packages/numpy/core/shape_base.py&quot;, line 283, in vstack
    return _nx.concatenate(arrs, 0)
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in concatenate
MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32
</code></pre>
",2020-07-24 19:08:11,2020-07-24 20:35:24,How to split a text file into sentences for word2vec/gensim,<python><spacy><gensim>,,,CC BY-SA 4.0,False,True,True,False,False
28600,63095512,2020-07-26 02:30:44,,"<p>I am trying to assess a doc2vec model based on the code from <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#assessing-the-model"" rel=""nofollow noreferrer"">here</a>. Basically, I want to know the percentual of  inferred documents are found to be most similar to itself. This is my current code an:</p>
<pre><code>    for doc_id, doc in enumerate(cur.execute('SELECT Text FROM Patents')):
        docs += 1
        doc = clean_text(doc)
        inferred_vector = model.infer_vector(doc)
        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
        rank = [docid for docid, sim in sims].index(doc_id)
        ranks.append(rank) 

    counter = collections.Counter(ranks)
    accuracy = counter[0] / docs
</code></pre>
<p>This code works perfectly with smaller datasets. However, since I have a huge file with millions of documents, this code becomes too slow, it would take months to compute. I profiled my code and most of the time is consumed by the following line: <code>sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))</code>.</p>
<p>If I am not mistaken, this is having to measure each document to every other document. I think computation time might be massively reduced if I change this to <code>topn=1</code> instead since the only thing I want to know is if the most similar document is itself or not. Doing this will basically take each doc (i.e., <code>inferred_vector</code>), measure its most similar document (i.e., <code>topn=1</code>), and then I just see if it is itself or not. How could I implement this? Any help or idea is welcome.</p>
",,2020-07-26 18:44:23,Assessing doc2vec accuracy,<gensim><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28603,63096897,2020-07-26 06:35:13,,"<p>i need to create bigrams, trigrams and barcharts using gensim. I have tried this using nltk ngrams. But need to replicate the same using gensim.</p>
<p>Example = ['I have completed the all the courses and passed in distinction']</p>
<pre><code>import nltk
from nltk import ngrams
nltk.download('wordnet')
import matplotlib.pyplot as plt

def basic_clean(text):
    wnl = nltk.stem.WordNetLemmatizer()
    stopwords = nltk.corpus.stopwords.words('english')
    text = (unicodedata.normalize('NFKD', text)
    .encode('ascii', 'ignore')
    .decode('utf-8', 'ignore')
    .lower())
    words = re.sub(r'[^\w\s]', '', text).split()
    return [wnl.lemmatize(word) for word in words if word not in stopwords]

words = basic_clean(''.join(str(Example)))

(pd.Series(nltk.ngrams(words, 2)).value_counts())[:10]
(pd.Series(nltk.ngrams(words, 3)).value_counts())[:10]

bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]
bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Bigrams')
plt.ylabel('Bigram')
plt.xlabel('# of Occurances')
           
trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]
trigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring trigrams')
plt.ylabel('trigrams')
plt.xlabel('# of Occurances')
</code></pre>
",,2020-07-26 06:35:13,How to create ngrams and bar charts using gensim similar to nltk ngram,<python><gensim>,,,CC BY-SA 4.0,True,False,True,False,False
28623,63065232,2020-07-24 00:53:22,,"<p>I use virtualenv in Python. I use gensim in a script. I get this error
name 'gensim' is not defined
I tried to install genism using pip and conda. I ended up updating conda packages after some suggested solution .
I see there is genism 3.8 after reunnig pip list, but I still have the error !. Could you please tell me what to do
P.S. I take input from a html form in a Flask function. Inside the function, I call the script that has genism. The program show the forms input buttons . After clicking the submit buton, I get the error message.</p>
<pre><code>import re
import numpy as np
import pandas as pd
from pprint import pprint

#database
import db
from db import *

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric

# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#matplotlib inline

from conn import *
from functions import *

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings(&quot;ignore&quot;,category=DeprecationWarning)
</code></pre>
<p>Thanks in advance</p>
",,2020-07-24 00:53:22,'gensim' is not defined even though it shows in the virtualenv packages,<python-3.x><flask><gensim>,,,CC BY-SA 4.0,False,True,True,False,False
28624,63068024,2020-07-24 06:25:57,,"<p>I am trying to use a pretrained word2vector model to create word embeddings but i am getting the following error when Im trying to create weight matrix from word2vec genism model:</p>
<p>Code:</p>
<pre><code>import gensim
w2v_model = gensim.models.KeyedVectors.load_word2vec_format(&quot;/content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz&quot;, binary=True)

vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)
EMBEDDING_DIM=300
# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)
</code></pre>
<p>Im getting the following error:</p>
<p><a href=""https://i.stack.imgur.com/HnCoC.png"" rel=""nofollow noreferrer"">Error</a></p>
",,2020-07-24 15:50:38,Using pretrained word2vector model,<nlp><lstm><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28626,63082446,2020-07-24 22:55:27,,"<p>I am trying to extract topics using Gensim library, LDA model from Persona-Chat dataset. After pre-processing part, I try to find the best number of topic in order to get the best keywords regarding to the topics.</p>
<p>So, for that I ran the code for different number of topics such as; 50-100-150-200 and 250.</p>
<p>The Coherence Score increases whenever number of topics are increased and at the same time, Perplexity Score decreases as well. So, from these information I understood I had to use 250, because as far as I know, Perplexity should be small and Coherence should be high. Below, you can see the scores of each number of topics (K)</p>
<ul>
<li>K is 50, Coherence Score is 0.5674564036353904 and Perplexity is
-30.41004925196529</li>
<li>K is 100, Coherence Score is 0.7057459566697354 and Perplexity is
-152.65336624931487</li>
<li>K is 150, Coherence Score is 0.8342139447628544 and Perplexity is
-298.6829528808594</li>
<li>K is 200, Coherence Score is 0.8342139447628546 and Perplexity is
-398.1058654785156</li>
<li>K is 250, Coherence Score is 0.8342139447628545 and Perplexity is
-497.6555480957031</li>
</ul>
<p>However, whenever I used 250 number of topics, my keywords have 0 weight and they are repeated in every topic.</p>
<ul>
<li>(0, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
<li>(1, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
<li>(2, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
</ul>
<p>I made some research and I found out that if keywords are repeated then maybe my number of topics are too much. So I decreased my number of topics from 250 to 50 but I want to know, is there any ways to fix this?</p>
<p>Since, the Perplexity and Coherence Scores shows that LDA model would work better with 250 number of  topics.</p>
<p>Below you can find the code as well.</p>
<pre><code>personality_dictionary = Dictionary(df['persona_sentences'])
corpus_personality = [personality_dictionary.doc2bow(text) for text in df['persona_sentences']]
lda = LdaModel(corpus=corpus_personality, num_topics=50, id2word=personality_dictionary)
lda_personality_topics = lda.print_topics(num_words=10, num_topics = -1)
for topic in lda_personality_topics:
    print(topic)
</code></pre>
<p>Best regards,</p>
",2020-07-27 00:24:48,2020-07-27 00:24:48,Topic Extraction Using Gensim,<python><nlp><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28636,63096909,2020-07-26 06:36:54,,"<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
",2020-07-29 08:24:56,2020-07-29 08:24:56,How to interpret output from gensim's Word2vec most similar method and understand how it's coming up with the output values,<python><nlp><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28648,63021096,2020-07-21 18:42:48,,"<p>Given a list of document words e.g. <code>[['cow','boy','hat','mat],['village','boy','water','cow']....]</code>, gensim can be used to get bi-grams as follows:</p>
<pre><code>bigrams = gensim.models.Phrases(data_words, min_count=1,threshold=1) 
bigram_model = gensim.models.phrases.Phraser(bigrams)
</code></pre>
<p>I was wondering as to how to get the score of each bi-gram detected in the bigram_model?</p>
",,2020-07-21 18:48:26,How to get the score of filtered bi-grams in gensim?,<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28651,63100943,2020-07-26 13:42:36,,"<p>I am currently trying to use dynamic topic modeling on some news crawled from the web.
Unfortunately, I receive a warning in the logs:</p>
<p>INFO : using serial LDA version on this node</p>
<pre><code>path/to/gensim/models/ldaseqmodel.py:1472: RuntimeWarning: invalid value encountered in double_scalars converged = np.fabs((lhood_old - lhood) / (lhood_old * total))
</code></pre>
<p>After using google to find out more about this issue, I learned that this numpy error is often produced by NaNs or null values. So with regards to dynamic topic modeling this probably refers to an empty document? but I dont have any empty documents in my dataframe</p>
",2020-07-27 07:29:09,2020-07-27 07:29:09,ldaseqmodel runtimewarning Invalid value in double_scalars,<nlp><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28652,63101674,2020-07-26 14:45:05,,"<p>I am using word2vec through gensim currently.  You can set the context size easily (that sets the number of words to the left and right of a center word to consider).  Sometimes it's better to consider the words to the left separately to the words to the the right. This would give two embeddings per word.</p>
<p>Can this be done in gensim or in any other python compatible tool?</p>
",,2020-07-26 18:54:52,Can you make word2vec use left context and right context separately?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28691,63087891,2020-07-25 11:40:54,,"<p>This question is of a more conceptual type.
I was using the pre-trained word-vectors of spacy (the <code>de_core_news_md</code> model).
The problem is that I have a lot of domain specific words which all get a 0-vector assignet and overall the results are in gerneral not too good.
I was wondering how one should proceed now.
should I try to fine tune the existing vectors? If so, how would one approach that?
Or, should I just not use the pre-trained word vectors of spacy and create my own?</p>
<p>Edit:
I want to fine tune the pre trained vectors. I've read, that I could train the already trained model again but on my data. Now my question is, how to do that. When I use spacy, i just load the model. Should I download the vectors of spacy and train a gensim model with them and afterwards again with my vectors? Or is there a better way?</p>
<p>Thank you in advance for any input!</p>
",2020-08-05 09:30:50,2020-08-05 09:30:50,fine tune spacy word vectors,<spacy><word-embedding>,,,CC BY-SA 4.0,False,True,True,False,False
28699,63107425,2020-07-27 01:14:07,,"<p>i try to use the methode <strong>tmtoolkit.topicmod.evaluate.metric_coherence_gensim</strong>  to calculate a coherence score an sklearn LDA  model, But it does not work.</p>
<p><strong>Code :</strong></p>
<pre><code>Coherence = metric_coherence_gensim(measure='c_v', 
                      topic_word_distrib = lda.components_, 
                      dtm = Matrice_document_terme, 
                      vocab = np.array([x for x in vectoriseur.vocabulary_.keys()]),
                      texts= df['lemme'])
</code></pre>
<p><strong>Output :</strong></p>
<pre><code>c:\users\big box\appdata\local\programs\python\python37\lib\site-packages\gensim\topic_coherence\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars
  m_lr_i = np.log(numerator / denominator)

c:\users\big box\appdata\local\programs\python\python37\lib\site-packages\gensim\topic_coherence\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars
  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))
</code></pre>
",,2020-07-27 01:14:07,How can I calculate the coherence score of an sklearn LDA model?,<python><scikit-learn><gensim><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,True
28701,63136275,2020-07-28 14:19:05,,"<p>I have a trained LDA model via gensim in the gensim format and as a pickle file. I would like to see the training parameters.</p>
<p>I've used:</p>
<pre><code>import pickle
infile = open(MODELPATH + '\\ldaModel.pickle','rb')                                            
new_dict = pickle.load(infile)                                                                               
infile.close()                                                                                               
print(new_dict)
</code></pre>
<p>to extract <strong>chunksize</strong>, I also need <strong>passes</strong> as well.</p>
<p>Is this possible?</p>
<p>CLARIFICATION I would like to be able to recreate the modeling process exactly given the same training corpus</p>
",2020-08-04 21:19:42,2020-08-04 21:19:42,Is there a way to see the training parameters of a trained gensim LDA model?,<python><parameters><model><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28727,63125904,2020-07-28 01:48:30,,"<p>I am novice python data analyst trying to preprocess the text data (jsonl format) before it goes into Neural networks for topic modelling(VAE). I was able to clean the data and turn it into numpy array, further I wanted to apply label encoding to the cleaned text data but fail to do so. **How can one apply label encoding to list of list data format **?. The input data into label encoding is list of list and ouput has to be in same format.</p>
<pre><code>numpy array format (type: &lt;class 'numpy.ndarray'&gt;)
[array([1131,  713,  857, 1130..........]) 
 array([ 142, 1346, 1918, 1893,   61,   62, 1922,967......]) ]) 
 array([135, 148,  14, 104, 154, 159, 136,  94, 149, 135, 117,  62, 130....]) 
 array([135, 148,  14, 104, 154, 159, 136......])...................................]
</code></pre>
<p>The code is this way(after cleaning):(list of list -strings)</p>
<pre><code>    dictionary = gensim.corpora.Dictionary(process_texts) # creating a dictionary
    label_covid_data =[list(filter(lambda x: x != -1, dictionary.doc2idx(doc))) for doc in     process_texts] # converint it into numeric according to dictionary
    covid_train_data,covid_test_data = train_test_split(label_covid_data, test_size=0.2, random_state = 3456) # dividing into train and test data
    covid_train_narray = np.array([np.array(i) for i in covid_train_data]) # converting into numpy array format
    label = preprocessing.LabelEncoder() # applying label encoding 
    covid_data_labels = label.fit_transform([label.fit_transform(i) for i in covid_train_narray])
</code></pre>
<p>Error I am getting:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode(values, uniques, encode, check_unknown)
    111         try:
--&gt; 112             res = _encode_python(values, uniques, encode)
    113         except TypeError:

    C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode_python(values, uniques, encode)
     59     if uniques is None:
---&gt; 60         uniques = sorted(set(values))
     61         uniques = np.array(uniques, dtype=values.dtype)
TypeError: unhashable type: 'numpy.ndarray'
During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-217-ebce4e37aad8&gt; in &lt;module&gt;
  4 label = preprocessing.LabelEncoder()
  5 #movie_line_labels = label.fit_transform(covid_train_narray[0])
 ----&gt; 6 covid_data_labels = label.fit_transform([label.fit_transform(i) for i in covid_train_narray])
  7 covid_data_labels
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in fit_transform(self, y)
250         &quot;&quot;&quot;
251         y = column_or_1d(y, warn=True)
 --&gt; 252         self.classes_, y = _encode(y, encode=True)
253         return y
254 

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode(values, uniques, encode, check_unknown)
    112             res = _encode_python(values, uniques, encode)
    113         except TypeError:
--&gt;     114             raise TypeError(&quot;argument must be a string or number&quot;)
    115         return res
    116     else:

    TypeError: argument must be a string or number



     
</code></pre>
",,2020-07-28 01:48:30,How to apply label encoding to text data(list of list),<python-3.x><nlp>,,,CC BY-SA 4.0,False,False,True,False,True
28794,63147796,2020-07-29 06:44:37,,"<p>I am using LDA for Topic Modelling in Python.Gensim implementation of LDA allows us to set alpha as 'auto' as below:</p>
<pre><code>alpha ({numpy.ndarray, str}, optional) 
    
            asymmetric: Uses a fixed normalized asymmetric prior of 1.0 / topicno.
    
            auto: Learns an asymmetric prior from the corpus (not available if distributed==True).
</code></pre>
<p>For LDA Mallet wrapper provided in Gensim there is no option of setting alpha as auto.</p>
<p>Is there way to learn alpha from the corpus in LDA Mallet?</p>
",,2020-07-29 11:31:32,LDA Gensim Mallet setting alpha as 'auto',<python><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28796,63162298,2020-07-29 20:56:06,,"<p>My dataset is in the following format where for each disease I am generating a 2D vector using word2vec.(Showing 2D vectors for example but in practice, vectors are in 100D )</p>
<pre><code>Disease                             Vectors

disease a, disease c         [[ 0.2520773 ,  0.433798],[0.38915345, 0.5541569]]

disease b                    [0.12321666, 0.64195603]

disease c, disease b         [[0.38915345, 0.5541569],[0.12321666, 0.64195603]]

disease c                    [0.38915345, 0.5541569]
</code></pre>
<p>From here I am generating a 1D array for each <code>disease/disease combination</code> by taking the average of the vectors. The issue with averaging word vectors is the fact that the combination of 2 or more diseases can have the same average vector as a totally different disease which is not at all relevant but the average vectors get matched. This makes the concept of averaging vectors flawed. To counter this, the understanding is with an increase in dimension of the vectors, this should be even less of a possibility.</p>
<p>So, couple of questions in all:</p>
<ol>
<li><p>Is there a better way than averaging the output from word2vec vectors to generate a 1D array?</p>
</li>
<li><p>These generated vectors will be treated as features to a classifier model that I am trying to build for each disease/disease combination so, if I generate a 100D feature vector from word2vec, shall I use something like a PCA on it to reduce the dimension or shall I just consider the 100D feature vector as 100 features to my classifier.</p>
</li>
</ol>
",,2020-07-29 20:56:06,What is a good substitute for averaging vectors generated from Word2vec,<vector><pca><gensim><word2vec><word-embedding>,,,CC BY-SA 4.0,False,False,True,False,False
28816,63249428,2020-08-04 14:49:56,,"<p>I currently have an AWS EMR with a linked notebook to that same cluster.</p>
<p>I would like to load a spacy model (<code>en_core_web_sm</code>) but first I need to download the model which is usually done using <code>python -m spacy download en_core_web_sm</code> but I really can't find how to do it in a PySpark Session.</p>
<p>Here is my config :</p>
<pre><code>%%configure -f
{
    &quot;name&quot;:&quot;conf0&quot;,
    &quot;kind&quot;: &quot;pyspark&quot;,
    &quot;conf&quot;:{
          &quot;spark.pyspark.python&quot;: &quot;python&quot;,
          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,
          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,
          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;/usr/bin/virtualenv&quot;
    },
    &quot;files&quot;:[&quot;s3://my-s3/code/utils/NLPtools.py&quot;,
            &quot;s3://my-s3/code/utils/Parse_wikidump.py&quot;,
            &quot;s3://my-s3/code/utils/S3_access.py&quot;,
            &quot;s3://my-s3/code/utils/myeval.py&quot;,
            &quot;s3://my-s3/code/utils/rank_metrics.py&quot;,
            &quot;s3://my-s3/code/utils/removeoutput.py&quot;,
            &quot;s3://my-s3/code/utils/secret_manager.py&quot;,
            &quot;s3://my-s3/code/utils/word2vec.py&quot;]
}
</code></pre>
<p>I'm able to run such command, which is kinda normal :</p>
<pre><code>sc.install_pypi_package(&quot;boto3&quot;)
sc.install_pypi_package(&quot;pandas&quot;)
sc.install_pypi_package(&quot;hdfs&quot;)
sc.install_pypi_package(&quot;NLPtools&quot;)
sc.install_pypi_package(&quot;numpy&quot;)
sc.install_pypi_package(&quot;tqdm&quot;)
sc.install_pypi_package(&quot;wikipedia&quot;)
sc.install_pypi_package(&quot;filechunkio&quot;)
sc.install_pypi_package(&quot;thinc&quot;)
sc.install_pypi_package(&quot;gensim&quot;)
sc.install_pypi_package(&quot;termcolor&quot;)
sc.install_pypi_package(&quot;boto&quot;)
sc.install_pypi_package(&quot;spacy&quot;)
sc.install_pypi_package(&quot;langdetect&quot;)
sc.install_pypi_package(&quot;pathos&quot;)
</code></pre>
<p>But of course, like I can't succeed to download the model, when trying to load it I got the following error :</p>
<pre><code>An error was encountered:
[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
Traceback (most recent call last):
  File &quot;/mnt/tmp/spark-eef27750-07a4-4a8a-82dc-b006827e7f1f/userFiles-ec6ecbe3-558b-42df-bd38-cd33b2340ae0/NLPtools.py&quot;, line 13, in &lt;module&gt;
    nlp = spacy.load('en_core_web_sm', disable=['parser', 'textcat'])
  File &quot;/tmp/1596550154785-0/lib/python2.7/site-packages/spacy/__init__.py&quot;, line 30, in load
    return util.load_model(name, **overrides)
  File &quot;/tmp/1596550154785-0/lib/python2.7/site-packages/spacy/util.py&quot;, line 175, in load_model
    raise IOError(Errors.E050.format(name=name))
IOError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
<p>I've tried to install it directly on the cluster (master/worker) but it's &quot;outside&quot; the PySpark Session so it won't be embedded. And commands like <code>!python -m spacy download en_core_web_sm</code> are not working in a PySpark notebook...</p>
<p>Thanks in advance !</p>
",,2020-08-04 14:49:56,I can't install spacy model in EMR PySpark notebook,<python><amazon-web-services><pyspark><amazon-emr><spacy>,,,CC BY-SA 4.0,False,True,True,False,False
28818,63200527,2020-08-01 00:11:49,,"<p>for an unseen text, we can find out the topic score using get_document_topics.</p>
<p>If it is for seen data, (one of many documents in corpus which is trained for LDA),
topic score varies when i apply ,suppose for 1st document ,</p>
<pre><code>LDA.get_document_topics(corpus[0]) and 


text_doc = 1st document 
test_doc = preprocess_text(test_doc) 
cor= gensim_dictionary.doc2bow(test_doc)
LDA.get_document_topics(cor)
</code></pre>
",,2020-08-01 00:11:49,Topic score/weight varies for seen text Lda,<python-3.x><nlp><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28819,63200943,2020-08-01 01:34:23,,"<p>I am trying to reproduce this code: <a href=""https://github.com/n0obcoder/skip-gram-model"" rel=""nofollow noreferrer"">https://github.com/n0obcoder/skip-gram-model</a></p>
<p>However I am getting this error:</p>
<pre><code>ModuleNotFoundError: No module named 'utils_modified'
</code></pre>
<p>could you please tell me what it is wrong there?</p>
",,2020-08-01 01:51:02,Word2vec from scratch: No module named 'utils_modified',<python><gensim><word2vec><torch>,,,CC BY-SA 4.0,False,False,True,False,False
28834,63218694,2020-08-02 16:54:45,,"<p>here is the problematic code:</p>
<pre><code>from gensim.corpora import Dictionary
tweets_dictionary = Dictionary(df.tokenized)
</code></pre>
<p>the Panda Dataframe df is build as followed with two columns &quot;created_at&quot; and &quot;tokenized&quot;. &quot;tokenized&quot; consists of a list of words:</p>
<p><a href=""https://i.stack.imgur.com/x3zxb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x3zxb.png"" alt=""df.head()"" /></a></p>
<p>I get the following error message while running the problematic code:</p>
<p><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</code></p>
<p>This is very bizzare to me, as the column tokenized is not a single string. I have tried converting the column into a single list, a list of lists and a tuple, but nothing has worked so far....thanks in advance for your help!</p>
",,2020-08-02 17:09:41,gensim.corpora Dictionary type error interprets tokenized column as single string,<python><dictionary><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28842,63299446,2020-08-07 09:58:07,,"<p>Following is the code :-</p>
<pre><code>modelDoc = Doc2Vec(size=300, window=5, dm=0, dbow_words=1, hs=0, negative=10, alpha=0.05, min_count=20,
                       workers=cores, sample=1e-5, seed=0, iter=10)
    modelDoc.build_vocab(finalSent)
  
    modelDoc.save(save_model)
</code></pre>
<p>my version :
gensim==3.8.1
numpy==1.16.2</p>
<p>after saving the model
only vocab_model file is generated</p>
<p><code>vocab_model.docvecs.doctag_syn0.npy</code> is not generated.
what is the use of this file and does it is necessary to generate this file.</p>
",,2020-08-07 18:18:11,vocab_model.docvecs.doctag_syn0.npy not generated after saving doc2vec model,<python><numpy><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28852,63284211,2020-08-06 13:03:03,,"<p>I am predicting similarities of documents using the pre trained spacy word embeddings. Because I have a lot of domain specific words, I want to fine tune my vectors on a rather small data set containing my domain specific vocabulary.</p>
<p>My idea was to just train the spacy model again with my data. But since the word vectors in spacy are built-in, I am not sure how to do that. Is there a way to train the spacy model again with my data?</p>
<p>After some research, I found out, that I can train my own vectors using Gensim. There I would have to download a pre trained model for example the Google News dataset model and afterwards I could train it again with my data set. Is this the only way? Or is there a way to proceed with my spacy model?</p>
<p>Any help is greatly appreciated.</p>
",,2020-10-13 11:28:08,how to fine tune spacys word vectors,<spacy><word-embedding>,,,CC BY-SA 4.0,False,True,True,False,False
28873,63335897,2020-08-10 07:32:35,,"<p>Thank you for stopping by. I need some help for logging issue in Spyder.
This is the page I looked :<a href=""https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html</a></p>
<p>And this what is written in the link:</p>
<p>I suggest the following way to choose iterations and passes. First, enable logging (as described in many Gensim tutorials), and set <code>eval_every = 1</code> in LdaModel. When training the model look for a line in the log that looks something like this:</p>
<pre><code>2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations
</code></pre>
<p>If you set <code>passes = 20</code> you will see this line 20 times. Make sure that by the final passes, most of the documents have converged. So you want to choose both passes and iterations to be high enough for this to happen.</p>
<p>The thing is, I don't know about logging and couldn't find about logging for lda using gensim tutorials. What I want to know is that coverage log so that I can set proper passes and iterations. Is it possible with Spyder?</p>
<p>For the one who is going to enlighten me, thank you in advance.</p>
",2020-08-10 07:54:12,2020-08-10 07:54:12,I have a question about logging function in Spyder (while using lda with gensim),<python-3.x><spyder><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
28909,63353553,2020-08-11 07:35:11,,"<p>So what I'm simply trying to do is save a Word2Vec model.</p>
<pre><code>import nltk
from nltk.corpus import product_reviews_1 as review
import gensim

model = gensim.models.Word2Vec(review.sents())
model.save('review.embedding')
</code></pre>
<p>The code runs normally and there's this file review.embedding on my directory, but it has nothing but an error that says</p>
<blockquote>
<p>the file is not UTF encoded and saving is disabled</p>
</blockquote>
<p>Couldn't figure out what is wrong.</p>
",,2020-08-11 07:35:11,word2vec embedding is not utf 8 encoded - saving disabled,<python><nltk><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
28912,63306635,2020-08-07 17:28:30,,"<p>I'm trying to run gensim dtm on colab.
I get a &quot;PermissionError: [Errno 13] Permission denied&quot; for the binary file location on my drive.
I tried  with os.chmod(path_to_dtm_binary, stat.S_IEXEC) but didn't solve the problem.
Any ideas?</p>
<p>Thanks!</p>
",,2020-08-07 17:28:30,Permission Error trying to run gensim dtm on google colab,<google-colaboratory><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
28940,63275259,2020-08-06 00:33:16,,"<p>Given a query and a document, I would like to compute a similarity score using Gensim doc2vec.
Each document consists of multiple fields (e.g., main title, author, publisher, etc)</p>
<p>For training, is it better to concatenate the document fields and treat each row as a unique document or should I split the fields and use them as different training examples?</p>
<p>For inference, should I treat a query like a document? Meaning, should I call the model (trained over the documents) on the query?</p>
",2020-08-06 10:22:36,2020-08-06 10:22:36,Query-document similarity with doc2vec,<machine-learning><gensim><word2vec><information-retrieval><doc2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28946,63345527,2020-08-10 17:55:23,,"<p>I am trying to build a Docker application that uses Python's gensim library, version 3.8.3, which is being installed via pip from a requirements.txt file.</p>
<p>However, Docker seems to have trouble while trying to do
RUN pip install -r requirements.txt</p>
<p>My Requirement.txt for reference -</p>
<pre><code>boto==2.49.0
boto3==1.14.33
botocore==1.17.33
certifi==2020.6.20
chardet==3.0.4
click==7.1.2
Cython==0.29.14
docutils==0.15.2
Flask==1.1.2
gensim==3.8.3
idna==2.10
itsdangerous==1.1.0
Jinja2==2.11.2
jmespath==0.10.0
MarkupSafe==1.1.1
numpy==1.19.1
python-dateutil==2.8.1
requests==2.24.0
s3transfer==0.3.3
scipy==1.5.2
six==1.15.0
smart-open==2.1.0
urllib3==1.25.10
Werkzeug==1.0.1
</code></pre>
<p>dockerFile</p>
<pre><code>FROM python:3.8.2-alpine
WORKDIR /project
ADD . /project
RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
RUN pip install --upgrade pip
RUN pip install -r requirements.txt
CMD [&quot;python&quot;,&quot;similarity.py&quot;] 
</code></pre>
<p>error:</p>
<pre><code>(venv) C:\Users\verma\PycharmProjects\flaskTest&gt;docker image build -t similarity-flask-api  .
Sending build context to Docker daemon  302.7MB
Step 1/7 : FROM python:3.8.2-alpine
 ---&gt; 6c32e2504283
Step 2/7 : WORKDIR /project
 ---&gt; Using cache
 ---&gt; 554b6bda89ad
Step 3/7 : ADD . /project
 ---&gt; d085a645ecb1
Step 4/7 : RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
 ---&gt; Running in e7117c1e18ff
+ apk add --no-cache build-base
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/18) Installing libgcc (9.2.0-r4)
(2/18) Installing libstdc++ (9.2.0-r4)
(3/18) Installing binutils (2.33.1-r0)
(4/18) Installing libmagic (5.37-r1)
(5/18) Installing file (5.37-r1)
(6/18) Installing gmp (6.1.2-r1)
(7/18) Installing isl (0.18-r0)
(8/18) Installing libgomp (9.2.0-r4)
(9/18) Installing libatomic (9.2.0-r4)
(10/18) Installing mpfr4 (4.0.2-r1)
(11/18) Installing mpc1 (1.1.0-r1)
(12/18) Installing gcc (9.2.0-r4)
(13/18) Installing musl-dev (1.1.24-r2)
(14/18) Installing libc-dev (0.7.2-r0)
(15/18) Installing g++ (9.2.0-r4)
(16/18) Installing make (4.2.1-r2)
(17/18) Installing fortify-headers (1.1-r0)
(18/18) Installing build-base (0.5-r1)
Executing busybox-1.31.1-r9.trigger
OK: 182 MiB in 52 packages
+ apk add --no-cache libexecinfo-dev
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/2) Installing libexecinfo (1.1-r1)
(2/2) Installing libexecinfo-dev (1.1-r1)
OK: 183 MiB in 54 packages
Removing intermediate container e7117c1e18ff
 ---&gt; 9e7a97f8bddc
Step 5/7 : RUN pip install --upgrade pip
 ---&gt; Running in 0286591e9e70
Collecting pip
  Downloading pip-20.2.1-py2.py3-none-any.whl (1.5 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.1
    Uninstalling pip-20.1:
      Successfully uninstalled pip-20.1
Successfully installed pip-20.2.1
Removing intermediate container 0286591e9e70
 ---&gt; ca837786d695
Step 6/7 : RUN pip install -r requirements.txt
 ---&gt; Running in 7f124c100c0b
Collecting boto==2.49.0
  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)
Collecting boto3==1.14.33
  Downloading boto3-1.14.33-py2.py3-none-any.whl (129 kB)
Collecting botocore==1.17.33
  Downloading botocore-1.17.33-py2.py3-none-any.whl (6.5 MB)
Collecting certifi==2020.6.20
  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)
Collecting chardet==3.0.4
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
Collecting Cython==0.29.14
  Downloading Cython-0.29.14.tar.gz (2.1 MB)
Collecting docutils==0.15.2
  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)
Collecting Flask==1.1.2
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting gensim==3.8.3
  Downloading gensim-3.8.3.tar.gz (23.4 MB)
Collecting idna==2.10
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting itsdangerous==1.1.0
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2==2.11.2
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting jmespath==0.10.0
  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)
Collecting MarkupSafe==1.1.1
  Downloading MarkupSafe-1.1.1.tar.gz (19 kB)
Processing /root/.cache/pip/wheels/df/b2/64/111c431ca7f7d49afb42126b7351fe1a4894803d75026360de/numpy-1.19.1-cp38-cp38-linux_x86_64.whl
Collecting python-dateutil==2.8.1
  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)
Collecting requests==2.24.0
  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)
Collecting s3transfer==0.3.3
  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)
Collecting scipy==1.5.2
  Downloading scipy-1.5.2.tar.gz (25.4 MB)
  Installing build dependencies: started
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'error'
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpoyjzx5wb
         cwd: /tmp/pip-install-r078skp_/scipy
    Complete output (139 lines):
    lapack_opt_info:
    lapack_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_lapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_clapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    flame_info:
    customize UnixCCompiler
      libraries flame not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    atlas_3_10_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;
      NOT AVAILABLE

    atlas_3_10_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;
      NOT AVAILABLE

    atlas_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;
      NOT AVAILABLE

    atlas_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;
      NOT AVAILABLE

    accelerate_info:
      NOT AVAILABLE

    lapack_info:
    customize UnixCCompiler
      libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    lapack_src_info:
      NOT AVAILABLE

      NOT AVAILABLE

    setup.py:460: UserWarning: Unrecognized setuptools command ('dist_info --egg-base /tmp/pip-modern-metadata-ujofw06w'), proceeding with generating Cython sources
and expanding templates
      warnings.warn(&quot;Unrecognized setuptools command ('{}'), proceeding with &quot;
    Running from SciPy source directory.
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 280, in &lt;module&gt;
        main()
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 263, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 133, in prepare_metadata_for_build_wheel
        return hook(metadata_directory, config_settings)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 157, in prepare_metadata_for_build_wheel
        self.run_setup()
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 248, in run_setup
        super(_BuildMetaLegacyBackend,
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 142, in run_setup
        exec(compile(code, __file__, 'exec'), locals())
      File &quot;setup.py&quot;, line 583, in &lt;module&gt;
        setup_package()
      File &quot;setup.py&quot;, line 579, in setup_package
        setup(**metadata)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/core.py&quot;, line 137, in setup
        config = configuration()
      File &quot;setup.py&quot;, line 477, in configuration
        raise NotFoundError(msg)
    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_bu
ild_wheel /tmp/tmpoyjzx5wb Check the logs for full command output.
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p><strong>I tried this thread - <a href=""https://stackoverflow.com/questions/44732303/docker-unable-to-install-numpy-scipy-or-gensim"">Docker unable to install numpy, scipy, or gensim</a>
As suggested I added line 4 and 5 in my dockerFile but it is still not working.</strong></p>
",2020-08-10 23:55:45,2020-08-11 21:39:42,Error when building image from requirement.txt in docker,<python><docker><gensim>,,,CC BY-SA 4.0,False,False,True,False,False
28955,63324116,2020-08-09 08:24:40,,"<p>Im using gensim version '3.8.3' <br></p>
<p>when im running for model Word2Vec and FastText <code>build_vocab</code> and <code>train</code> <br>
the logs from those functions are missing the values</p>
<p>for example part of the logs of  <code>build_vocab</code> of FastText</p>
<pre><code>08/09/2020 08:19:18 AM [INFO] collecting all words and their counts
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
</code></pre>
<p>the index is missing and printed as <code>i</code></p>
<p>is there a way to solve it? is it a version bug?</p>
",,2020-08-10 16:47:51,word2vec logging missing values,<python-3.x><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28973,63377398,2020-08-12 13:19:01,,"<p>I am trying to create a custom word2vec for LOINC Longnames which contained in List2.However when i am tring to match with &quot;discharge summary&quot; which is alredy present in list am not able find it when searched with similar word. The code i used is below. How to train a model with list of words.</p>
<pre><code>import pandas as pd
import gensim
from gensim import corpora
from pprint import pprint
texts = [[text for text in doc.split()] for doc in List2]

# Create dictionary
#dictionary = corpora.Dictionary(texts)

# Get information about the dictionary
#print(dictionary)

df = pd.DataFrame(list(zip(List1, List2)), columns =['LOINC', 'LONGNAME'] )
print(List2[0:10])
res = [i.strip(&quot;[]&quot;).split(&quot;,&quot;) for i in List2]
print(res[0:5])
model = Word2Vec(List2, min_count=1)
print(model)
words = list(model.wv.vocab)
print(words)
model.save('model.bin')
new_model = Word2Vec.load('model.bin')
print(new_model)
model.wv.most_similar(positive=[&quot;Discharge summary&quot;])

#print(dictionary.token2id)
</code></pre>
",,2020-08-12 13:19:01,Custom Word2vec Vocabulary gensim,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28979,63332046,2020-08-09 22:43:31,,"<p>I have data from 10x different sources <em>(i.e. people from Instagram)</em> which I want to fine-tune Word2Vec model on. Afterwards I wish to use model in WEB API.</p>
<p>My question is, since each user should be analyzed on vocabulary that is based on <strong>ITS DATA alone</strong>, is it okay to fine-tune 1 Word2Vec model on all 10 different users, but keep vocabularies separate, so that when I analyze particular user I filter Word2Vec results based on that users vocabulary, or should I train Word2Vec model for each user individually?</p>
",,2020-08-09 22:43:31,Gensim Fine-Tuning Word2Vec model,<nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
28985,63362101,2020-08-11 16:06:30,,"<p>I am doing LDA topic modeling in Python and the folloing is my code for visualization:</p>
<pre><code>import pyLDAvis.gensim
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)
vis
</code></pre>
<p>I am looking for a way to export the Intertopic Distance Map graph to PDF or at least plot it using matplotlib then save as pdf, any idea?</p>
",,2020-08-11 16:06:30,Export pyLDAvis graphs as pdf,<python><matplotlib><lda><pyldavis>,,,CC BY-SA 4.0,False,False,True,False,False
28991,63411408,2020-08-14 10:54:23,,"<p>I'm quite new to DataScience so I apologize in advance should I make some mistakes in my reasoning.
I'm trying to write an application that for NLP in the form of Emotion Analysis (i found there's 4 main categories of emotions: Anger, Joy, Sadness and Fear/Surprise) and I'm going to use this to process real time voice stream, but I'm actually stuck. I turned to word vectorization to calculate the distance of a word from the relative emotion. Taking the word &quot;evil&quot; for example, I'd calculate the distance between &quot;evil&quot; and every emotion, then I'd check the distance from the 4 emotions word vectors to find the word polarity, excluding all emotions that are too distant (still WIP, need to think about this). Since the language needs to be Italian, I'm having a bit of a hard time understanding how to train my model. I understand that Sense2Vec and Gensim seem to work well in tandem for this kind of operations, but I'm frankly quite lost in the documentation, so I'd like to ask for your help. I'm also open to change modules if there's other ways to do this.</p>
",,2020-08-14 10:54:23,Emotion Analysis in Python with Sense2Vec for IT language,<python><nlp><data-science><sense2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29045,63419318,2020-08-14 19:59:19,,"<p>I have the following piece of code:</p>
<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('model2')
X = model[model.wv.vocab]
</code></pre>
<p>This piece of code works on one of my machines but not another. The model file is the same. What's going on? The error message I get is the following:</p>
<pre><code>  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1330, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 427, in load
    obj._load_specials(fname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/npyio.py&quot;, line 440, in load
    pickle_kwargs=pickle_kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/format.py&quot;, line 771, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 16777184 into shape (134441,128)
</code></pre>
<p>To install gensim, I used <code>conda install -c anaconda gensim</code></p>
",,2020-08-14 23:08:19,Array reshape error when loading word2vec model,<python><amazon-ec2><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29053,63385272,2020-08-12 22:01:03,,"<p>I am trying to load a pre-trained word2vec model in pkl format taken from <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">here</a></p>
<p>The line of code I use to load it:</p>
<pre><code>model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl') 
</code></pre>
<p>However, i keep getting the following error (full traceback):</p>
<pre><code>UnpicklingError                           Traceback (most recent call last)
&lt;ipython-input-15-ebd5780b6636&gt; in &lt;module&gt;
     55 
     56 #Load pretrained word2vec
---&gt; 57 model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl',mmap='r')
     58 

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
   1551     @classmethod
   1552     def load(cls, fname_or_handle, **kwargs):
-&gt; 1553         model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)
   1554         if isinstance(model, FastTextKeyedVectors):
   1555             if not hasattr(model, 'compatible_hash'):

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    226     @classmethod
    227     def load(cls, fname_or_handle, **kwargs):
--&gt; 228         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    229 
    230     def similarity(self, entity1, entity2):

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    433         compress, subname = SaveLoad._adapt_by_suffix(fname)
    434 
--&gt; 435         obj = unpickle(fname)
    436         obj._load_specials(fname, mmap, compress, subname)
    437         logger.info(&quot;loaded %s&quot;, fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1396         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1397         if sys.version_info &gt; (3, 0):
-&gt; 1398             return _pickle.load(f, encoding='latin1')
   1399         else:
   1400             return _pickle.loads(f.read())

UnpicklingError: invalid load key, ':'.
</code></pre>
<p>I tried loading it with load_word2vec_format, but no luck. Any ideas what might be wrong with it?</p>
",,2020-08-13 02:02:57,How to fix unpickling key error when loading word2vec (gensim)?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29057,63371536,2020-08-12 07:08:40,,"<p>I have two lists . One list containing Documents names another list also containing document names.How can we find the document name in one list which is semantically simalar to other list. how can we build our own custom embeddings.</p>
",,2020-08-12 07:08:40,Lists semantic similarity,<python><list><gensim><word2vec><taxonomy>,,,CC BY-SA 4.0,False,False,True,False,False
29081,63472132,2020-08-18 15:45:17,,"<p>I have created GloVE vectors in R previously using <code>text2vec</code> library.</p>
<p>Is there any easy way to export these for use in Python where I have scripts to compare/contract with Gensim created word vectors?  I know there is a specific word2vec c_format, but Im not sure if R has the capability of producing this.</p>
",,2020-08-21 13:19:34,Export R text2vec Vectors for use in Gensim in Python,<python><r><gensim><text2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29091,63459657,2020-08-17 22:55:40,,"<p>So I have multiple text files(around 40). and each file has around 2000 articles (average of 500 words each). And each document is a single line in the text file.</p>
<p>So because of the memory limitations I wanted to use dynamic loading of these text files for training. (Perhaps a iterator class?)</p>
<p>so how do I proceed?</p>
<ul>
<li>train each text file -&gt; save the model -&gt; load the model and rerun on new data?</li>
<li>is there a way with iterator class to do this automatically?</li>
<li>should I give sentence by sentence, article by article or text file by text file as input to model training?</li>
</ul>
",,2020-08-17 23:15:24,How to load large dataset to gensim word2vec model,<python><iterator><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29123,63524493,2020-08-21 14:11:48,,"<p>Using the Gensim package, I have trained a word2vec model on the corpus that I am working with as follows:</p>
<pre><code>word2vec = Word2Vec(all_words, min_count = 3, size = 512, sg = 1)
</code></pre>
<p>Using Numpy, I have initialized a random array with the same dimensions:</p>
<pre><code>vector = (rand(512)-0.5) *20
</code></pre>
<p>Now, I would like to find the words from the word2vec that are most similar to the random vector that I initialized.</p>
<p>For words in the word2vec, you can run:</p>
<pre><code>word2vec.most_similar('word')
</code></pre>
<p>And the output is a list with most similar words and their according distance.</p>
<p>I would like to get a similar output for my initialized array.</p>
<p>However, when I run:</p>
<pre><code>word2vec.most_similar(vector)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-297-3815cf183d05&gt;&quot;, line 1, in &lt;module&gt;
    word2vec.most_similar(vector)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\utils.py&quot;, line 1461, in new_func1
    return func(*args, **kwargs)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 1383, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 549, in most_similar
    for word, weight in positive + negative:

TypeError: cannot unpack non-iterable numpy.float64 object
</code></pre>
<p>What can I do to overcome this error and find the most similar words to my arrays?</p>
<p>I've checked <a href=""https://stackoverflow.com/questions/54273077/cannot-unpack-non-iterable-numpy-float64-object-python3-opencv"">this</a> and <a href=""https://stackoverflow.com/questions/59357940/typeerror-cannot-unpack-non-iterable-numpy-float64-object"">this</a> page. However, it is unclear to me how I could solve my problem with these suggestions.</p>
",2020-08-21 16:04:19,2020-08-21 16:04:19,Find most similar words to randomy initialized array,<python><numpy><typeerror><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29129,63537713,2020-08-22 15:10:38,,"<p>I am using doc2vec of gensim to train document embedding, now after training, even when i give the infer_Vector function some random words, it still gives back some vector and doesn't throw an exception or anything, but word2vec reacts differently and throws an exception when this happens, why?</p>
<p>for example :</p>
<pre><code>inferred_vector = model.infer_vector(['random1' , 'random2' , 'random3'])
</code></pre>
<p>still returns a vector to me, my question is :</p>
<ol>
<li><p>how is this vector computed when non of the words were in the training corpus? is it random?</p>
</li>
<li><p>if it is random, then isn't it better for it to return 0 if non of the words were in the corpus? i have a classification model that uses these vectors and giving it random values instead of 0s will just confuse the model, is there anyway i can make it so it gives me vector of 0s if non them were in the training corpus?</p>
</li>
<li><p>is this overall a good vector to give my model for sentence classification, or is there any better method to use instead of Doc2vec? my corpus is basically non english word sentences.</p>
</li>
</ol>
",2020-08-22 23:53:16,2020-08-22 23:53:16,What happens when you give a word to infer_vector of gensim which was not in the training corpus?,<python><deep-learning><gensim><doc2vec>,2020-08-23 00:26:30,,CC BY-SA 4.0,False,False,True,False,False
29149,63479304,2020-08-19 02:49:30,,"<p>I trained an LDA model according to gensim documentation, and am attempting to get the main topic from each document. This takes forever. How can I speed it up?</p>
<pre><code>topics = [max(topics, key=lambda i: i[1])[0] for topics in lda_model.get_document_topics(corpus_bow)]
</code></pre>
<p>My model is stored in <code>lda_model</code>, and my bag-of-words vectors are stored in <code>corpus_bow</code>. The model was trained on about 145,000 documents.</p>
",2020-08-19 22:20:56,2020-08-19 22:20:56,How to optimize extracting main topics from LDA model,<python><optimization><gensim><lda>,,,CC BY-SA 4.0,False,False,True,False,False
29151,63481736,2020-08-19 07:10:22,,"<p>i have created a word2vec model with gensim and am now looking for a way to visualize this.</p>
<p>I have already created a 2D plot, but it is very confusing. Assuming one performs a dimensionality reduction to 3 dimensions, is there a possibility to visualize these data points &quot;interactively&quot; within a jupyter notebook?
By interactive I mean that you might be able to navigate through the 3D plot and have a closer look at different points.</p>
<p>Thanks a lot.</p>
",,2020-08-19 08:43:56,Interactive 3D visualization of word2vec model (gensim),<python><data-visualization><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29170,63559660,2020-08-24 10:56:32,,"<p>I am writing a python script that deal with sentiment analysis and I did the pre-process for the text and vectorize the categorical features and split the dataset, then I use the LogisticRegression model and I got <strong>accuracy 84%</strong></p>
<p>When I upload a new dataset and try to deploy the created model I got <strong>accuracy 51,84%</strong></p>
<h1>code:</h1>
<pre><code>    import pandas as pd
    import numpy as np
    import re
    import string
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer
    from sklearn.model_selection import train_test_split
    from nltk.stem import PorterStemmer
    from nltk.stem import WordNetLemmatizer
    # ML Libraries
    from sklearn.metrics import accuracy_score
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import GridSearchCV
    
    stop_words = set(stopwords.words('english'))  
    import joblib
    
    def load_dataset(filename, cols):
        dataset = pd.read_csv(filename, encoding='latin-1')
        dataset.columns = cols
        return dataset
    
    dataset = load_dataset(&quot;F:\AIenv\sentiment_analysis\input_2_balanced.csv&quot;, [&quot;id&quot;,&quot;label&quot;,&quot;date&quot;,&quot;text&quot;])
    dataset.head()
    
    dataset['clean_text'] = dataset['text'].apply(processTweet)
    
    # create doc2vec vector columns
    from gensim.test.utils import common_texts
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    
    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset[&quot;clean_text&quot;].apply(lambda x: x.split(&quot; &quot;)))]
    
    # train a Doc2Vec model with our text data
    model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
    
    # transform each document into a vector data
    doc2vec_df = dataset[&quot;clean_text&quot;].apply(lambda x: model.infer_vector(x.split(&quot; &quot;))).apply(pd.Series)
    doc2vec_df.columns = [&quot;doc2vec_vector_&quot; + str(x) for x in doc2vec_df.columns]
    dataset = pd.concat([dataset, doc2vec_df], axis=1)
    
    # add tf-idfs columns
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(min_df = 10)
    tfidf_result = tfidf.fit_transform(dataset[&quot;clean_text&quot;]).toarray()
    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())
    tfidf_df.columns = [&quot;word_&quot; + str(x) for x in tfidf_df.columns]
    tfidf_df.index = dataset.index
    dataset = pd.concat([dataset, tfidf_df], axis=1)
    
    x = dataset.iloc[:,3]
    y = dataset.iloc[:,1]
    
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 42)
    
    from sklearn.pipeline import Pipeline
    # create pipeline
    pipeline = Pipeline([
        ('bow', CountVectorizer(strip_accents='ascii',
                                stop_words=['english'],
                                lowercase=True)),  
        ('tfidf', TfidfTransformer()),  
        ('classifier', LogisticRegression(C=15.075475376884423,penalty=&quot;l2&quot;)),
    ])
    
    
    # Parameter grid settings for LogisticRegression
    parameters = {'bow__ngram_range': [(1, 1), (1, 2)],
                  'tfidf__use_idf': (True, False),
                    
                 }
    grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, verbose=1,n_jobs=-1)
    grid.fit(X_train,y_train)
    
    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
    #get predictions from best model above
    y_preds = grid.predict(X_test)
    cm = confusion_matrix(y_test, y_preds)
    
    print(&quot;accuracy score: &quot;,accuracy_score(y_test,y_preds))
    print(&quot;\n&quot;)
    print(&quot;confusion matrix: \n&quot;,cm)
    print(&quot;\n&quot;)
    print(classification_report(y_test,y_preds))
    
    joblib.dump(grid,&quot;F:\\AIenv\\sentiment_analysis\\RF_jupyter.pkl&quot;)
    RF_Model = joblib.load(&quot;F:\\AIenv\\sentiment_analysis\\RF_jupyter.pkl&quot;)
    
    test_twtr_preds = RF_Model.predict(test_twtr[&quot;clean_text&quot;])
</code></pre>
",2020-08-25 23:35:53,2020-08-28 13:21:46,How to improve the ML model in order to improve accuracy,<python><logistic-regression><sentiment-analysis>,,,CC BY-SA 4.0,True,False,True,False,True
29171,63564117,2020-08-24 15:31:32,,"<p>I was thinking underlying dictionary are the same for both supervised (classification) and unsupervised(Word Embedding). I want to analyze the classification model (word vectors) that I built for supervised problem using Gensim. But I got following error. I know <strong>Gensim has not implemented Supervised learning</strong> part of Fastext and only focusing on Word Embedding. But I just want to load the dictionary to analyze. Any pointers?</p>
<pre><code>Traceback (most recent call last):
  File &quot;fasttext_model_analysis.py&quot;, line 2, in &lt;module&gt;
    model = FastText.load_fasttext_format('model_ups_tickets_rca.bin')
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 274, in load_fasttext_format
    model.load_binary_data(encoding=encoding)
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 301, in load_binary_data
    self.load_dict(f, encoding=encoding)
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 332, in load_dict
    raise NotImplementedError(&quot;Supervised fastText models are not supported&quot;)
NotImplementedError: Supervised fastText models are not supported
</code></pre>
",,2020-08-25 15:57:13,Why does Gensim reject to load supervised model dict built by Fasttext (Facebook) library?,<gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
29177,63581374,2020-08-25 14:43:51,,"<p>I was trying to use this project :</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>for embedding non english sentences, the language is not a human speaking language, its machine language (x86)</p>
<p>but the problem is i cannot find a simple example where it shows how can i embed sentences using a custom dataset without any labels or similarity values of the sentences.</p>
<p>basically i have an array of sentences lists without any labels for sentences or similarity values for them, and i want to embed them into vectors in a way that it preserves the semantic of the sentence the best way possible, so far i have used word2vec and doc2vec using gensim library so i wanted to try this method to see if its any better?</p>
<p>(also any other suggestion for methods to use is appreciated)</p>
",2020-08-25 17:02:57,2020-08-25 17:02:57,Can we use sentence transformers to embed non english sentences without labels?,<nlp><word-embedding><doc2vec><bert-language-model><sentence-similarity>,,,CC BY-SA 4.0,False,False,True,False,False
29178,63547850,2020-08-23 14:03:20,,"<p>I am doing topic modelling in Python with the gensim package. I want to seed prior probabilities for specific words using the eta parameter. I am not sure if the probability of word w is: number of occurrences of w in a topic/total number of tokens in that topic?
I tried to manually check for this by inspecting the lda.get_topics and lda.get_term_topics. They all have a shape of numtopics_numwords, just like the dictionary I am intending to pass through eta.
However, when I call the first term, these 2 give different results.</p>
<pre><code>from gensim.models.ldamulticore import LdaMulticore
from gensim.test.utils import datapath


lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=3, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=5,
                                           alpha='asymmetric',
                                           eta='auto',
                                           per_word_topics=True)

w = lda_model.get_topics()
w[0,0:3]
array([0.00347201, 0.00134237, 0.00135214], dtype=float32)

w2 = lda_model.get_term_topics('account', minimum_probability=0.00000001)
w2
[(0, 0.0031849854), (1, 0.006998436), (2, 0.0028895985)]
</code></pre>
<p>I thought w and w2 should give the same values for the same indexes. Can you explain how these are different?</p>
<p>Once I understand that, I would know how to calculate the values to pass through eta for my keywords. But working examples of that would be nice.</p>
<p>Thanks.</p>
",,2020-08-23 14:03:20,Topic modelling - seeding specific words (Python),<gensim><lda><seeding><eta>,,,CC BY-SA 4.0,False,False,True,False,False
29180,63528600,2020-08-21 18:54:47,,"<p>I'm trying to figure out how to identify unigrams and bigrams in a text in R, and then keep both in the final output based on a threshold. I've done this in Python with gensim's Phraser model, but haven't figured out how to do it in R.</p>
<p>For example:</p>
<pre><code>strings &lt;- data.frame(text = 'This is a great movie from yesterday', 'I went to the movies', 'Great movie time at the theater', 'I went to the theater yesterday')
#Pseudocode below
bigs &lt;- tokenize_uni_bi(strings, n = 1:2, threshold = 2)
print(bigs)
[['this', 'great_movie', 'yesterday'], ['went', 'movies'], ['great_movie', 'theater'], ['went', 'theater', 'yesterday']]
</code></pre>
<p>Thank you!</p>
",,2020-08-22 08:40:25,Output text with both unigrams and bigrams in R,<r><nlp><n-gram>,,,CC BY-SA 4.0,False,False,True,False,False
29188,63499110,2020-08-20 05:56:51,,"<p>Hoping you can help. Im creating a web app with python and Flask. One of the the things that my web app will do is provide a smart document search. You can enter text and it will fetch results of documents similar to the portion of text you entered.</p>
<p>Ive used Flask for the front end to serve the HTML, manage any DB interactions required and display results. It will pass the query through to a Gensim similarity model and query it.</p>
<p>My question here is what is the best way to host these? Ive explored loading the model as part of loading flask but it slows things down quite a lot (its c. 6gb in memory) but it works. I can then query the model quite easily as its all within the same program scope.</p>
<p>My concern is that this would then not be scalable and possibly not best practice and that I may be better to host the model separately and make API calls to it from my Flask web app.</p>
<p>Thoughts and views would be much appreciated.</p>
<p>Thanks,
Pete</p>
",,2020-08-20 06:36:30,Best practice for deploying machine learning web app with Flask,<python><flask><gensim>,2020-08-20 21:29:48,,CC BY-SA 4.0,False,False,True,False,False
29196,63509864,2020-08-20 16:58:37,,"<p>I am using the Word2vec module of Gensim library to train a word embedding, the dataset is 400k sentences with 100k unique words (its not english)</p>
<p>I'm using this code to monitor and calculate the loss :</p>
<pre class=""lang-py prettyprint-override""><code>class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(&quot;Model loss:&quot;, model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))


monitor = MonitorCallback([&quot;MyWord&quot;])  # monitor with demo words

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT  , callbacks=[monitor])

w2v_model.build_vocab(tokenized_corpus)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(&quot;Vocab size&quot;, vocab_size)

print(&quot;[*] Training...&quot;)

# Train Word Embeddings
w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=W2V_EPOCH)
</code></pre>
<p>The problem is from epoch 1 the loss is 0 and the vector of the monitored words dont change at all!</p>
<pre><code>[*] Training...
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
</code></pre>
<p>so what is the problem here? is this normal?  the tokenized corpus is a list of lists that are something like tokenized_corpus[0] = [ &quot;word1&quot; , &quot;word2&quot; , ...]</p>
<p>I googled and seems like some of the old versions of gensim had problem with calculating loss function, but they are from almost a year ago and it seems like it should be fixed right now?</p>
<p>I tried the code provided in the answer of this question as well but still the loss is 0 :</p>
<p><a href=""https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim"">Loss does not decrease during training (Word2Vec, Gensim)</a></p>
<p>EDIT1 : after adding compute_loss=True, the loss shows up, but it keeps going higher and higher, and the top similar words and their similarity doesn't change at all :</p>
<pre><code>Model loss: 2187903.5
Model loss: 3245492.0
Model loss: 4103624.5
Model loss: 4798541.0
Model loss: 5413940.0
Model loss: 5993822.5
Model loss: 6532631.0
Model loss: 7048384.5
Model loss: 7547147.0
</code></pre>
",2020-08-20 18:23:32,2020-08-20 18:23:32,Gensim's word2vec has a loss of 0 from epoch 1?,<nlp><pytorch><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29198,63549977,2020-08-23 17:24:27,,"<p>I am doing research that requires direct manipulation &amp; embedding of one-hot vectors and I am trying to use gensim to load a pretrained word2vec model for this.</p>
<p>The problem is they don't seem to have a direct api for working with 1-hot-vectors. And I am looking for work arounds.</p>
<p>So I wanted to know if anyone knows of a way to do this? Or more specifically if these vocab indices (which are defined quite ambiguously). Could be indices into corresponding 1-hot-vectors?</p>
<p>Context I have found:</p>
<ul>
<li>Seems <a href=""https://stackoverflow.com/questions/40458742/gensim-word2vec-accessing-in-out-vectors"">this question</a> is related but I tried accessing the 'input embeddings' (assuming they were one-hot representations), via model.syn0 (from link in answer), but I got a non-sparse matrix...</li>
<li>Also appears <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">they refer to word indices as 'doctags'</a> (search for Doctag/index).</li>
<li><a href=""https://stackoverflow.com/questions/47117569/how-to-get-word2index-from-gensim"">Here</a> is another question giving some context to the indices (although not quite answering my question).</li>
<li><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">Here</a> is the official documentation:</li>
</ul>
<p>################################################</p>
<p>class gensim.models.keyedvectors.Vocab(**kwargs)
Bases: object</p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info, and for constructing binary trees (incl. both word leaves and inner nodes).</p>
<p>################################################</p>
",,2020-08-23 18:14:19,Is a gensim vocab index the index in the corresponding 1-hot-vector?,<gensim><word2vec><one-hot-encoding>,,,CC BY-SA 4.0,False,False,True,False,False
29203,63551484,2020-08-23 20:03:59,,"<p>I'm testing feeding gensim's Word2Vec different sentences with the same overall vocabulary to see if some sentences carry &quot;better&quot; information than others. My method to train Word2Vec looks like this</p>
<pre><code>def encode_sentences(self, w2v_params, sentences):
    model = Word2Vec(sentences, **w2v_params)
    
    idx_order = torch.tensor([int(i) for i in model.wv.index2entity], dtype=torch.long)
    X = torch.zeros((idx_order.max()+1, w2v_params['size']), dtype=torch.float)
    
    # Put embeddings back in order
    X[idx_order] = torch.tensor(model.wv.vectors)    
    return X, y
</code></pre>
<p>What I'm hoping for here, is each time w2v runs, it starts with a fresh model and trains from scratch. However, I'm testing 3 kinds of sentences, so my test code looks like this:</p>
<pre><code>def test(sentence):
    w2v = {'size': 128, 'sg': 1}
    X = encode_sentences(w2v, sentence)
    evaluate(X) # Basic cluster analysis stuff here

# s1, s2 and s3 are the 3 sets of sentences with the same vocabulary in different order/frequency
[print(test(s) for s in [s1, s2, s3]]
</code></pre>
<p>However, I noticed if I remove one of the test sets, and only test <code>s1</code> and <code>s2</code> (or any combination of 2 sets of the three), the overall quality of the clusterings decreases. If I go back into <code>encode_sentences</code> and add <code>del model</code> before the <code>return</code> call, the overall cluster quality also goes down but remains consistent no matter how many datasets are tested.</p>
<p>What gives? Is the constructor not actually building a fresh model each time with new weights? The docs and source code give no indication of this. I'm quite sure it isn't my evaluation method, as everything was fixed after the <code>del model</code> was added. I'm at a loss here... Are these runs actually independent, or is each call to <code>Word2Vec(foo, ...)</code> equivalent to retraining the previous model with <code>foo</code> as new data?</p>
<p>And before you ask, no <code>model</code> is nowhere outside of the scope of the <code>encode_sentence</code> variable; that's the only time that variable name is used in the whole program. Very odd.</p>
<h3>Edit with more details</h3>
<hr />
<p>If it's important, I'm using Word2Vec to build node embeddings on a graph the way Node2Vec does with different walk strategies. These embeddings are then fed to a Logistic Regression model (<code>evaluate(X)</code>) and which calculates area under the roc.</p>
<p>Here is some sample output of the model before adding the <code>del model</code> call to the <code>encode_sentences</code> method averaged over 5 trials:</p>
<pre><code>Random walks:   0.9153 (+/-) 0.002
Policy walks:   0.9125 (+/-) 0.005
E-greedy walks: 0.8489 (+/-) 0.011
</code></pre>
<p>Here is the same output with the only difference being <code>del model</code> in the encoding method:</p>
<pre><code>Random walks:   0.8627 (+/-) 0.005
Policy walks:   0.8527 (+/-) 0.002
E-greedy walks: 0.8385 (+/-) 0.009
</code></pre>
<p>As you can see, in each case, the variance is very low (the +/- value is the standard error) but the difference between the two runs is almost a whole standard deviation. It seems odd that if each call to <code>Word2Vec</code> was truly independent that manually freeing the data structure would have such a large effect.</p>
",2020-08-24 13:48:04,2020-08-24 13:48:04,Does the gensim `Word2Vec()` constructor make a completely independent model?,<python><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29229,63633739,2020-08-28 12:24:03,,"<p>I 've been some trouble updating a model from gensim. I use the following command to create the model.</p>
<p><code>model = gensim.models.Word2Vec(sentences,size=100, window=20, min_count=10, workers=24, iter=200, callbacks=[epoch_saver])</code>
and to save the model, I used:</p>
<p><code>model.save(type+&quot;/&quot;+&quot;word2vec_&quot;+name+&quot;_&quot;+type+&quot;.&quot;+version)</code></p>
<p>As far as I remember, the first step, when training a model, is to create a vocab. Anyway, I had to stop the training on iter=147. So, now I want to load and continue training. This is how I load the model:</p>
<p><code>model = gensim.models.Word2Vec.load(&quot;encoded_op_op/temporary_model/word2vec.model&quot;)</code></p>
<p>But how do I use train() method to continue update ? I am trying :</p>
<p><code>model = model.train(sentences, epochs=53, callbacks=[epoch_saver])</code></p>
<p>but it gives an error:</p>
<blockquote>
<p>You must specify either total_examples or total_words, for proper job parameters updationand progress
calculations. The usual value is total_examples=model.corpus_count.</p>
</blockquote>
<p>Anyway, where could I define the same parameters used when creating the model: size=100, window=20, min_count=10, workers=24. Ok. i believe size is already defined. but what about workers ?</p>
<p>Thanks in advance</p>
",,2020-08-28 18:34:26,How to update a trained Word2vec model in gensim with same parameters,<parameters><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29232,63646428,2020-08-29 11:26:11,,"<p>This is officially doing my head in. I am web scraping a collection of tweets for text analysis. The tweets have been scraped and put into a dataframe, where each row is a string containing the entire tweet. I can't for the life of me remove quotation marks or apostrophes, but removing all other punctuation is OK.</p>
<p>What I am trying to do is extract just the verbs, nouns and adjectives from each of the scraped tweets, which I have done, but anything in quotation marks is excluded.</p>
<p>The code that I have been using so far is below, but I can't for the life of me add quotation marks or apostrophes. I have also tried every other method I can find on this site , but it either does nothing, or produces errors.</p>
<pre><code>tweets['Text_processed'] = tweets['Text'].map(lambda x: re.sub('[,\@#.!?]', &quot;&quot;, x)) 
</code></pre>
<p>The entire code base up until this point is:</p>
<pre><code>import GetOldTweets3 as got
import pandas as pd
import re  
from wordcloud import WordCloud# Join the different processed titles together.
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter(&quot;ignore&quot;, DeprecationWarning)# Load the LDA model from sk-learn
from sklearn.decomposition import LatentDirichletAllocation as LDA
import os
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import plotly as py
import plotly.graph_objs as go
import gensim
from gensim import corpora, models, similarities
import logging
import tempfile
from nltk.corpus import stopwords
from string import punctuation
from collections import OrderedDict
import pyLDAvis.gensim
import tempfile

%matplotlib inline

init_notebook_mode(connected=True) #do not miss this line
warnings.filterwarnings(&quot;ignore&quot;)

# Function that pulls tweets based on a general search query and turns to csv file
# Parameters: (text query you want to search), (max number of most recent tweets to pull from)

def text_query_to_csv(text_query, count):
    # Creation of query object
    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query)\
                                                .setMaxTweets(count)
    # Creation of list that contains all tweets
    tweets = got.manager.TweetManager.getTweets(tweetCriteria)
    # Creating list of chosen tweet data
    text_tweets = [[tweet.date, tweet.text] for tweet in tweets]
    # Creation of dataframe from tweets
    tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text'])
    # Converting tweets dataframe to csv file
    tweets_df.to_csv('{}-{}-tweets.csv'.format(text_query, int(count)), sep=',')
</code></pre>
<pre><code>############################################
# Search word and number of tweets to scrape
############################################

text_query = '#barackobama'

count = 5

# Calling function to query X amount of relevant tweets and create a CSV file
text_query_to_csv(text_query, count)

filename = '#barackobama-5-tweets.csv'

tweets = pd.read_csv(filename)

# Convert tweets to strings and lower case
tweets['Text'] = tweets['Text'].astype(str)
tweets['Text'] = tweets['Text'].map(lambda x: x.lower())

tweets
</code></pre>
<p>This is the offending bit of code below...</p>
<pre><code># remove punctuation
tweets['Text_processed'] = tweets['Text'].map(lambda x: re.sub('[,\@#.!?]', &quot;&quot;, x)) 
tweets['Text_processed'].head()
</code></pre>
<pre><code>#####################################
# Extract nouns, verbs and adjectives
#####################################

import nltk
from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet

from IPython.display import display
lemmatizer = nltk.WordNetLemmatizer()

def leaves(tree):
    &quot;&quot;&quot;Finds NP (nounphrase) leaf nodes of a chunk tree.&quot;&quot;&quot;
    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):
        yield subtree.leaves()
        
def get_word_postag(word):
    if pos_tag([word])[0][1].startswith('J'):
        return wordnet.ADJ
    if pos_tag([word])[0][1].startswith('V'):
        return wordnet.VERB
    if pos_tag([word])[0][1].startswith('N'):
        return wordnet.NOUN
    else:
        return wordnet.NOUN
    
def normalise(word):
    &quot;&quot;&quot;Normalises words to lowercase and stems and lemmatizes it.&quot;&quot;&quot;
    word = word.lower()
    postag = get_word_postag(word)
    word = lemmatizer.lemmatize(word,postag)
    return word

def get_terms(tree):    
    for leaf in leaves(tree):
        terms = [normalise(w) for w,t in leaf]
        yield terms
        
tidied_tweets = []
        
for t in tweets['Text']:
    #word tokenizeing and part-of-speech tagger
    document = t
    tokens = [nltk.word_tokenize(sent) for sent in [document]]
    postag = [nltk.pos_tag(sent) for sent in tokens][0]
    
    # Rule for NP chunk and VB Chunk
    grammar = r&quot;&quot;&quot;
        NBAR:
            {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns
            {&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;JJ&gt;*&lt;VB.?&gt;+&lt;VB&gt;?} # Verbs and Verb Phrases
            
        NP:
            {&lt;NBAR&gt;}
            {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...
            
    &quot;&quot;&quot;
    #Chunking
    cp = nltk.RegexpParser(grammar)
    
    # the result is a tree
    tree = cp.parse(postag)
    
    terms = get_terms(tree)
    
    features = []
    for term in terms:
        _term = ''
        for word in term:
            _term += ' ' + word
        features.append(_term.strip())
        
    tidied_tweets.append(features)
    
tidied_tweets
</code></pre>
<p>The code base I have after this works OK, but the inability to remove quoted text or anything with an apostrophe is causing real problems.</p>
<p><strong>EDITED TO ADD</strong></p>
<p>I've managed to solve the problem, but in doing so, created another. The latest bit of code to extract the words sans the punctuation is:</p>
<pre><code>tweet_list = []
ind_tweet = []

for tweets in tidied_tweets:
    for words in tweets:
        a = re.findall(r&quot;[\w']+&quot;, words)
        ind_tweet.append(a)
    tweet_list.append(ind_tweet)
</code></pre>
<p><strong>re.findall(r&quot;[\w']+&quot;, words)</strong> does the job of extracting the word, but I can't create the same structured list I started with. What I wanted is 'tweet_list' to act as the parent list, and 'ind_tweet' to act as a sucession of child lists (nested). When I print out the result of the code above, I'm not able to create the nested list I am looking for. ind_tweet produces the output but in a single list with no nesting, and tweet_list duplicates ind_tweet. It probably isn't helping that it's 2:30am on a Saturday, but this should be much easier than I am making it...</p>
",2020-08-29 16:29:05,2020-08-29 17:07:46,Removing all punctuation from string in dataframe,<python><nlp>,,,CC BY-SA 4.0,True,False,True,False,True
29238,63567713,2020-08-24 19:40:43,,"<p>I have trained a word2vec model called <code>word_vectors</code>, using the Gensim package with size = 512.</p>
<pre><code>fname = get_tmpfile('word2vec.model')
word_vectors = KeyedVectors.load(fname, mmap='r')
</code></pre>
<p>Now, I have created a new Numpy array (also of size 512) which I have added to the word2vec as follows:</p>
<pre><code>vector = (rand(512)-0.5) *20
word_vectors.add('koffie', vector)
</code></pre>
<p>Doing this seems to go fine and even when I call</p>
<pre><code>word_vectors['koffie']
</code></pre>
<p>I get the array as output, as expected.</p>
<p>However, when I want to look for the most similar words in my model and run the following code:</p>
<pre><code>word_vectors.most_similar('koffie')
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-283-ce992786ce89&gt;&quot;, line 1, in &lt;module&gt;
    word_vectors.most_similar('koffie')

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 553, in most_similar
    mean.append(weight * self.word_vec(word, use_norm=True))

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 461, in word_vec
    result = self.vectors_norm[self.vocab[word].index]

IndexError: index 146139 is out of bounds for axis 0 with size 146138


word_vector.size()
Traceback (most recent call last):

  File &quot;&lt;ipython-input-284-2606aca38446&gt;&quot;, line 1, in &lt;module&gt;
    word_vector.size()

NameError: name 'word_vector' is not defined
</code></pre>
<p>The error seems to indicate that my indexing isn't correct here. But since I am only indexing indirectly (with a key rather than an actual numeric index), I don't see what I need to change here.</p>
<p>Who knows what goes wrong here? And what can I do to overcome this error?</p>
",,2020-08-24 21:10:23,IndexError: index is out of bounds - word2vec,<python-3.x><numpy><gensim><word2vec><index-error>,,,CC BY-SA 4.0,False,False,True,False,False
29266,63590239,2020-08-26 03:54:04,,"<p>So, I have a training data which is in a json file. Now, I am trying to use word Embedding(Word2Vec) on that. However, I have no idea how to proceed. Mostly everything I looked up just deals with direct sentences given and not with json file. Right now all I have done is to read a json file using python.</p>
<p>Please guide me on what could be done next. I trying to use gensim</p>
",2020-08-26 09:53:23,2020-08-26 09:53:23,Word2Vec with a json file,<python><json><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
29277,63513647,2020-08-20 21:55:48,,"<p>I'm trying to find an alternative for LDA , and i needs to calculate <strong>coherence score</strong> of subject extracted from my new model how can i calculate the coherence score without using the lda ( latent dirichlet allocation ) model. HELP plz !</p>
<p>here is an example of using the gensim.models.coherencemodel () method :</p>
<pre><code>model = LdaModel(common_corpus, 5, common_dictionary
coherence_score = CoherenceModel(model=model, corpus=common_corpus, coherence='u_mass')
</code></pre>
",,2020-08-20 21:55:48,"can we calculate coherence score of subjects like the gensim.models.coherencemodel () method does , without using the lda model?",<nlp><lda><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
29278,63514464,2020-08-20 23:28:52,,"<p>I have a list of sentences of a few topics (two) like the below:</p>
<pre><code>Sentences
Trump says that it is useful to win the next presidential election. 
The Prime Minister suggests the name of the winner of the next presidential election.
In yesterday's conference, the Prime Minister said that it is very important to win the next presidential election. 
The Chinese Minister is in London to discuss about climate change.
The president Donald Trump states that he wants to win the presidential election. This will require a strong media engagement.
The president Donald Trump states that he wants to win the presidential election. The UK has proposed collaboration. 
The president Donald Trump states that he wants to win the presidential election. He has the support of his electors. 
</code></pre>
<p>As you can see there is similarity in sentences.</p>
<p><a href=""https://i.stack.imgur.com/TVJ8f.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TVJ8f.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to relate multiple sentences and visualise the characteristics of them by using a graph (directed). The graph is built from a similarity matrix, by applying row ordering of sentences as shown above.
I created a new column, Time, to show the order of sentences, so first row (Trump says that....) is at time 1; second row (The Prime Minister suggests...) is at time 2, and so on.
Something like this</p>
<pre><code>Time    Sentences
1           Trump said that it is useful to win the next presidential election. 
2           The Prime Minister suggests the name of the winner of the next presidential election.

3           In today's conference, the Prime Minister said that it is very important to win the next presidential election. 

...
</code></pre>
<p>I would like then to find the relationships in order to have a clear overview of the topic.
Multiple paths for a sentence would show that there are multiple information associated with it.
To determine similarity between two sentences, I tried to extract nouns and verbs as follows:</p>
<pre><code>noun=[]
verb=[]
for  index, row in df.iterrows():
      nouns.append([word for word,pos in pos_tag(row[0]) if pos == 'NN'])
      verb.append([word for word,pos in pos_tag(row[0]) if pos == 'VB'])
</code></pre>
<p>as they are keywords in whatever sentence.
So when a keyword (noun or verb) appears in sentence x but not in the other sentences, it represents a difference between these two sentences.
I think a better approach, however, could be using word2vec or gensim (WMD).</p>
<p>This similarity has to be calculated for each sentence.
I would like to build a graph which shows the content of the sentence in my example above.
Since there are two topics (Trump and Chinese Minister), for each of them I need to look for sub-topics. Trump has sub-topic presidential election, for example. A node in my graph should represent a sentence. Words in each node represent differences for the sentences, showing new info in the sentence. For example, the word <code>states</code> in sentence at time 5 is in adjacent sentences at time 6 and 7.
I would like just to find a way to have similar results as shown in picture below. I have tried using mainly nouns and verbs extraction, but probably it is not the right way to proceed.
What I tried to do has been to consider sentence at time 1 and compare it with other sentences, assigning a similarity score (with noun and verbs extraction but also with word2vec), and repeat it for all the other sentences.
But my problem is now on how to extract difference to create a graph that can make sense.</p>
<p>For the part of the graph, I would consider to use networkx (DiGraph):</p>
<pre><code>G = nx.DiGraph()
N = Network(directed=True) 
</code></pre>
<p>to show direction of relationships.</p>
<p>I provided a different example to make it be clearer (but if you worked with the previous example, it would be fine as well. Apologies for the inconvenience, but since my first question was not so clear, I had to provide also a better, probably easier, example).</p>
",2020-10-10 18:25:31,2020-10-10 20:09:20,Graph to connect sentences,<python><nlp><nltk><networkx><word2vec>,,,CC BY-SA 4.0,True,False,True,False,False
29282,63637245,2020-08-28 15:59:06,,"<p>I am new to deep learning and I am trying to play with a pretrained word embedding model from a <a href=""https://www.cse.iitb.ac.in/%7Epb/papers/sltu-ccurl20-il-we.pdf"" rel=""nofollow noreferrer"">paper</a>. I  downloaded the following files:</p>
<p>1)sa-d300-m2-fasttext.model</p>
<p>2)sa-d300-m2-fasttext.model.trainables.syn1neg.npy</p>
<p>3)sa-d300-m2-fasttext.model.trainables.vectors_ngrams_lockf.npy</p>
<p>4)sa-d300-m2-fasttext.model.wv.vectors.npy</p>
<p>5)sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy</p>
<p>6)sa-d300-m2-fasttext.model.wv.vectors_vocab.npy</p>
<p>If in  case these details are needed
sa - sanskrit
d300 - embedding dimension
fastText - fastText</p>
<p>I dont have a prior experience with gensim, how can load the model into gensim or into tensorflow.</p>
<p>I tried</p>
<pre><code>from gensim.models.wrappers import FastText
FastText.load_fasttext_format('/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy')
</code></pre>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: '/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy.bin'</p>
</blockquote>
",2020-08-28 18:06:55,2020-08-29 03:33:16,How to load pre-trained fastText model in gensim with .npy extension,<gensim><pre-trained-model><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
29291,63625873,2020-08-28 00:32:53,,"<p>Well the issue is I have 1000s of the document and I passed all the documents for the training of Gensim model and I successfully trained and saved the model in .model format.</p>
<p>But with the current format, 2 new files have also been generated</p>
<ol>
<li>doc2vec.model</li>
<li>doc2vec.model.trainables.syn1neg.npy</li>
<li>doc2vec.model.wv.vectors.npy</li>
</ol>
<p>Due to the limitation of Hardware I trained and saved the model on Google Colab and Google Driver respectively. When I downloaded the generated models and extra files in my local machine and ran the code it's giving me a File Not Found Error, whereas I have added the particular files where the .py file is or current working directory is.</p>
<p>Well I used below code</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

files = readfiles(&quot;CuratedData&quot;)
data = [TaggedDocument(words=word_tokenize(_d.decode('utf-8').strip().lower()), tags=[str(i)]) for i, _d in enumerate(files)]

max_epochs = 100
vec_size = 300
alpha = 0.025

model = Doc2Vec(vector_size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm=1)

model.build_vocab(data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;doc2vec.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>Code for Loading the Model</p>
<pre><code>    webVec = &quot;&quot;
    try:

        path = os.path.join(os.getcwd(), &quot;doc2vec.model&quot;)

        model = Word2Vec.load(path)

        data = word_tokenize(content['htmlResponse'].lower())

        # Webvector
        webVec = model.infer_vector(data)
    except ValueError as ve:
        print(ve)
    except (TypeError, ZeroDivisionError) as ty:
        print(ty)
    except:
        print(&quot;Oops!&quot;, sys.exc_info()[0], &quot;occurred.&quot;)
</code></pre>
<p>Any help would be greatly appreciated. Thanks, Cheers</p>
",2020-08-28 10:16:56,2020-08-28 10:16:56,Gensim Model : class 'FileNotFoundError',<python><python-2.7><gensim><word2vec><doc2vec>,,,CC BY-SA 4.0,True,False,True,False,False
29324,63740186,2020-09-04 11:20:58,,"<p>I have trained FastText model for french language using Gensim library.
Suddenly, this trained model is not getting loaded into memory.</p>
<p>I am using below code :-</p>
<pre><code>from gensim.models import FastText
fname = &quot;filename&quot;
model = FastText.load(fname)
</code></pre>
<p>and it throws following error : -</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/fasttext.py&quot;, line 1070, in load
    model = super(FastText, cls).load(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/utils.py&quot;, line 426, in load
    obj = unpickle(fname)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/utils.py&quot;, line 1384, in unpickle
    return _pickle.load(f, encoding='latin1')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x86 in position 14072054: invalid start byte
</code></pre>
<p>As this model is trained on large dataset, is there any way to recover/load this model?</p>
",,2020-09-04 11:25:43,Unable to load gensim Fasttext model - UTF-8 unicode error,<python><encoding><pickle><gensim><fasttext>,,,CC BY-SA 4.0,False,False,True,False,False
29405,63749621,2020-09-05 01:17:56,,"<p>I have been trying topic modelling using gensim in Python. I have the following dataset:</p>
<p>Docs</p>
<pre><code>&quot;Sugar is bad to consume. My sister likes to have sugar, but not my father.&quot;
&quot;My father spends a lot of time driving my sister around to dance practice.&quot;
&quot;Doctors suggest that driving may cause increased stress and blood pressure.&quot;
&quot;Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.&quot;
&quot;Health experts say that Sugar is not good for your lifestyle.&quot;
</code></pre>
<p>I tried to lemmatise it as follows:</p>
<pre><code>texts = map(gensim.utils.lemmatize,Docs)
</code></pre>
<p>and run LDA:</p>
<pre><code>dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(doc) for doc in texts]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)
ldamodel.print_topics()
</code></pre>
<p>However I am getting an error. Do you know how to fix it?</p>
<p>thanks</p>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-15-b36df3b5374b&gt; in &lt;module&gt;
----&gt; 1 import pattern
      2 
      3 dictionary = gensim.corpora.Dictionary(Docs)
      4 corpus = [dictionary.doc2bow(doc) for doc in Docs]
      5 Lda = gensim.models.ldamodel.LdaModel

ModuleNotFoundError: No module named 'pattern'
</code></pre>
<p>The whole error message:</p>
<pre><code>---&gt; 3 dictionary = gensim.corpora.Dictionary(Docs)
      4 corpus = [dictionary.doc2bow(doc) for doc in Docs]
      5 Lda = gensim.models.ldamodel.LdaModel

/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py in __init__(self, documents, prune_at)
     82 
     83         if documents is not None:
---&gt; 84             self.add_documents(documents, prune_at=prune_at)
     85 
     86     def __getitem__(self, tokenid):

/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py in add_documents(self, documents, prune_at)
    195 
    196         &quot;&quot;&quot;
--&gt; 197         for docno, document in enumerate(documents):
    198             # log progress &amp; run a regular check for pruning, once every 10k docs
    199             if docno % 10000 == 0:

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in lemmatize(content, allowed_tags, light, stopwords, min_length, max_length)
   1676     if not has_pattern():
   1677         raise ImportError(
-&gt; 1678             &quot;Pattern library is not installed. Pattern library is needed in order to use lemmatize function&quot;
   1679         )
   1680     from pattern.en import parse

ImportError: Pattern library is not installed. Pattern library is needed in order to use lemmatize function
</code></pre>
",2020-09-05 21:22:51,2020-09-05 21:47:48,Topic modelling with gensim,<python><gensim><topic-modeling>,,,CC BY-SA 4.0,False,False,True,False,False
29414,63752033,2020-09-05 08:40:41,,"<p>I want to train a previous-trained word2vec model in a increased way that is update the word's weights if the word has been seen in the previous training process and create and update the weights of the new words that has not been seen in the previous training process. For example:</p>
<pre><code>from gensim.models import Word2Vec
# old corpus
corpus = [[&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;2&quot;, &quot;3&quot;, &quot;1&quot;]]
# first train on old corpus
model = Word2Vec(sentences=corpus, size=2, min_count=0, window=2)
# checkout the embedding weights for word &quot;1&quot;
print(model[&quot;1&quot;])

# here comes a new corpus with new word &quot;4&quot; and &quot;5&quot;
newCorpus = [[&quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;1&quot;, &quot;5&quot;, &quot;2&quot;]]

# update the previous trained model
model.build_vocab(newCorpus, update=True)
model.train(newCorpus, total_examples=model.corpus_count, epochs=1)

# check if new word has embedding weights:
print(model[&quot;4&quot;])  # yes

# check if previous word's embedding weights are updated
print(model[&quot;1&quot;])  # output the same as before
</code></pre>
<p>It seems that the previous word's embedding is not updated even though the previous word's context has benn changed in the new corpus. Could someone tell me how to make the previous embedding weights updated?</p>
",2020-09-07 06:26:29,2020-09-07 06:26:29,Gensim Word2vec model is not updating the previous word's embedding weights during increased training,<python><nlp><gensim><word2vec>,,,CC BY-SA 4.0,False,False,True,False,False
