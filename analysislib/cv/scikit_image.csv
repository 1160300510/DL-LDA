,Id,CreationDate,DeletionDate,Body,LastEditDate,LastActivityDate,Title,Tags,ClosedDate,CommunityOwnedDate,ContentLicense,opencv,matlab,emgucv,scikit-image,ffmpeg
860,15766012,2013-04-02 13:56:26,,"<p>Is it possibile deleting small branches during skeleton procedure?</p>

<p>As in this picture:
<a href=""http://felix.abecassis.me/wp-content/uploads/2011/09/skel_opencv.png"" rel=""nofollow noreferrer"">http://felix.abecassis.me/wp-content/uploads/2011/09/skel_opencv.png</a></p>

<p>Only O letter is perfect but not all the others letters.</p>

<p>THere is a way during the procedure or after to deleting this little branches?
I use python opencv, but a solution also with pymorph or scikit-image is good.
Here there is the code i used for skeletonization:
<a href=""https://stackoverflow.com/questions/15135676/problems-during-skeletonization-image-for-extracting-contours"">Code</a></p>

<p>Original image: <a href=""http://felix.abecassis.me/wp-content/uploads/2011/09/opencv.png"" rel=""nofollow noreferrer"">http://felix.abecassis.me/wp-content/uploads/2011/09/opencv.png</a></p>
",2017-05-23 11:45:32,2015-09-03 13:44:26,It is possible deleting Skeleton small branches in opencv or scikit image?,<opencv><image-processing><mathematical-morphology><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
1395,12934156,2012-10-17 12:17:42,,"<p>As I test <code>scikit-image</code> methods ,I came across <code>skimage.measure.perimeter(image)</code> but couldn't explain the output of this function.  </p>

<pre><code>import numpy as np  
image=np.zeros((100,100))  
image[10:30,10:30]=1                   # this creates a white square  
from skimage.measure import perimeter  
x=perimeter(image)  
print x                               #Should be (20+20+20+20) = 80  
76.0        &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; it returns this value
</code></pre>

<p>Did I misunderstand what this function should return . I know that <strong>perimeter</strong> is a path that surrounds an area .<br>
Note:-<br>
(1) The difference between the calculated perimeter and the returned perimeter not always by 4.as sometimes it can be 6  no matter it's a square a rectangle or any other polygon.<br>
Update:=<br>
(1) <a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#perimeter"" rel=""nofollow"">The function page</a> </p>
",2012-10-22 08:29:42,2012-10-22 08:29:42,Can anyone explain why `skimage.measure.perimeter(img)` returns this result?,<python><image-processing><scikits><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
2382,13021206,2012-10-22 23:02:34,,"<p>I am wondering how to <strong>smart sharpen</strong> an image using <code>python</code> or any  related image library like <code>ndimage</code> ,<code>skimage</code> or even <code>PIL</code>.I could find methods that actually sharpen my image but with a lot of noise and pixelating when zooming in .So since I know <code>Photoshop</code> I tried to get that smart sharpen effect which sharpens the image with a less noising and with a nice sweet contrast through python but I failed.  </p>

<p>Notes:-<br>
(1) methods has been tested:-  </p>

<pre><code>&gt;&gt;&gt; # The 1st Method:  
&gt;&gt;&gt; import Image                 
&gt;&gt;&gt; import ImageFilter
&gt;&gt;&gt; image.filter(ImageFilter.SHARPEN)               
&gt;&gt;&gt; Image.filter(ImageFilter.EDGE_ENHANCE_MORE)    #Look down:1st image created  

&gt;&gt;&gt; # The 2nd Method:
&gt;&gt;&gt; blurred_l=scipy.ndimage.gaussian_filter(b,3)  
&gt;&gt;&gt; filter_blurred_l = scipy.ndimage.gaussian_filter(blurred_l, 1)  
&gt;&gt;&gt; alpha =30  
&gt;&gt;&gt; sharpened = blurred_l + alpha * (blurred_l - filter_blurred_l)  

&gt;&gt;&gt; # The 3rd Method:  
&gt;&gt;&gt; shar=imfilter(Image,'sharpen')               #Look down:2nd image created
</code></pre>

<p>(2) I found a piece of code but it's in <code>Perl</code> . I only know <code>Python</code> <a href=""http://astoryworthtelling.wordpress.com/2011/03/28/smart-sharpening-in-imagemagick/"" rel=""nofollow noreferrer"">In here</a>  or <a href=""http://www.columbia.edu/~jhb2147/smartsharp.pl"" rel=""nofollow noreferrer""> directly</a>
(3) Here are 2 of the sharpened image using above methods the third done with <em>smartsharp</em>:<br>
Original<br>
<a href=""http://imageshack.us/a/img600/6640/babyil.jpg"" rel=""nofollow noreferrer"">Original image http://imageshack.us/a/img600/6640/babyil.jpg</a><br>
First ....................................................................second<br>
<a href=""http://imageshack.us/a/img803/3897/sharp1.png"" rel=""nofollow noreferrer"">1st http://imageshack.us/a/img803/3897/sharp1.png</a>  <a href=""http://imageshack.us/a/img809/2235/sharp2.png"" rel=""nofollow noreferrer"">2nd http://imageshack.us/a/img809/2235/sharp2.png</a><br>
third MY GOAL>that's the effect I want<br>
<a href=""http://imageshack.us/a/img832/4563/smartsharp.jpg"" rel=""nofollow noreferrer"">3rd http://imageshack.us/a/img832/4563/smartsharp.jpg</a><br>
(4) Here are the tool I used to create the third image above:<br>
<a href=""http://imageshack.us/a/img210/2747/smartsharpentoolm.jpg"" rel=""nofollow noreferrer"">smrsharpm http://imageshack.us/a/img210/2747/smartsharpentoolm.jpg</a>    <a href=""http://imageshack.us/a/img193/490/smartsharpentool.jpg"" rel=""nofollow noreferrer"">smrsharp http://imageshack.us/a/img193/490/smartsharpentool.jpg</a></p>
",2012-10-23 10:40:17,2012-10-24 21:06:02,"How can I get the ""smart sharpen"" effect on my images with python?",<python><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
2611,15914684,2013-04-10 00:05:10,,"<p>I have many skeletonized images like this: </p>

<p><img src=""https://i.stack.imgur.com/vdosF.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/u52Gg.gif"" alt=""enter image description here""></p>

<p>How can i detect a cycle, a loop in the skeleton?
Are there ""special"" functions that do this or should I implement it as a graph?</p>

<p>In case there is only the graph option, can the python graph library NetworkX can help me?</p>
",2013-04-10 15:42:33,2013-05-17 11:01:03,How can i find cycles in a skeleton image with python libraries?,<python><opencv><image-processing><mathematical-morphology><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
2613,15915387,2013-04-10 01:24:57,,"<p>I create skeleton images from binary images, <a href=""http://homepages.inf.ed.ac.uk/rbf/HIPR2/images/art7skl1.gif"" rel=""nofollow"">like that</a></p>

<p>I detect end points of that skeletons with mahotas python library but it returns me the full image array with 1 value for end point and zero the others.
I prefer to detect end points' coordinates.
How can i get them?</p>

<p>My Code that compute endpoints:</p>

<pre><code>branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])
branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])
branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])
branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])
branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])
branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])
branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])
branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])
branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])

endpoint1=np.array([[0, 0, 0], [0, 1, 0], [2, 1, 2]])
endpoint2=np.array([[0, 0, 0], [0, 1, 2], [0, 2, 1]])
endpoint3=np.array([[0, 0, 2], [0, 1, 2], [0, 2, 1]])
endpoint4=np.array([[0, 2, 1], [0, 1, 2], [0, 0, 0]])
endpoint5=np.array([[2, 1, 2], [0, 1, 0], [0, 0, 0]])
endpoint6=np.array([[1, 2, 0], [2, 1, 0], [0, 0, 0]])
endpoint7=np.array([[2, 0, 0], [1, 1, 0], [2, 0, 0]])
endpoint8=np.array([[0, 0, 0], [2, 1, 0], [1, 2, 0]])

jpg = 'skel.jpg'
skel = cv2.imread(jpg, 0)

sk = pymorph.binary(skel)

complete_path = 'skel.jpg'
print pymorph.gray(sk).dtype


br1=mah.morph.hitmiss(sk,branch1)
br2=mah.morph.hitmiss(sk,branch2)
br3=mah.morph.hitmiss(sk,branch3)
br4=mah.morph.hitmiss(sk,branch4)
br5=mah.morph.hitmiss(sk,branch5)
br6=mah.morph.hitmiss(sk,branch6)
br7=mah.morph.hitmiss(sk,branch7)
br8=mah.morph.hitmiss(sk,branch8)
br9=mah.morph.hitmiss(sk,branch9)

ep1=mah.morph.hitmiss(sk,endpoint1)
ep2=mah.morph.hitmiss(sk,endpoint2)
ep3=mah.morph.hitmiss(sk,endpoint3)
ep4=mah.morph.hitmiss(sk,endpoint4)
ep5=mah.morph.hitmiss(sk,endpoint5)
ep6=mah.morph.hitmiss(sk,endpoint6)
ep7=mah.morph.hitmiss(sk,endpoint7)
ep8=mah.morph.hitmiss(sk,endpoint8)

br=br1+br2+br3+br4+br5+br6+br7+br8+br9
ep=ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8
</code></pre>

<p>br and ep are the array with all branches and endpoint from i want get coordinates.</p>
",,2013-04-10 01:41:56,How can i get coordinates point of a skeleton?,<python><opencv><image-processing><mathematical-morphology><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
2754,13055096,2012-10-24 18:04:43,,"<p>This is an extended question of <a href=""https://stackoverflow.com/questions/13021206/how-can-i-get-the-smart-sharpen-effect-on-my-images-with-python"">this ques </a>.Thanks to <em>Kindall</em> and <em>Stephan Van der Wallt</em> ,it turns out in order to solve the previous problem, I need to understand how to apply <code>deconvolution</code> process on an image using <code>python</code> with any related packages.<br>
since I only know <code>python</code> , you may want to show me how to convert the <code>MatLab</code> code in this link <a href=""http://yuzhikov.com/articles/BlurredImagesRestoration1.htm"" rel=""nofollow noreferrer"">MatLap code</a> using python and I'm only interested in  ""Convolution theorem - practice part"",that would be a great help.<br>
I need also to understand what a <strong>convolve or deconvolve method</strong> does to the image.I googled trying to figure it out  but there are a lot of equations that I couldn't fully understand.  </p>

<p>Note:<br>
(1) Any explanation on how <code>deconvolve</code> works ,it certainly will be appreciated.<br>
Thanks   </p>
",2017-05-23 11:51:38,2012-10-24 21:43:34,how to apply a deconvolution method on an image?,<image-processing><numpy><python-2.7><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
2920,13073375,2012-10-25 16:47:35,,"<p>I know you can write <code>python</code> scripts for <code>GIMP</code> and then through <code>GIMP</code> you can use these scripts as plug-ins but what I'm trying to achieve is to use <code>GIMP's functions</code> through <code>Python</code> as if you would use <code>PIL</code> or <code>scikit-image</code> functions through <code>Python</code>.I don't want to disturb my program user to actually download <code>Gimp</code> only to execute the one function I need from it.  </p>

<p>Can I do such a thing ??? or I'm going too far?<br>
If no, what is the best alternative thing I can do to force <code>Gimp</code> does the work undercover without disturbing the program user.in this case I think I would be forced to install <code>GIMP</code> within the installation process which is something I hate.  </p>

<p>Update:-<br>
(1) considering <code>libgimp</code> after googling I  found this <a href=""http://gimp.1065349.n5.nabble.com/libGimp-td17666.html"" rel=""nofollow"">discussion</a>.in brief words > "" <code>libgimp</code> is just used to allow the <code>GIMP core</code> and it's plug-ins to communicate"" so it doesn't work as I hope.<br>
(2) I thought of accessing the <code>GIMP</code> code and deleting all unnecessary functions I don't need and keep the only function I'm gonna use,that would be a one function GIMP.Do you think I should do something like that?  </p>

<p><strong>Update</strong>:-<br>
Yes the problem was solved but I'm still interested in using <code>GIMP</code> as a library by any means even if it was undercover as <em>Marawan</em> has suggested.Hope you can help. </p>

<p>Thanks.</p>
",2012-10-26 21:03:36,2014-06-06 20:52:24,Can I write a stand alone python script using gimp commands?,<python><image-processing><gimp>,,,CC BY-SA 3.0,False,False,False,True,False
4625,16094485,2013-04-18 22:59:57,,"<p>I have a skeleton image of wheels with 2,4, or 6 diameters drawn.
I have also branched point coordinates.</p>

<p>I think about 2 ways for detecting the different wheels:</p>

<ol>
<li>Counting black areas inside the circle</li>
<li>counting diameters drawn</li>
</ol>

<p>In both cases i do not know how could i implement them.</p>

<p><img src=""https://i.stack.imgur.com/ojLMB.jpg"" alt=""wheel 1"">
<img src=""https://i.stack.imgur.com/y8YBi.jpg"" alt=""wheel 2"">
<img src=""https://i.stack.imgur.com/3uEjc.jpg"" alt=""wheel 3"">
<img src=""https://i.stack.imgur.com/ooN6s.jpg"" alt=""wheel 4"">
<img src=""https://i.stack.imgur.com/N3Mpd.jpg"" alt=""wheel 5""></p>

<p>As you can see, wheels are not perfect skeletonized, and so it is harder detecting differences.</p>

<p>This is the code i use for skeletonization:</p>

<p>first of all i binarize image, i dilate and then skeletonize.</p>

<pre><code>from skimage import io
import scipy
from skimage import morphology
import cv2
from scipy import ndimage as nd
import mahotas as mah
import pymorph as pm
import pymorph

complete_path = ""wheel1.jpg""
gray = cv2.imread(complete_path,0)
print(gray.shape)
cv2.imshow('graybin',gray)
cv2.waitKey()

ret,thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY_INV) 
imgbnbin = thresh
print(""shape imgbnbin"")
print(imgbnbin.shape)
cv2.imshow('binaria',imgbnbin)
cv2.waitKey()

element = cv2.getStructuringElement(cv2.MORPH_CROSS,(6,6)) 
graydilate = cv2.dilate(imgbnbin, element) #imgbnbin
graydilate = cv2.dilate(graydilate, element)
#graydilate = cv2.erode(graydilate, element)

cv2.imshow('dilate',graydilate)
cv2.waitKey()   

#SKELETONIZE
out = morphology.skeletonize(graydilate&gt;0)
skel = out.astype(float)
cv2.imshow('scikitimage',skel)
cv2.waitKey()
io.imsave('wheel.jpg', skel)    
sk = skel
print(sk.shape)
</code></pre>

<p>Original images:</p>

<p><img src=""https://i.stack.imgur.com/ANG8p.jpg"" alt=""wheel 1"">
<img src=""https://i.stack.imgur.com/z0Qqg.jpg"" alt=""wheel 2"">
<img src=""https://i.stack.imgur.com/B8ASe.jpg"" alt=""wheel 3"">
<img src=""https://i.stack.imgur.com/kl4vi.jpg"" alt=""wheel 4"">
<img src=""https://i.stack.imgur.com/JgP3X.jpg"" alt=""wheel 5""></p>
",,2013-05-01 18:10:37,How can i detect number of segments in a circle in a skeletonized image?,<python><opencv><image-processing><mathematical-morphology><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
5400,16156788,2013-04-22 21:10:18,,"<p>I tried to find contour with cv2 python library in a skeletonized image created with scikit-image and i got this error:</p>

<pre><code>    contours, hierarchy = cv2.findContours(skel,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
TypeError: &lt;unknown&gt; data type = 0 is not supported
</code></pre>

<p>My question is: What i have to do to convert to cv2 and viceversa?</p>

<p>I know that opencv use numpy.uint8 type to represent binary images instead scikit-image numpy.float64</p>

<p>I used also mahotas (numpy.bool) and pymorph libraries.
How can i convert from scikit-image to these libraries and viceversa?</p>
",2013-04-22 23:00:15,2018-11-25 09:43:34,How can i convert images from scikit-image to opencv2 and other libraries?,<python><opencv><image-processing><contour><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
6338,16241708,2013-04-26 17:06:00,,"<p>I have 2 functions that use mahotas python library for detecting branched point and end point in an image.</p>

<p>The 2 functions:</p>

<pre><code>def branchedPoints(skel):
    branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])
    branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])
    branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])
    branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])
    branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])
    branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])
    branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])
    branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])
    branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])
    br1=mh.morph.hitmiss(skel,branch1)
    br2=mh.morph.hitmiss(skel,branch2)
    br3=mh.morph.hitmiss(skel,branch3)
    br4=mh.morph.hitmiss(skel,branch4)
    br5=mh.morph.hitmiss(skel,branch5)
    br6=mh.morph.hitmiss(skel,branch6)
    br7=mh.morph.hitmiss(skel,branch7)
    br8=mh.morph.hitmiss(skel,branch8)
    br9=mh.morph.hitmiss(skel,branch9)
    return br1+br2+br3+br4+br5+br6+br7+br8+br9

def endPoints(skel):
    endpoint1=np.array([[0, 0, 0],[0, 1, 0],[2, 1, 2]])
    endpoint2=np.array([[0, 0, 0],[0, 1, 2],[0, 2, 1]])
    endpoint3=np.array([[0, 0, 2],[0, 1, 1],[0, 0, 2]])
    endpoint4=np.array([[0, 2, 1],[0, 1, 2],[0, 0, 0]])
    endpoint5=np.array([[2, 1, 2],[0, 1, 0],[0, 0, 0]])
    endpoint6=np.array([[1, 2, 0],[2, 1, 0],[0, 0, 0]])
    endpoint7=np.array([[2, 0, 0],[1, 1, 0],[2, 0, 0]])
    endpoint8=np.array([[0, 0, 0],[2, 1, 0],[1, 2, 0]])
    ep1=mh.morph.hitmiss(skel,endpoint1)
    ep2=mh.morph.hitmiss(skel,endpoint2)
    ep3=mh.morph.hitmiss(skel,endpoint3)
    ep4=mh.morph.hitmiss(skel,endpoint4)
    ep5=mh.morph.hitmiss(skel,endpoint5)
    ep6=mh.morph.hitmiss(skel,endpoint6)
    ep7=mh.morph.hitmiss(skel,endpoint7)
    ep8=mh.morph.hitmiss(skel,endpoint8)
    ep = ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8
    return ep
</code></pre>

<p>There is a way to obtain these functions with Scikit-image library?
<a href=""http://scikit-image.org/docs/0.7.0/api/skimage.morphology.html"" rel=""nofollow"">Morphology section</a> of scikit image hasn't got hit and miss transform.</p>
",,2013-06-24 20:16:33,hit and miss transform for detecting branched point and endpoint in scikit-image,<python><image-processing><scikit-image><mathematical-morphology>,,,CC BY-SA 3.0,False,False,False,True,False
6872,13407470,2012-11-15 22:36:27,,"<p>I have an image and I want to get the pixels that cross through its medial axis.
I tried to use <em>skeletonize</em> and <em>medial axis</em> methods in order to get them but both methods return one dimensional line which is shorter than the corresponding object.  </p>

<p>Here's the code with a sample image:-</p>

<pre><code>&gt;&gt;&gt; import skimage.filter  
&gt;&gt;&gt; import skimage.morphology  
&gt;&gt;&gt; import numpy as np  
&gt;&gt;&gt; import scipy.misc
&gt;&gt;&gt; im=scipy.misc.imread('img.jpg')   
&gt;&gt;&gt; thr=skimage.filter.threshold_otsu(im)  
&gt;&gt;&gt; im=im &gt; thr # Threshold the image  
&gt;&gt;&gt; im_sk=skimage.morphology.skeletonize(im)  
&gt;&gt;&gt; mask=np.where(im_sk==1)     # pixels of skeleton  
&gt;&gt;&gt; im[mask]= 0                 # this will color the skeleton with black  
</code></pre>

<p>Original    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Result<br>
<a href=""http://imageshack.us/a/img23/9035/testwus.jpg"" rel=""nofollow noreferrer"">or_im http://imageshack.us/a/img23/9035/testwus.jpg</a> <a href=""http://imageshack.us/a/img585/1910/imgskx.jpg"" rel=""nofollow noreferrer"">sk_im http://imageshack.us/a/img585/1910/imgskx.jpg</a>  </p>

<p>As you can see the black line isn't connected to the tip of the shape.<br>
(1) How can I get a fully connected one dimensional medial axis line that represents the length of the shapes in the image.<br>
(2) How can I get the pixels that are perpendicular to the medial axis (as I want to draw perpendicular lines from one side to another crossing the medial axis of the shape)  </p>

<ul>
<li>I need any python library that can do this stuff.</li>
</ul>

<p>Thanks</p>
",2012-11-16 20:40:07,2012-11-17 01:57:07,How can I get a full medial-axis line with its perpendicular lines crossing it?,<image-processing><opencv><computer-vision><scipy><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
7296,14402011,2013-01-18 15:23:21,,"<p>Which is the difference between WeightedNormalizedMoments, WeightedHuMoments and HuMoments? (<a href=""http://scikit-image.org/docs/0.6/api/skimage.measure.html"" rel=""nofollow noreferrer"">http://scikit-image.org/docs/0.6/api/skimage.measure.html</a>)</p>

<p>There are other shape properties scale-rotation invariant except HuMoment? There are example that show me how can i implement them? I find this example in c++ <a href=""https://stackoverflow.com/questions/8675878/opencvc-calculating-moments-from-contour"">OpenCV(C): calculating moments FROM contour</a> but i prefer working in python</p>
",2017-05-23 12:18:48,2013-02-22 07:10:18,There are other shape properties scale-rotation invariant except HuMoment?,<python><image-processing><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
7452,12484849,2012-09-18 20:51:53,,"<p>I am very interested in python image analysis and especially in Scikit-image.</p>

<p>As a newbie would it be possible to get an explanation how to convert the Matlab codes below to python+scikit-image.</p>

<p>Detecting a Cell Using Image Segmentation</p>

<p><a href=""http://www.mathworks.com/help/images/examples/detecting-a-cell-using-image-segmentation.html"" rel=""nofollow"">http://www.mathworks.com/help/images/examples/detecting-a-cell-using-image-segmentation.html</a></p>

<p>Granulometry of Snowflakes</p>

<p><a href=""http://www.mathworks.com/products/image/examples.html?file=/products/demos/shipping/images/ipexsnow.html"" rel=""nofollow"">http://www.mathworks.com/products/image/examples.html?file=/products/demos/shipping/images/ipexsnow.html</a></p>

<p>Thanks a lot </p>
",2013-01-14 15:52:03,2014-05-24 19:29:34,Segmentation and Granulometry with scikit-image,<python><image><analysis><image-segmentation>,,,CC BY-SA 3.0,False,True,False,True,False
7610,16347499,2013-05-02 20:51:11,,"<p>I would like to delete the circumference (the outermost perimeter of a sign) of this wheel skeleton sign except what's inside.
I think about a function findcontours() and delete the largest contour that i found</p>

<p>This is the input image:</p>

<p><img src=""https://i.stack.imgur.com/gWtis.jpg"" alt=""Original image""></p>

<p>Skeletonized:</p>

<p><img src=""https://i.stack.imgur.com/wCsEj.jpg"" alt=""skeleton image""></p>

<p>but unfortunately this is my output image:</p>

<p><img src=""https://i.stack.imgur.com/ZE4fx.jpg"" alt=""skeleton image without largest contour""></p>

<p>Why it does not remain only with 2 crossed segments and a segment is made by a lot of dots</p>

<pre><code>from __future__ import division
import mahotas as mh
import pymorph as pm
import numpy as np

import os
import math

import cv2
from skimage import io
import scipy
from skimage import morphology

complete_path = 'DUPLInuova/ruote 7/e (11).jpg'

fork = mh.imread(complete_path)  
fork = fork[:,:,0]# extract one component, ex R 

#structuring elements
disk7 = pm.sedisk(3)#size 7x7: 7=3+1+3
disk5 = pm.sedisk(2)

#Just a simple thresholding with white background
bfork = fork &lt; 150
bfork = mh.morph.dilate(bfork, disk7)

gray = cv2.imread(complete_path,0)
originale = gray
print(""gray"")
print(gray.shape)
cv2.imshow('graybin',gray)
cv2.waitKey()

ret,thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY_INV) 
imgbnbin = thresh
print(""shape imgbnbin"")
print(imgbnbin.shape)
cv2.imshow('binaria',imgbnbin)
cv2.waitKey()
shape = list(gray.shape)
w = int( (shape[0]/100 )*5)
h = int((shape[1]/100)*5)
print(w)
print(h)
element = cv2.getStructuringElement(cv2.MORPH_CROSS,(w,h)) #con 4,4 si vede tutta la stella e riconosce piccoli oggetti
from skimage.morphology import square

graydilate = np.array(imgbnbin, dtype=np.float64)
graydilate = morphology.binary_dilation(graydilate, square(w))
graydilate = morphology.binary_dilation(graydilate, square(w))

out = morphology.skeletonize(graydilate&gt;0)
img = out.astype(float)
cv2.imshow('scikitimage',img)
cv2.waitKey()
img = img.astype(np.uint8)
cv2.imshow('scikitconvert',img)
cv2.waitKey()

contours, hierarchy = cv2.findContours(img,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
print(len(contours))

# calculating area for deleting little signs
Areacontours = list()
calcarea = 0.0
unicocnt = 0.0
for i in range (0, len(contours)):
    area = cv2.contourArea(contours[i])
    #print(""area"")
    print(area)
    if (area &gt; 90 ): 
        if (calcarea&lt;area):
            calcarea = area
            unicocnt = contours[i]

cnt = unicocnt
ara = cv2.contourArea(cnt)
print(""cnt"")
print(ara)

#delete largest contour
cv2.drawContours(img,[cnt],0,(0,255,0),1)
cv2.imshow('img del contour',img)
cv2.waitKey()
</code></pre>

<p><strong>UPDATE SOLUTION</strong> (and new question):</p>

<p>if i make a deep copy of the skeletonized img after this line of code: 
    img = img.astype(np.uint8) #after skeletonization procedure</p>

<p>I can use find_contour with copied image and apply a draw_contour to original image and that's all!</p>

<p>My questions are: </p>

<p>Why find contour edit my image and i'm forced to use a temporary image?
Why matplotlib show me the right result and cv2 imshow don't (it show me a black image)?</p>

<p>New part of code:</p>

<pre><code>import copy
imgcontour = copy.copy(img)

imgcnt = img
contours, hierarchy = cv2.findContours(imgcontour,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE )
print(len(contours))

cnt = contours[0]

cv2.drawContours(img,[cnt],0,(0,0,0),1)

cv2.imshow('imgcv2black',img)
cv2.waitKey()

plt.gray()
plt.subplot(121)
plt.imshow(img)
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/1JKXw.png"" alt=""correct result in matplotlib""></p>

<p><strong>UPDATE FLOODFILE+DILATE:</strong></p>

<p>It is correct the floodfill-dilate procedure?
Where its' wrong?</p>

<pre><code>a = np.ones((212,205), dtype=np.uint8)
#myMask = zeros(a.shape[0:2], dtype = uint8)

maskr = np.zeros(a.shape,np.uint8)
print(maskr.shape)
print(img[0])

cv2.floodFill(img,mask =maskr, seedPoint = (0,0), newVal = 1)
element = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)) 
img = cv2.dilate(img, element)

cv2.imshow('flood',img)
cv2.waitKey()
plt.gray()
plt.subplot(121)
plt.imshow(img)
plt.show()
</code></pre>

<p>and i unfortunately obtain this:</p>

<p><img src=""https://i.stack.imgur.com/UzYRP.jpg"" alt=""floodfil+dilate""></p>
",2013-05-06 23:27:22,2013-11-04 03:14:23,How can i delete largest contour in a skeleton image in python?,<python><opencv><image-processing><contour><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
11525,16691924,2013-05-22 12:34:05,,"<p>I have binary skeletonized images and I use python library mahotas for extracting end-points and branched points.</p>

<p>I do not like mahotas <em>thin</em> function (there are too many little branches) and so I chose scikit-image <em>skeletonize</em> function.</p>

<p>Now troubles begin: in some images it doesn't more extract branched point.
Why?</p>

<p><a href=""http://scikit-image.org/docs/dev/api/skimage.morphology.html#skeletonize"" rel=""nofollow noreferrer"">Scikit image function</a> accepts both boolean and integer values (mahotas uses boolean).</p>

<p><img src=""https://i.stack.imgur.com/cYQ1K.jpg"" alt=""image with no branched point detected""></p>

<p><img src=""https://i.stack.imgur.com/iHhFG.jpg"" alt=""iamge with branched point detected""></p>

<pre><code>from skimage import morphology
import mahotas as mh
import pymorph as pm
import numpy as np
import cv2
from matplotlib import pyplot as plt
import scipy

def branchedPoints(skel):
    branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])
    branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])
    branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])
    branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])
    branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])
    branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])
    branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])
    branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])
    branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])
    br1=mh.morph.hitmiss(skel,branch1)
    br2=mh.morph.hitmiss(skel,branch2)
    br3=mh.morph.hitmiss(skel,branch3)
    br4=mh.morph.hitmiss(skel,branch4)
    br5=mh.morph.hitmiss(skel,branch5)
    br6=mh.morph.hitmiss(skel,branch6)
    br7=mh.morph.hitmiss(skel,branch7)
    br8=mh.morph.hitmiss(skel,branch8)
    br9=mh.morph.hitmiss(skel,branch9)
    return br1+br2+br3+br4+br5+br6+br7+br8+br9

def endPoints(skel):
    endpoint1=np.array([[0, 0, 0],[0, 1, 0],[2, 1, 2]])
    endpoint2=np.array([[0, 0, 0],[0, 1, 2],[0, 2, 1]])
    endpoint3=np.array([[0, 0, 2],[0, 1, 1],[0, 0, 2]])
    endpoint4=np.array([[0, 2, 1],[0, 1, 2],[0, 0, 0]])
    endpoint5=np.array([[2, 1, 2],[0, 1, 0],[0, 0, 0]])
    endpoint6=np.array([[1, 2, 0],[2, 1, 0],[0, 0, 0]])
    endpoint7=np.array([[2, 0, 0],[1, 1, 0],[2, 0, 0]])
    endpoint8=np.array([[0, 0, 0],[2, 1, 0],[1, 2, 0]])
    ep1=mh.morph.hitmiss(skel,endpoint1)
    ep2=mh.morph.hitmiss(skel,endpoint2)
    ep3=mh.morph.hitmiss(skel,endpoint3)
    ep4=mh.morph.hitmiss(skel,endpoint4)
    ep5=mh.morph.hitmiss(skel,endpoint5)
    ep6=mh.morph.hitmiss(skel,endpoint6)
    ep7=mh.morph.hitmiss(skel,endpoint7)
    ep8=mh.morph.hitmiss(skel,endpoint8)
    ep = ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8
    return ep

def pruning(skeleton, size):

    for i in range(1, size):
        endpoints = endPoints(skeleton)
        endpoints = np.logical_not(endpoints)
        skeleton = np.logical_and(skeleton,endpoints)
    return skeleton


path = 'signs/a (0).jpg'

fork = mh.imread(path)  
imgbnbin = fork[:,:,0]

shape = list(fork.shape)

w =  (shape[0]/100 )*3.5

#structuring elements
disk7 = pm.sedisk(w)
disk5 = pm.sedisk(3)
disk3 = pm.sedisk(0.5)      

bfork = imgbnbin &lt; 150

plt.gray()
plt.subplot(121)
plt.title(""after binarization"")
plt.imshow(bfork)
plt.show()

bfork = mh.morph.dilate(bfork, disk7)

bfork = np.array(bfork, dtype=np.bool)
#Pota cose inutili

bfork = mh.morph.close(bfork, disk3)

# Skeleton+Pruning
#skelFk = mh.thin(bfork)
bfork = np.array(bfork, dtype=np.uint8)
skelFk = morphology.skeletonize(bfork)
skelFk = np.array(skelFk, dtype=np.bool)

skelF_pruned = pruning(skelFk, 15)

#end points (Ep) from skeletons
## fork (Fk) sign
print(""skelfpruned before of endpoint"")
print(skelF_pruned[70])
EpFk = endPoints(skelF_pruned)
EpFk_p = endPoints(skelF_pruned)
EpFk_p = mh.dilate(EpFk_p,disk5)

# counting end-points
lab_Ek, n1 = mh.label(EpFk)
lab_Ekp, n1p = mh.label(EpFk_p)

print n1, ' end points on fork like image'
print n1p, ' end points on fork like image, after pruning'

#branched points
## Merge too close points by morphological dilation
### Fork
BpFk = branchedPoints(skelF_pruned)# br points on Fork

print(""branched point"")
bcols,brows = np.where(BpFk)
print(brows)
print(bcols)

print(""end point"")
ecols,erows = np.where(EpFk)
print(erows)

img = skelF_pruned

# viene dilatato per mostrare meglio il punto di giunzione
BpFk = mh.morph.dilate(BpFk, disk5)

## count branched points
lab_Ek, n3 = mh.label(BpFk)

print n3, ' branched points on fork like image'

#Overlay:
#Display end-points in blue
#        branched-points in yellow
#        skeleton in red 
display_Fk = pm.overlay(imgbnbin, red = img&gt;0, blue = EpFk_p&gt;0, yellow = BpFk&gt;0)     
plt.gray()
plt.subplot(121)
plt.imshow(imgbnbin)
plt.imshow(display_Fk)
plt.show()
</code></pre>
",2013-10-26 00:02:35,2013-10-26 00:02:35,How can I detect points after a scikit image's skeletonization?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
11627,16697391,2013-05-22 16:49:30,,"<p>I want to decide about a Python computer vision library. I had used OpenCV in C++, and like it very much. However this time I need to develop my algorithm in Python. My short list has three libraries:
1- <em>OpenCV</em> (Python wrapper)
2- <em>PIL</em> (Python Image Processing Library)
3- <em>scikit-image</em></p>

<p><strong>Would you please help me to compare these libraries?</strong></p>

<p>I use <em>numpy, scipy, scikit-learn</em> in the rest of my code. The performance and ease of use is an important factor, also, portability is an important factor for me.</p>

<p>Thanks for your help</p>
",,2013-05-22 17:40:20,Comparing computer vision libraries in python,<python><opencv><image-processing><computer-vision><scikit-learn>,2013-05-22 21:25:20,,CC BY-SA 3.0,True,False,False,True,False
11923,20954361,2014-01-06 16:22:02,,"<p>i have an image like this:</p>

<p><img src=""https://i.stack.imgur.com/rPrj7.jpg"" alt=""star""></p>

<p>after I skeletonize it by scikit image's skeletonize  function</p>

<pre><code>from skimage import morphology
out = morphology.skeletonize(gray&gt;0)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Bju1h.png"" alt=""enter image description here""></p>

<p>There is a way for counting the number of black spaces? (in this picture six) except the background in scikit-image or mahotas? </p>
",,2014-03-19 13:46:53,How can i count number of black spaces in scikit image or mahotas?,<image-processing><numpy><scikit-image><mahotas>,,,CC BY-SA 3.0,False,False,False,True,False
13284,21061814,2014-01-11 11:45:06,,"<p>How can I read an image from an Internet URL in Python cv2?</p>

<p>This <a href=""https://stackoverflow.com/questions/11253820/python-urllib2-and-opencv"">Stack&nbsp;Overflow answer</a>,</p>

<pre><code>import cv2.cv as cv
import urllib2
from cStringIO import StringIO
import PIL.Image as pil
url=""some_url""

img_file = urllib2.urlopen(url)
im = StringIO(img_file.read())
</code></pre>

<p>is not good because Python reported to me:</p>

<pre><code>TypeError: object.__new__(cStringIO.StringI) is not safe, use cStringIO.StringI.__new__
</code></pre>
",2018-03-01 12:31:12,2019-03-06 15:42:50,"How can I read an image from an Internet URL in Python cv2, scikit image and mahotas?",<python><opencv><image-processing><scikit-image><mahotas>,,,CC BY-SA 3.0,True,False,False,True,False
15014,22333700,2014-03-11 18:36:35,,"<p>I take the equalization code from <a href=""http://scikit-image.org/docs/0.9.x/auto_examples/plot_equalize.html"" rel=""nofollow"">here</a></p>

<pre><code>import numpy as np
from skimage import morphology
from skimage import color
from skimage import io
from matplotlib import pyplot as plt
from skimage import data, img_as_float
from skimage import exposure


img = color.rgb2gray(io.imread(path))

# Contrast stretching
p2 = np.percentile(img, 2)
p98 = np.percentile(img, 98)
#img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))
img_rescale = exposure.rescale_intensity(img, out_range=(0, 255))

# Equalization
img_eq = exposure.equalize_hist(img)

# Adaptive Equalization
img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)
</code></pre>

<p>and when i use this kind of image: </p>

<p><a href=""http://i.stack.imgur.com/pzBWU.jpg"" rel=""nofollow"">image</a></p>

<p>i got this error:</p>

<pre><code>Run time Error!
</code></pre>

<p>python stopped and respond: </p>

<pre><code>this application has requested the runtime to terminate it in an unusual way!
</code></pre>

<p>at this line:</p>

<pre><code>p2 = np.percentile(img, 2)
</code></pre>
",2014-03-11 22:25:44,2014-03-11 22:25:44,Why python raise a runtime error while i run numpy.percentile for equalization by scikit-image?,<python><image-processing><numpy><runtime-error><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
15087,22339555,2014-03-12 00:26:18,,"<p>I'm working in python with many images.
While I'm analyzing this <img src=""https://i.stack.imgur.com/pzBWU.jpg"" alt=""image""></p>

<p>when I use:</p>

<pre><code>from skimage import color
from skimage import io
img = color.rgb2gray(io.imread(path))
</code></pre>

<p>I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\wamp\www\NewIESP\FUNZIONApy\up1feature.py"", line 180, in &lt;module&gt;
    updatefeatures(infile)
  File ""C:\wamp\www\NewIESP\FUNZIONApy\up1feature.py"", line 43, in updatefeatures
    temp = compareup1features.comparison(path)
  File ""C:\wamp\www\NewIESP\FUNZIONApy\compareup1features.py"", line 496, in comparison
    skellist = moduloSkeleton.skelfeatures(path1img)
  File ""C:\wamp\www\NewIESP\FUNZIONApy\moduloSkeleton.py"", line 1418, in skelfeatures
    img = color.rgb2gray(io.imread(path))
  File ""C:\Python27\Lib\site-packages\skimage\color\colorconv.py"", line 661, in rgb2gray
    return _convert(gray_from_rgb, rgb[:, :, :3])[..., 0]
  File ""C:\Python27\Lib\site-packages\skimage\color\colorconv.py"", line 462, in _convert
    return np.ascontiguousarray(out)
  File ""C:\Python27\Lib\site-packages\numpy\core\numeric.py"", line 409, in ascontiguousarray
    return array(a, dtype, copy=False, order='C', ndmin=1)
MemoryError
</code></pre>

<p>How can I solve this memory error?
If it is not possible, how can I say to ""recognize"" this kind of image and skip it!?</p>
",2014-03-12 08:25:09,2014-03-12 08:25:09,How can i solve a memory error of python scikit image function color.rgb2gray()?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
15447,23516870,2014-05-07 11:42:56,,"<p>One of my friend was working on following project:</p>

<p>Below is the microscopic (SEM) image of the surface of stainless steel. </p>

<p><img src=""https://i.stack.imgur.com/inN4c.jpg"" alt=""enter image description here""></p>

<p>But you can see, it is corroded a little bit (after long exposure to marine environment) and some pits are formed on the surface. Some of the pits are marked in red circle. </p>

<p>He needs to find number of pits in the image and he was counting it manually (imagine, there are nearly 150 images). So I thought of automating this process with any image processing tool.</p>

<blockquote>
  <p><strong>Question:</strong></p>
  
  <p>How can I find the number of pits in this image?</p>
</blockquote>

<hr>

<p><strong>What I tried:</strong></p>

<p>As a first step, I improved the contrast a little bit by closing operation.</p>

<pre><code>import numpy as np
import cv2
from matplotlib import pyplot as plt

img = cv2.imread('6.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))

close = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)

close2 = cv2.add(close,1)
div = (np.float32(gray)+1)/(close2)
div2 = cv2.normalize(div,None, 0,255, cv2.NORM_MINMAX)
div3 = np.uint8(div2)
</code></pre>

<p>Result:</p>

<p><img src=""https://i.stack.imgur.com/joVHG.jpg"" alt=""enter image description here""></p>

<p>Then I applied some threshold for 127 and find contours in it. Later these contours are filtered based on their area (there is no specific information on the area, I took a range of 1-10 as an empirical value).</p>

<pre><code>ret, thresh = cv2.threshold(div3, 127,255, cv2.THRESH_BINARY_INV)
temp, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

res = np.zeros(gray.shape,np.uint8)

for cnt in contours:
    if 1.0 &lt; cv2.contourArea(cnt) &lt; 10.0:
        res = cv2.drawContours(res, [cnt], 0, 255, -1)

plt.subplot(121); plt.imshow(img, 'gray'); plt.subplot(122); plt.imshow(res,'gray'); plt.show() 
</code></pre>

<p>But it ended up in a lot of extra noise. See the result below:</p>

<p><img src=""https://i.stack.imgur.com/t8j35.jpg"" alt=""enter image description here""></p>

<hr>

<p><strong>Additional Information:</strong></p>

<p>Some test images:</p>

<p><img src=""https://i.stack.imgur.com/y2cmk.jpg"" alt=""enter image description here""> 
<img src=""https://i.stack.imgur.com/F6nxx.jpg"" alt=""enter image description here""></p>
",,2014-08-11 20:04:24,Finding pits in an image,<matlab><opencv><image-processing><computer-vision><scikit-image>,,,CC BY-SA 3.0,True,True,False,True,False
15754,22391254,2014-03-13 21:25:04,,"<p>To better understand gabor filters and kernels in image processing, I am trying to place my own images into a Gabor texture comparison template from the <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_gabor.html"" rel=""nofollow"">scikit-image</a> site. </p>

<p>The code modification I used was adding first line below and pointing second line to my new variable. Previous structured paralleled following line in code quoted below.</p>

<pre><code>img=plt.imread('greenBalloon.png') #This is code I added to read in the local png file

brick = img_as_float(img)[shrink] #Using pre-existing line with my swapped variable
</code></pre>

<p>I continue to get errors when trying to swap out the included sample data file with another png present in the working directory.  I get the following error:</p>

<p>""RuntimeError: filter weights array has incorrect shape."" </p>

<p>How should I re-write this code to pull in a locally saved image or, preferably, set of images in place of the sample(s)?</p>

<pre><code>from __future__ import print_function

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage as nd

from skimage import data
from skimage.util import img_as_float
from skimage.filter import gabor_kernel


def compute_feats(image, kernels):
    feats = np.zeros((len(kernels), 2), dtype=np.double)
    for k, kernel in enumerate(kernels):
        filtered = nd.convolve(image, kernel, mode='wrap')
        feats[k, 0] = filtered.mean()
        feats[k, 1] = filtered.var()
    return feats


def match(feats, ref_feats):
    min_error = np.inf
    min_i = None
    for i in range(ref_feats.shape[0]):
        error = np.sum((feats - ref_feats[i, :])**2)
        if error &lt; min_error:
            min_error = error
            min_i = i
    return min_i


# prepare filter bank kernels
kernels = []
for theta in range(4):
    theta = theta / 4. * np.pi
    for sigma in (1, 3):
        for frequency in (0.05, 0.25):
            kernel = np.real(gabor_kernel(frequency, theta=theta,
                                          sigma_x=sigma, sigma_y=sigma))
            kernels.append(kernel)

shrink = (slice(0, None, 3), slice(0, None, 3))

img=plt.imread('greenBalloon.png') #This is code I added to read in the local png file

brick = img_as_float(img)[shrink] #Using pre-existing line with my swapped variable
grass = img_as_float(data.load('grass.png'))[shrink]
wall = img_as_float(data.load('rough-wall.png'))[shrink]
image_names = ('brick', 'grass', 'wall')
images = (brick, grass, wall)

# prepare reference features
ref_feats = np.zeros((3, len(kernels), 2), dtype=np.double)
ref_feats[0, :, :] = compute_feats(brick, kernels)
ref_feats[1, :, :] = compute_feats(grass, kernels)
ref_feats[2, :, :] = compute_feats(wall, kernels)

print('Rotated images matched against references using Gabor filter banks:')

print('original: brick, rotated: 30deg, match result: ', end='')
feats = compute_feats(nd.rotate(brick, angle=190, reshape=False), kernels)
print(image_names[match(feats, ref_feats)])

print('original: brick, rotated: 70deg, match result: ', end='')
feats = compute_feats(nd.rotate(brick, angle=70, reshape=False), kernels)
print(image_names[match(feats, ref_feats)])

print('original: grass, rotated: 145deg, match result: ', end='')
feats = compute_feats(nd.rotate(grass, angle=145, reshape=False), kernels)
print(image_names[match(feats, ref_feats)])


def power(image, kernel):
    # Normalize images for better comparison.
    image = (image - image.mean()) / image.std()
    return np.sqrt(nd.convolve(image, np.real(kernel), mode='wrap')**2 +
                   nd.convolve(image, np.imag(kernel), mode='wrap')**2)

# Plot a selection of the filter bank kernels and their responses.
results = []
kernel_params = []
for theta in (0, 1):
    theta = theta / 4. * np.pi
    for frequency in (0.1, 0.4):
        kernel = gabor_kernel(frequency, theta=theta)
        params = 'theta=%d,\nfrequency=%.2f' % (theta * 180 / np.pi, frequency)
        kernel_params.append(params)
        # Save kernel and the power image for each image
        results.append((kernel, [power(img, kernel) for img in images]))

fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(5, 6))
plt.gray()

fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)

axes[0][0].axis('off')

# Plot original images
for label, img, ax in zip(image_names, images, axes[0][1:]):
    ax.imshow(img)
    ax.set_title(label, fontsize=9)
    ax.axis('off')

for label, (kernel, powers), ax_row in zip(kernel_params, results, axes[1:]):
    # Plot Gabor kernel
    ax = ax_row[0]
    ax.imshow(np.real(kernel), interpolation='nearest')
    ax.set_ylabel(label, fontsize=7)
    ax.set_xticks([])
    ax.set_yticks([])

    # Plot Gabor responses with the contrast normalized for each filter
    vmin = np.min(powers)
    vmax = np.max(powers)
    for patch, ax in zip(powers, ax_row[1:]):
        ax.imshow(patch, vmin=vmin, vmax=vmax)
        ax.axis('off')

plt.show()
</code></pre>

<p>Any other insights on processing images with gabor patches would be quite helpful.</p>
",,2014-09-21 22:46:31,Gabor filter bank - modification of display template,<python><image-processing><filtering><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
16433,24703005,2014-07-11 17:06:18,,"<p>I want to train my classifier with some images, some of which have different dimensions.</p>

<p>They all fall under the following dimensions:</p>

<ul>
<li>100x50 </li>
<li>50x100 </li>
<li>64x72 </li>
<li>72x64</li>
</ul>

<p>However, with 9 orientation bins, and 8 pixels per cell, each of these generates 648 HoG features. </p>

<p>I actually chose all images to be of one of these sizes so that they would end up having the same number of HoG features so that training is uniform. </p>

<p>The reason I opted for this is because the object of interest in the training images sometimes has a different aspect ratio, hence cropping all the images the same size for some of the images left too much background in there.</p>

<p>Now my question is - does it matter what the aspect ratio/image dimensions of the training images are, as long as the number of HoG features is consistent? (My training algorithm only takes in the HoG features).</p>
",,2016-01-12 18:09:21,Training a classifier using images of different dimensions but same number of HoG features,<image><opencv><image-processing><classification><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
16937,20199683,2013-11-25 17:41:40,,"<p>I want to route through an array with <a href=""http://scikit-image.org/docs/dev/api/skimage.graph.html#route-through-array"" rel=""nofollow"">skimage.graph.route_through_array</a>. I have one start point and multiple destination points. I am wondering if it is possible to insert the multiple destinations into the function. I know I could also loop over the function, but I am looking for something faster.</p>
",2020-08-01 05:50:48,2020-08-01 05:50:48,Skimage graph route to multiple destinations,<python><arrays><opencv><routes><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
17067,21371827,2014-01-27 01:56:56,,"<p>I used histogram equalization and adaptation for erase illumination from the <strong>grayscale images</strong>, but  after the <a href=""http://scikit-image.org/docs/0.9.x/auto_examples/plot_equalize.html"" rel=""nofollow noreferrer"">histogram equalization</a> (i used scikit image python library) was good, during image conversion in <strong>mahotas</strong> something goes wrong. I got a picture total black. How can i fix it?</p>

<ul>
<li>Source image:</li>
</ul>

<p><img src=""https://i.stack.imgur.com/WPinF.jpg"" alt=""Source image""></p>

<ul>
<li>Histogram equalization and adaptation;</li>
</ul>

<p><img src=""https://i.stack.imgur.com/2uCVQ.png"" alt=""histogram equalization on an image""></p>

<ul>
<li>Result after mahotas conversion.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/Ek0RX.png"" alt=""maho conv""></p>

<p>conversion code from scikit to mahotas:</p>

<pre><code>binimg = np.array(img_adapteq, dtype=np.bool)
</code></pre>

<p>Source code:</p>

<pre><code>import scipy
import numpy as np
import pymorph as pm
import mahotas as mh
from skimage import morphology
from skimage import io
from matplotlib import pyplot as plt
from skimage import data, img_as_float
from skimage import exposure
def plot_img_and_hist(img, axes, bins=256):
    """"""Plot an image along with its histogram and cumulative histogram.

    """"""
    img = img_as_float(img)
    ax_img, ax_hist = axes
    ax_cdf = ax_hist.twinx()

    # Display image
    ax_img.imshow(img, cmap=plt.cm.gray)
    ax_img.set_axis_off()

    # Display histogram
    ax_hist.hist(img.ravel(), bins=bins, histtype='step', color='black')
    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))
    ax_hist.set_xlabel('Pixel intensity')
    ax_hist.set_xlim(0, 1)
    ax_hist.set_yticks([])

    # Display cumulative distribution
    img_cdf, bins = exposure.cumulative_distribution(img, bins)
    ax_cdf.plot(bins, img_cdf, 'r')
    ax_cdf.set_yticks([])

    return ax_img, ax_hist, ax_cdf


mhgray = mh.imread(path,0)
binimg = mhgray[:,:,0]
print(type(binimg[0][0]))
thresh = mh.otsu(binimg)
gray =( binimg&lt; thresh)

shape = list(gray.shape)
w = 0
if (shape[0] &gt; shape[1]):
    shape = shape[0]
else:
    shape = shape[1]

if (shape &lt; 100):
    w =  int((shape/100 )*1.5)
elif(shape &gt; 100 and shape &lt;420):
    w =  int((shape/100 )*2.5)
else:
    w = int((shape/100)*4)
disk7 = pm.sedisk(w)

img = binimg

# Contrast stretching
p2 = np.percentile(img, 2)
p98 = np.percentile(img, 98)
img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))

# Equalization
img_eq = exposure.equalize_hist(img)

# Adaptive Equalization
img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)

# Display results
f, axes = plt.subplots(2, 4, figsize=(8, 4))

ax_img, ax_hist, ax_cdf = plot_img_and_hist(img, axes[:, 0])
ax_img.set_title('Low contrast image')

y_min, y_max = ax_hist.get_ylim()
ax_hist.set_ylabel('Number of pixels')
ax_hist.set_yticks(np.linspace(0, y_max, 5))

ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_rescale, axes[:, 1])
ax_img.set_title('Contrast stretching')

ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_eq, axes[:, 2])
ax_img.set_title('Histogram equalization')

ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_adapteq, axes[:, 3])
ax_img.set_title('Adaptive equalization')

ax_cdf.set_ylabel('Fraction of total intensity')
ax_cdf.set_yticks(np.linspace(0, 1, 5))

# prevent overlap of y-axis labels
plt.subplots_adjust(wspace=0.4)
plt.show()


plt.gray()
plt.subplot(121)
plt.title(""after histo"")
plt.imshow(img_adapteq)
plt.show()

binimg = np.array(img_adapteq, dtype=np.bool)#uint16

plt.gray()
plt.subplot(121)
plt.title(""after otsu"")
plt.imshow(binimg)
plt.show()

imgbnbin = mh.morph.dilate(binimg, disk7)

#2     
plt.gray()
plt.subplot(121)
plt.title(""after dilate before close"")
plt.imshow(imgbnbin)
plt.show()

imgbnbin = mh.morph.close(imgbnbin, disk7)
#2     
plt.gray()
plt.subplot(121)
plt.title(""before skeletonize"")
plt.imshow(imgbnbin)
plt.show()

imgbnbin = mh.morph.close(imgbnbin, disk7)
out = morphology.skeletonize(imgbnbin&gt;0)
</code></pre>
",2014-01-27 11:06:17,2014-01-27 14:56:30,Why performing an histogram equalization by scikit image to a binary image i got a black image after mahotas conversion?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
17723,21430938,2014-01-29 12:18:42,,"<p>I am trying to convert an image from RGB to XYZ using scikit-image. I found out that there are some differences depending the input type:</p>

<pre><code>from numpy import array,uint8
import skimage.color

rgb = array([array([[56,79,132],[255,100,70]])]) 
i1 = skimage.color.rgb2xyz(rgb)#rgb.dtype -&gt;dtype('int32')
i2 = skimage.color.rgb2xyz(rgb.astype(uint8))
i3 = skimage.color.rgb2xyz(rgb.astype(float))

print i1[0,1,:]
print i2[0,1,:]
print i3[0,1,:]
</code></pre>

<p>This is the output:</p>

<pre><code>[  5.55183419e-09   4.73226247e-09   3.02426596e-09]
[ 0.46907236  0.3082294   0.09272133]
[ 240644.54537677  153080.21825017   39214.47581034]
</code></pre>

<p>The cause of the differences is the function <code>img_to_float</code> which is used inside <code>rgb2xyz</code> (see <a href=""https://stackoverflow.com/questions/21429261/array-conversion-using-scikit-image-from-integer-to-float"">this question</a>). </p>

<p>But I am wondering: What is the correct way to use <code>rgb2xyz</code>? </p>

<p>Regarding <a href=""https://stackoverflow.com/questions/17764744/why-is-there-such-a-difference-between-rgb-to-xyz-color-conversions"">this question</a> there are multiple solutions, depending on the formula, but again: what is the correct image type that is <strong>required</strong> by <code>rgb2xyz</code>? It seems that <code>unit8</code>, but why? Thanks!</p>
",2017-05-23 12:11:44,2014-01-29 14:30:57,RGB to XYZ in Scikit-Image,<python><image-processing><numpy><colors><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
17899,24822209,2014-07-18 09:57:07,,"<p>How we can read the 16 uint jpeg images in python 
please suggest me the libraries which can read the these type of files in python.
i tried matplotlib, scipy, scikit-image, medpy ,Pil ,opencv, numpy libraries. 
when we are using these libraries i am getting the out put as:</p>

<pre><code>raise IOError(""cannot identify image file"")

IOError: cannot identify image file
</code></pre>

<p>please help me</p>

<p>find the file from the link</p>

<p><a href=""https://drive.google.com/file/d/0B4l5GiM7kBXraDEyMXdseENfUlE/edit?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B4l5GiM7kBXraDEyMXdseENfUlE/edit?usp=sharing</a></p>
",2014-07-19 06:35:03,2015-01-02 08:46:22,How we can read 16 un signed integer(16 uint) jpeg files in python,<python><opencv><numpy><matplotlib>,,,CC BY-SA 3.0,True,False,False,True,False
18208,21466829,2014-01-30 20:31:04,,"<p>I used histogram equalization and adaptation for erase illumination from the grayscale images:</p>

<pre><code>import scipy
import numpy as np
import pymorph as pm
import mahotas as mh
from skimage import morphology
from skimage import io
from matplotlib import pyplot as plt
from skimage import data, img_as_float
from skimage import exposure

mhgray = io.imread(path)
mhgray = mhgray[:,:,0]

#thresh = mh.otsu(binimg)
#gray =( binimg&lt; thresh)
img = color.rgb2gray(mhgray)   
#img = mhgray #binimg

#from skimage import exposure
#print dir(exposure)

# Contrast stretching
p2 = np.percentile(img, 2)
p98 = np.percentile(img, 98)
#img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))
img_rescale = exposure.rescale_intensity(img, out_range=(0, 255))

# Equalization
img_eq = exposure.equalize_hist(img)

# Adaptive Equalization
img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)
</code></pre>

<p>but after the <a href=""http://scikit-image.org/docs/0.9.x/auto_examples/plot_equalize.html"" rel=""nofollow noreferrer"">histogram equalization</a>, i use otsu method:</p>

<pre><code>thresh = mh.otsu(binimg) 
gray =( binimg&lt; thresh)
</code></pre>

<p>the thresh value for the next example is: 16329</p>

<p>Source image: </p>

<p><img src=""https://i.stack.imgur.com/O4nzP.jpg"" alt=""enter image description here""></p>

<p>After histogram equalization and adaptation:</p>

<p><img src=""https://i.stack.imgur.com/fZ9xi.png"" alt=""source image""></p>

<p>After Otsu method:</p>

<p><img src=""https://i.stack.imgur.com/7Ebk3.png"" alt=""Image after Otsu""></p>

<p>The image before Otsu is an array of uint16, after Otsu is a numpy array of bool.</p>

<p>In <a href=""https://stackoverflow.com/questions/21152497/how-can-i-subtract-the-background-from-an-image-in-mahotas-and-opencv-in-python"">stackoverflow</a> suggested me to use histogram equalization to avoid illumination problems.</p>

<p>It is for the grey background?
How can i fix it?</p>
",2017-05-23 12:11:44,2014-02-02 15:21:10,Why after histogram equalization (scikit image) and Otsu mahotas method in some images get out big white squares?,<image-processing><scikit-image><mahotas>,,,CC BY-SA 3.0,False,False,False,True,False
18560,22619506,2014-03-24 20:04:59,,"<p>How can i see in if a binary image is almost all black or all white in numpy or scikit-image modules ?</p>

<p>I thought about <code>numpy.all</code> function or <code>numpy.any</code> but i do not know how neither for a total black image nor for a almost black image.</p>
",2014-03-24 20:13:03,2014-03-24 20:23:49,How can i check in numpy if a binary image is almost all black?,<python><opencv><image-processing><numpy><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
18829,24893824,2014-07-22 17:21:44,,"<p>I'm moving my Matlab image processing algorithms to Python using scikit-image tools, and I'm calculating the gray level co-occurrence matrix (<a href=""http://www.fp.ucalgary.ca/mhallbey/the_glcm.htm"" rel=""nofollow noreferrer"">GLCM</a>) using <a href=""http://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=greycomatrix#skimage.feature.greycomatrix"" rel=""nofollow noreferrer"">greycomatrix</a>. I have a problem if the parameter <code>levels</code> is lesser than the maximum value of the intensity image (<code>image.max()</code>). For instance: </p>

<pre><code>import numpy as np
from skimage.feature import greycomatrix
image = np.array([[0, 0, 1, 1],[0, 0, 1, 1],[0, 2, 2, 2],[2, 2, 3, 3]], dtype=np.uint8)
result = greycomatrix(image, distances = [1], angles = [0], levels = 4, symmetric=True)
</code></pre>

<p>The output is:</p>

<pre><code>glcm = result[:,:,0,0]

array([[4, 2, 1, 0],
   [2, 4, 0, 0],
   [1, 0, 6, 1],
   [0, 0, 1, 2]], dtype=uint32)
</code></pre>

<p>which is correct, a 4x4 matrix. But if <code>levels=3</code>, I can't calculate the GLCM, and the error is:</p>

<pre><code>result = greycomatrix(image, distances = [1], angles = [0], levels = 3, symmetric=True)

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/lib/python3.4/site-packages/skimage/feature/texture.py"", line 97, in greycomatrix
assert image.max() &lt; levels
AssertionError
</code></pre>

<p>And of course ... I get the error, but I should be able to calculate a GLCM (3x3 matrix) with levels lesser than <code>image.max()</code>. For instance, for:</p>

<pre><code>result = greycomatrix(image, distances = [1], angles = [0], levels = 3, symmetric=True)
</code></pre>

<p>I should get the following GLCM (I can do it in Matlab):</p>

<pre><code>4     3     0
3    10     1
0     1     2
</code></pre>

<p>When I work with huge images I reduce the the levels of the GLCM in order to reduce the calculation time. Is there any problem with the <code>greycomatrix</code> or I'm thinking wrong? 
Thanks in advance.</p>
",2017-05-18 22:41:20,2017-06-05 19:09:01,levels parameter in greycomatrix scikit-image python,<python><numpy><image-processing><scikit-image><glcm>,,,CC BY-SA 3.0,False,True,False,True,False
19183,23824147,2014-05-23 08:12:56,,"<p>I'm using HoG features for object detection via classification.</p>

<p>I'm confused about how to deal with HoG feature vectors of different lengths.</p>

<p>I've trained my classifier using training images that all have the same size.</p>

<p>Now, I'm extracting regions from my image on which to run the classifier - say, using the sliding windows approach. Some of the windows that I extract are a lot bigger than the size of images the classifier was trained on. (It was trained on the smallest possible size of the object that might be expected in test images).</p>

<p>The problem is, when the windows I need to classify are bigger than the training image sizes, then the HoG feature vector is also much bigger than the trained model's feature vector.</p>

<p>So how can I use the model's feature vector to classify the extract window?</p>

<p>For example, let's take the dimensions of one extracted window, which is 360x240, and call it <code>extractedwindow</code>. Then let's take one of my training images, which is only 20x30, and call it <code>trainingsample</code>.</p>

<p>If I take the HoG feature vectors, like this:</p>

<pre><code>fd1, hog_image1 = hog(extractedwindow, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True, normalise=True)

fd2, hog_image2 = hog(trainingsample, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualise=True, normalise=True)

print len(fd1)
print len(fd2)
</code></pre>

<p>Then this is the difference in length between the feature vectors:</p>

<pre><code>2640
616
</code></pre>

<p>So how is this dealt with? Are extracted windows supposed to be scaled down to the size of the samples the classifier was trained on? Or should the parameters for HoG features be changed/normalized according to each extracted window? Or is there another way to do this? </p>

<p>I'm personally working in python, using scikit-image, but I guess the problem is independent of what platform I'm using. </p>
",2014-05-23 08:28:09,2014-05-23 08:28:09,Choosing/Normalizing HoG parameters for object detection?,<python><computer-vision><feature-extraction><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
19356,21566326,2014-02-05 00:22:55,,"<p>I want to modify image files (bmp, jpg, and png) by adding text on top of it in a specific area. For example if I were to draw an picture for an envelope and print different addresses on top in a designated area with specified fonts and colors which library would I use?</p>

<p>I am looking for something like this <a href=""http://python-catalin.blogspot.com/2010/06/add-text-on-image-with-pil-module.html"" rel=""nofollow"">http://python-catalin.blogspot.com/2010/06/add-text-on-image-with-pil-module.html</a>
But with a BSD license that can work with numpy and scipy with Python 3 since I just switched.</p>

<p>Is this a job for Scikit-image or Pillow (Not sure if it is BSD)? if not what can I use and which part of the library?</p>
",2014-02-05 01:15:39,2014-02-05 06:55:57,Python image processing library,<python><image-processing><scikit-image>,2014-02-08 19:08:37,,CC BY-SA 3.0,False,False,False,True,False
19531,22700033,2014-03-27 21:55:36,,"<p>I wrote a little script to transform pictures of chalkboards into a form that I can print off and mark up.</p>

<p>I take an image like this:</p>

<p><img src=""https://i.stack.imgur.com/7Esco.jpg"" alt=""enter image description here""></p>

<p>Auto-crop it, and binarize it. Here's the output of the script:</p>

<p><img src=""https://i.stack.imgur.com/ba92D.png"" alt=""enter image description here""></p>

<p>I would like to remove the largest connected black regions from the image. Is there a simple way to do this?</p>

<p>I was thinking of eroding the image to eliminate the text and then subtracting the eroded image from the original binarized image, but I can't help thinking that there's a more appropriate method.</p>
",2014-03-28 03:51:54,2014-04-01 15:52:58,Remove features from binarized image,<python><image-processing><numpy><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
19580,24956179,2014-07-25 12:47:58,,"<p>the morphology operators differ in Scipy ndimage and Scikit image. I suppose, boundary conditions are treated in different way:</p>

<pre><code>import numpy as np
from scipy import ndimage
from skimage import morphology

scp = ndimage.binary_erosion(np.ones((10,10),dtype=""uint8""),).astype(""uint8"")
sci = morphology.binary_erosion(np.ones((10,10),dtype=""uint8""),morphology.disk(1))
</code></pre>

<p>scp results as expected, but sci does not:</p>

<pre><code>&gt;&gt;&gt;&gt; scp
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)

&gt;&gt;&gt;&gt; sci
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)
</code></pre>

<p>How can I set boundary condition in  scikit-image morphology operators?</p>

<p>Best regards</p>
",2014-07-25 13:15:09,2014-07-25 16:43:10,Morphology erosion - difference betwen Scipy ndimage and Scikit image,<python><image-processing><numpy><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
19635,22709407,2014-03-28 09:57:04,,"<p>Does anyone know if I can use the OpenCV library in python 3.X? or should I stick to using <code>scikit-image</code> for task like object recognition?</p>

<p>I want to try out openCV but decided last year to swich from 2.7.</p>

<p>I searched for packets here:</p>

<p><a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv</a></p>

<p>But none exist for python 3</p>
",,2015-06-06 19:49:17,OpenCV for python 3.X,<opencv><python-3.x>,,,CC BY-SA 3.0,True,False,False,True,False
19782,21601334,2014-02-06 11:15:36,,"<p>I never used computer vision stuff before and thought I can use python for analysis of Gel Electrophoresis. <a href=""http://www.youtube.com/watch?v=mN5IvS96wNk"" rel=""nofollow noreferrer"">Here</a> is a video explaining what is happening if you are not familiar with the process. </p>

<p>So I took a pic from wikipedia of a gel then use a grayscale filter, then a bilateral filter to get rid of smudges and artifacts, and then I used a Otsu filter to separate out the prominent bands.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

from skimage import data, io
from skimage.filter import threshold_otsu, denoise_bilateral
from skimage.morphology import closing, square
from skimage.measure import regionprops
from skimage.color import label2rgb, rgb2gray


image = io.imread('http://upload.wikimedia.org/wikipedia/commons/6/60/Gel_electrophoresis_2.jpg')


#grayscaling
gray_image = rgb2gray(image)

# bilateral filtering
bilat=denoise_bilateral(gray_image, sigma_range=0.05, sigma_spatial=20)

# apply threshold Otsu
thresh = threshold_otsu(bilat)
bw = closing(bilat &gt; thresh, square(1))

#print process
def show_images(images,titles=None):
    """"""Display a list of images""""""
    n_ims = len(images)
    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]
    fig = plt.figure()
    n = 1
    for image,title in zip(images,titles):
        a = fig.add_subplot(1,n_ims,n) 
        if image.ndim == 2: 
            plt.gray() 
        plt.imshow(image)
        a.set_title(title)
        n += 1
    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)
    plt.show()

#print data
show_images(images=[image, bilat, bw], titles=['Normal', 'Bilateral filter', 'Otsu Threshold'])
</code></pre>

<p>Here is what the results currently look like
<img src=""https://i.stack.imgur.com/BIahX.png"" alt=""Wikipedia Gel electrophoresis""></p>

<p>I have 4 problems I got stuck on:</p>

<ol>
<li><p>Using the otsu threshold causes some data loss from light color bands is there better way get the band data?</p></li>
<li><p>Is there a way to return the results from each row to a numpy/pandas array where the bands are displayed on a matrix? (ie 0 for no bands, 1 for light band, 2 for medium band, 3 for heavy band) This will allow detecting bands that are matching with the DNA Ladder(reference row).</p></li>
<li><p>What method can be used to calculate the distance from the wells to the bands.</p></li>
<li><p>If the picture is not taken straight would I need something called <a href=""http://en.wikipedia.org/wiki/Image_registration"" rel=""nofollow noreferrer"">Image registration</a>? If so where do I find it in <code>scikit-image</code>?</p></li>
</ol>

<p>Last thing I am using python 3 and the last stable version of scikit-image if it matters.</p>
",,2014-02-06 13:39:01,Python Scikit-image processing of Gel electrophoresis data,<python><numpy><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
20000,21618252,2014-02-07 02:38:13,,"<p>I'm trying to get blue colored contours using scikit-image. I'm sure there are functions in opencv that are also available in scikit-image.</p>

<p>I am aware of the find_contours method which works well however it gets ALL colors of contours. I just wnat to get the blue contours.</p>

<p><a href=""http://scikit-image.org/docs/dev/api/skimage.measure.find_contours.html"" rel=""nofollow"">http://scikit-image.org/docs/dev/api/skimage.measure.find_contours.html</a></p>

<p>Any ideas of how to do this? My guess is to preprocess the image somehow to remove every color other than blue.</p>
",,2014-02-09 16:05:41,Get blue colored contours using scikit-image/opencv,<opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
20081,20455793,2013-12-08 16:31:51,,"<p><img src=""https://i.stack.imgur.com/8kmZd.png"" alt=""&quot;Figure 1&quot;""></p>

<p><img src=""https://i.stack.imgur.com/5VerJ.png"" alt=""&quot;Figure 2&quot;""></p>

<p>I am going to make a feature extraction from fingerprint images. So far I have tried many methods to simply enhance the image and skeletonize it. Methods I tried;
Local Histogram Equalization (11x11 neighborhood) + Binarization with Adaptive Thresholding + Morphological Thinning (With Erode+Dilate+Substract so called White Top Hat). I used built-in functions come with OpenCV , Scipy and Scikit-Image. Didn't work pretty well.
  I tried a different approach , Local Histogram + Wiener Filtering + Adaptive Thresholding Binarization + Skeletonize.
  Results are varying , some are perfectly good , some are terrible with background noise and billions of false connections. I also tried applying Gaussian or Median blurring before any action taken. 
  For example figure 1 is one of my good resulting examples with wiener filtering. Except the borderline effect. On the borders of fingerprint, there seems to be millions of false connections and algorithm tends to draw a border around the fingerprint. But still i accept this as a good result but also need suggestions to get over this border effect.
  On the other hand , as you see , figure 2 is a horrible piece. All of the bits are just bitwise not'ed to have background and valleys black and ridges white. It is still the same algorithm. Any suggestions for fingerprint image enhancement in OpenCV or/and any library for Python?
NOTE: Added raw images due request. </p>

<p><img src=""https://i.stack.imgur.com/f932k.png"" alt=""&quot;Figure 1 not processed&quot;""></p>

<p><img src=""https://i.stack.imgur.com/lbsZq.png"" alt=""&quot;Figure 2 not processed&quot;""></p>
",2013-12-08 20:25:47,2013-12-18 16:03:42,Fingerprint Image Enhancement,<image-processing><computer-vision>,,,CC BY-SA 3.0,True,False,False,True,False
21234,22845247,2014-04-03 17:50:03,,"<p>I'm using Python + Scipy + Scikit-image + numpy for the first time.</p>

<p>I'm using <a href=""http://www.tp.umu.se/~nylen/pylect/advanced/image_processing/index.html"" rel=""nofollow"">this</a> page for help, and I've only changed the given code a bit to pick up an image of my liking:</p>

<pre><code>tree = misc.imread('C:\\Users\\app\\Pictures\\treephoto1.jpg')
type(tree)
&lt;type 'numpy.ndarray' &gt;
tree.shape, tree.dtype((512, 512), dtype('uint8'))
</code></pre>

<p>But I'm getting the following error:</p>

<pre><code> type(tree) &lt;type 'numpy.ndarray'&gt;
                                   ^
SyntaxError: invalid syntax
</code></pre>

<p>What's wrong with the syntax? I'm using python 2.7 on Windows, and all the related toolkits are also according to Python 2.7.</p>

<p>I need to convert the image to a 2-D numpy array so that I can use the canny edge detector with it.</p>
",2014-04-03 18:49:17,2014-04-04 10:11:54,Converting image to numpy array in python,<python><image-processing><numpy><computer-vision><scikits>,,,CC BY-SA 3.0,False,False,False,True,False
22800,25218465,2014-08-09 12:15:04,,"<p>I'm trying to scale the colors of images to predefined ranges. Based on least-squared error from palette's range of colors, a color is assigned to output pixel.</p>

<p>I have written the code in python loops is there a better vectorized way to do this?</p>

<pre><code>import numpy as np
import skimage.io as io

palette = [
            [180, 0 , 0],
            [255, 150, 0],
            [255, 200, 0],
            [0, 128, 0]
        ]

IMG = io.imread('lena.jpg')[:,:,:3]
DIM = IMG.shape
IOUT = np.empty(DIM)

for x in range(DIM[0]):
    for y in range(DIM[1]):
        P = ((np.array(palette)-IMG[x,y,:])**2).sum(axis=1).argmin()
        IOUT[x,y,:] = palette[P]
</code></pre>

<p>Can the loops be avoided and solved using numpy operations itself?</p>
",2014-08-09 12:36:54,2014-08-09 14:14:10,Working on multidimensional arrays,<python><image-processing><numpy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
23158,20712311,2013-12-20 21:36:42,,"<p>I have large file 9600x7000 pixel jpg file I am trying to see if I can do a edge detection.  I tried loading the large (25Mb) file using:</p>

<pre><code>from PIL import Image
image = Image.open(""C:\\pathtofile\\test-tac.jpg"")
image.show()
</code></pre>

<p>However python interpreter will crash.  I am using Pycharm running Python 2.7.</p>

<p>So, I used a GDAL (used for <strong>large</strong> GEO refererencing files) to load the file.  It will load the file into memory without any problem.</p>

<pre><code>#reference http://www.gdal.org/gdal_tutorial.html
import gdal
from gdalconst import *

dataset = gdal.Open(""C:\\pathtofile\\test-tac.jpg"", GA_ReadOnly )
if dataset is None:
   print ""error loading file in gdal""
</code></pre>

<p>This will load file. However, I am trying to run following edge detection on it:</p>

<pre><code>from matplotlib import pyplot as plt

from skimage import data
from skimage.feature import corner_harris, corner_subpix, corner_peaks
from skimage.transform import warp, AffineTransform
from skimage.draw import ellipse

# running corner Harris on the image object to detect image corners. 
#(reference http://scikit-image.org/docs/dev/auto_examples/plot_corner.html)
coords = corner_peaks(corner_harris(image), min_distance=3) #5
coords_subpix = corner_subpix(image, coords, window_size=13)

plt.gray()
plt.imshow(image, interpolation='nearest')
plt.plot(coords[:, 1], coords[:, 0], '.b', markersize=9)  # dots
plt.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15) # +
plt.plot(coords_subpix[:, 1][1], coords_subpix[:, 0][1], '*r', markersize=20)  #X_Point1=Subpix[:,1][1], Y_Point1=Subpix[:,0][1]

N=len(coords_subpix[:,0])
labels = ['point{0}'.format(i) for i in range(N)]

#Label corners in image
for label, x, y in zip(labels, coords_subpix[:,1], coords_subpix[:,0]):
   plt.annotate(label,
    xy=(x,y), xytext = (-10,10),
    textcoords = 'offset points', ha = 'right', va = 'bottom',
    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),
    arrowprops = dict(arrowstyle = '-&gt;', connectionstyle = 'arc3,rad=0'))

   plt.axis((0, 9672, 7272, 0))            # (Y_start, Y_Stop, X_Stop, X_Start) ((0, 9672, 7272, 0))
   plt.show()
</code></pre>

<p>This would work if I generate image using following code:</p>

<pre><code> tform = AffineTransform(scale=(1.3, 1.1), rotation=1, shear=0.8,
                    translation=(210, 50))
 image = warp(data.checkerboard(), tform.inverse, output_shape=(350, 350))
 rr, cc = ellipse(310, 175, 10, 100)
 image[rr, cc] = 1
 image[180:230, 10:60] = 1
 image[230:280, 60:110] = 1
</code></pre>

<p>My problem is I am not understanding Python much about the data format from the 'image' variable versus dataset variable generated by GDAL.  My end goal is to be able to run edge detection on large (10000x7000) pixel jpg image using Python scikit-image library.  If there is better way that GDAL to read large jpg images I am open to it.</p>

<p>If I set:</p>

<pre><code>image=dataset
</code></pre>

<p>and run it, I get following error:</p>

<pre><code>coords = corner_peaks(corner_harris(image), min_distance=3) #5
 File ""C:\Python27\lib\site-packages\skimage\feature\corner.py"", line 171, in corner_harris
 Axx, Axy, Ayy = _compute_auto_correlation(image, sigma)
 File ""C:\Python27\lib\site-packages\skimage\feature\corner.py"", line 54, in _compute_auto_correlation
 if image.ndim == 3:
 AttributeError: 'Dataset' object has no attribute 'ndim'
</code></pre>

<p>This error message points that I am not understanding the datatype between dataset and image variables.  </p>

<pre><code>type(dataset)
</code></pre>

<p>Gives:</p>

<pre><code>&lt;class 'osgeo.gdal.Dataset'&gt;
</code></pre>

<p>and </p>

<p>type(image)</p>

<p>Gives:</p>

<pre><code>(350,350) float64.
</code></pre>

<p>For your large source file use:
<a href=""http://www.lib.utexas.edu/maps/tpc/txu-pclmaps-oclc-22834566_a-2c.jpg"" rel=""nofollow"">http://www.lib.utexas.edu/maps/tpc/txu-pclmaps-oclc-22834566_a-2c.jpg</a> to give it a try.</p>
",2013-12-20 22:15:51,2013-12-21 00:22:25,Python Large Image Edge Detection Using Scikit-image and GDAL,<python><image-processing><gdal><edge-detection><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
23268,21900090,2014-02-20 06:36:30,,"<p>When i'm installing <code>scikit-image</code> it giving me this error. I am new in python image processing. I installed all the dependencies of this library but still it is not working.</p>

<p>It failed with:</p>

<pre><code>ImportError: /usr/lib/liblapack.so.3gf: undefined symbol: ATL_chemv
</code></pre>

<p>How can I deal with it? How fix it?</p>

<p>Thank you in advance.</p>
",2014-02-20 07:05:12,2016-07-11 12:46:44,scikit-image installation error,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
23425,24185972,2014-06-12 13:41:01,,"<p>Can I get some ideas on how to morph the face in a live video using opencv? I have tried <a href=""https://github.com/arturoc/FaceSubstitution"" rel=""nofollow"">Face substitution</a> but it is implemented using <a href=""http://www.openframeworks.cc/"" rel=""nofollow"">openFrameworks</a>. </p>

<p>I would like to implement the same using opencv. Is there any other methods available in opencv than diirectly porting Face substituion code from openFrameworks to Opencv?</p>

<p>I have also gone through this <a href=""http://engineeering.blogspot.in/2008/07/image-morphing-with-opencv.html"" rel=""nofollow"">link</a>, but few people have mentioned as the <a href=""http://code.opencv.org/issues/2711"" rel=""nofollow"">face morphing is deprecated in opencv</a>?</p>
",2019-09-01 21:55:42,2019-09-01 22:01:58,Face morphing using opencv,<python><opencv><image-processing><computer-vision><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
23526,24191545,2014-06-12 18:33:12,,"<p>I need to shear and skew some images using python.
I've come across <a href=""http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.AffineTransform"" rel=""nofollow"">this skimage module</a> but I don't seem able to understand exactly how I'm supposed to use this.</p>

<p>I've tried a few things, which obviously gave me errors, because as I suddenly realized later, I'm not passing in my image to the function. I then noticed that the function doesn't take my image as an input parameter in the first place. So how should the transformation be applied? Or is this even the right function to be looking at in order to skew or shear an image?</p>
",,2018-03-07 09:31:13,skewing or shearing an image in python,<python><image-processing><numpy><transformation><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
23565,25284229,2014-08-13 10:51:49,,"<p><img src=""https://i.stack.imgur.com/mQRyU.png"" alt=""enter image description here""></p>

<p>I'm detecting blobs on image using <code>skimage.feature.blob_doh</code> and 'm getting my blob areas in format:</p>

<blockquote>
  <p>A = array([[121, 271,  30],
             [123,  44,  23],
             [123, 205,  20],
             [124, 336,  20],
             [126, 101,  20],
             [126, 153,  20],
             [156, 302,  30],
             [185, 348,  30],
             [192, 212,  23],
             [193, 275,  23],
             [195, 100,  23],
             [197,  44,  20],
             [197, 153,  20],
             [260, 173,  30],
             [262, 243,  23],
             [265, 113,  23],
             [270, 363,  30]])</p>
</blockquote>

<p>A : (n, 3) ndarray
        A 2d array with each row representing 3 values, <code>(y,x,sigma)</code>
        where <code>(y,x)</code> are coordinates of the blob and <code>sigma</code> is the
        standard deviation of the Gaussian kernel (it's approximatly just a radius of my area)</p>

<p>So the question is - how to select all these areas for further data processing (calculating average features, making some clustering and classification). Now I just draw them on plot, but can't migrate them to bitmap\array variables.</p>

<p>And I don't want use for this task OpenCV library, I have to do it using numpy/scipy/skimage and other libs.</p>

<pre><code> fig, ax = plt.subplots(1, 1)
    ax.set_title(title)
    ax.imshow(image, interpolation='nearest')
    for blob in blobs:
            y, x, r = blob
            c = plt.Circle((x, y), r, color=color, linewidth=2, fill=False)
            print c
            ax.add_patch(c)
    plt.show()
</code></pre>

<p>Thank you for any help!</p>

<p><strong>UPD: got some code for cropping, but it's doing something strange... it crops well, but what is with coordinates?</strong></p>

<pre><code>def crop_and_save_blobs(image, blobs):
    image = np.asarray(image)
    for blob in blobs:
            y, x, radius = blob
            center = (x, y)
            mask = np.zeros((image.shape[0],image.shape[1]))
            for i in range(image.shape[0]):
                for j in range(image.shape[1]):
                    if (i-center[0])**2 + (j-center[0])**2 &lt; radius**2:
                        mask[i,j] = 1

            # assemble new image (uint8: 0-255)
            newImArray = np.empty(image.shape,dtype='uint8')
            # colors (three first columns, RGB)
            newImArray[:,:,:3] = image[:,:,:3]
            # transparency (4th column)
            newImArray[:,:,3] = mask*255 
            newIm = Image.fromarray(newImArray, ""RGBA"")
            plt.imshow(newIm)
            plt.show() 
</code></pre>
",2014-08-13 11:17:45,2014-08-13 13:21:17,Selecting circle areas on image for further processing,<python><image><image-processing><numpy><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
23824,19619996,2013-10-27 15:48:47,,"<p>I have a <a href=""http://i.imgur.com/grnpDpP.jpg"" rel=""nofollow"">numpy array binary</a> (black and white) image and coordinates in a list of tuples like:</p>

<pre><code>coordlist =[(110, 110), (110, 111), (110, 112), (110, 113), (110, 114), (110, 115), (110, 116), (110, 117), (110, 118), (110, 119), (110, 120), (100, 110), (101, 111), (102, 112), (103, 113), (104, 114), (105, 115), (106, 116), (107, 117), (108, 118), (109, 119), (110, 120)]
</code></pre>

<p>or as:</p>

<pre><code>coordx = [110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
coordy = [110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]
</code></pre>

<p>How can i check if there is a ""white"" pixel in the image with that coordinates list?
I also would like check the white pixels that are around 3 pixels range far from that coordinates list.</p>

<p>i.e.:</p>

<pre><code>for i, j in coordx, coordy:
    for k in a range (k-3, k + 3)
        for l in a range (l-3, l + 3)
            #checking white pixels also for pixel near coordinates list
</code></pre>

<p>I thought about ""where"" function.</p>

<pre><code>from skimage import morphology
import numpy as np

path = 'image/a.jpg'
col = mh.imread(path)
bn0 = col[:,:,0]
bn = (bn0 &lt; 127)
bnsk = morphology.skeletonize(bn)
bnskInt = np.array(bnsk, dtype=np.uint8)

#finding if there are white pixel in the coord list and around that in a 5 pixel range
for i in coordlist:
np.where(?)
</code></pre>

<p><strong>UPDATE</strong>.</p>

<p>I tried to use  shape (128, 128) instead of (128, 128, 3) because my image have this shape: (a,b) but now it does not find the white pixels!
Why in this way does it find anything?</p>

<pre><code>    white_pixel = np.array([255, 255])
    img = np.random.randint(0, 256, (128, 128))
    print(img[150])
    print(img.shape)
    img[110, 110] = 255
    img[109, 110] = 255

    mask = np.zeros((128, 128), dtype=bool)
    mask[coordx, coordy] = 1
    #structure = np.ones((3, 3, 1))
    #mask = scipy.ndimage.morphology.binary_dilation(mask, structure)

    is_white = np.all((img * mask) == white_pixel, axis=-1)

    # This will tell you which pixels are white
    print np.where(is_white)

    # This will tell you if any pixels are white
    print np.any(is_white)
</code></pre>

<p>output:</p>

<pre><code>(array([], dtype=int32),)
False
</code></pre>
",2013-10-30 17:29:24,2013-10-30 17:39:27,How can i know if there are white pixels in a binary image in some coordinates (stored in a list) in python?,<python><image-processing><numpy><scikit-image><mahotas>,,,CC BY-SA 3.0,False,False,False,True,False
24109,24240039,2014-06-16 09:04:33,,"<p>I am working with 2D floating-point numpy arrays that I would like to save to greyscale .png files with high precision (e.g. 16 bits). I would like to do this using the scikit-image <code>skimage.io</code> package if possible.</p>

<p>Here's the main thing I've tried:</p>

<pre><code>import numpy as np
from skimage import io, exposure, img_as_uint, img_as_float

im = np.array([[1., 2.], [3., 4.]], dtype='float64')
im = exposure.rescale_intensity(im, out_range='float')
im = img_as_uint(im)
im
</code></pre>

<p>produces:</p>

<pre><code>array([[    0, 21845],
       [43690, 65535]], dtype=uint16)
</code></pre>

<p>First I tried saving this as an image then reloading using the Python Imaging Library:</p>

<pre><code># try with pil:
io.use_plugin('pil')
io.imsave('test_16bit.png', im)
im2 = io.imread('test_16bit.png')
im2
</code></pre>

<p>produces:</p>

<pre><code>array([[  0,  85],
       [170, 255]], dtype=uint8)
</code></pre>

<p>So somewhere (in either the write or read) I have lost precision. I then tried with the matplotlib plugin:</p>

<pre><code># try with matplotlib:
io.use_plugin('matplotlib')
io.imsave('test_16bit.png', im)
im3 = io.imread('test_16bit.png')
im3
</code></pre>

<p>gives me a 32-bit float:</p>

<pre><code>array([[ 0.        ,  0.33333334],
       [ 0.66666669,  1.        ]], dtype=float32)
</code></pre>

<p>but I doubt this is really 32-bits given that I saved a 16-bit uint to the file. It would be great if someone could point me to where I'm going wrong. I would like this to extend to 3D arrays too (i.e. saving 16 bits per colour channel, for 48 bits per image).</p>

<h2>UPDATE:</h2>

<p>The problem is with imsave. The images are 8 bits per channel. How can one use io.imsave to output a high bit-depth image? </p>
",2014-06-16 09:19:44,2014-06-16 10:26:27,Save numpy array as image with high precision (16 bits) with scikit-image,<python><image-processing><numpy><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
24298,19660582,2013-10-29 14:08:49,,"<p>I would like to use python to perform a geometric transform over an image, to 'straighten' or rectify an image along a given curve. It seems that scikit-image <code>ProjectiveTransform()</code> and <code>warp()</code> are very good for this, but the documentation is sparse. I followed the documentation <a href=""http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html#parameter-estimation"" rel=""nofollow noreferrer"">here</a>, but I couldn't get it to work properly for a sample case.</p>

<p>Here's an example: I'll create an image with two concentric circles, and the goal is to rectify one quarter of these circles, so that the resulting image are two parallel lines. Here is the sample data:</p>

<pre><code>import numpy as np
a = np.zeros((500, 500))

# create two concentric circles with a thickness of a few pixels:
for i in range(500):
    for j in range(500):
        r = np.sqrt((i - 250)**2 + (j - 250)**2) 
        if r &gt; 50 and r &lt; 52:
            a[i, j] = 10
        if r &gt; 100 and r &lt; 102:
            a[i, j] = 10
# now create the coordinates of the control points in the original image:
(x0, y0) = (250, 250)
r = 30   # inner circle
x = np.linspace(250 - r, 250, 50)
y = np.sqrt(r ** 2 - (x - x0) ** 2) + x0
r2 = 120   # outer circle
x2 = np.linspace(250 - r2, 250, 50)
y2 = np.sqrt(r2 ** 2 - (x2 - x0) ** 2) + x0
dst = np.concatenate((np.array([x, y]).T, np.array([x2, y2]).T))
</code></pre>

<p>And this can be plotted, e.g.:</p>

<pre><code>imshow(a, cmap='gist_gray_r')
plot(x, y, 'r.')
plot(x2, y2, 'r.')
</code></pre>

<p><img src=""https://i.stack.imgur.com/pspvE.png"" alt=""enter image description here""></p>

<p>So my goal is to rectify the image in the quadrant given by the red control points. (In this case, this is the same as a Cartesian to polar transformation.) Using scikit image from the documentation example, I've done:</p>

<pre><code># create corresponding coordinates for control points in final image:
xi = np.linspace(0, 100, 50)
yi = np.zeros(50)
xi2 = xi
yi2 = yi + (r2 - r)
src = np.concatenate((np.array([xi, yi]).T, np.array([xi2, yi2]).T))

# transform image
from skimage import transform, data
tform3 = transform.ProjectiveTransform()
tform3.estimate(src, dst)
warped = transform.warp(a, tform3)
</code></pre>

<p>I was expecting this <code>warped</code> image to show two parallel lines, but instead I get:
<img src=""https://i.stack.imgur.com/AzmZ8.png"" alt=""enter image description here""></p>

<p>What am I doing wrong here? </p>

<p>Note that while in this case it is a Cartesian to polar transform, in the most general case I'm looking for a transformation from some arbitrary curve. If someone knows of a better way using some other package, please let me know. I can solve this problem by using <code>ndimage.map_coordinates</code> for a bunch of radial lines, but was looking for something more elegant.</p>
",2013-10-29 17:02:28,2016-09-29 10:22:22,Geometric warp of image in python,<python><image-processing><numpy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
24390,25349178,2014-08-17 12:28:04,,"<p>In testing an object detection algorithm in large images, we check our detected bounding boxes against the coordinates given for the ground truth rectangles.</p>

<p>According to the Pascal VOC challenges, there's this:</p>

<blockquote>
  <p>A predicted bounding box is considered correct if it overlaps more
  than 50% with a ground-truth bounding box, otherwise the bounding box
  is considered a false positive detection. Multiple detections are
  penalized. If a system predicts several bounding boxes that overlap
  with a single ground-truth bounding box, only one prediction is
  considered correct, the others are considered false positives.</p>
</blockquote>

<p>This means that we need to calculate the percentage of overlap. Does this mean that the ground truth box is 50% covered by the detected boundary box? Or that 50% of the bounding box is absorbed by the ground truth box?</p>

<p>I've searched but I haven't found a standard algorithm for this - which is surprising because I would have thought that this is something pretty common in computer vision. (I'm new to it). Have I missed it? Does anyone know what the standard algorithm is for this type of problem?</p>
",2014-08-17 15:01:55,2019-12-21 12:32:07,"Calculating percentage of Bounding box overlap, for image detector evaluation",<python><computer-vision><bounding-box><object-detection><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
24546,23121416,2014-04-16 22:37:02,,"<p>I'm trying to extract a single long boundary from a rather noisy image (forgive the green, the image is converted to grayscale in any case). I've tried running various edge detection and threshold algorithms to extract the boundary. The closest I've gotten so far is by using the local Otsu threshold bundled with scikit-image:</p>

<p><img src=""https://i.stack.imgur.com/4g1te.png"" width=""277"" height=""324""><img src=""https://i.stack.imgur.com/rbsSe.png"" width=""291"" height=""324""></p>

<p>Even so, I'm still unable to extract any meaningful boundary - when I try to use edge detection on the image, it gets caught up in the noise, which is drastically amplified by the thresholding - the boundary detection algorithms are so heavily dependent on calculating the derivative, so the sharp transitions in a binary image really hurts their performance, but I believe it's necessary since no other method has managed to distinguish the boundary at all.</p>

<p>Is there some way to either force the local Otsu threshold to flatten out the noise under a particular global threshold, or get one of the boundary extraction algorithms to ignore things that look like? </p>

<p><img src=""https://i.stack.imgur.com/AXJ1i.png"" alt=""enter image description here""></p>

<p>Or is it best to write a replacement based on the local Otsu thresholding, that only applies the threshold when it returns a pattern resembling a line?</p>

<p>Any help finding the right way to get the relevant boundary is appreciated.</p>
",2014-04-17 11:55:41,2014-04-18 01:54:08,Long Boundary Detection in a Noisy Image,<python><image-processing><matplotlib><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
24784,25379752,2014-08-19 09:18:15,,"<p>How can I extract the boundary curve of an image region enumerated by <a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#regionprops"" rel=""nofollow""><code>measure.regionprops</code></a>?</p>

<p>By boundary curve, I mean a list of border pixels of the region in, say, clockwise direction around the region's perimeter, such that I can, for example, represent the region with a polygon. Note that I want  the exact coordinates of all border pixels, not a convex hull approximation.</p>

<p>I've read the docs and googled, and I have the impression its done somewhere under the hood, but I just cannot find the function.</p>
",2014-08-19 17:59:53,2014-08-19 17:59:53,How can I extract the boundary curve of an image region in scikit-image?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
24887,26821827,2014-11-08 20:39:32,,"<p>I'm trying to extract the boundary between two regions in an image programmatically. I've got the hard bits figured out, so that I have a binary image that contains the boundary and plenty of noise.
<img src=""https://i.stack.imgur.com/ctyZ6.jpg"" alt=""Sample Image"">. Cropping the areas outside isn't an issue.</p>

<p>The boundary in the image is afflicted both by noise (bottom-left for example) and some areas of discontinuity. That means I can't simply select the shape based on one known pixel.</p>

<p>The problem left to me is pretty simple - I only really need to fill the gaps in the boundary and smooth it out, so that I am left with something smooth and continuous that I can extract afterwards. That doesn't sound like a particularly hard problem for images like this, but I'm completely lost. What algorithms or strategies could I possible use in order to turn this image into something useful?</p>

<p>The output I'm looking for is something that can be cropped to give <img src=""https://i.stack.imgur.com/98O2n.png"" alt=""Sample Output"">.</p>
",2014-11-08 22:57:42,2014-11-08 22:57:42,Smooth Boundary Extraction from a Binary Image,<python><math><image-processing><scientific-computing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
25223,24325773,2014-06-20 11:01:44,,"<p>I'm working on some object detection code, however my objects don't have a fixed size, so;</p>

<pre><code>skimage.feature.hog(obj)
</code></pre>

<p>doesn't give me equal length vectors(since it uses fixed sized cells), and therefore I can't use learning algorithms on them.</p>

<p>So, I tried dynamically assigning HOG feature length:</p>

<pre><code>from __future__ import division

def describe_object(obj, div=8):
    width, height = obj.shape
    f = skimage.feature.hog(obj, normalise=True,
                            pixels_per_cell=(height//div, width//div))
    return f
</code></pre>

<p>But, now it <em>mostly</em> gives <code>2916</code> sized vectors, but sometimes it gives longer vectors (like <code>3402</code> elements long) too. </p>

<p>I believe this happens when some specific ratio between bin size and object's shape, but don't know why exactly.</p>

<p>Can you help me?</p>
",2014-06-20 11:07:35,2017-06-16 11:52:10,Fixed-length HOG descriptor for variable sized images,<python><image-processing><numpy><feature-detection><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
25314,23183157,2014-04-20 13:45:05,,"<p>I've pasted all my code here in case you'd need that to understand my question: <a href=""https://stackoverflow.com/questions/23178535/plotting-a-graph-on-axes-but-getting-no-results-while-trying-to-classify-image-b"">Plotting a graph on axes but getting no results while trying to classify image based on HoG features</a></p>

<p>My question is: given approximately 500 images (the Caltech ""<a href=""http://www.vision.caltech.edu/html-files/archive.html"" rel=""nofollow noreferrer"">Cars 2001</a>"" dataset) with 48 HoG features each, what possible reasons can there be for the boosting to terminate early? What could cause a perfect fit, or a problem with the boosted sample weights, and how can such problems be solved? The specific algorithm I'm using is SAMME, a multiclass Adaboost classifier. I'm using Python 2.7 on Anaconda.</p>

<p>When I checked certain variables during the classification of my dataset, setting the <code>n_estimators</code> parameter to be 600, I found that:</p>

<ul>
<li>discrete_test_errors: consisted of 1 item instead of being an array of 600 values</li>
<li>discrete_estimator_errors: was again one single value instead of of being an array of 600 values</li>
<li>real_test_errors is just one item again instead of 600</li>
<li>discrete_estimator_weights: array ([1.]) ""</li>
<li>n_trees_discrete and n_trees_real: 1 instead of 600</li>
</ul>
",2017-05-23 11:57:28,2014-04-20 14:45:46,Image classification using cascaded boosting in scikit-learn - why would classification terminate early?,<python-2.7><computer-vision><scikit-learn><scikit-image><cascade-classifier>,,,CC BY-SA 3.0,False,False,False,True,False
25562,26877311,2014-11-12 00:44:05,,"<p><br>
I am trying to play with image processing on python, and was recommended the use of the skimage module (Scikit-image full name).</p>

<p>I am using a Windows Conda Python 2.7 installation.</p>

<p>When trying to use the canny function,<code>skimage.feature.canny()</code>, I ran into the following error:</p>

<pre><code>AttributeError: 'module' object has no attribute 'canny'`
</code></pre>

<p>Printing out the module version, I appear to be updated.</p>

<pre><code>print skimage.__version__
0.10.1
</code></pre>

<p>According to <a href=""http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.canny"" rel=""nofollow"">documentation</a> though, this function should exist.<br></p>

<p>I'm not entirely sure what to do from here. Any help?</p>

<p>EDIT - </p>

<pre><code>&gt;&gt;&gt; print dir(skimage.feature)
['BRIEF', 'CENSURE', 'ORB', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', '_daisy', '_hessian_det_appx', '_hog', '_texture', 'blob', 'blob_dog', 'blob_doh', 'blob_log', 'brief', 'brief_cy', 'censure', 'censure_cy', 'corner', 'corner_cy', 'corner_fast', 'corner_foerstner', 'corner_harris', 'corner_kitchen_rosenfeld', 'corner_moravec', 'corner_orientations', 'corner_peaks', 'corner_shi_tomasi', 'corner_subpix', 'daisy', 'greycomatrix', 'greycoprops', 'hessian_matrix', 'hessian_matrix_det', 'hessian_matrix_eigvals', 'hog', 'local_binary_pattern', 'match', 'match_descriptors', 'match_template', 'orb', 'orb_cy', 'peak', 'peak_local_max', 'plot_matches', 'structure_tensor', 'structure_tensor_eigvals', 'template', 'texture', 'util']
</code></pre>
",2014-11-12 01:00:38,2014-11-12 01:06:30,skimage module lacking documented features,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
25657,27838531,2015-01-08 11:02:33,,"<p>I had a number of images from which I extracted HoG features and saved them. </p>

<p>I've lost the images now, that was apparently one dataset I hadn't backed up before the machine got messed up.</p>

<p>However I have the files containing the HoG features of those images.</p>

<p>If I had the images now, I would apply shearing and rotation to the images to create more samples, and then take the HoG features of those edited images.</p>

<p>But since I don't have the images... is it possible to somehow work with just the HoG features of the originals in order to get the HoG features of the edited ones?</p>

<p>This is the code I would use to edit the images, if I still had them, from which I would then extract HoG features for object classification:</p>

<pre><code>import numpy as np
from skimage import data, io, filter, color, exposure
from skimage.feature import hog
import skimage.transform as tf
from skimage.transform import resize, rescale, rotate, setup, warp, AffineTransform
import matplotlib.pyplot as plt
import os
from os import listdir
from os.path import isfile, join
import pickle
import Image

def generateSamples(path, readfile):
    print ""generating samples from  "" + path+""\\""+readfile
    img = color.rgb2gray(io.imread(path+""\\""+readfile))
    img = resize(img, (50,100))
    filename = os.path.splitext(readfile)[0]
    angles = [3, 0, -3]
    shears = [0.13, 0.0, -0.13]
    i = 0
    no_samples = len(angles) * len(shears)
    samples = np.empty((no_samples, int(img.shape[0]), int(img.shape[1])), dtype=object)
    for myangle in angles:
        myimg = rotate(img, angle=myangle, order=2)
        for myshear in shears:
            afine_tf = tf.AffineTransform(shear=myshear)
            mymyimg = tf.warp(myimg, afine_tf)
            samples[i] = np.array(mymyimg)
            i+=1
            #io.imshow(mymyimg)
            #io.show()  
    newfile = filename + ""_samples.vec""
    pickle.dump(samples, file(path+""\\""+newfile,'w'))
    print ""saved vec file""
</code></pre>
",,2015-01-08 11:02:33,Getting HoG features of edited images from HoG features of originals directly,<python><image><image-processing><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
25706,27648195,2014-12-25 14:44:39,,"<p>I want to be able to find the length of a path in a picture, this could be the length of a worm, a curly human hair, Amazon river, etc. Consider this Amazon river picture:
<img src=""https://i.stack.imgur.com/EOKfS.jpg"" alt=""enter image description here""></p>

<p>I tried to make an skeleton of my picture after its binarization, but the problem is that the skeletons (obtained by two methods) have many small branches that causes their area to be much more than the approximate length of the path. I used <a href=""http://scikit-image.org/"" rel=""nofollow noreferrer"">scikit-image</a> to do this. Here's the code and results:</p>

<pre><code>from skimage.filter import threshold_otsu
from skimage import io
from skimage.filter.rank import median
from skimage.morphology import disk,skeletonize,medial_axis,remove_small_objects
import matplotlib.pyplot as plt


input_image = io.imread('Amazon-river2.jpg',
                    as_grey=True, plugin=None, flatten=None)
image = median(input_image, disk(15)) 

thresh = threshold_otsu(image)
image = image &lt; thresh

skel1=skeletonize(image)
skel2=medial_axis(image)

min_size=sum(sum(skel1))/2

remove_small_objects(skel1,min_size=min_size,connectivity=5,in_place=True)

remove_small_objects(skel2,min_size=min_size,connectivity=5,in_place=True)


fig2, ax = plt.subplots(2, 2, figsize=(24, 12))

ax[0,0].imshow(input_image,cmap=plt.cm.gray)
ax[0,0].set_title('Input image')
ax[0,0].axis('image')
ax[0,1].imshow(image, cmap=plt.cm.gray)
ax[0,1].set_title('Binary image')
ax[0,1].axis('image')
ax[1,0].imshow(skel1, cmap=plt.cm.gray)
ax[1,0].set_title('Skeleton')
ax[1,0].axis('image')
ax[1,1].imshow(skel2,cmap=plt.cm.gray)
ax[1,1].set_title('Sleleton - Medial axis')
ax[1,1].axis('image')

plt.show()


print (""Length 1: {0}"".format(sum(sum(skel1))))
print (""Length 2: {0}"".format(sum(sum(skel2))))
</code></pre>

<p><img src=""https://i.stack.imgur.com/AWTdi.png"" alt=""enter image description here""></p>

<p>Any suggestions to solve this problem? Any other idea to measure the length of an arc ?</p>
",,2020-01-02 19:44:36,How to find the length of a path (curve) in a picture?,<image-processing><measure><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
26038,27910187,2015-01-12 20:25:36,,"<p>I am trying my hand at image processing and my goal is to output the measurements of a human hand given an image of a human hand as the input. My current thought process is to include a quarter in the image to provide a reference value. Therefore, my input looks like this:</p>

<p><img src=""https://i.stack.imgur.com/HXbWq.jpg"" alt=""hand-image""></p>

<p>I am currently using scikit-image for image processing, and my code looks like this:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

from skimage import data
from skimage.filter import threshold_otsu
from skimage.segmentation import clear_border
from skimage.morphology import label, closing, square
from skimage.measure import regionprops
from skimage.color import label2rgb
from skimage import io, color


#image = data.coins()[50:-50, 50:-50]
filename = io.imread(""hand2.JPG"")
image = color.rgb2gray(filename)

# apply threshold
thresh = threshold_otsu(image)
bw = closing(image &gt; thresh, square(3))

# remove artifacts connected to image border
cleared = bw.copy()
#clear_border(cleared)

# label image regions
label_image = label(cleared)
borders = np.logical_xor(bw, cleared)
label_image[borders] = -1
image_label_overlay = label2rgb(label_image, image=image)

fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12, 12))
ax.imshow(image_label_overlay)

for region in regionprops(label_image):

    # skip small images
    if region.area &lt; 1000:
        continue

    print ""Perimeter: ""
    print region.perimeter
    print ""Area: ""
    print region.area
    print """"

    # draw rectangle around segments
    minr, minc, maxr, maxc = region.bbox
    rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,
                              fill=False, edgecolor='red', linewidth=2)
    ax.add_patch(rect)

plt.show()
</code></pre>

<p>I am able to segment my image into regions, but I don't know how to convert my hand segment into measurements for the individual fingers and width of the hand. I think I'm close, I just don't quite know how to proceed!</p>

<p><strong>EDIT: Maybe I should be using opencv for this?</strong></p>
",2015-01-12 21:00:38,2015-01-13 20:05:40,How do I calculate the measurements of a hand using scikit-image?,<python><image><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
26391,27704490,2014-12-30 11:19:52,,"<p><strong>Short version:</strong> is there a Python method for displaying an image which shows, in real time, the pixel indices and intensities?  So that as I move the cursor over the image, I have a continually updated display such as <code>pixel[103,214] = 198</code> (for grayscale) or <code>pixel[103,214] = (138,24,211)</code> for rgb?</p>

<p><strong>Long version:</strong></p>

<p>Suppose I open a grayscale image saved as an ndarray <code>im</code> and display it with <code>imshow</code> from matplotlib:</p>

<pre><code>im = plt.imread('image.png')
plt.imshow(im,cm.gray)
</code></pre>

<p>What I get is the image, and in the bottom right of the window frame, an interactive display of the pixel indices.  Except that they're not quite, as the values are not integers: <code>x=134.64   y=129.169</code> for example.</p>

<p>If I set the display with correct resolution:</p>

<pre><code>plt.axis('equal')
</code></pre>

<p>the x and y values are still not integers.</p>

<p>The <code>imshow</code> method from the <code>spectral</code> package does a better job:</p>

<pre><code>import spectral as spc
spc.imshow(im)
</code></pre>

<p>Then in the bottom right I now have <code>pixel=[103,152]</code> for example.</p>

<p>However, none of these methods also shows the pixel values.  So I have two questions:</p>

<ol>
<li>Can the <code>imshow</code> from <code>matplotlib</code> (and the <code>imshow</code> from <code>scikit-image</code>) be coerced into showing the correct (integer) pixel indices?</li>
<li>Can any of these methods be extended to show the pixel values as well?</li>
</ol>
",,2019-06-11 16:54:44,Interactive pixel information of an image in Python?,<python><image><image-processing><matplotlib><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
26462,28020521,2015-01-19 08:28:42,,"<p>My goal is to recognize specific types of traffic signs: red circles on video in real time.</p>
",,2018-03-23 06:44:11,What to choose to begin with ComputerVision: Scikit-image or OpenCV?,<python><opencv><computer-vision><image-recognition><scikit-image>,2019-01-18 19:01:27,,CC BY-SA 3.0,True,False,False,True,False
26528,28078530,2015-01-21 23:04:15,,"<p>I have the following image:</p>

<p><img src=""https://i.stack.imgur.com/WhGXN.png"" alt=""Image with gapped contours""></p>

<p>and I would like to fill in its contours (i.e. I would like to gap fill the lines in this image).</p>

<p>I have tried a morphological closing, but using a rectangular kernel of size <code>3x3</code> with <code>10</code> iterations does not fill in the entire border. I have also tried a <code>21x21</code> kernel with <code>1</code> iteration and also not had luck.</p>

<p><strong>UPDATE:</strong></p>

<p>I have tried this in OpenCV (Python) using:</p>

<pre><code>cv2.morphologyEx(img, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (21,21)))
</code></pre>

<p>and</p>

<pre><code>cv2.morphologyEx(img, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (3,3)), iterations=10)
</code></pre>

<p>and <a href=""http://scikit-image.org/docs/dev/auto_examples/applications/plot_morphology.html"" rel=""nofollow noreferrer"">scikit-image</a>:</p>

<pre><code>closing(img, square(21))
</code></pre>

<p>My end goal is to a have a filled version of that entire image without distorting the area covered.</p>
",2015-01-21 23:39:05,2015-01-22 01:02:30,Gap Filling Contours / Lines,<python><opencv><image-segmentation><scikit-image><image-morphology>,,,CC BY-SA 3.0,True,False,False,True,False
26608,28084908,2015-01-22 09:13:12,,"<p>I read an image with <a href=""https://docs.scipy.org/doc/scipy/reference/ndimage.html"" rel=""nofollow noreferrer""><code>ndimage</code></a>, which results in a binary image like this:</p>

<p><img src=""https://i.stack.imgur.com/pirqF.png"" alt=""enter image description here""></p>

<p>I would like to invert the image such that white turns into black, and vice versa.</p>

<p>Help is appreciated.</p>
",2019-11-14 09:43:59,2019-12-23 19:35:34,How to invert black and white with scikit-image?,<python><image><image-processing><scipy><scikit-image>,,,CC BY-SA 4.0,False,False,False,True,False
26637,28141559,2015-01-25 21:11:02,,"<p>I am creating a rotation matrix in python as follows:</p>

<pre><code>import numpy as np
def make_rot(angle):
    cost = np.cos(np.deg2rad(angle))
    sint = np.sin(np.deg2rad(angle))
    rot = np.array([[cost, -sint, 0],
                   [sint, cost, 0],
                   [0, 0, 1]])
    return rot
</code></pre>

<p>This is as defined in the wikipedia page here: <a href=""http://en.wikipedia.org/wiki/Rotation_matrix"" rel=""nofollow"">http://en.wikipedia.org/wiki/Rotation_matrix</a></p>

<p>I run it with the angle parameter as 45 degrees and I get something like:</p>

<pre><code>[[ 0.70710678 -0.70710678  0.        ]
 [ 0.70710678  0.70710678  0.        ]
 [ 0.          0.          1.        ]]
</code></pre>

<p>Now, I use the OpenCV <code>getRotationMatrix2D</code> API as follows:</p>

<pre><code>import cv2
M = cv2.getRotationMatrix2D((0, 0), 45, 1)
</code></pre>

<p>The matrix I get is the inverse of the matrix (the transpose as it is a rotation matrix). The result is as follows:</p>

<pre><code>[[ 0.70710678  0.70710678  0.        ]
 [-0.70710678  0.70710678  0.        ]]
</code></pre>

<p>As you can see it is the inverse. I have not found anything in the OpenCV documentation which says anything about this behaviour.</p>

<p>Now, I can use this matrix in OpenCV and <code>skimage</code> to transform an image as follows:</p>

<pre><code># openCV
M = cv2.getRotationMatrix2D((0, 0), 45, 1)
dst = cv2.warpAffine(image2, M, (coumns, rows))

# skimage
from skimage import transform as tf
tform = tf.AffineTransform(matrix=make_rot(45))
dst = tf.warp(image_2, tform)
</code></pre>

<p>The surprising thing is that the result from using my matrix and OpenCV matrix is the same. </p>

<p>My question is why is OpenCV working with the inverse of the transformation? I am wondering if this is something they are not mentioning in the documentation or if I am using this wrong somehow.</p>
",2015-01-26 04:28:50,2018-07-17 03:27:15,rotation matrix in openCV,<python><opencv><image-processing><transformation><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
26766,28242274,2015-01-30 18:22:46,,"<p>I am trying to find the number of objects in a given image using watershed segmentation. Consider for example the <a href=""http://docs.opencv.org/trunk/_images/water_coins.jpg"" rel=""nofollow"">coins image</a>. Here I would like to know the number of coins in the image. I implemented the code available at <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_watershed.html"" rel=""nofollow"">Scikit-image</a> documentation and tweaked with it a little and got results similar to those displayed on the documentation page.</p>

<p>After looking at functions used in the code in detail I found out that ndimage.label() also returns number of unique objects found in the image (mentioned in it's documentation), but when I print that value I am getting 53 which is very high as compared to the number of coins in the actual image.</p>

<p>Can somebody suggest some method to find the number of objects in an image.</p>
",,2015-09-14 12:35:17,Count number of objects using watershed algorithm - Scikit-image,<opencv><image-processing><scikit-image><watershed>,,,CC BY-SA 3.0,True,False,False,True,False
26800,28281742,2015-02-02 16:02:58,,"<p>I have been using skim age's thresholding algorithms to get some binary mask. For example, I obtain binary images like this:</p>

<p><img src=""https://i.stack.imgur.com/3J3qz.jpg"" alt=""Binary image obtained as a result of Otsu thresholding""></p>

<p>What I am trying to figure out is how can I fit a circle to this binary mask. The constraint is the circle should cover as much of the white areas as possible and the whole circumference of the circle should lie entirely on the white parts.</p>

<p>I have been wrecking my head on how I can do this efficiently but have come up with no solution that works. </p>

<p>One approach I thought that might be something is:</p>

<ul>
<li>Find some optimal center of the image/circle (I am not sure how to do this yet. Perhaps some raster scan like approach).</li>
<li>Compute circle for increasing radiii and figure out when it starts getting out of the white area or outside the image.</li>
<li>Then the centroid and radius will describe the circle.</li>
</ul>
",,2015-02-03 00:06:29,fitting a circle to a binary image,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
26835,25871115,2014-09-16 14:12:37,,"<p>I'm trying to do OCR to some forms that, however, have some texture as follows:</p>

<p><img src=""https://i.stack.imgur.com/MjdUw.png"" alt=""Original Image""></p>

<p>This texture causes the OCR programs to ignore it tagging it as an image region.</p>

<p>I considered using morphology. A closing operation with a star ends up as follows:</p>

<p><img src=""https://i.stack.imgur.com/9gZHu.png"" alt=""Closing operation""></p>

<p>This result is still not good enough for the OCR.</p>

<p>When I manually erase the 'pepper' and do adaptive thresholding an image as follows gives good results on the OCR:</p>

<p><img src=""https://i.stack.imgur.com/8XKWa.png"" alt=""Edited and thresholded""></p>

<p>Do you have any other ideas for the problem?</p>

<p>thanks</p>
",2014-09-16 14:29:20,2014-09-16 15:14:48,How to filter a texture from an image for OCR,<opencv><image-processing><ocr><scikit-image><leptonica>,,,CC BY-SA 3.0,True,False,False,True,False
26879,28289374,2015-02-03 00:31:10,,"<p>I'm looking for a robust way to extract the foreground from an image where the background has some noise in it.</p>

<p>So, the image I want to use it on is:</p>

<p><img src=""https://i.stack.imgur.com/QQ4xVm.jpg"" alt=""Image with some background noise""></p>

<p>My attempt was to use the <code>Otsu thresholding</code>. I did that in Python as follows:</p>

<pre><code>from skimage.filter import threshold_otsu
import os.path as path
import matplotlib.pyplot as plt

img = io.imread(path.expanduser('~/Desktop/62.jpg'))
r_t = threshold_otsu(img[:, :, 0])
g_t = threshold_otsu(img[:, :, 1])
b_t = threshold_otsu(img[:, :, 2])

m = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)
mask = (img[:, :, 0] &lt; r_t) &amp; (img[:, :, 1] &lt; g_t) &amp; (img[:, :, 2] &lt; b_t)
m[~mask] = 255

plt.imshow(m)
plt.show()
</code></pre>

<p>This gives the R, G, B threshold as (62 67 64), which is a bit high. The result is:</p>

<p><img src=""https://i.stack.imgur.com/wFkoYm.png"" alt=""Otsu result""></p>

<p>This image is also one of the images where <code>Otsu thresholding</code> worked best. If I use a manual threshold like a value of 30, it works quite well. The result is:</p>

<p><img src=""https://i.stack.imgur.com/9E5WXm.png"" alt=""Manual""></p>

<p>I was wondering if there are some other approaches that I should try. Segmentation really is not my area of expertise and what I can do out of the box seem limited.</p>
",2015-02-03 14:26:50,2015-02-03 14:26:50,Extracting foreground image as mask by thresholding,<python><image-processing><image-segmentation><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
27076,28392695,2015-02-08 10:03:12,,"<p>I can compute the SLIC boundaries using skimage as follows:</p>

<pre><code>def compute_superpixels(frame, num_pixels=100, std=5, iter_max=10,
                        connectivity=False, compactness=10.0):

    return slic(frame, n_segments=num_pixels, sigma=std, max_iter=iter_max,
                enforce_connectivity=connectivity, compactness=compactness)
</code></pre>

<p>Now, what I would like to do is get the index of pixels which form the boundary of each label. So my idea was to get all pixels belonging to a given segment and then check which pixels have a change in all two directions</p>

<pre><code>def boundary_pixels(segments, index):
    # Get all pixels having a given index
    x, y = np.where(segments == index)

    right = x + 1
    # check we are in bounds
    right_mask = right &lt; segments.shape[0]
    down = y + 1
    down_mask = down &lt; segments.shape[1]
    left = x - 1
    left_mask = left &gt;= 0  
    up = y - 1
    up_mask = up &gt;= 0  

    neighbors_1 = np.union1d(right_n, down_n)
    neighbors_2 = np.union1d(left_n, up_n)
    neighbors = np.union1d(neighbors_1, neighbors_2)

    # Not neighbours to ourselves
    neighbors = np.delete(neighbors, np.where(neighbors == i))
</code></pre>

<p>However, with this all I managed to do was to get the neighbours in the 4 directions of a given label. Can someone suggest some way to actually get all pixels on the border of the label. </p>
",,2015-02-08 13:33:03,Detecting border pixel of a segmentation label,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
27103,27011995,2014-11-19 08:20:05,,"<p>I would like to robustly estimate a polynomial geometric transform with scikit-image skimage.transform and skimage.measure.ransac</p>

<p>The ransack documentation gives a very nice example of how to do exactly that but with a Similarity Transform. Here is how it goes:</p>

<pre><code>from skimage.transform import SimilarityTransform
from skimage.measure import ransac
model, inliers = ransac((src, dst), SimilarityTransform, 2, 10)
</code></pre>

<p>I need to use <a href=""http://scikit-image.org/docs/dev/api/skimage.transform.html#polynomialtransform"" rel=""nofollow"">skimage.transform.PolynomialTransform</a> instead of SimilarityTransform, and I need to be able to specify the polynomial order.</p>

<p>But the RANSAC call takes as input the PolynomialTransform(), which does not take any input parameters. The desired polynomial order is indeed specified in the estimate attribute of PolynomialTransform()... So the RANSAC call uses the default value for the polynomial order, which is 2, while I would need a 3rd or 4th order polynomial.</p>

<p>I suspect it's a basic python problem? 
Thanks in advance!</p>
",,2014-11-19 17:20:05,Robustly estimate Polynomial geometric transformation with scikit-image and RANSAC,<python><image-processing><transformation><scikit-image><ransac>,,,CC BY-SA 3.0,False,False,False,True,False
27615,28636080,2015-02-20 18:46:49,,"<p>I have been confusing myself more and more with the skimage coordinate system. So when I load an image with skimage, the images are loaded with the shape (rows, columns) where rows are running from top to the bottom of the image and columns are running from left to right.</p>

<p>Now, I am trying to estimate a projective transformation between set of image keypoints and if I understand correctly, the points have to be supplied in the more conventional (x, y) format where x is columns and y is rows. If so, I need to permute the points. However, I am getting sometimes plausible and sometimes fairly rubbish results using both conventions and would like to know which one to use.</p>

<p>ultimately, the transformation that should be returned with ransac would be used to warp an image using the skimage.transform.warp function.</p>

<p>[UPDATE]: With some code. </p>

<p>Here is the usage, I am confused with:</p>

<pre><code>from skimage.transform import ProjectiveTransform
from skimage.measure import ransac
from skimage.feature import plot_matches
from skimage.feature import match_descriptors

# target, source are 2-d images.
# Compute SIFT is done outside skimage but returns c, r indexes.
k_1, d_1 = compute_sift(target)  # c, r indexing
k_2, d_2 = compute_sift(source)

# Match SIFT features and descriptors
matches = match_descriptors(d_1, d_2)

# This is all still c, r indexing
fp = k_1[[[0], [1]], matches[:, 0]] # This is shape (2, n)
tp = k_2[[[0], [1]], matches[:, 1]] # This is shape (2, n)

model_estimate, inliers = ransac((tp.T, fp.T), ProjectiveTransform,
                           min_samples=4, residual_threshold=10,
                           max_trials=100)
</code></pre>

<p>So, the input to RANSAC is in the form (c, r). Since, this is using the transform object internally, I am guessing this is correct. At the moment, I get mixed result with both (c, r) and (r, c), so I would like to eliminate the uncertainty here and verify I am doing the right thing.</p>

<p>If I may also clarify one more thing. skimage uses the nomenclature (src, dst) and if I use the estimate() method on any transform object, is it true that the returned transformation T has the relationship :</p>

<pre><code>T * src_coords = dst_coords
</code></pre>

<p>I am using version 0.10.1 of skimage.</p>
",2015-02-22 19:29:33,2015-02-22 20:32:24,"skimage coordinate system: Confusion about row, column ordering",<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
27786,27069133,2014-11-21 19:46:14,,"<p>I've used the scikit tutorial (<a href=""http://scikit-image.org/docs/dev/auto_examples/plot_blob.html"" rel=""nofollow"">http://scikit-image.org/docs/dev/auto_examples/plot_blob.html</a>) to play with blob detection. I've changed the code to get only the difference of Gaussian</p>

<pre><code>from matplotlib import pyplot as plt
from skimage import data
from skimage.feature import blob_dog
from skimage.color import rgb2gray

image = data.imread('Img.png')

blobs_dog = blob_dog(image, max_sigma=30, threshold=.1)

blobs = [blobs_dog]
colors = ['red']
titles = ['Difference of Gaussian']
sequence = zip(blobs, colors, titles)

for blobs, color, title in sequence:
    fig, ax = plt.subplots(1, 1)
    ax.set_title(title)
    ax.imshow(image, interpolation='nearest')
    for blob in blobs:
        y, x, r = blob
        c = plt.Circle((x, y), r, color=color, linewidth=1, fill=False)
        ax.add_patch(c)

plt.show()
</code></pre>

<p>Now my question is: I used a grayscale image (I didn't modified it with the rgb2gray function), but when I run the code, as output I have a ""colored"" image (almost all cyan with some red and yellow spots). 
If I use a RGB image and then I transform it to grayscale I don't have problems. Why is it so? </p>
",,2014-11-21 19:46:14,Blob detection in grayscale images (skimage),<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
27899,28688349,2015-02-24 04:47:24,,"<p>We are using Python 2.7.9.and scikit image library. We are not able to use <code>skimage.feature.greycomatrix</code> because there is no file such as <strong>greycomatrix.py</strong> in the <strong>feature</strong> folder. Their documentation seems to be wrong as it says this function is available. We get an error module attribute has no object feature. Is there any other image processing library in Python which will help us achieve this goal?</p>
",2017-03-15 16:19:28,2017-06-15 17:53:56,How to find GLCM matrix in python?,<python><image-processing><scikit-image><glcm>,,,CC BY-SA 3.0,False,False,False,True,False
28753,27152624,2014-11-26 15:14:43,,"<p>Looking up at different feature matching tutorials I've noticed that it's tipical to illustrate how the matching works by plotting side by side the same image in two different version (one normal and the other one rotated or distorted). I want to work on feature matching by using two distinct images (same scene shot from slightly different angles). How do I plot them together side by side? 
I'm willing to use skimage on python 2.7</p>
",2014-11-26 15:32:50,2014-11-26 18:38:29,Plot two images side by side with skimage,<python-2.7><image-processing><plot><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
28888,29216179,2015-03-23 17:10:07,,"<p>I am using the measure.regionprops method available in scikit-image to measure the properties of the connected components. It computes a bunch of properties (<a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#regionprops/"" rel=""noreferrer"" title=""regionprops"">Python-regionprops</a>). However, I just need the area of each connected component. Is there a way to compute just a single property and save computation?</p>
",,2019-04-15 18:57:15,Calculating just a specific property in regionprops python,<python><image-processing><scikit-image><connected-components>,,,CC BY-SA 3.0,False,False,False,True,False
29007,29240191,2015-03-24 18:28:57,,"<p>I'm trying to find coins at different images and mark their location. Coins always are perfect circles (not ellipses), but they can touch or even overlap. <a href=""https://gist.github.com/aplavin/9505eef4175d21249b19"" rel=""nofollow noreferrer"">Here</a> are some example images, as well as results of my tries (a Python script using skimage and its outputs), but it doesn't seem to perform well.</p>

<p>The script:</p>

<pre><code>def edges(img, t):
    @adapt_rgb(each_channel)
    def filter_rgb(image):
        sigma = 1
        return feature.canny(image, sigma=sigma, low_threshold=t/sigma/2, high_threshold=t/sigma)

    edges = color.rgb2hsv(filter_rgb(img))
    edges = edges[..., 2]
    return edges

images = io.ImageCollection('*.bmp', conserve_memory=True)

for i, im in enumerate(images):
    es = edges(im, t=220)
    output = im.copy()
    circles = cv2.HoughCircles((es*255).astype(np.uint8), cv2.cv.CV_HOUGH_GRADIENT, dp=1, minDist=50, param2=50, minRadius=0, maxRadius=0)

    if circles is not None:
        circles = np.round(circles[0, :]).astype(""int"")

        for (x, y, r) in circles:
            cv2.circle(output, (x, y), r, (0, 255, 0), 4)
            cv2.rectangle(output, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1)

    # now es is edges
    # and output is image with marked circles
</code></pre>

<p>A couple of example images, with detected edges and circles:</p>

<p><img src=""https://i.stack.imgur.com/gbxMM.png"" alt="""">
<img src=""https://i.stack.imgur.com/5d9pX.png"" alt=""""></p>

<p>I am using canny edge detection &amp; hough transform, which is the most common way to detect circles. However, with the same parameters it finds almost nothing on some photos, and finds way too many circles on other.</p>

<p>Can you give me any pointers and suggestions on how to do this better?</p>
",,2016-01-15 08:01:19,Find coins on image,<opencv><computer-vision><geometry><hough-transform><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
29234,27188425,2014-11-28 12:12:55,,"<p>I'm trying to match the patches I got using dog blob detection. I've been through Jan Solem book about computer vision with python and I have found :</p>

<pre><code>def match(desc1,desc2,threshold=0.5):

    n = len(desc1[0])

    d = -ones((len(desc1),len(desc2)))
    for i in range(len(desc1)):
        for j in range(len(desc2)):
            d1 = (desc1[i] - mean(desc1[i])) / std(desc1[i])
            d2 = (desc2[j] - mean(desc2[j])) / std(desc2[j])
            ncc_value = sum(d1 * d2) / (n-1)
            if ncc_value &gt; threshold:
                d[i,j] = ncc_value
    ndx = argsort(-d)
    matchscores = ndx[:,0]
    return matchscores
</code></pre>

<p>which is useful if you use harris corner detection, but with my features raise a ValueError: operands could not be broadcast together with shapes (81,) (0,).</p>

<p>How could I implement an effective code for a different blob detector which does the same things that this does (compute distances using negative cross correlation)?</p>
",2014-11-28 12:32:29,2014-11-28 12:32:29,compute distance between descriptors extracted with dog,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
29279,29313667,2015-03-28 04:23:27,,"<p><img src=""https://i.stack.imgur.com/SYxmp.jpg"" alt=""Image_1""></p>

<p>I want to remove the background of this image to get the person only. I have thousand of images like this, basically, a person and a somewhat whitish background.</p>

<p>What I have done is to use edge detector like canny edge detector or sobel filter (from <code>skimage</code> library). Then what I think possible to do is, whiten the pixels within the edges and blacken the pixels without. Afterwards, the original image can be mask to get the picture of the person only.</p>

<p>However, it's hard to get a closed boundary using canny edge detector. Result using Sobel filter is not that bad, however I don't how to proceed from there.</p>

<p><img src=""https://i.stack.imgur.com/Vroyo.png"" alt=""Sobel_result""></p>

<p>EDIT:</p>

<p>Is it possible to also remove the background between the right hand and the skirt and between hairs?</p>
",2016-12-15 08:22:09,2020-04-29 05:42:09,How do I remove the background from this kind of image?,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
29663,29568459,2015-04-10 18:34:25,,"<p>Iam trying to use built-in filters in python using scikit image. 
However on executing this sample code,</p>

<pre><code>    from skimage import data, io, filters
    image = data.coins() # or any NumPy array!
    edges = filters.sobel(image)
    io.imshow(edges)
    io.show() 
</code></pre>

<p>I get the error: "" from skimage import data, io, filters 
ImportError: cannot import name filters"". 
I have scikit image package listed in my project interpreter.How to resolve this error?</p>
",,2015-04-10 18:34:25,Error using built in gabor filters in scikit-image in Python,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
29965,29738077,2015-04-20 01:46:39,,"<p>I have pictures from a microscope and I am trying to count the total number of crystal as well as those with specific colors (red, blue or yellow). In principle it looks possible to do it just by looking at the picture but considering I have to do it for over 200, I believe a script would be more effective.</p>

<p>My first approach was to follow the suggestions of a very similar problem (<a href=""https://stackoverflow.com/questions/5298884/finding-number-of-colored-shapes-from-picture-using-python"">Finding number of colored shapes from picture using Python</a> )however, my issue is that the crystal color and the background color (an alcohol solution) are almost the same (transparent), and therefore it has been impossible to get any successful result so far.</p>

<p>I have also checked the ""skimage"" module (<a href=""http://scikit-image.org/"" rel=""nofollow noreferrer"">http://scikit-image.org/</a>) but I got the same issues with the crystal/background.</p>

<p>With the following code I can got something that start making sense, but it is still far from my objective:</p>

<pre><code>import scipy
from scipy import ndimage

sand = scipy.misc.imread(img) # gray-scale image
sandf = ndimage.gaussian_filter(sand, 16)
T = 160 # set threshold by hand to avoid installing `mahotas`

# find connected components
#labeled, nr_objects = ndimage.label(sandf&gt; T) #black-white
labeled, nr_objects = ndimage.label(sandf[:,:,0] &gt; T) #colour
print (""Number of objects is %d "" % nr_objects)


import matplotlib.pyplot as plt
plt.imshow(labeled)
plt.show()
</code></pre>

<p>I will really appreciate if anyone can give me a hand with this. I have tried even modifying the way I take the pictures but nothing. Again, I have two objectives: count the total number of particles and then count those particles that have a tint (red, blue or yellow)</p>

<p>Please find below one typical picture:</p>

<p><img src=""https://i.stack.imgur.com/NAFF0.jpg"" alt=""Cell Image""></p>
",2017-09-26 04:51:17,2017-09-26 08:49:05,python count crystals (total and by color),<python><image-processing>,,,CC BY-SA 3.0,False,False,False,True,False
29973,29718877,2015-04-18 14:51:54,,"<p>I am extremely new to scikit-image (<code>skimage</code>) library in Python for image processing (started few minutes ago!). I have used <code>imread</code> to read an image file in a <code>numpy.ndarray</code>. The array is 3 dimensional where the size of the third dimension is 3 (namely one for each of Red, Green and Blue components of an image). </p>

<pre><code>rgb_image = imread(""input_rgb_image.jpg"")
rgb_image.shape # gives (1411L, 1411L, 3L)
</code></pre>

<p>I tried to extract green channel as:</p>

<pre><code>green_image = rgb_image[:,:,1]
</code></pre>

<p>But when I write this image matrix to an output file as:</p>

<pre><code>imsave(""green_output_image.jpg"",green_image)
</code></pre>

<p>I get an image which doesn't really look ONLY green!</p>
",2017-07-28 19:37:47,2017-07-28 19:37:47,How to extract green channel from RGB image in Python using Scikit-Image library?,<python><image><image-processing><scikit-image><imread>,,,CC BY-SA 3.0,False,False,False,True,False
30192,29773369,2015-04-21 13:18:58,,"<p>I want to detect objects inside cells of microscopy images. I have a lot of annotated images (app. 50.000 images with an object and 500.000 without an object).</p>

<p>So far I tried to extract features using HOG and classifying using logistic regression and LinearSVC. I have tried several parameters for HOG or color spaces (RGB, HSV, LAB) but I don't see a big difference, the predication rate is about 70 %. </p>

<p>I have several questions. How many images should I use to train the descriptor? How many images should I use to test the prediction?</p>

<p>I have tried with about 1000 images for training,  which gives me 55 % positive and 5000, which gives me about 72 % positive. However, it also depends a lot on the test set, sometimes a test set can reach 80-90 % positive detected images.</p>

<p>Here are two examples containing an object and two images without an object:</p>

<p><img src=""https://i.stack.imgur.com/va7ek.jpg"" alt=""object 01""></p>

<p><img src=""https://i.stack.imgur.com/iHEEY.jpg"" alt=""object 02""></p>

<p><img src=""https://i.stack.imgur.com/SvaKt.jpg"" alt=""cell 01""></p>

<p><img src=""https://i.stack.imgur.com/3etmk.jpg"" alt=""cell 02""></p>

<p>Another problem is, sometimes the images contain several objects:</p>

<p><img src=""https://i.stack.imgur.com/2MAOa.jpg"" alt=""objects""></p>

<p>Should I try to increase the examples of the learning set? How should I choose the images for the training set, just random? What else could I try?</p>

<p>Any help would be very appreciated, I just started to discover machine learning. I am using Python (scikit-image &amp; scikit-learn).</p>
",2015-04-21 13:25:33,2015-04-23 17:42:57,Object detection in images (HOG),<python><image><scikit-learn><object-detection><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
30416,26165941,2014-10-02 17:11:26,,"<p>I have to create a linear grayscale gradient, with black shade on top and white shade at the bottom. I have to use skimage and numpy. </p>

<p>I've found on scikit the code for a color linear gradient that goes horizontally instead of vertically here: <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_tinting_grayscale_images.html"" rel=""nofollow"">http://scikit-image.org/docs/dev/auto_examples/plot_tinting_grayscale_images.html</a>.</p>

<p>I would like an explanation of this code and some hints on how put everything in grayscale and vertical instead of colored and horizontal. Thanks!</p>
",,2014-10-02 20:57:35,grayscale gradient with skimage or numpy,<image-processing><numpy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
30795,30113847,2015-05-08 00:27:24,,"<p>I am trying to extract shape descriptors such as Hu Moments, spherical harmonics, etc. from 3D binary images and/or their mesh representation.  I came across plenty of packages in both Matlab and Python that can be used to extract shape descriptors from 2D image.</p>

<p>For example there is scikit-image,</p>

<p><a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.moments"" rel=""nofollow"">http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.moments</a></p>

<p>in python and a number of factory-built and user-built functions in Matlab.</p>

<p>The only thing that I found for 3D image is SPHARM-MAT for Matlab
<a href=""http://imaging.indyrad.iupui.edu/projects/SPHARM/SPHARM-docs/C01_Introduction.html"" rel=""nofollow"">http://imaging.indyrad.iupui.edu/projects/SPHARM/SPHARM-docs/C01_Introduction.html</a></p>

<p>This toolbox calculates spherical harmonics from both 3d binary image and mesh representation of it. But it has not been updated since its release and I am struggling with lots of errors.</p>

<p>So, in the mean time I am looking for alternatives.</p>

<p>Are there any image processing packages to extract shape descriptors from 3d binary images or the mesh representation of the deformable surfaces? (preferably in Python and Matlab)</p>
",,2019-10-10 07:15:57,How to extract shape descriptors from 3D binary image or it's mesh representation (vertices and faces)?,<python><matlab><opencv><3d><shape>,,,CC BY-SA 3.0,True,True,False,True,False
31099,30282909,2015-05-17 02:57:11,,"<p>Calling the following function in MATLAB applies the Canny filter to an image:</p>

<pre><code>edges = edge(image, 'Canny');
</code></pre>

<p>There are two Python functions I know of that implement the Canny filter:</p>

<pre><code>import cv2

edges = cv2.Canny(image)
</code></pre>

<p>and</p>

<pre><code>from skimage import feature

edges = feature.canny(image)
</code></pre>

<p>However, neither of these Python functions is capable of computing the filter's high and low threshold in the same manner MATLAB does.  According to <a href=""http://www.sciencedirect.com/science/article/pii/S1053811908001171"" rel=""nofollow"">this neuroimaging paper</a>,</p>

<blockquote>
  <p>. . . the default MATLAB algorithm [generates] the two thresholds such that the high threshold is calculated to be the lowest value at which no more than 30% of the pixels are detected as edges, and the low threshold is defined as 40% of the high threshold.</p>
</blockquote>

<p>Is there a Python implementation that can do this?</p>
",,2018-10-24 14:15:04,Python Implementation of MATLAB's Canny Filter,<python><matlab><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,True,False,True,False
31464,30397832,2015-05-22 13:20:56,,"<p>I want to train a SVM to perform a classification of Images of Digits (0-9) and then use it to read images with numerical values(a low level OCR).</p>

<p>My idea is to read images one by one and store them in numpy array and then to put all those arrays in an array, so as to make my sample_array.</p>

<p>As from the Scikit-Learn documentation "" As other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array y of class labels (strings or integers), size [n_samples]:""</p>

<p>My question is what should be the features of the Images, and how to define them?</p>

<p>I read the Handwriting Recognition example on the tutorial page of the scikit-learn page, but over there the data is already in a dataset(even after a searching the web quite intensively I still don't know how to get my PICS to convert to a dataset) so you see it's a different situation.</p>

<p>A Secondary question: will scikit-image be helpful to use in this situation? I'm using standard PIL for reading images. </p>
",2015-05-22 13:32:18,2015-05-22 13:44:20,Python Scikit-learn - An attempt at a low level OCR,<python><numpy><scikit-learn><ocr>,,,CC BY-SA 3.0,False,False,False,True,False
31839,30634891,2015-06-04 04:23:24,,"<p>I want to select the staff lines as individual blobs in either of these images:</p>

<p><img src=""https://i.stack.imgur.com/7LJ25.png"" alt=""""></p>

<p><img src=""https://i.stack.imgur.com/V95Le.png"" alt=""""></p>

<p>Is this possible?</p>

<p><strong>Edit</strong>: The regions that I want to select are shown in this image (roughly, I selected them manually): </p>

<p><img src=""https://i.imgur.com/4bpZLNJ.png"" alt=""""> </p>

<p>It's ok if other blobs are getting selected too (I can filter them by their area afterwards). </p>
",2015-06-04 05:55:57,2015-06-04 07:39:44,Getting blobs from similar colored contours in grayscale image,<opencv><image-processing><computer-vision><aforge><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
32727,31036971,2015-06-24 21:02:31,,"<p>I am attempting to read a 4-band (red, green, blue, near-infrared) geotiff (<a href=""ftp://dataworks.library.unr.edu/naip/2010/quarter_quad/41117/"" rel=""nofollow noreferrer"">example data</a>) and perform a <a href=""http://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift"" rel=""nofollow noreferrer"">quickshift segmentation</a> using the <code>scikit-image</code> module in Python.</p>

<p>I have created the following script (based on the <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_segmentations.html#quickshift-image-segmentation"" rel=""nofollow noreferrer"">scikit example</a>):</p>

<pre><code>from __future__ import print_function
from osgeo import gdal
import matplotlib.pyplot as plt
import numpy as np

from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

image = r'C:\path\to\my\geotiff.tif'
img = io.imread(image, as_grey=False, plugin=""gdal"")
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: the input array must be have a shape == (.., ..,[ ..,] 3)), got (4, 436, 553)
</code></pre>

<p>I am pretty sure the numpy array needs to be reshaped somehow.  How can I properly read the multiband geotiff into a numpy array and perform the image segmentation?</p>
",2018-01-03 03:46:34,2018-01-03 03:46:34,How to perform image segmentation on 4-band geotiff using Python's scikit-image?,<python><image-processing><numpy><scikit-image><geotiff>,,,CC BY-SA 3.0,False,False,False,True,False
33510,26428644,2014-10-17 15:41:35,,"<p>I'm trying to write a script in python to detect and count objects inside an image, but I'm failing miserably. </p>

<p>It is the first time I get interested and try something by means of computer vision. I have tried using cv2 module (open cv) following the tutorials about Feature Matching and Template Matching present <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/"" rel=""nofollow"">here</a>. I have also tried with scikit-image but I cannot achieve a good result either. I have also thought about finding contours and then making 2d-curve matching.</p>

<p>Let me explain a little bit more the problem. I have a set of icons which are the ones composing an big image. The composition of this image or scene is done by a plain background or an transparent one, and a number of images from the iconset. These images from the iconset can undergo basically 3 basic transformations: scale, rotation and translation. They can also be overlapped.</p>

<p><a href=""http://pastebin.com/G9vHAdBt"" rel=""nofollow"">An quick example by means of Android version icons.</a></p>

<p>The output of the desired script will be something like:</p>

<p>C -> 1</p>

<p>D -> 1</p>

<p>E -> 1</p>

<p>F -> 1</p>

<p>G -> 1</p>

<p>H -> 2</p>

<p>I -> 3</p>

<p>J -> 0</p>

<p>I'm going to try now with Dlib and see if I can achieve something with machine learning algorithms. I think that I'm trying to solve my problem by something much more complex that actually needed. Any advice on how to do it will be great, I'm also open to any library for python.</p>

<p>PS: sorry for not publishing here the images but I do not have enough reputation yet.</p>
",,2014-10-18 22:47:40,Detect and count objects in an image,<python><opencv><computer-vision><object-detection><template-matching>,,,CC BY-SA 3.0,True,False,False,True,False
33644,31436514,2015-07-15 16:56:15,,"<p>I am quite new to Image Processing, CV and OCR. So far I think its an amazing subject and that I am willing to dig it further.</p>

<p>Imagine I have this original image:
<img src=""https://i.stack.imgur.com/WfhA6.jpg"" alt=""Original page""></p>

<p>I resize it to this:
<img src=""https://i.stack.imgur.com/3uGI9.jpg"" alt=""resized""></p>

<p>Then I found regional maxima and get this image(to avoid lighter backgrounds and too noisy ones):
<img src=""https://i.stack.imgur.com/3B52h.jpg"" alt=""Regional Maxima""></p>

<p>Then submit the above image to a threshold and with processing have this image:
<img src=""https://i.stack.imgur.com/80Ps9.jpg"" alt=""Tresholded"">
This image seems to me that is not 100% binary... if I zoom it in it shows some gray pixels inside characters... </p>

<p>I was thinking that this last image should be enough/(very good, indeed) for OCR, don't you think? But there is no text coming out of it...</p>

<p>My code:</p>

<pre><code>#http://stackoverflow.com/questions/18813300/finding-the-coordinates-of-maxima-in-an-image
from PIL import *
from PIL import Image
import numpy as np
from skimage import io
from skimage import img_as_float
from scipy.ndimage import gaussian_filter
from skimage.morphology import reconstruction
import pytesseract

im111 = Image.open('page.jpg')

basewidth = 1000
wpercent = (basewidth / float(im111.size[0]))
hsize = int((float(im111.size[1]) * float(wpercent)))
image_resized = im111.resize((basewidth, hsize), Image.ANTIALIAS)
image_resized.save('page2.jpg')

image = img_as_float(io.imread('page2.jpg', as_grey=True))
image = gaussian_filter(image, 1)
seed = np.copy(image)
seed[1:-1, 1:-1] = image.min()
mask = image
dilated = reconstruction(seed, mask, method='dilation')
image = image - dilated

#print type(image)

#io.imsave(""RegionalMaxima.jpg"", image)

im = np.array(image * 255, dtype = np.uint8)
a = np.asarray(im)
img = Image.fromarray(a)

#img.show()

#print type(img)
#img.save('RegionalMaximaPIL.jpg')

#image2 = Image.open('RegionalMaxima.jpg')

minima, maxima = img.getextrema()
print ""------Extrema1----------"" + str(minima), str(maxima)
mean = int(maxima/4)
im1 = img.point(lambda x: 0 if x&lt;mean else maxima, '1')
im1.save('Thresh_calculated.jpg')
#im1.show()

mini, maxi = im1.getextrema()
print ""-------Extrema2(after1stTresh)---------"" + str(mini), str(maxi)

im2 = im1.point(lambda x: 0 if x&lt;128 else 255, '1')
im2.save('Thresh_calculated+++.jpg')
im2.show()

text = pytesseract.image_to_string(im2)
print ""-----TEXT------"" + text
</code></pre>

<p>What am I doing wrong? pytesseract.image_to_string(im1) with the thresholded image should be retrieving some text already :/</p>

<p>Other doubts:
the second ""getextrema()"" the results should not be 0 and 255??? I am confused since they still present me the same numbers before the first threshold... so the image resulted from the second treshold is all black.</p>

<p>Thanks so much for your time and help.</p>
",2015-07-16 08:59:29,2017-08-04 16:26:55,"Good image but no text coming from OCR, why? Python, Skimage, PIL, Tesseract",<image-processing><python-imaging-library><ocr><tesseract><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
33767,27571511,2014-12-19 18:04:56,,"<p>I have tried the dHash algorithm which is applied on each image, then a hamming_distance is calculated on both hashes, the lower the number, the higher the similarity.</p>

<pre><code>from PIL import Image
import os
import shutil
import glob
from plotData import *

def hamming_distance(s1, s2):
    #Return the Hamming distance between equal-length sequences
    if len(s1) != len(s2):
        raise ValueError(""Undefined for sequences of unequal length"")
    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))


def dhash(image, hash_size = 8):
    # Grayscale and shrink the image in one step.
    image = image.convert('L').resize(
        (hash_size + 1, hash_size),
        Image.ANTIALIAS,
    )

    pixels = list(image.getdata())

    # Compare adjacent pixels.
    difference = []
    for row in xrange(hash_size):
        for col in xrange(hash_size):
            pixel_left = image.getpixel((col, row))
            pixel_right = image.getpixel((col + 1, row))
            difference.append(pixel_left &gt; pixel_right)

    # Convert the binary array to a hexadecimal string.
    decimal_value = 0
    hex_string = []
    for index, value in enumerate(difference):
        if value:
            decimal_value += 2**(index % 8)
        if (index % 8) == 7:
            hex_string.append(hex(decimal_value)[2:].rjust(2, '0'))
            decimal_value = 0

    return ''.join(hex_string)




orig = Image.open('imageA.png')
modif = Image.open('imageA.png')
hammingDistanceValue = hamming_distance(dhash(orig),dhash(modif))
    print hammingDistanceValue
</code></pre>

<p>Unfortunately, this approach produces false positives because it does not really look at the line chart shapes as primary similarity feature. I guess, I'd need some kind of machine learning approach maybe from openCV or so. Can anyone guide me into the right direction to something that compares with high precision?</p>

<p><strong>this is the initial image to compare against a collection of similar images.</strong></p>

<p><img src=""https://i.stack.imgur.com/GjpFk.png"" alt=""enter image description here""></p>

<p><strong>this is a <em>positive</em> match</strong>
<img src=""https://i.stack.imgur.com/61GJE.png"" alt=""enter image description here""></p>

<p><strong>this is a <em>false</em> match</strong>
<img src=""https://i.stack.imgur.com/XmZ06.png"" alt=""enter image description here""></p>

<p><strong>update:</strong> I added some opencv magic to jme's suggestion below. I try to detect significant features first. Howeve, it still produces false positives, since the overall indicator for similarity is the cummulated value over all features and does not take differences into account that can give a line chart a totally different meaning.</p>

<p><strong>False Positive example</strong>
<img src=""https://i.stack.imgur.com/9i7oa.png"" alt=""enter image description here""></p>

<p><strong>Example of preprocessed image with significant features marked as red dots</strong>
<img src=""https://i.stack.imgur.com/TAvvo.png"" alt=""enter image description here""></p>

<pre><code>from PIL import Image
import os
import numpy as np
from scipy.interpolate import interp1d
import os.path
import shutil
import glob
from plotData import *
import cv2
from matplotlib import pyplot as plt

def load_image(path):
    #data = Image.open(path)
    img = cv2.imread(path)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

    corners = cv2.goodFeaturesToTrack(gray,25,0.01,10)
    corners = np.int0(corners)

    for i in corners:
        x,y = i.ravel()
        cv2.circle(img,(x,y),3,255,-1)

    return np.mean((255 - np.array(img))**2, axis=2)


symbol = ""PBYI""
x = np.arange(1000)

if not os.path.exists('clusters1DSignal/'+symbol+'/'):
    os.mkdir('clusters1DSignal/'+symbol+'/')
else:
    shutil.rmtree('clusters1DSignal/'+symbol+'/')
    os.mkdir('clusters1DSignal/'+symbol+'/')

shutil.copyfile('rendered/'+symbol+'.png', ""clusters1DSignal/""+symbol+""/""+symbol+'.png')


img1 = load_image('rendered/'+symbol+'.png')
y1 = np.argmax(img1, axis=0)
f1 = interp1d(np.linspace(0, 1000, len(y1)), y1)
z1 = f1(x)

for filename in glob.iglob('rendered/*.png'):
    try:
        img2 = load_image(filename)
    except:
        continue
    y2 = np.argmax(img2, axis=0)
    f2 = interp1d(np.linspace(0, 1000, len(y2)), y2)
    z2 = f2(x)

    result = np.linalg.norm(z1 - z2)
    if result &lt; 2100:
        print str(result) +"": "" +filename
        symbolCompare = filename.split(""/"")[1].replace("".png"","""")
        shutil.copyfile('rendered/'+symbolCompare+'.png', ""clusters1DSignal/""+symbol+""/""+str(result)+""_""+symbolCompare+"".png"")
</code></pre>
",2014-12-20 10:50:09,2014-12-20 10:50:09,What is a good way to get a similarity measure of two images that contain a line chart?,<python><opencv><scipy><scikit-learn><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
33862,31576512,2015-07-23 01:00:15,,"<p>I have a <a href=""https://drive.google.com/file/d/0B0Vt3k6ChHIseXhpX1BTbTRjQU0/view"" rel=""nofollow"">RGB *.TIFF image</a> imported with <a href=""http://scikit-image.org/"" rel=""nofollow"">scikit-image library</a>. I wish to apply the <em>Gaussian filter</em> and save a TIFF file again. </p>

<p>When i open this file with a viewer software (es: IrfanView) i get a message about the file is invalid or unsupported as TIFF file. Inside Python i can plot without problem the array and see the effect of the filter.</p>

<pre><code>from skimage.filters import gaussian_filter
from skimage import io
import numpy as np
from skimage import filters
from skimage import img_as_float
from skimage import exposure
from PIL import Image
import os

myimage = '~prova2.tiff'
im = io.imread(myimage)

array([[[11368, 10479,  7155],
        [11632,  9333,  7155],
        [11893, 10107,  7574],
        ..., 
        ...,
        ..., 
        [15262, 13399, 13285],
        [15782, 13878, 13186],
        [15782, 13666, 13087]]], dtype=uint16)

im.dtype
dtype('uint16')
im.shape
(4024L, 6024L, 3L)
</code></pre>

<p>on this image i apply a gaussian filter implemented in skimage </p>

<pre><code>im_gaussian = gaussian_filter(im, sigma=5, multichannel=True)
array([[[ 0.17088626,  0.16434972,  0.1129538 ],
        [ 0.17117972,  0.16493729,  0.11318613],
        [ 0.17150206,  0.16562064,  0.11345394],
        ..., 
        ...,
        ..., 
        [ 0.22970595,  0.21051914,  0.19862151],
        [ 0.22982556,  0.21036883,  0.19824348],
        [ 0.23003602,  0.21031404,  0.19787011]]])

io.imsave(""~prova2_gaussin.tiff"", im_gaussian)
</code></pre>
",2015-07-23 02:51:32,2015-07-23 02:51:32,TIFF File invalid or unsupported after gaussin filter in scikit-image library,<python><image-processing><save><filtering><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
34164,31705355,2015-07-29 15:52:46,,"<p>I have a figure flame of the form shown below:</p>

<p><a href=""https://i.stack.imgur.com/3JxQ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3JxQ8.png"" alt=""enter image description here""></a></p>

<p>I am trying to detect the outer edge of the camera's view and centre the figure so that circular view of the flame is exactly at the centre of the plot. As the position of the circle might change with the image capture date. Sometimes it might be at the upper half, sometimes lower half, etc.</p>

<p>Are there any modules in Python that can detect the view and centre it?</p>

<p><strong>Reproducible code</strong></p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img=mpimg.imread('flame.png')
lum_img = img[:,:,0]
img_plot = plt.imshow(lum_img)
img_plot.set_cmap('jet')
plt.axis('Off')
plt.show()
</code></pre>
",2015-07-29 15:59:07,2015-07-29 19:13:35,How to detect circlular region in images and centre it with Python?,<python><image-processing><matplotlib><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
34494,31878214,2015-08-07 12:53:07,,"<p>I want to create a skeleton based on an existing segmentation, similar to what is done here (from <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_medial_transform.html"" rel=""nofollow noreferrer"">sk-image</a>):</p>

<p><a href=""https://i.stack.imgur.com/55lV3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/55lV3.png"" alt=""my goal""></a></p>

<p>However I want to do this on 3D data. Is there code for that somewhere out there? Preferably in python but any language helps.</p>

<p>I am aware of <a href=""http://www.inf.u-szeged.hu/~palagyi/skel/skel.html"" rel=""nofollow noreferrer"">this</a> great site, however I think they don't offer any code.</p>

<p>I am planning on using that on volumes of about 500x500x500 Pixels, so it should scale well...</p>
",2015-08-07 13:03:28,2017-04-25 19:45:06,3d skeleton from segmentation,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
34763,32049122,2015-08-17 11:22:58,,"<p>I am using Python 2.7 with sklearn, skimage and OpenCV.</p>

<p>I want to convert a folder of images into a .mat image dataset containing features and labels. On the dataset, I will use the HOG descriptor for feature extraction. I am trying to access the image folder using the following code:</p>

<p><code>from sklearn import datasets</code></p>

<p><code>dataset = datasets.fetch_mldata(""C:\\Users\\local.user\\Desktop\\image-folder"")</code></p>

<p>But how to create a <strong>.mat</strong> image dataset using Python which contains a feature-label pair for each image ?</p>
",2016-05-17 05:10:07,2019-02-13 17:18:53,Python: How to convert an image folder into a dataset containing features and labels?,<python><opencv><computer-vision><scikit-learn><scikit-image>,,,CC BY-SA 3.0,True,True,False,True,False
34929,32102122,2015-08-19 17:38:37,,"<p>I am working on removing the background of <a href=""http://i.stack.imgur.com/oYxzi.jpg"" rel=""nofollow"">this image</a> where the object is the top the model is wearing. I have to do this on a range of images, some of them have very distinct background colors, some have very noisy/colorful backgrounds, and some are like this one where the background color and object color are very similar.</p>

<p>Right now, the process I'm taking is from image -> grayscale -> use otsu threshold to get black and white image -> canny edge detection -> opening and closing -> find contour -> overlay on original image</p>

<p>This works well with some of the images and doesn't work at all for others, like the one above. I don't know where to go from here and also have the one process automatically remove backgrounds for all the different images.</p>

<p>I'm using the opencv2-python, PIL, and scikit-image packages.</p>

<hr>

<p>So I tried out grabcut and it works reasonably well. I ended up using the tutorial codes <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_grabcut/py_grabcut.html"" rel=""nofollow"">here</a> and modified it slightly.</p>

<pre><code>import numpy as np
import cv2
from matplotlib import pyplot as plt

img = cv2.imread(filename)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #convert to RGB color
img = cv2.copyMakeBorder(img,1,1,1,1,cv2.BORDER_CONSTANT,value= (0,255,255) #add thin border so images where foreground bleeds off page are not cut out
img = np.array(Image.fromarray(img).resize((200,200))) #make it run faster and standardize size of the img
mask = np.zeros(img.shape[:2],np.uint8)

bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

rect = (1,1,200,200)
cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)

mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')
new_img = img.copy()
new_img[np.where((mask2==0))] = np.array([0,255,255]).astype('uint8') #this allows you to change the background color after grabcut
</code></pre>
",2015-08-21 14:07:19,2015-08-21 14:07:19,How do I remove the background on this type of images when object color is very similar to the background?,<python><opencv><python-imaging-library><image-recognition><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
35129,26574303,2014-10-26 14:54:41,,"<p>I want to do something similar to what in image analysis would be a standard 'image registration' using features.</p>

<p>I want to find the best transformation that transforms a set of 2D coordinates A in another one B.
But I want to add an extra constraint being that the transformation is 'rigid/Euclidean transformation' Meaning that there is no scaling but only translation and rotation.
Normally allowing scaling I would do:</p>

<pre><code> from skimage import io, transform
 destination = array([[1.0,2.0],[1.0,4.0],[3.0,3.0],[3.0,7.0]])
 source = array([[1.2,1.7],[1.1,3.8],[3.1,3.4],[2.6,7.0]])
 T = transform.estimate_transform('similarity',source,destination)
</code></pre>

<p>I believe<code>estimate_transform</code> under the hood just solves a least squares problem.
But I want to add the constraint of no scaling.</p>

<p>Are there any function in skimage or other packages that solve this?
Probably I need to write my own optimization problem with scipy, CVXOPT or cvxpy.
Any help to phrase/implement this optimization problem?</p>

<p><strong>EDIT</strong>:
My implementation thanks to Stefan van der Walt Answer</p>

<pre><code>from matplotlib.pylab import *
from scipy.optimize import *

def obj_fun(pars,x,src):
    theta, tx, ty = pars
    H = array([[cos(theta), -sin(theta), tx],\
         [sin(theta), cos(theta), ty],
         [0,0,1]])
    src1 = c_[src,ones(src.shape[0])]
    return sum( (x - src1.dot(H.T)[:,:2])**2 )

def apply_transform(pars, src):
    theta, tx, ty = pars
    H = array([[cos(theta), -sin(theta), tx],\
         [sin(theta), cos(theta), ty],
         [0,0,1]])
    src1 = c_[src,ones(src.shape[0])]
    return src1.dot(H.T)[:,:2]

res = minimize(obj_fun,[0,0,0],args=(dst,src), method='Nelder-Mead')
</code></pre>
",2014-10-27 09:18:51,2018-04-14 16:33:39,Estimate Euclidean transformation with python,<python><image-processing><scikit-image><homogenous-transformation>,,,CC BY-SA 3.0,False,False,False,True,False
35241,32529149,2015-09-11 17:36:06,,"<p>I'm trying to run the canny edge detector on this image:</p>

<p><a href=""https://i.stack.imgur.com/SgQKf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SgQKf.jpg"" alt=""enter image description here""></a></p>

<p>With this code:</p>

<pre><code>def edges(img):
    from skimage import feature
    img = Image.open(img)
    img.convert('L')
    array = np.array(img)    
    out = feature.canny(array, sigma=1, )
    return Image.fromarray(out,'L')

edges('Q_3.jpg').save('Q_3_edges.jpg')
</code></pre>

<p>But I'm just getting a black image back.  Any ideas what I could be doing wrong?  I tried sigma of 1 and of 3.</p>

<p><a href=""https://i.stack.imgur.com/SS44O.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SS44O.jpg"" alt=""enter image description here""></a></p>
",,2020-07-18 12:20:15,Trouble with Canny Edge Detector - Returning black image,<python><computer-vision><scikit-image><canny-operator>,,,CC BY-SA 3.0,False,False,False,True,False
35550,32435488,2015-09-07 09:38:34,,"<p>I want in the x-ray image below to (by using Python):</p>

<ol>
<li>identify the rotation of the (imperfect) rectangle block</li>
<li>rotate the image so that it is in vertical (portrait form)</li>
<li>remove by cropping the remaining white space</li>
</ol>

<p>I guess this partly the reverse of <a href=""https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders"">this question</a> where the tools are most likely identical with the addition of a <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_corner.html"" rel=""nofollow noreferrer"">corner detector</a>. I'm not entirely sure of how to best approach this and it seems like a problem that someone has solved.</p>

<p><a href=""https://i.stack.imgur.com/qLQbF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qLQbF.png"" alt=""enter image description here""></a></p>
",2017-05-23 11:55:16,2015-09-09 14:19:53,"Align x-ray images: find rotation, rotate and crop",<python><opencv><pillow><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
35641,32622935,2015-09-17 05:06:08,,"<p>I want to use OCR to capture the bowling scores from the monitor at the lances.  I had a look at <a href=""https://stackoverflow.com/questions/10196198/how-to-remove-convexity-defects-in-a-sudoku-square/11366549#11366549"">this sudoku solver</a>, as I think its pretty similar - numbers and grids right?  It has trouble finding the horizontal lines.  Has anyone got any tips for pre-processing this image to make it easier to detect the lines (or numbers!).  Also any tips for how to deal with the split (the orange ellipse around some of the 8's int he image)?</p>

<p>So far I have got the outline of the score area and cropped it.</p>

<pre><code>import matplotlib
matplotlib.use('TkAgg')
from skimage import io
import numpy as np
import matplotlib.pyplot as plt
from skimage import measure
from skimage.color import rgb2gray
# import pytesseract
from matplotlib.path import Path
from qhd import *


def polygonArea(poly):
    """"""
    Return area of an unclosed polygon.

    :see: https://stackoverflow.com/a/451482
    :param poly: (n,2)-array
    """"""
    # we need a plain list for the following operations
    if isinstance(poly, np.ndarray):
        poly = poly.tolist()
    segments = zip(poly, poly[1:] + [poly[0]])
    return 0.5 * abs(sum(x0*y1 - x1*y0
                         for ((x0, y0), (x1, y1)) in segments))

filename = 'good.jpg'
image = io.imread(filename)
image = rgb2gray(image)

# Find contours at a constant value of 0.8
contours = measure.find_contours(image, 0.4)

# Display the image and plot all contours found
fig, ax = plt.subplots()

c = 0
biggest = None
biggest_size = 0

for n, contour in enumerate(contours):
    curr_size = polygonArea(contour)
    if  curr_size &gt; biggest_size:
        biggest = contour
        biggest_size = curr_size

biggest = qhull2D(biggest)


# Approximate that so we just get a rectangle.
biggest = measure.approximate_polygon(biggest, 500)

# vertices of the cropping polygon
yc = biggest[:,0]
xc = biggest[:,1]
xycrop = np.vstack((xc, yc)).T

# xy coordinates for each pixel in the image
nr, nc = image.shape
ygrid, xgrid = np.mgrid[:nr, :nc]
xypix = np.vstack((xgrid.ravel(), ygrid.ravel())).T

# construct a Path from the vertices
pth = Path(xycrop, closed=False)

# test which pixels fall within the path
mask = pth.contains_points(xypix)

# reshape to the same size as the image
mask = mask.reshape(image.shape)

# create a masked array
masked = np.ma.masked_array(image, ~mask)

# if you want to get rid of the blank space above and below the cropped
# region, use the min and max x, y values of the cropping polygon:

xmin, xmax = int(xc.min()), int(np.ceil(xc.max()))
ymin, ymax = int(yc.min()), int(np.ceil(yc.max()))
trimmed = masked[ymin:ymax, xmin:xmax]

plt.imshow(trimmed, cmap=plt.cm.gray), plt.title('trimmed')
plt.show()
</code></pre>

<p><a href=""https://imgur.com/LijB85I"" rel=""nofollow"">https://imgur.com/LijB85I</a> is an example of how the score is displayed.</p>
",2015-09-17 05:59:08,2015-09-17 05:59:08,10 Pin Bowling score capture,<python><opencv><computer-vision><ocr><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
36262,33037073,2015-10-09 11:25:10,,"<p>I am using scikit-image's greycomatrix (GLCM) to extract features from an image. This method is quite fast, but when creating a feature map of an image with GLCM one needs to glide a window over the image and call greycomatrix once for each window. For a 256x256 image that is 65536 calls. This becomes very slow with simple Python loops. I have been searching high and low, but I cannot find an existing implementation of this window gliding method, so I have made it myself so far. It seems strange that this doesn't already exist within scikit-image since this is the primary way to use the GLCM.</p>

<p>In the scikit-image GLCM tutorial (<a href=""http://scikit-image.org/docs/dev/auto_examples/plot_glcm.html"" rel=""nofollow"">http://scikit-image.org/docs/dev/auto_examples/plot_glcm.html</a>) they only analyse a few individual windows and say nothing about gliding windows. That is not very helpful.</p>

<pre><code>import numpy as np
from skimage.feature import greycomatrix
from skimage.data import coins

def glide(image, w, d, theta, levels=16):

    image = np.pad(image, int(w/2), mode='reflect')  # Add padding.
    M, N = image.shape
    feature_map = np.zeros((M, N))  # Placeholder for some feature.

    for m in xrange(0, M):
        for n in xrange(0, N):
            window = image[m:m+w, n:n+w]
            glcm = greycomatrix(window, d, theta, levels)
            # Do something with glcm: Find variance, entropy, etc.
            feature_map[m,n] = 1.  # Compute something here.

feature_map = glide(coins(), w=21, d=[5], theta=[0], levels=256)
</code></pre>

<p>This is my custom gliding window method. Does a more efficient version of this already exist within scikit-image or similar?</p>
",2017-03-15 16:04:23,2017-03-15 16:04:23,Does efficient GLCM gliding window exist in Python library?,<python><image-processing><scikit-image><glcm>,,,CC BY-SA 3.0,False,False,False,True,False
36396,33128658,2015-10-14 14:51:02,,"<p>I would like to transform a .jpg into a categorical array. For each pixel of the images I have RGB values and I would like to associate this values to a unique value (see images). Have you any idea to do this? I've made some research in scikit image and other image processing modules but without success.</p>

<p><a href=""http://i.stack.imgur.com/LhCIT.png"" rel=""nofollow"">jpg to categorical</a></p>
",,2015-10-15 14:21:20,Categorical image in python,<python><image><image-processing>,,,CC BY-SA 3.0,False,False,False,True,False
36458,33022983,2015-10-08 18:06:41,,"<p>An indexed color image is an image with pixels that are integers (1,2, .. N), and for each integer, an associated color maps to this pixel from a given color map.  In MATLAB, reading in an indexed color image can be done in the following way:</p>

<pre><code>[im, colormap] = imread('indexed.png');
</code></pre>

<p>How can I do the same thing in Python? I have tried OpenCV, <code>scikit-image</code> but they all convert to RGB automatically.</p>
",2015-10-08 20:50:58,2015-10-09 22:01:42,Read in an indexed color image in Python,<python><image><matlab><image-processing>,,,CC BY-SA 3.0,True,True,False,True,False
36626,33287613,2015-10-22 18:01:20,,"<p>I'm using skimage to crop a rectangle in a given image, now I have (x1,y1,x2,y2) as the rectangle coordinates, then I had loaded the image </p>

<pre><code> image = skimage.io.imread(filename)
 cropped = image(x1,y1,x2,y2)
</code></pre>

<p>However this is the wrong way to crop the image, how would I do it in the right way in skimage</p>
",2019-04-22 20:38:09,2020-05-12 17:34:19,crop image in skimage?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
36664,33331847,2015-10-25 16:09:54,,"<p>I have a Python script that uses PIL and the HSV color space to detect all the red objects in an image.
<a href=""https://i.stack.imgur.com/hEew4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hEew4.jpg"" alt=""in action""></a></p>

<p>It can output a list of red pixels in the image that looks like <code>[(x,y),(x,y),(x,y),
   etc.]</code></p>

<p>Using this list, I can find the center of all red pixels with <code>[sum(list(x))/len(list(x)) for x in zip(*list)]</code></p>

<p>What I'd like to do is find the respective centers of all red objects (Both the center of the trash can AND the center of the folder). To do this, I want an efficient way to divide the list into multiple lists, one for each solid object. Then, I can take these lists and exclude the ones with fewer than 20 pixels to account for disconnected specks of color. </p>

<p>How can I best separate lists of (x, y) coordinates into multiple lists of connected pixels?</p>

<p>NOTE: I do not have <code>scipy</code>, <code>OpenCV</code>, or <code>scikit-image</code> available. I have <code>PIL</code> and <code>Numpy</code></p>
",2015-10-25 16:52:28,2016-07-16 15:40:33,"Lists of (x, y) coordinates into multiple lists of connected pixels",<python><arrays><python-2.7><image-processing><python-imaging-library>,,,CC BY-SA 3.0,False,False,False,True,False
36686,33294595,2015-10-23 03:52:03,,"<p>I've written this algorithm in Python for reading CAPTCHAs using scikit-image:</p>

<pre><code>from skimage.color import rgb2gray
from skimage import io

def process(self, image):
    """"""
    Processes a CAPTCHA by removing noise

    Args:
        image (str): The file path of the image to process
    """"""

    input = io.imread(image)
    histogram = {}

    for x in range(input.shape[0]):
        for y in range(input.shape[1]):
            pixel = input[x, y]
            hex = '%02x%02x%02x' % (pixel[0], pixel[1], pixel[2])

            if hex in histogram:
                histogram[hex] += 1
            else:
                histogram[hex] = 1

    histogram = sorted(histogram, key = histogram.get, reverse=True)
    threshold = len(histogram) * 0.015

    for x in range(input.shape[0]):
        for y in range(input.shape[1]):
            pixel = input[x, y]
            hex = '%02x%02x%02x' % (pixel[0], pixel[1], pixel[2])
            index = histogram.index(hex)

            if index &lt; 3 or index &gt; threshold:
                input[x, y] = [255, 255, 255, 255]

    input = rgb2gray(~input)
    io.imsave(image, input)
</code></pre>

<p><strong>Before:</strong></p>

<p><a href=""https://i.stack.imgur.com/phWoh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/phWoh.png"" alt=""Before""></a></p>

<p><strong>After:</strong></p>

<p><a href=""https://i.stack.imgur.com/27onz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/27onz.jpg"" alt=""After""></a></p>

<p>It works fairly well and I get decent results after running it through Google's Tesseract OCR, but I want to make it better. I think that straightening the letters would yield a much better result. My question is how do I do that?</p>

<p>I understand I need to box the letters somehow, like so:</p>

<p><a href=""https://i.stack.imgur.com/aFemx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aFemx.jpg"" alt=""Boxed""></a></p>

<p>Then, for each character, rotate it some number of degrees based on a vertical or horizontal line.</p>

<p>My initial thought was to identify the center of a character (possibly by finding clusters of most used colors in the histogram) and then expanding a box until it found black, but again, I'm not so sure how to go about doing that.</p>

<p>What are some common practices used in image segmentation to achieve this result?</p>

<p><strong>Edit:</strong></p>

<p>In the end, further refining the color filters and limiting Tesseract to only characters yielded a nearly 100% accurate result without any deskewing.</p>
",2015-10-24 07:03:33,2018-12-31 22:08:49,Segmenting letters in a Captcha image,<python><image-processing><captcha><image-segmentation><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
36796,33303947,2015-10-23 13:47:50,,"<p>I am looking for a best approach for the following problem:</p>

<p>I have 10.000 images, pictures (art or cliparts) to be more precise. I am looking for a method to search for those images appearing in the evaluation image ( the evaluation image may contain any number of 10.000 images, but in most cases will be none to 4 ).</p>

<p>Here is an example:
10.000 images are pictures published in newspaper in a last year. The evaluation image is the newspaper scan. I want to find which picture ( of 10.000 ) appears on the scan.
It's important to know <strong>which</strong> image appears and not just ""one of training set"" appears. </p>

<p>Ideas?</p>
",2015-10-23 14:40:16,2015-10-26 13:36:59,Multiple images/objects recognition in a single image,<opencv><computer-vision><scikit-learn><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
36809,33499295,2015-11-03 12:34:33,,"<p>I was looking at some code in python skimage toolkit regarding representing affine transformations in 2D and there is an <code>AffineTransform</code> class which is defined as:</p>

<pre><code>Parameters
----------
matrix : (3, 3) array, optional
    Homogeneous transformation matrix.
scale : (sx, sy) as array, list or tuple, optional
    Scale factors.
rotation : float, optional
    Rotation angle in counter-clockwise direction as radians.
shear : float, optional
    Shear angle in counter-clockwise direction as radians.
translation : (tx, ty) as array, list or tuple, optional
    Translation parameters.
</code></pre>

<p>I notice that the shearing only takes one parameter (shearing angle in counter-clockwise direction). However, why should this not be two parameters? I can shear in x AND in y direction. How come these two operations map into one free parameter in 2D?</p>
",,2015-11-10 07:46:36,Shearing in 2D affine transformations,<geometry><computer-vision><affinetransform><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
37145,33603304,2015-11-09 06:03:00,,"<p>Here's what I'm trying to do:</p>

<p>I have an image.</p>

<p>I want to take a circular region in the image, and have it appear as normal. </p>

<p>The rest of the image should appear darker. </p>

<p>This way, it will be as if the circular region is ""highlighted"".</p>

<p>I would much appreciate feedback on how to do it in Python. </p>

<p>Manually, in Gimp, I would create a new layer with a color of gray (less than middle gray). I would then create a circualr region on that layer, and make it middle gray. Then I would change the blending mode to soft light. Essentially, anything that is middle gray on the top layer will show up without modification, and anything darker than middle gray would show up darker.</p>

<p>(Ideally, I'd also blur out the top layer so that the transition isn't abrupt). </p>

<p>How can I do this algorithmically in Python? I've considered using the Pillow library, but it doesn't have these kinds of blend modes. I also considered using the Blit library, but I couldn't import (not sure it's maintained any more). Am open to scikit-image as well. I just need pointers on the library and some relevant functions. </p>

<p>If there's no suitable library, I'm open to calling command line tools (e.g. imagemagick) from within the Python code.</p>

<p>Thanks!</p>
",,2018-03-17 10:45:47,"Create a ""spotlight"" in an image using Python",<python><image-processing>,,,CC BY-SA 3.0,False,False,False,True,False
37159,33698068,2015-11-13 17:03:34,,"<p>I am creating a database from historical records which I have as photographed pages from books (+100K pages). I wrote some python code to do some image processing before I OCR each page. Since the data in these books does not come in well formatted tables, I need to segment each page into rows and columns and then OCR each piece separately.</p>

<p>One of the critical steps is to align the text in the image.</p>

<p>For example, this is a typical page that needs to be aligned:
<a href=""https://i.stack.imgur.com/2q4Qr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2q4Qr.png"" alt=""page to align""></a></p>

<p>A solution I found is to smudge the text horizontally (I'm using skimage.ndimage.morphology.binary_dilation) and find the rotation that maximizes the sum of white pixels along the horizontal dimension.</p>

<p>This works fine, but it takes about 8 seconds per page, which given the volume of pages I am working with, is way too much.</p>

<p>Do you know of a better, faster way of accomplishing aligning the text?</p>

<h3>Update:</h3>

<p>I use scikit-image for image processing functions, and scipy to maximize the count of white pixels along the horizontal axis.</p>

<p>Here is a link to an html view of the Jupyter notebook I used to work on this. The code uses some functions from a module I've written for this project so it cannot be run on its own.</p>

<p>Link to notebook (dropbox): <a href=""https://db.tt/Mls9Tk8s"" rel=""noreferrer"">https://db.tt/Mls9Tk8s</a></p>

<h3>Update 2:</h3>

<p>Here is a link to the original raw image (dropbox): <a href=""https://db.tt/1t9kAt0z"" rel=""noreferrer"">https://db.tt/1t9kAt0z</a></p>
",2015-11-14 18:39:55,2015-12-17 01:23:13,Align text for OCR,<python><image-processing><ocr>,,,CC BY-SA 3.0,False,False,False,True,False
37185,33700132,2015-11-13 19:10:25,,"<p>I would like o perform erosion on a 3D CT volume using python. I already tried the erosion functions prom SciPy and Scikit-image, but they don't seem to be working properly (neither the dilation). Here is what I tried:</p>

<pre><code>import numpy as np
from scipy import ndimage
from skimage import morphology

np_image_data = sitk.GetArrayFromImage(imageData) #Numpy array with CT data
boneMask = np_image_data&gt;=1000
struct = ndimage.generate_binary_structure(3, 1)

# Scipy erosion
erodedMask1 = ndimage.binary_erosion(boneMask.astype(uint), structure=struct, iterations=1)

# Scikit-image erosion
erodedMask2 = morphology.binary_erosion(boneMask.astype(uint), struct)
</code></pre>

<p>Both of them erode almost the whole image (there are only some voxels left). Are there any suggestions? I tried BinaryErodeImageFilter from SimpleITK, but it is slow and uses a lot of memory.</p>

<p>Thank you all in advance!</p>
",,2019-11-09 16:53:52,3D image erosion - Python,<python><image-processing><multidimensional-array><3d><image-morphology>,,,CC BY-SA 3.0,False,False,False,True,False
37565,33979742,2015-11-29 05:35:11,,"<p>After applying <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_skeleton.html"" rel=""nofollow noreferrer"">skeletonization</a> on an image(),<a href=""https://i.stack.imgur.com/YV7eX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YV7eX.png"" alt=""enter image description here""></a></p>

<p>I want to measure the longest branch, or spine of the skeleton using python. ImageJ has several tools that do this job one is <a href=""https://i.stack.imgur.com/YV7eX.png"" rel=""nofollow noreferrer"">Measure_Skeleton_length</a>, another is <a href=""http://fiji.sc/AnalyzeSkeleton"" rel=""nofollow noreferrer"">AnalyzeSkeleton</a>. Any tools or suggestions in python?</p>
",,2020-04-02 07:23:50,Ideas how to measure the length of a skeleton using python,<python><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
37703,34024503,2015-12-01 15:53:51,,"<p>So I have this map:
<a href=""https://i.stack.imgur.com/IODrs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IODrs.jpg"" alt=""original map""></a></p>

<p>I've already done some filtering and now I have the following image:
<a href=""https://i.stack.imgur.com/H7mMb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H7mMb.png"" alt=""filtered map""></a></p>

<p>Finally, I want to find the coordinates for each of the polygons in the image, but using the find_contours function is not giving me good results, as you can see on the following image.</p>

<p>On the left side, you can see all of the contours (polygons) found (one color for each), and on the right side, it's an example of a polygon it has found (which is clearly wrong).
<a href=""https://i.stack.imgur.com/0pwIq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0pwIq.png"" alt=""contours""></a></p>

<p>Having the black and white image, how can I find the coordinates for each of the polygons in it?</p>
",,2017-04-10 11:33:47,Approximate polygons from image (map),<python><numpy><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
38188,34356635,2015-12-18 13:18:08,,"<p>I have an grayscale image of a comic strip page that features several dialogue bubbles (=speech baloons, etc), that are enclosed areas with white background and solid black borders that contain text inside, i.e. something like that:</p>

<p><a href=""https://i.stack.imgur.com/gIEXY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gIEXY.png"" alt=""Sample comic strip image""></a></p>

<p>I want to detect these regions and create a mask (binary is ok) that will cover all the inside regions of dialogue bubbles, i.e. something like:</p>

<p><a href=""https://i.stack.imgur.com/bIbYp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bIbYp.png"" alt=""Sample resulting mask image""></a></p>

<p>The same image, mask overlaid, to be totally clear:</p>

<p><a href=""https://i.stack.imgur.com/Ru9ip.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ru9ip.png"" alt=""Sample image with transparent mask overlay""></a></p>

<p>So, my basic idea of the algorithm was something like:</p>

<ol>
<li>Detect where the text is  plant at least one pixel in every bubble. Dilate these regions somewhat and apply threshold to get a better starting ground; I've done this part:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/YFRh9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YFRh9.png"" alt=""Text positions outlined""></a></p>

<ol start=""2"">
<li><p>Use a flood fill or some sort of graph traversal, starting from every white pixel detected as a pixel-inside-bubble on step 1, but working on initial image, flooding white pixels (which are supposed to be inside the bubble) and stopping on dark pixels (which are supposed to be borders or text).</p></li>
<li><p>Use some sort of <a href=""http://scikit-image.org/docs/dev/api/skimage.morphology.html#binary-closing"" rel=""noreferrer"">binary_closing</a> operation to remove dark areas (i.e. regions that correspond to text) inside bubbles). This part works ok.</p></li>
</ol>

<p>So far, steps 1 and 3 work, but I'm struggling with step 2. I'm currently working with <a href=""http://scikit-image.org"" rel=""noreferrer"">scikit-image</a>, and I don't see any ready-made algorithms like flood fill implemented there. Obviously, I can use something trivial like breadth-first traversal, basically <a href=""http://arcgisandpython.blogspot.ru/2012/01/python-flood-fill-algorithm.html"" rel=""noreferrer"">as suggested here</a>, but it's really slow when done in Python. I suspect that intricate morphology stuff like <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.morphology.binary_erosion.html#scipy.ndimage.morphology.binary_erosion"" rel=""noreferrer"">binary_erosion</a> or <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.morphology.generate_binary_structure.html#scipy.ndimage.morphology.generate_binary_structure"" rel=""noreferrer"">generate_binary_structure</a> in ndimage or scikit-image, but I struggle to understand all that morphology terminology and basically how do I implement such a custom flood fill with it (i.e. starting with step 1 image, working on original image and producing output to separate output image).</p>

<p>I'm open to any suggestions, including ones in OpenCV, etc.</p>
",,2016-04-09 00:57:31,Detecting comic strip dialogue bubble regions in images,<python><numpy><scipy><computer-vision><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
38559,34574714,2016-01-03 08:34:54,,"<p>I am training a convolutional neural network, but have a relatively small dataset. So I am implementing techniques to augment it. Now this is the first time i am working on a core computer vision problem so am relatively new to it. For augmenting, i read many techniques and one of them that is mentioned a lot in the papers is random cropping. Now i'm trying to implement it ,i've searched a lot about this technique but couldn't find a proper explanation. So had a few queries:</p>

<p>How is random cropping actually helping in data augmentation? Is there any library (e.g OpenCV, PIL, scikit-image, scipy) in python implementing random cropping implicitly? If not, how should i implement it?</p>
",,2018-02-09 02:25:12,Random cropping data augmentation convolutional neural networks,<python><opencv><image-processing><deep-learning><conv-neural-network>,,,CC BY-SA 3.0,True,False,False,True,False
39237,34981144,2016-01-24 20:36:46,,"<p>I am trying to find a way to break the split the lines of text in a scanned document that has been adaptive thresholded. Right now, I am storing the pixel values of the document as unsigned ints from 0 to 255, and I am taking the average of the pixels in each line, and I split the lines into ranges based on whether the average of the pixels values is larger than 250, and then I take the median of each range of lines for which this holds. However, this methods sometimes fails, as there can be black splotches on the image.</p>

<p>Is there a more noise-resistant way to do this task?</p>

<p>EDIT: Here is some code. ""warped"" is the name of the original image, ""cuts"" is where I want to split the image.</p>

<pre><code>warped = threshold_adaptive(warped, 250, offset = 10)
warped = warped.astype(""uint8"") * 255

# get areas where we can split image on whitespace to make OCR more accurate
color_level = np.array([np.sum(line) / len(line) for line in warped])
cuts = []
i = 0
while(i &lt; len(color_level)):
    if color_level[i] &gt; 250:
        begin = i
        while(color_level[i] &gt; 250):
            i += 1
        cuts.append((i + begin)/2) # middle of the whitespace region
    else:
        i += 1
</code></pre>

<p>EDIT 2: Sample image added
<a href=""https://i.stack.imgur.com/sxRq9.jpg""><img src=""https://i.stack.imgur.com/sxRq9.jpg"" alt=""enter image description here""></a></p>
",2016-01-24 20:53:55,2020-08-26 20:24:34,Split text lines in scanned document,<python><opencv><ocr><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
39488,35192550,2016-02-04 04:31:44,,"<p>I am trying to implement the Wiener Filter to perform deconvolution on blurred image. My implementation is like this</p>

<pre><code>import numpy as np
from numpy.fft import fft2, ifft2

def wiener_filter(img, kernel, K = 10):
    dummy = np.copy(img)
    kernel = np.pad(kernel, [(0, dummy.shape[0] - kernel.shape[0]), (0, dummy.shape[1] - kernel.shape[1])], 'constant')
    # Fourier Transform
    dummy = fft2(dummy)
    kernel = fft2(kernel)
    kernel = np.conj(kernel) / (np.abs(kernel) ** 2 + K)
    dummy = dummy * kernel
    dummy = np.abs(ifft2(dummy))
    return np.uint8(dummy)
</code></pre>

<p>This implementation is based on the <a href=""https://en.wikipedia.org/wiki/Wiener_deconvolution"" rel=""nofollow noreferrer"">Wiki Page</a>.</p>

<p>The TIFF image used is from : <a href=""http://www.ece.rice.edu/~wakin/images/lena512color.tiff"" rel=""nofollow noreferrer"">http://www.ece.rice.edu/~wakin/images/lena512color.tiff</a><br>
But here is a PNG version:
<br>
<img src=""https://i.stack.imgur.com/yCF9i.png"" width=""400"" /></p>

<p>I have a input image motion blurred by a diagonal kernel and some gaussian additive noise is added to it. The lena picture is 512x512 and the blurring kernel is 11x11.</p>

<p>When I apply my wiener_filter to this image the result is like this.
<a href=""https://i.stack.imgur.com/wp7UA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wp7UA.png"" alt=""enter image description here""></a>.</p>

<p>I think this deblurred image is not of good quality. So I would like to ask if my implementation is correct.</p>

<p><strong>Update</strong> the way I add noise.</p>

<pre><code>from scipy.signal import gaussian, convolve2d

def blur(img, mode = 'box', block_size = 3):
    # mode = 'box' or 'gaussian' or 'motion'
    dummy = np.copy(img)
    if mode == 'box':
        h = np.ones((block_size, block_size)) / block_size ** 2
    elif mode == 'gaussian':
        h = gaussian(block_size, block_size / 3).reshape(block_size, 1)
        h = np.dot(h, h.transpose())
        h /= np.sum(h)
    elif mode == 'motion':
        h = np.eye(block_size) / block_size
    dummy = convolve2d(dummy, h, mode = 'valid')
    return np.uint8(dummy), h

def gaussian_add(img, sigma = 5):
    dummy = np.copy(img).astype(float)
    gauss = np.random.normal(0, sigma, np.shape(img))
    # Additive Noise
    dummy = np.round(gauss + dummy)
    # Saturate lower bound
    dummy[np.where(dummy &lt; 0)] = 0
    # Saturate upper bound
    dummy[np.where(dummy &gt; 255)] = 255
    return np.uint8(dummy)
</code></pre>
",2020-01-21 15:11:24,2020-01-21 15:11:24,Wiener Filter for image deblur,<python><image-processing><convolution><scikit-image><motion-blur>,,,CC BY-SA 4.0,False,False,False,True,False
39543,35082033,2016-01-29 10:21:32,,"<p>I am studying scikit image and I have this <a href=""http://i.stack.imgur.com/O3DHy.png"" rel=""nofollow"">image</a> named ""unknown.png""</p>

<p>I used ""from skimage.color import rgb2hed"" to transform the colors: </p>

<pre><code>im = io.imread(""C:\Users\Gerard\Desktop\Demo\folder\unknown.png"")
im_hed = rgb2hed(im)
plt.imshow(im_hed)
plt.show()
</code></pre>

<p>it gives me a good result showing the <a href=""http://i.stack.imgur.com/XUcaY.jpg"" rel=""nofollow"">color transformation</a>. However, when I choose another image, im = io.imread(""C:\Users\Gerard\Desktop\Demo\folder\sample_1.png"") from the same folder it gives me the following error:</p>

<pre><code>ihc_hed = rgb2hed(im)
File ""C:\Python27\lib\site-packages\skimage\color\colorconv.py"", line 1163, in rgb2hed
return separate_stains(rgb, hed_from_rgb)
File ""C:\Python27\lib\site-packages\skimage\color\colorconv.py"", line 1255, in separate_stains
stains = np.dot(np.reshape(-np.log(rgb), (-1, 3)), conv_matrix)
File ""C:\Python27\lib\site-packages\numpy\core\fromnumeric.py"", line 225, in reshape
return reshape(newshape, order=order)
ValueError: total size of new array must be unchanged
</code></pre>

<p>Any advice on this? Any form of standardization so that every time I choose a new image the same error wont pop out again and again? Or is the error dependent of the size of the image? (which makes me believe that there is something wrong with scikit-image?</p>

<p>Thanks so much for the help!</p>
",,2016-01-29 10:21:32,ValueError: total size of new array must be unchanged when image is changed,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
39705,35247211,2016-02-06 22:22:15,,"<p>I am getting a Zero Division Error with some images (Even though a lot of them work just fine) :  </p>

<p>Here's the code :</p>

<pre><code>image = skimage.io.imread('test.png', False)
image_gray = skimage.io.imread('test.png', True)
blurred = cv2.GaussianBlur(img_as_ubyte(image_gray), (5, 5), 0)
thresh = threshold_li(blurred)
binary = blurred &gt; thresh
binary_cv2 = img_as_ubyte(binary)

# find contours in the thresholded image
cnts = cv2.findContours(binary_cv2.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if imutils.is_cv2() else cnts[1]

# loop over the contours
for c in cnts:
    # compute the center of the contour
    M = cv2.moments(c)
    cX = int(M[""m10""] / M[""m00""])
    cY = int(M[""m01""] / M[""m00""])

    # draw the contour and center of the shape on the image
    cv2.drawContours(img_as_ubyte(image), [c], -1, (0, 255, 0), 2)
    cv2.circle(img_as_ubyte(image), (cX, cY), 7, (255, 255, 255), -1)
    cv2.putText(img_as_ubyte(image), ""center"", (cX - 20, cY - 20),
    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

viewer = ImageViewer(image)
viewer.show()
</code></pre>

<hr>

<pre><code>Traceback (most recent call last):
  File ""Center.py"", line 26, in &lt;module&gt;
    cX = int(M[""m10""] / M[""m00""])
ZeroDivisionError: float division by zero
</code></pre>

<p>Thanks in advance!</p>
",,2018-11-19 21:58:25,ZeroDivisionError (Python),<python><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
39793,35355711,2016-02-12 05:52:50,,"<p>I have a group of RGBA images saved in a folder, my goal is to convert these images into another folder in a pgm format, below is the code:</p>

<pre><code>path1 = file/path/where/image/are/stored
path2 = file/path/where/pgm/images/will/be/saved

list = os.listdir(path1)
for file in listing:
    #Transforms an RGBA with channel into an RGB only
    image_rgb = Image.open(file).convert('RGB')

    #Color separation stains to detect microscopic cells
    ihc_hed = rgb2hed(image_rgb)

    #Trasnforms the image into a numpy array of the UINT8 Type
    cv_img = ihc_hed.astype(np.uint8)

    # create color boundaries boundaries detecting black and blue stains
    lower = np.array([0,0,0], dtype = ""uint8"")
    upper = np.array([0,0,255], dtype = ""uint8"")
    #calculates the pixel within the specified boundaries and create a mask
    mask = cv2.inRange(cv_img, lower, upper)
    img = Image.fromarray(mask,'L')
    img.save(path2+file,'pgm')
</code></pre>

<p>however I get an error stating KeyError: 'PGM', it seems that the 'pgm' format is not in the modes</p>

<p>Thanks for the advice :)</p>
",,2016-02-12 08:30:32,Converting RGBA Images in a folder and save it to another folder in '.pgm' format,<python><image-processing><scikit-image><pgm>,,,CC BY-SA 3.0,False,False,False,True,False
39936,35518793,2016-02-20 03:31:49,,"<p>I am following <a href=""http://scikit-image.org/docs/0.11.x/auto_examples/plot_daisy.html"" rel=""nofollow"">http://scikit-image.org/docs/0.11.x/auto_examples/plot_daisy.html</a>, but it's not very clear what does desc[0],desc[1] and desc[2] mean. I am trying to compare the descriptor vector of two images using Brutte Force or Flann matcher. But the size of the descriptor vectors of two images are not same.
Can anyone please suggest me the way to solve this?  </p>
",,2016-02-20 08:51:38,Comparing Daisy descriptors of two images,<python><image-processing><feature-detection><feature-extraction>,,,CC BY-SA 3.0,False,False,False,True,False
40258,35541915,2016-02-21 20:54:52,,"<p>I'm new to image segmentation, but I need to do it to get a database for the  machine learning classifier. </p>

<p>Essentially I have a video similar to this image:</p>

<p><a href=""https://i.stack.imgur.com/lIgtO.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lIgtO.jpg"" alt=""Cow with a rectangle""></a></p>

<p>My job is to identify cows in the foreground, or at least any cow at all. I realize there is an occlusion problem, but for a starter I'd like to correctly segment a lonely cow, like the one with the red rectangle around it (hand-drawn).</p>

<p>In less challenging problems, such as this, I discriminate by adding a threshold for every pixel, that either becomes (0,0,0) for the object or (255,255,255) for the background:</p>

<p><a href=""https://i.stack.imgur.com/ZuNzW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZuNzW.png"" alt=""Megasteak""></a>  </p>

<p>Then I label the pixels with the same values to get classes and obtain the rectangle for large enough 'blobs'.</p>

<p>For the image above this approach will not work as the objects and the background are similar + there are a lot of shadows, side lighting etc, so I'm not sure how to approach it. Any suggestions are welcome.     </p>
",,2017-10-25 06:45:05,Challenging image segmentation: background and objects are similar,<python><image><image-processing><image-segmentation><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
40502,35622420,2016-02-25 08:52:50,,"<p>I'm trying to make opencv VideoWriter to generate avi from images modified by skimage-kit. And I encountered color difference between skimage.io.imsave and VideoWriter.write"".</p>

<p>When I see the movie generated by opencv,it looks having only blue. However, if I save the frame by skimage.io.imsave, the jpg file has correct colors.
I assume normalization happens in skimage causes something bad to me but I'm not sure how I can convert them to collect color for opencv.</p>

<p>Can anyone advice why the difference comes from and how I should solve it?
I really appreciate any advice.</p>

<p><a href=""http://i.stack.imgur.com/JZxku.jpg"" rel=""nofollow"">result of imsave,which I want</a>
<a href=""http://i.stack.imgur.com/wafU0.png"" rel=""nofollow"">result of output.avi</a></p>

<pre><code>import numpy
import cv2
from colorizer import Colorizer  #returns modified 224*224 color image using skimage kit
import skimage

c=Colorizer()
cap = cv2.VideoCapture('test.mp4')
fourcc = cv2.cv.CV_FOURCC(*'MJPG')
out = cv2.VideoWriter('output.avi',fourcc, 12.0, (224,224))
i=0
while(cap.isOpened()):
    ret, frame = cap.read()
    cframe=c.colorize(frame)
    #it outputs correct color image
    skimage.io.imsave(""result/%d.jpg"" % i, cframe)

    #it outputs wrong color
    out.write(skimage.img_as_ubyte(cframe))
    i=i+1
cap.release()
out.release()
cv2.destroyAllWindows()
</code></pre>
",,2016-02-26 04:16:43,Why different color between skimage.io.imsave and opencv.VideoWriter,<python><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
40858,35938740,2016-03-11 11:12:08,,"<p>I've been using <a href=""http://scikit-image.org/"" rel=""nofollow noreferrer"">scikit-image</a> to classify road features with some success. See below: <a href=""https://i.stack.imgur.com/zRJNh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRJNh.jpg"" alt=""image processed by scikit-image""></a>. I am having trouble doing the next step which is to classify the features. For example, let's say these features are located in the box (600, 800) and (1400, 600).</p>

<p>The code I'm using to extract the information is:</p>

<pre><code>from skimage import io, segmentation as seg
color_image = io.imread(img)  
plt.rcParams['image.cmap'] = 'spectral'
labels = seg.slic(color_image, n_segments=6, compactness=4)
</code></pre>

<p>The objective is to have a table in the following form:</p>

<pre><code>Image, feature_type, starting_pixel, ending_pixel
001    a             (600, 600),     (1300, 700) 
002    b             (600, 600),     (1100, 700)
002    undefined     (700, 700),     (900, 800)
</code></pre>

<p><code>feature_type</code> would be based on colours, ideally shoulders would be one colour, trees and brush would be another, etc.</p>

<p>How can I extract the data I need? (i.e: have scikit break the image into different components where I know the location of each component. I can then pass each component to a classifier which will identify what each component is) Thanks!</p>
",2016-03-19 20:30:26,2016-03-19 20:30:26,Extracting attributes from images using Scikit-image,<python><machine-learning><computer-vision><scikit-learn><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
40918,36101352,2016-03-19 11:51:41,,"<p>Original Image:</p>

<p><a href=""https://i.stack.imgur.com/xCXal.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xCXal.png"" alt=""Original Image""></a></p>

<p>Code:</p>

<pre><code>image = skimage.img_as_float(original_image) # datattype of original image is uint8
blurred_image = filters.gaussian_filter(image, 3)    
filter_blurred_image = filters.gaussian_filter(blurred_image,1)
alpha = 30
sharpened = blurred_image + alpha * (blurred_image - filter_blurred_image)
</code></pre>

<p>original_image array: <a href=""https://drive.google.com/file/d/0B2EliTzXsTI3VFJHVVgxTHl6UTg/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B2EliTzXsTI3VFJHVVgxTHl6UTg/view?usp=sharing</a></p>

<p>The above code is giving me a different result than what I expected:
<a href=""https://i.stack.imgur.com/3TzGK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3TzGK.png"" alt=""enter image description here""></a></p>

<p>I want to sharpen it and then ""thin"" the lines so that I can detect its end points.</p>
",,2016-03-19 14:33:26,How to sharpen and then thin an image of a handwritten digit in Python?,<python><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
41071,35958510,2016-03-12 13:55:34,,"<p>I have a code which recognizes faces on images (dlib implementation, 68 points)</p>

<p>I want to rotate a bit some images but after a get following trouble: my image becomes somehow spoiled</p>

<pre><code>from skimage import io
from skimage import transform as tf
img = io.imread(f)
tform = tf.SimilarityTransform(rotation=np.deg2rad(10),translation=(10,12))
img = tf.warp(img,tform)
</code></pre>

<p>I plot image with 2 ways:</p>

<pre><code>plt.imshow(img) #the right picture (matplotlib)
win = dlib.image_window() #the left picture (dlib)
win.set_image(img) #the left picture
</code></pre>

<p>As you can see dlib image is broken. Also algorithm which can find facial keypoints stopps working.</p>

<p>Without SimilarityTransform dlib works correctly.</p>

<p>Help me please! I want to rotate an image and to pass it to dlib</p>

<p><a href=""https://i.stack.imgur.com/mKhBw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKhBw.jpg"" alt=""the prolem""></a></p>
",,2016-03-12 14:16:08,Skimage SimilarityTransform spoils image,<python><image><image-processing><image-recognition><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
41139,36256661,2016-03-28 05:43:13,,"<p>Let me start with where I am: </p>

<p><a href=""https://i.stack.imgur.com/rX635.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rX635.png"" alt=""enter image description here""></a></p>

<p>I created the above image with the following code: </p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

color_palette_name = 'gist_heat'
cmap = plt.cm.get_cmap(color_palette_name)
bgcolor = cmap(np.random.rand())

f = plt.figure(figsize=(12, 12), facecolor=bgcolor,)
ax = f.add_subplot(111)
ax.axis('off')

t = np.linspace(0, 2 * np.pi, 1000)

x = np.cos(t) + np.cos(6. * t) / 2.0 + np.sin(14. * t) / 3.0
y = np.sin(t) + np.sin(6. * t) / 2.0 + np.cos(14. * t) / 3.0

ax.plot(x, y, color=cmap(np.random.rand()))
ax.fill(x, y, color=cmap(np.random.rand()))
plt.tight_layout()
plt.savefig(""../demo/tricky.png"", facecolor=bgcolor, edgecolor=cmap(np.random.rand()), dpi=350)
</code></pre>

<p>Is there a way to fill the loops (or the triangle-like regions) that are created when the line crosses itself with some other color? It doesn't have to be matplotlib, it could be scikit-image or some other library.</p>

<p>I'm thinking some pseudo-code like: </p>

<pre><code>for region in regions:
    ax.fill(region, color=cmap(np.random.rand()))
</code></pre>

<p>But I have no idea how to get the <code>regions</code>, or how filling it would work. </p>
",2016-03-28 18:01:41,2016-03-28 18:01:41,How can I fill arbitrary closed regions in Matplotlib?,<python><opencv><matplotlib><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
41227,36166486,2016-03-22 22:31:24,,"<p>[cross-posted from stats and datascience]</p>

<p>I'm working on an animal classification problem, with the data extracted from a video feed. The recording was made in a pen, so the problem is quite challenging with a dark background and many shadows: <a href=""https://i.stack.imgur.com/2B5BE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2B5BE.png"" alt=""enter image description here""></a></p>

<p>Initially I tried scikit-image, but then someone helped me with an advanced tool called crf-rnn (<a href=""http://crfasrnn.torr.vision/"" rel=""nofollow noreferrer"">http://crfasrnn.torr.vision/</a>) that does a great job segmenting and labelling objects in an image. I did the following: </p>

<pre><code>import caffe
net = caffe.Segmenter(MODEL_FILE, PRETRAINED)
IMAGE_FILE = '0045_crop2.png'
input_image = caffe.io.load_image(IMAGE_FILE)
from PIL import Image as PILImage
image = PILImage.fromarray(np.uint8(input_image))
image = np.array(image)
mean_vec = [np.mean(image[:,:,vals]) for vals in range(image.shape[2])]
im = image[:, :, ::-1]
im = im - reshaped_mean_vec
cur_h, cur_w, cur_c = im.shape
pad_h = 750 - cur_h
pad_w = 750 - cur_w
print(pad_h, pad_w, ""999"")
im = np.pad(im, pad_width=((0, max(pad_h,0)), (0, max(pad_w,0)), (0, 0)), mode = 'constant', constant_values = 255)
segmentation = net.predict([im])
segmentation2 = segmentation[0:cur_h, 0:cur_w]
</code></pre>

<p>The resulting image segmentation is rather poor (although two cows are recognized correctly):
<a href=""https://i.stack.imgur.com/eBEr4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eBEr4.png"" alt=""enter image description here""></a></p>

<p>I use a trained crf-rnn (MODEL_FILE, PRETRAINED), which works well for other problems, but this one is harder. I would appreciate any suggestions on how to pre-process this sort of image to extract the shape of most cows. </p>
",,2016-03-22 22:31:24,Image segmentation with a challenging background,<image><neural-network><image-segmentation><caffe><crf>,,,CC BY-SA 3.0,False,False,False,True,False
41258,36263738,2016-03-28 13:50:02,,"<p>I've got an image that I apply a Gaussian Blur to using both <code>cv2.GaussianBlur</code> and <code>skimage.gaussian_filter</code> libraries, but I get significantly different results.  I'm curious as to why, and what can be done to make <code>skimage</code> look more like <code>cv2</code>.  I know <code>skimage.gaussian_filter</code> is a wrapper around <code>scipy.scipy.ndimage.filters.gaussian_filter</code>.  To clearly state the question, why are the two functions different and what can be done to make them more similar?</p>
<p>Here is my test image:</p>
<p><a href=""https://i.stack.imgur.com/bg4dZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bg4dZ.png"" alt=""Original Image"" /></a></p>
<p>Here is the <code>cv2</code> version (appears blurrier):</p>
<p><a href=""https://i.stack.imgur.com/zmVUu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zmVUu.png"" alt=""cv2 image"" /></a></p>
<p>Here is the <code>skimage</code>/<code>scipy</code> version (appears sharper):</p>
<p><a href=""https://i.stack.imgur.com/Tlmbr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tlmbr.png"" alt=""skimage version"" /></a></p>
<p>Details:</p>
<p><code>skimage_response = skimage.filters.gaussian_filter(im, 2, multichannel=True, mode='reflect')</code></p>
<p><code>cv2_response = cv2.GaussianBlur(im, (33, 33), 2)</code></p>
<p>So sigma=2 and the size of the filter is big enough that it shouldn't make a difference.  Imagemagick <code>covnert -gaussian-blur 0x2</code> visually agrees with <code>cv2</code>.</p>
<p>Versions: <code>cv2</code>=2.4.10, <code>skimage</code>=0.11.3, <code>scipy</code>=0.13.3</p>
",2020-06-20 09:12:55,2019-06-20 00:54:28,Why is Gaussian Filter different between cv2 and skimage?,<python><image><opencv><scikit-image><gaussianblur>,,,CC BY-SA 3.0,True,False,False,True,False
41307,36248527,2016-03-27 14:34:33,,"<p>I am trying to convert an RGB image to grayscale using <strong>skimage</strong> in Python. Here's what I do:</p>

<pre><code>for im_path in glob.glob(os.path.join(pos_raw, ""*"")):
    im = imread(im_path)
    im = color.rgb2gray(im)
    image_name = os.path.split(im_path)[1].split(""."")[0] + "".pgm""
    image_path = os.path.join(pos_img_path, image_name)
    imwrite(image_path, im)
</code></pre>

<p>for a bunch of image files.
My input image looks like this:</p>

<p><a href=""https://i.stack.imgur.com/r9iXG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r9iXG.jpg"" alt=""Color image""></a></p>

<p>And the output image looks like this:</p>

<p><a href=""https://i.stack.imgur.com/2JxZB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2JxZB.jpg"" alt=""Black image""></a></p>

<p>The expected output is this:</p>

<p><a href=""https://i.stack.imgur.com/PVfm8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PVfm8.jpg"" alt=""Gray image""></a></p>

<p>What can be the issue here?</p>
",2016-03-27 14:45:16,2016-03-27 14:45:16,Converting an RGB image to grayscale in Python,<python><image><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
41421,36354577,2016-04-01 10:57:49,,"<h2>Introduction</h2>
<p>I have an image stack (<code>ImgStack</code>) made of 42 planes each of 2048x2048 px and a function that I use for the analysis:</p>
<pre><code>def All(ImgStack):
    some filtering
    more filtering
</code></pre>
<p>I determined that the most efficient way to process the array with dask (on my computer) is to make <code>chunks=(21,256,256)</code>.</p>
<p>When I run <code>map_blocks</code>:</p>
<pre><code>now=time.time()
z=da.from_array(ImgStack,chunks=(21,256,256))
g=da.ghost.ghost(z, depth={0:10, 1:50,2:50},boundary={0: 'periodic',1:'periodic',2:'periodic'})
g2=g.map_blocks(All)
result = da.ghost.trim_internal(g2, {0: 10, 1: 50,2:50})
print('Time=',str(time.time()-now))
</code></pre>
<p><strong>Time= 1.7090258598327637</strong></p>
<p>Instead when I run <code>map_overlap</code></p>
<pre><code>now=time.time()
z=da.from_array(ImgStack,chunks=(21,256,256))
y=z.map_overlap(All,depth={0:10, 1:50,2:50},boundary={0: 'periodic', 1: 'periodic',2:'periodic'})
y.compute()
print('Time=',str(time.time()-now))
</code></pre>
<p><strong>Time= 228.19104409217834</strong></p>
<p>I guess the big time difference is due to the conversion from dask.array to np.array in map_overlap because if I add the conversion step to the map_block script the execution time became comparable.</p>
<pre><code>now=time.time()
z=da.from_array(ImgStack,chunks=(21,256,256))
g=da.ghost.ghost(z, depth={0:10, 1:50,2:50},boundary={0: 'periodic', 1: 'periodic',2:'periodic'})
g2=g.map_blocks(All)
result = da.ghost.trim_internal(g2, {0: 10, 1: 50,2:50})
I=np.array(result)
print('Time=',str(time.time()-now))
</code></pre>
<p><strong>Time= 209.68917989730835</strong></p>
<h2>Issue</h2>
<p>So the best way will be to keep the dask.array but the problem shows up when I am saving the data on h5 file:</p>
<pre><code>now=time.time()
result.to_hdf5('/Users/simone/Downloads/test.h5','/Dask2',compression='lzf')
print('Time=',str(time.time()-now))
</code></pre>
<p><strong>Time= 243.1597340106964</strong></p>
<p>but if I save the corresponding np.array</p>
<pre><code>test=h5.File('/Users/simone/Downloads/test.h5','r+')
DT=test.require_group('NP')
DT.create_dataset('t', data=I,dtype=I.dtype,compression=&quot;lzf&quot;)
now=time.time()
print('Time=',str(time.time()-now))
</code></pre>
<p><strong>Time= Time= 4.887580871582031e-05</strong></p>
<h2>Question</h2>
<p>So I would like to be able to run the filtering and saving the arrays in the lowest amount of time possible. Is there a way to speed up the conversion from dask.array--&gt; np.array() or to speed up the da.to_hdf5?</p>
<p>Thanks! Any comment will be appreciated.</p>
",2020-06-20 09:12:55,2016-04-01 15:28:28,Difference in processing time between map_block and map_overlap is it due to dask.array to np.array conversion?,<python><image-processing><h5py><dask><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
41507,36556921,2016-04-11 18:53:25,,"<p>Is it possible to detect the upper side of a dice? While this will be an easy task if you look from the top, from many perspectives multiple sides are visible.</p>

<p>Here is an example of a dice, feel free to take your own pictures:
<a href=""https://i.stack.imgur.com/JxGjS.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JxGjS.jpg"" alt=""enter image description here""></a></p>

<p>You usually want to know the score you have achieved. It is easy for me to extract ALL dots, but how to only extract those on the top? In this special case, the top side is the largest, but this might not always be true. I am looking for someting which evaluates the distortion of the top square (or circle in this case, which I can extract) in relation to the perspective given by the grid in the bottom.</p>

<p>Example program with some results is given below.</p>

<pre><code>import numpy as np
import cv2

img = cv2.imread('dice.jpg')


# Colour range to be extracted
lower_blue = np.array([0,0,0])
upper_blue = np.array([24,24,24])

# Threshold the BGR image 
dots = cv2.inRange(img, lower_blue, upper_blue)

# Colour range to be extracted
lower_blue = np.array([0,0,0])
upper_blue = np.array([226,122,154])

# Threshold the BGR image 
upper_side_shape = cv2.inRange(img, lower_blue, upper_blue)

cv2.imshow('Upper side shape',upper_side_shape)
cv2.imshow('Dots',dots)

cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>

<p>Some resulting images:
<a href=""https://i.stack.imgur.com/ZQoab.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZQoab.png"" alt=""enter image description here""></a></p>
",2016-04-11 18:59:03,2016-04-11 20:15:28,Detecting the upper side of a dice,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
41671,36294025,2016-03-29 19:59:54,,"<p>Is there an implementation using OpenCV or scikit-image that is equivalent to Matlab's grayscale image <strong>imfill</strong> funciton (i.e. grayscale hole filling)? </p>

<p>See the imfill section for grayscale (<strong>I2= imfill(I)</strong>) in the following example link <a href=""http://www.mathworks.com/help/images/ref/imfill.html"" rel=""noreferrer"">matlab_imfill</a>. Or see image: <a href=""https://i.imgur.com/z5FnNk7.png"" rel=""noreferrer"">matlab_tire_ex</a></p>

<p>Here's a link to the tire image in the example </p>

<p><img src=""https://i.imgur.com/HFxTJS3.png"" alt=""tire""></p>

<p>I've been trying to replicate the Matlab output using <strong>scipy.ndimage.grey_closing</strong> function with varying the size parameter, but have not been successful.</p>

<p>I'm using Python 3.5.</p>
",2016-03-29 20:06:41,2017-12-24 04:55:01,Python equivalent to Matlab funciton 'imfill' for grayscale?,<python><matlab><opencv><image-processing>,,,CC BY-SA 3.0,True,True,False,True,False
41759,36635124,2016-04-14 22:28:58,,"<p>I have an <a href=""https://postimg.org/image/tnutzqvml/"" rel=""nofollow noreferrer"">Image</a> obtained from a camera (inner part of my VW T5 Van). I would like to know how accurately I can reproduce the shapes. I corrected for lens distortion using lensfun and gimp. I would like to quantify the remaining distortions by utilizing the rectangular features. I can detect the rectangles using <code>match_template</code> but I do not know how to detect the rectangles.</p>

<p>In the end I would like to label them and measure the width and length. In order to visualize the centers better I tried to locate the centers but this is not as straightforward as I thought because a simple threshold counts the same spots several times. How can I find the centers of all detected rectangles ?</p>

<p>Can this be done with skimage ? Or should I use opencv ? </p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import ndimage as ndi
from scipy.ndimage.filters import maximum_filter
from scipy.ndimage import center_of_mass
from skimage.color import rgb2grey
from skimage.feature import match_template
from skimage import feature, io
from skimage import img_as_float
%matplotlib inline

image = io.imread('../lensfun_cut.JPG')
imgray = rgb2grey(image)

    template = imgray[1365:1390,445:470]
    result = match_template(imgray, template)

    # 
    hit = np.where(result&gt;0.90)
    fig, ax = plt.subplots(ncols=1,figsize=(8,8))
    ax.imshow(result,cmap=plt.cm.spectral)

    # loop over the detected regions and draw a circe around them
    for i in range(len(hit[0])):
        rect = plt.Circle((hit[1][i],hit[0][i]), 20,linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZKmZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZKmZD.png"" alt=""enter image description here""></a></p>
",2016-04-16 07:06:22,2016-04-16 07:06:22,"find rectangles in image, preferably with skimage",<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
42305,36703964,2016-04-18 20:44:36,,"<p>I have a jigsaw puzzle and want to automatically distinguish between ""normal"", ""edge"", and ""corner"" pieces in that puzzle (I hope that the definition of those words is obvious for anyone who has ever done jigsaw puzzle)</p>

<p>To make things easier, I started with a selection of 9 parts, 4 of them normal, 4 are edges and one being a corner. The original image looks like this:
<a href=""https://i.stack.imgur.com/9ZLU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9ZLU7.png"" alt=""enter image description here""></a></p>

<p>My first idea now was to detect the 4 ""major corners"" of each single piece, and then to proceed as follows:</p>

<ul>
<li>It's an edge if the contour between two adjacent ""major corners"" is a straight line</li>
<li>It's a corner if the two contours between three adjacent ""major corners"" are straight lines</li>
<li>It's a normal part if there are no straight lines between two adjacent ""major corners"".</li>
</ul>

<p>However, I have problems extracting the four ""major corners"" for each piece (I was trying to use Harris corners for this)</p>

<p>My code, including some preprocessing, is attached below, together with some resulting, including the Harris corners I get. Any input appreciated.</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread('image.png')
gray= cv2.imread('image.png',0)

# Threshold to detect rectangles independent from background illumination
ret2,th3 = cv2.threshold(gray,220,255,cv2.THRESH_BINARY_INV)

# Detect contours
_, contours, hierarchy = cv2.findContours( th3.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)

# Draw contours
h, w = th3.shape[:2]
vis = np.zeros((h, w, 3), np.uint8)
cv2.drawContours( vis, contours, -1, (128,255,255), -1)

# Print Features of each contour and select some contours
contours2=[]
for i, cnt in enumerate(contours):
    cnt=contours[i]
    M = cv2.moments(cnt)

    if M['m00'] != 0:
        # for definition of features cf http://docs.opencv.org/3.1.0/d1/d32/tutorial_py_contour_properties.html#gsc.tab=0
        cx = int(M['m10']/M['m00'])
        cy = int(M['m01']/M['m00'])
        area = cv2.contourArea(cnt)
        x,y,w,h = cv2.boundingRect(cnt)
        aspect_ratio = float(w)/h
        rect_area = w*h
        extent = float(area)/rect_area        

        print i, cx, cy, area, aspect_ratio, rect_area, extent

        if area &lt; 80 and area &gt; 10:
            contours2.append(cnt)

# Detect Harris corners
dst = cv2.cornerHarris(th3,2,3,0.04)

#result is dilated for marking the corners, not important
dst = cv2.dilate(dst,None, iterations=5)

# Threshold for an optimal value, it may vary depending on the image.
harris=img.copy()
print harris.shape
harris[dst&gt;0.4*dst.max()]=[255,0,0]

titles = ['Original Image', 'Thresholding', 'Contours', ""Harris corners""]
images = [img, th3, vis, harris]
for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/mgQXJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mgQXJ.png"" alt=""enter image description here""></a></p>
",,2016-04-18 20:44:36,"Want to detect edge and corner parts in a jigsaw puzzle, but can't find the 4 corners of each piece",<python><opencv><image-processing><scikit-image><ndimage>,,,CC BY-SA 3.0,True,False,False,True,False
42512,36979367,2016-05-02 09:41:53,,"<p>I am doing watershed segmentation via scikit and now I need to determine the main (average) color of every segment and its position. The aim is to get a graph with the colors as vertices and weighted edges that illustrate the distance between segments. Can I do it with some scikit or opencv functions?  </p>
",,2016-05-02 09:41:53,Turning an image into a graph,<python><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
42892,37038260,2016-05-04 21:22:06,,"<p>I need to fill holes in images using python.
This is the image with objects that I managed to get - they are really edges of objects I want, so I need to fill them.
<a href=""https://i.stack.imgur.com/IpauJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IpauJ.png"" alt=""enter image description here""></a></p>

<p>It seemed very straightforward using <code>ndimage.binary_fill_holes(A)</code>, but the problem is that it produces this (manually filled with red colour):</p>

<p><a href=""https://i.stack.imgur.com/koNd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/koNd2.png"" alt=""enter image description here""></a></p>

<p>But I need this:</p>

<p><a href=""https://i.stack.imgur.com/kO9x0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kO9x0.png"" alt=""enter image description here""></a></p>

<p>Any way this can be solved?</p>

<p>This is the first image without the axes if you want to give it a try: <a href=""https://i.stack.imgur.com/9JiQB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JiQB.png"" alt=""enter image description here""></a></p>
",2016-05-06 06:58:34,2016-05-06 18:54:45,Tricky filling holes in an image,<python><opencv><numpy><scipy><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
42909,37234413,2016-05-15 04:00:09,,"<p>I'd like to create two images from a source RAW image, a Canon CR2 in this case. I've got the RAW conversion sorted and some of the processing. My final images need to be a PNG with an alpha mask and a 95% quality JPG with the alpha area instead filled with black. I've got a test image set here showing how far I've got with detecting the subject:</p>

<p><a href=""http://imgur.com/a/Q8k3w/all"" rel=""nofollow noreferrer"">http://imgur.com/a/Q8k3w/all</a></p>

<p>So basically, as you can see I want to isolate the subjects from the grey background. I also want to mask out any shadows cast on the grey background as much as possible and ideally in entirety. I'm using a Python2 script I've written and so far mostly scikit-image. I would swap to another Python compatible image processing lib if required. Also, I need to do all steps in memory so that I only save out once at the end of all image processing with the PNG and then the JPG. So no subprocess.Popen etc..</p>

<p>You'll see from the sample images that, I think at least, that I've got some way to a solution already. I've used scikit-image and its Canny edge algorithm for the images you see in my examples.</p>

<p>What I need to do now is figure out how to fill the subject in the Canny images with white so that I can get a proper solid white mask. In most of my example images, with Canny filter applied, it appears that there is good edge detection for the subjects themselves, usually with a major unbroken border. But, I'm guessing I might get some images in the future where this does not happen and there may be small breaks in the major border. I need to handle this occurrence if it looks like it'll be an issue for later processing steps.</p>

<p>Also, I'm wondering if I need to increase the overall border by one pixel and set it to the same color as my 0,0 pixel (i.e. first pixel top/left in background) and then run my Canny filter and then shrink my border by 1px again? This should allow for the bottom edge to be detected and for when subjects break the top or the sides of the frame? </p>

<p>So really I'm just looking for advice and wondering where to go next to get a nice solid mask. It needs to stay binary as a binary mask, (i.e. everything outside of the main subject needs to be totally masked to 0). This means that  I'll need to run something that looks for isolated islands of pixels below a certain pixel volume at some point - probably last step and add them to the mask (e.g. 50px or so).</p>

<p>Also, overall, the rule of thumb would be that it's better if a little bit of the subject gets masked rather than less of the background being masked (i.e. I want all or as much as possible of the background/shadow areas to be masked.)</p>

<p>I've tried a few things, but not quite getting there. I'm thinking something along the lines of find_contours in sci_kit might help. But I can't quite see from the scikit-image examples how I'd pick and then turn my detected contour into a mask. I've killed quite a bit of time experimenting without success today and so I thought I'd ask here and see if anyone has any better ideas. </p>

<p>This is an OpenCV based method that looks promising:</p>

<p><a href=""http://funcvis.org/blog/?p=44"" rel=""nofollow noreferrer"">http://funcvis.org/blog/?p=44</a></p>

<p>I'd like to stick with scikit-image or some other interchangeable numpty image library for python if possible. However, if it's just easier and faster with OpenCV or another library then I'm open to ideas so long as I can stick with Python. </p>

<p>Also worth bearing in mind that for my application I will always have an image of the background without a subject. So maybe I should be pursuing this route. Problem is that I don't think a simple difference approach deals very well with shadows. It seems to me like some sort of edge detection is required at some point for a superior masking approach.</p>

<p><img src=""https://i.imgur.com/AbHQN7s.jpg"" alt=""1""> ""Source 1""</p>

<p><img src=""https://i.imgur.com/LE5Y7yj.jpg"" alt=""2""> ""Source 2""</p>

<p><img src=""https://i.imgur.com/SwYXazt.jpg"" alt=""3""> ""Source 3""</p>

<p><img src=""https://i.imgur.com/FyCSMEu.jpg"" alt=""1""> ""Result 1""</p>

<p><img src=""https://i.imgur.com/LH9UkKt.jpg"" alt=""2""> ""Result 2""</p>

<p><img src=""https://i.imgur.com/CrMrDwV.jpg"" alt=""3""> ""Result 3""</p>
",,2016-05-15 06:50:41,Background removal/masking in Python using edge detection and scikit-image,<python><opencv><imagemagick><scikit-image><mahotas>,,,CC BY-SA 3.0,True,False,False,True,False
42917,37079738,2016-05-06 19:07:25,,"<p>For tasks like clustering or classification on images we generally convert images to numerical feature vectors. Now, instead of calculating feature vectors for an entire image, I would like to generate features for segments of an image (not constrained to rectangular segments). For example, using the SLIC algorithm (<a href=""http://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic"" rel=""nofollow"" title=""slic"">skimage.segmentation.slic</a>) I can segment an image into super pixels. Now I would like to generate features (region size, location, color, shape and texture features) for each segment as described in section 5.3 of</p>

<blockquote>
  <p>Gould, Stephen, et al. ""Multi-class segmentation with relative location prior."" International Journal of Computer Vision 80.3 (2008): 300-316.</p>
</blockquote>

<p>Are there existing libraries in python that can help me generate those features, given an image and a mask of segments? Can I do this using skimage?</p>
",2016-05-06 19:53:56,2019-07-30 15:44:17,Calculating feature vectors for image segments (super-pixels),<python><opencv><image-processing><classification><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
43349,37520501,2016-05-30 08:00:07,,"<p>`</p>

<pre><code>import numpy 
import skimage.io
from skimage.transform import rotate
tr_1 = numpy.random.rand(5,300)
training_inputs = [numpy.reshape(tr_1[x,:], (3,10,10)) for x in range(len(tr_1))]
f = rotate(training_inputs[1], 90, resize=True)
</code></pre>

<p>The above code is giving an output of size (10,4,10). But image should be rotated and it's size should be of (3,10,10).</p>

<p>Any Suggestions and how to proceed with the code? </p>
",,2016-05-30 08:18:28,Rotating an Image in Python,<image><python-2.7><numpy><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
43789,37733838,2016-06-09 18:42:15,,"<p>I am getting hog features of a human body in a image which resulted after grabcut. However I have to get the weighted hog features i.e. give special weight to face and arms then rest of the body. So I have detected the face and arms region however I am uncertain how to get the weighted combination of the hog features.</p>

<p>Image size - (307,220)</p>

<p>Number of Hog Features - 1976</p>

<p>Settings - <code>fd, hog_image = hog(im, orientations=8, pixels_per_cell=(16, 16),
                    cells_per_block=(1, 1), visualise=True);</code></p>

<p>I am using skimage in python.</p>

<p>Please give any pointers as I am not able to think of any starting point to do so.</p>

<p>Thanks</p>
",,2016-06-09 18:42:15,Calculating weighted hog features,<python><image-processing><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
44537,38078519,2016-06-28 14:08:48,,"<p>When I try to install scikit-image with pip2 there is show error. I have tried to find this problem solution.</p>

<p>Command python setup.py egg_info failed with error code 1 in /tmp/pip_build_root/scikit-image
Storing debug log for failure in /home/nshakib/.pip/pip.log</p>
",,2016-07-08 20:48:59,Problems installing scikit-image in ubuntu 14.04,<opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
44663,38271928,2016-07-08 16:53:41,,"<p>I was hoping somebody could help me with an image processing problem. My fly embryos are stained for the cell nuclei in the peripheral nervous system (PNS) - attachment 1. These cell nuclei form clusters in each of the embryonic segments of the fly. My goal is to use these clusters of cell nuclei to label each of the embryonic segments (i.e. one blob or point per segment) - attachment 2. </p>

<p><a href=""http://i.stack.imgur.com/mge1v.png"" rel=""nofollow"">PNS Stain</a></p>

<p><a href=""http://i.stack.imgur.com/htqgU.png"" rel=""nofollow"">PNS Stain showing blobs</a></p>

<p>I have had some success by doing Gaussian blur of the labeled nuclei (so that each cluster of nuclei form a blob) and then using adaptive thresholding to identify these ""blobs"". However, it isn't a very robust method - some clusters don't form or multiple clusters stick together. I'm using scikit-image to do my analysis, here is the relevant portion of my code that I have been using:</p>

<blockquote>
  <p>EmbryoBlur = gaussian(Embryo, sigma=(10,5))</p>
  
  <p>ClusteredCells= threshold_adaptive(EmbryoBlur , block_size=151, method=""mean"")</p>
</blockquote>

<p>Does anybody have any other strategies to suggest, so that it robustly forms a single point or blob per cluster of nuclei?  </p>

<p>Even if somebody wants to explain a strategy conceptually, I could also trying implement in scikit-image.</p>

<p>Thank you!</p>
",2016-07-08 19:46:15,2019-11-21 00:06:33,Imaging Processing - Clustering Cells into groups in Fly Embryos Images,<image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
44675,38308480,2016-07-11 13:40:26,,"<p>I want to skeletonize an image using the scikit-image module for skeletonization. This image is pre processed by OpenCV library. Given an Image 'Feb_16-0.jpg', I convert it to gray scale, perform the morphological transformation of opening the image, then apply the Gaussian Blur and adaptive thresholding using OpenCV and Python:</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt
from skimage.morphology import skeletonize
from skimage.viewer import ImageViewer
img = cv2.imread('Feb_16-0.jpg',0)
kernel = np.ones((1,1),np.uint8)
opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
blur = cv2.GaussianBlur(opening,(1,1),0)
ret3,th4 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
</code></pre>

<p>I now want to skeletonize the image using scikit-image skimage.morphology.skeletonize. I have tried writing code for performing erosion and dilation to manually skeletonize the image using OpenCV and Python. But, this proved to be a highly inefficient processing so i decided to switch to the scikit-image library at this point. However, when I pass the numpy array preprocessed by OpenCV to the scikit-image module using the code:</p>

<pre><code>skel = skeletonize(th4)
</code></pre>

<p>and try to view the results of the same, I end up with the error:</p>

<pre><code>Image contains values other than 0 and 1
</code></pre>

<p>I am unable to interpret the cause for the same. Can anyone kindly help me out in resolving this datatype error? </p>
",2016-07-11 14:09:26,2016-07-11 14:33:49,Unable to Process an image transformed in OpenCV via scikit-image,<python><image><opencv><preprocessor><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
44864,38440520,2016-07-18 15:23:42,,"<p>A black and white image showing a smile face inside a frame.</p>

<p><a href=""https://i.stack.imgur.com/sbqcu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sbqcu.jpg"" alt=""smiling face""></a></p>

<p>What I want is to find out the location of most right point of the smile face. (in this case, color black shall be at around 184,91 of the image)</p>

<p>By using below I hope to list the colors in the image, then see what can look for further.</p>

<pre><code>from PIL import Image
im = Image.open(""face.jpg"")
print im.convert('RGB').getcolors() # or print im.getcolors()
</code></pre>

<p>However it returns <code>None</code>, and I am stuck.  </p>

<p>How can I get the most right point of the face?</p>
",2017-03-21 09:00:33,2017-03-21 09:00:33,How to find the location of the rightmost black pixel,<python><numpy><image-processing><python-imaging-library><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
44955,38297765,2016-07-11 00:52:10,,"<p>I am trying to find the GLCM of an image using <code>greycomatrix</code> from <em>skimage</em> library. I am having issues with the selection of levels. Since it's an 8-bit image, the obvious selection should be 256; however, if I select values such as 8 (for the purpose of binning and to prevent sparse matrices from forming), I am getting errors. </p>

<p>QUESTIONS:</p>

<ul>
<li>Does anyone know why?</li>
<li>Can anyone suggest any ideas of binning these values into a 8x8 matrix instead of a 256x256 one?</li>
</ul>
",2017-03-16 08:57:43,2017-03-16 08:57:43,Grey Level Co-Occurrence Matrix // Python,<python><image-processing><scikit-image><glcm>,,,CC BY-SA 3.0,False,False,False,True,False
44963,38366439,2016-07-14 05:53:40,,"<p>What is the best way to get a floorplan external contour?</p>

<p><a href=""https://i.stack.imgur.com/1tl6D.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1tl6D.jpg"" alt=""enter image description here""></a></p>

<p>Snakes algorithm doesn't work well because some floorplans are too convex.</p>
",,2016-07-14 07:22:55,How to get the external contour of a floorplan in python?,<opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45004,38528363,2016-07-22 13:59:07,,"<p>Trying to complete a simple school exercise but I'm stuck at the beginning.
This is the code that doesn't work:</p>

<pre><code>from skimage import io

img = io.imread('mypic.jpg')
io.imshow(img)
io.show()
</code></pre>

<p>After execution I get this error:</p>

<pre><code>Exception in Tkinter callback Traceback (most recent call last):
File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 1536, in __call__
return self.func(*args)
File ""/usr/lib/python2.7/site-packages/matplotlib/backends/backend_tkagg.py"", line 283, in resize
self.show()
File ""/usr/lib/python2.7/site-packages/matplotlib/backends/backend_tkagg.py"", line 355, in draw
tkagg.blit(self._tkphoto, self.renderer._renderer, colormode=2)
File ""/usr/lib/python2.7/site-packages/matplotlib/backends/tkagg.py"", line 26, in blit
_tkagg.tkinit(tk.interpaddr(), 1)

OverflowError: Python int too large to convert to C long
</code></pre>

<p>I also tried the same thing with OpenCV library and I got a similar result:
Code</p>

<pre><code>import numpy as np
import cv2

img = cv2.imread('mypic.jpg',0)
cv2.imshow('image',img)
waitKey(0)
</code></pre>

<p>Error:</p>

<pre><code>OpenCV Error: Assertion failed (size.width&gt;0 &amp;&amp; size.height&gt;0) in imshow, file /builddir/build/BUILD/opencv-2.4.12.3/modules/highgui/src/window.cpp, line 261
Traceback (most recent call last):
File ""imagetry.py"", line 6, in &lt;module&gt;
cv2.imshow('lasta',img)
cv2.error: /builddir/build/BUILD/opencv-2.4.12.3/modules/highgui/src/window.cpp:261: error: (-215) size.width&gt;0 &amp;&amp; size.height&gt;0 in function imshow
</code></pre>

<p>I'm running this on 32-bit Fedora 24 and Python 2.7.
I would really appreciate your help! </p>
",,2019-10-11 07:33:48,How to display a jpg image in Python with Scikit-Image library,<python><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45179,38545611,2016-07-23 19:19:35,,"<p>In scikit image there is a feature descriptor called <a href=""http://scikit-image.org/docs/0.12.x/api/skimage.feature.html#hog"" rel=""nofollow"">HoG</a></p>

<p>The code below will calculate the descriptor for a single frame ""greyVideoFrame""</p>

<pre><code>HoGDescriptor = hog(greyVideoFrame, orientations=8, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualise=False)
</code></pre>

<p>Which returns a flattened (using ravel) nd array, below are some properties of the individual arrays</p>

<pre><code>('NDarray Shape', (251328,))
('NDarray Number of Dimensions', 1)
('NDarray length of 1 array element in Bytes', 8)
('NDarray Total bytes consumed by elements', 2010624)
('NDarray DataType', dtype('float64'))
</code></pre>

<p>My question is how would i collate* the results from multiple frames together which would be able to handle efficiently <strong>10,000 results</strong> of <code>shape (251328, )</code> .</p>

<p>*I'm not sure of the correct term to use here and whether I need to append or concatenate result 1, result 2,...result 10k and would welcome the communities guidance</p>
",2016-07-24 15:09:54,2016-08-22 19:57:00,What is the most efficient way to collect HoG descriptors (ndarrays) together for processing by Kmeans,<python><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45269,38609599,2016-07-27 09:54:54,,"<p>Given a <code>numpy.ndarray</code> of the kind</p>

<pre><code>myarray= 
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1])
</code></pre>

<p>I want to use <code>scikit-image</code> on the array (which is already labelled) to derive some properties.</p>

<p>This is what I do:</p>

<pre><code>myarray.reshape((11,11))                  
labelled=label(myarray)
props=sk.measure.regionprops(labelled)
</code></pre>

<p>But then I get this error:
<code>TypeError: Only 2-D and 3-D images supported.</code>, pointing at <code>props</code>. <strong>What is the problem? The image I am passing to <code>props</code> is already a 2D object.</strong></p>

<p>Shape of <code>myarray</code>:</p>

<pre><code>In [17]: myarray
Out[17]: 
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
</code></pre>
",2016-07-27 09:57:43,2018-04-01 20:24:29,Python: TypeError: Only 2-D and 3-D images supported with scikit-image regionprops,<python><numpy><image-processing><2d><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45287,38763530,2016-08-04 09:28:42,,"<p>I have used canny edge detector on an image.
It detected some areas in the image and other areas it displays nothing.
Now, I want that on the original image it would mask the areas that were completely black.
How can I do it?</p>

<p>I am using python and skimage or opencv (doesn't matter which one)</p>

<pre><code>from skimage.feature import canny
from skimage.morphology import closing
import skimage.io
import numpy as np
import os
import matplotlib.pyplot as plt
import cv2

img = skimage.io.imread(""test.jpg"",as_grey=True)
fig, ax = plt.subplots(1, 1, figsize=(20,20))
ax.imshow(img,'gray')
ax.set_axis_off()
plt.show()

edges = canny(img)

close = closing(edges)

fig, ax = plt.subplots(1, 1, figsize=(20,20))
ax.imshow(close,'gray')
ax.set_axis_off()
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/osYuB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/osYuB.jpg"" alt=""Original Image""></a>
<a href=""https://i.stack.imgur.com/hbkMt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hbkMt.png"" alt=""After canny and closing""></a></p>

<p>Now what I want is that the white part(in the second image) would be the only part that would be displayed in the original image ( Masking )</p>
",2016-08-04 12:23:40,2016-08-04 12:58:25,python edge detector - mask the area were it's completly black,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45303,38612909,2016-07-27 12:25:09,,"<p>An image with multiple objects is labelled like this:</p>

<pre><code>image=
[[ 1  0  0  0  2  2  0  3  3  3  3]
 [ 3  0  0  0  0  0  0  0  4  4  4]
 [ 4  0  5  5  0  0  0  0  6  6  6]
 [ 6  0  0  0  0  0  0  0  0  7  7]
 [ 7  0  0  0  0  0  0  0  0  8  8]
 [ 0  0  0  0  0  0  0  9  9  9  9]
 [ 9  0  0  0  0  0 10 10 10 10 10]
 [ 0  0  0  0  0  0  0  0  0  0 11]
 [ 0  0  0  0  0  0  0  0  0  0 12]
 [12 12  0  0 13 13  0  0  0  0 14]
 [14 14  0  0  0  0  0  0  0  0 15]]
</code></pre>

<p>Since I want to know about the major axis length of the equivalent ellipse, I use this function of image processing:</p>

<pre><code>import skimage as sk
from skimage import measure
props=sk.measure.regionprops(image)
maj_ax_le=round(props[0].major_axis_length,3)
</code></pre>

<p>But when I ask for the result, I get:</p>

<pre><code>In [1]: maj_ax_le
Out[1]: 0.0
</code></pre>

<p>Is this because of the presence of multiple objects (15, in this case)? If so, how can I compute the individual <code>maj_ax_le</code> for all the objects?</p>
",,2016-07-27 13:19:44,Scikit-image: why major axis length is zero with multiple objects?,<python><for-loop><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45354,38769153,2016-08-04 13:45:50,,"<p>How do I do cv2.min operation in skimage?
Is there a method for that in skimage or do I need to write it?</p>
",,2016-08-04 22:22:49,How do I do cv2.min operation in skimage?,<opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45370,38617317,2016-07-27 15:28:33,,"<p>Both <code>cv2.imread</code> and <code>skimage.io.imread</code> loads image into a <code>ndarray</code>, but with different shapes.</p>



<pre><code>import cv2
from skimage import io

im = cv2.imread(imfile)
print im.shape  # (2592, 1936, 3)

im = io.imread(imfile)
print im.shape # (1936, 2592, 3)
</code></pre>

<p>What is the real width/height of this image ? Why those shapes are different ?</p>

<p><strong>Edit:</strong>  This problem only appears with some images.</p>

<p><strong>Solution</strong>: It seems that OpenCV 3.1+ handles <a href=""http://www.impulseadventure.com/photo/exif-orientation.html"" rel=""nofollow"">EXIF orientation</a>, so it loads and rotate the image and <code>io.imread</code> don't.</p>

<p>So the real orientation of the image is the one returned by <code>io.imread</code>.</p>
",2016-07-27 15:42:21,2016-07-27 18:40:14,What is the real shape of this image?,<python><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45421,38620129,2016-07-27 18:00:48,,"<p>I'm trying to create a Region Adjacency Graph from after segmenting an image using the tools in the <code>Skimage</code> package. Using the examples in the documentation I can segment an image using SLIC and create the RAG successfully.</p>

<pre class=""lang-py prettyprint-override""><code>from skimage import data
from skimage import segmentation
from skimage.future import graph
import matplotlib.pyplot as plt

#Load Image
img = data.coffee()

#Segment image
labels = segmentation.slic(img, compactness=30, n_segments=800)
#Create RAG
g = graph.rag_mean_color(img, labels)
#Draw RAG
gplt = graph.draw_rag(labels, g, img)
plt.imshow(gplt)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Kffnm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kffnm.png"" alt=""Successful RAG""></a></p>

<p>However, if I use either <code>segmentation.quickshift</code> or <code>segmentation.felzenszwalb</code> to segment the image and then create the RAG, I get an error at <code>draw_rag()</code>.</p>

<pre class=""lang-py prettyprint-override""><code>labels = segmentation.quickshift(img, kernel_size=5, max_dist=5, ratio=0.5)
g = graph.rag_mean_color(img, labels)
gplt = graph.draw_rag(labels, g, img)

labels = segmentation.felzenszwalb(img, scale=100, sigma=0.5, min_size=50)
g = graph.rag_mean_color(img, labels)
gplt = graph.draw_rag(labels, g, img)
</code></pre>

<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File ""C:\Anaconda\lib\site-packages\IPython\core\interactiveshell.py"", line 3032, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-34-c0784622a6c7&gt;"", line 1, in &lt;module&gt;
    gplt = graph.draw_rag(labels, g, img)
  File ""C:\Anaconda\lib\site-packages\skimage\future\graph\rag.py"", line 429, in draw_rag
    out[circle] = node_color
IndexError: index 600 is out of bounds for axis 1 with size 600
</code></pre>

<p>The documentation seems to suggest that RAG methods should be compatible with segments from any of these methods, so I'm not sure if I'm doing something wrong, there's a bug, or RAG can only be used with the SLIC segmentation method. Any suggestions?</p>
",2016-07-27 19:54:24,2016-07-27 21:21:18,Skimage Region Adjacency Graph (RAG) from quickshift segmentation,<python><python-2.7><classification><image-segmentation><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45449,38589419,2016-07-26 12:04:25,,"<p>I have a (100,64,64) Numpy array which stores 100 64*64 images, and now I want to get the OTSU thresholds of each image, how can I achieve that without a for loop as it is slow?</p>

<p>Many thanks in advance.</p>
",,2016-07-26 12:04:25,How can I get the OTSU thresholds of 100 images in an array without using for loop?,<python><opencv><numpy><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45464,38820743,2016-08-08 03:08:06,,"<p>I am doing a <strong>license-plate recognition</strong>. I have crop out the plate but it is very <strong>blurred</strong>. Therefore I cannot split out the digits/characters and recognize it.</p>

<p>Here is my image:</p>

<p><a href=""https://i.stack.imgur.com/qQ8CF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qQ8CF.jpg"" alt=""enter image description here""></a></p>

<p>I have tried to <strong>denoise</strong> it through using <strong>scikit image</strong> function.</p>

<p>First, import the libraries:</p>

<pre><code>import cv2
from skimage import restoration
from skimage.filters import threshold_otsu, rank
from skimage.morphology import closing, square, disk
</code></pre>

<p>then, I read the image and convert it to <strong>gray scale</strong></p>

<pre><code>image = cv2.imread(""plate.jpg"")
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
</code></pre>

<p>I try to <strong>remove the noise</strong>:</p>

<pre><code>denoise = restoration.denoise_tv_chambolle(image , weight=0.1)
thresh = threshold_otsu(denoise)
bw = closing(denoise  &gt; thresh, square(2))
</code></pre>

<p>What I got is :</p>

<p><a href=""https://i.stack.imgur.com/udt2O.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/udt2O.jpg"" alt=""enter image description here""></a></p>

<p>As you can see, all the digits are <strong>mixed together</strong>. Thus, I <strong>cannot separate</strong> them and recognize the characters one by one.</p>

<p>What I expect is something like this (I draw it):</p>

<p><a href=""https://i.stack.imgur.com/OHQN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OHQN3.png"" alt=""enter image description here""></a></p>

<p>I am looking for help how can I better filter the image? Thank you.</p>

<p>=====================================================================
<strong>UPDATE</strong>:</p>

<p>After using <code>skimage.morphology.erosion</code>, I got:</p>

<p><a href=""https://i.stack.imgur.com/Z0Pak.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z0Pak.jpg"" alt=""enter image description here""></a></p>
",2016-08-08 08:01:34,2016-08-18 21:46:54,Denoise and filter an image,<python><image><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
45566,38828143,2016-08-08 11:31:58,,"<p><strong>1. The problem</strong></p>

<p>Given the images of a house roof, I am trying to find the contours of the roofs.  I have labelled data available (as polygon vertices) which I interpolate and create the truth image which is shown below
<a href=""https://i.stack.imgur.com/OhNKI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OhNKI.png"" alt=""ground truth from annotations""></a></p>

<p>I use canny, hough-lines, LBP features to train an ML model the results look decent. the model output is shown in the middle, and overlay on test image is shown on right.</p>

<p><a href=""https://i.stack.imgur.com/ZybaY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZybaY.png"" alt=""left original image, middle Model output, right: Output over-layed to original image""></a></p>

<p><strong>2. What I need.</strong> </p>

<p>The final output should really be a set of polygons and I need to find the points on which these polygons should be drawn (see the highlighted points in image below). So the output can be set of n line segments. where each line segment is 2 points [(x1,y1),(x2,y2)]</p>

<p><a href=""https://i.stack.imgur.com/70HAu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/70HAu.png"" alt=""marked image with vertices coded in color""></a></p>

<p><strong>3. What are my thoughts/ideas;</strong></p>

<p>a. Erosion,Dilation,Opening,closing,skeletonize operations</p>

<p>While these operations make the lines in the above image much neater, they dont help me find the polygon vertices I am looking for.</p>

<p>I'd like to fit (a number of) lines to the white pixels in the image (something like hough lines). </p>

<p>The intersections of these lines would give me the vertices for the polygons I am looking for.</p>

<p>I am wondering if there is a more standard/better way of accomplishing the above.</p>
",2016-08-09 07:56:53,2016-08-09 19:42:12,How to find polygon vertices from edge detection images?,<python><numpy><image-processing><signal-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45650,38834495,2016-08-08 16:42:09,,"<p>Looking for examples of how to use image processing tools to ""describe"" images and shapes of any sort, I have stumbled upon the <a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.moments_central"" rel=""nofollow"">Scikit-image</a> <code>skimage.measure.moments_central(image, cr, cc, order=3)</code> function.</p>

<p>They give an example of how to use this function:</p>

<pre><code>from skimage import measure #Package name in Enthought Canopy
import numpy as np

image = np.zeros((20, 20), dtype=np.double) #Square image of zeros
image[13:17, 13:17] = 1 #Adding a square of 1s
m = moments(image)
cr = m[0, 1] / m[0, 0] #Row of the centroid (x coordinate)
cc = m[1, 0] / m[0, 0] #Column of the centroid (y coordinate)

In[1]: moments_central(image, cr, cc)
Out[1]:
array([[ 16.,   0.,  20.,   0.],
       [  0.,   0.,   0.,   0.],
       [ 20.,   0.,  25.,   0.],
       [  0.,   0.,   0.,   0.]])
</code></pre>

<p><strong>1) What do each of the values represent?</strong> Since the (0,0) element is 16, I get this number corresponds to the area of the square of 1s, and therefore it is mu zero-zero. But how about the others?</p>

<p><strong>2) Is this always a symmetric matrix?</strong> </p>

<p><strong>3) What are the values associated with the famous <em>second central moments</em>?</strong></p>
",,2016-08-09 20:52:23,Scikit-image and central moments: what is the meaning?,<python><image-processing><canopy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45827,38948366,2016-08-15 02:02:28,,"<p>Now, I can get the intensity function of along horizontal and vertical direction using following code:</p>

<pre><code>import matplotlib.pyplot as plt
from skimage import data

# Load the image
image = data.coins()

imgslice = 120

# Vertical intensity function
plt.plot(image[imgslice], 'r')
</code></pre>

<p>But I want to get intensity function along certain directions. For example along a line from point A to point B. How can I achieve this?</p>
",,2017-05-12 02:22:16,"How to get intensity function along specific direction in Python image processing (module: numpy, scipy, skimage)",<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
45878,38955970,2016-08-15 13:26:15,,"<p>I am working on 6641x2720 image to generate its feature images (Haralick features like contrast, second moment etc) using a moving GLCM(Grey level Co-occurrence matrix ) window. But it takes forever to run. <strong>The code works fine, as I have tested it on smaller images.</strong> But, I need to make it run faster. Reducing the dimensions to 25% (1661x680) it takes <strong>30 minutes</strong> to run. How can I make it run faster ? Here's the code:</p>

<pre><code>from skimage.feature import greycomatrix, greycoprops
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import time
start_time = time.time()
img = Image.open('/home/student/python/test50.jpg').convert('L')
y=np.asarray(img, dtype=np.uint8)
#plt.imshow(y, cmap = plt.get_cmap('gray'), vmin = 0, vmax = 255)
contrast = np.zeros((y.shape[0], y.shape[1]), dtype = float)

for i in range(0,y.shape[0]):
    for j in range(0,y.shape[1]):
        if i &lt; 2 or i &gt; (y.shape[0]-3) or j &lt; 2 or j &gt; (y.shape[1]-3):
            continue
        else:
            s = y[(i-2):(i+3), (j-2):(j+3)]
            glcm = greycomatrix(s, [1], [0],  symmetric = True, normed = True )
            contrast[i,j] = greycoprops(glcm, 'contrast')
print(""--- %s seconds ---"" % (time.time() - start_time))
plt.imshow(contrast, cmap = plt.get_cmap('gray'), vmin = 0, vmax = 255)
</code></pre>
",2017-03-16 08:20:20,2017-03-16 08:20:20,Reduce running time in Texture analysis using GLCM [Python],<python><image-processing><scikit-image><glcm>,,,CC BY-SA 3.0,False,False,False,True,False
46053,39162261,2016-08-26 08:54:45,,"<p>I want to fit a set of points on a image with a <strong>smooth</strong> curve in python. The curve may be open or closed. Moreover, I want to get the curve plotted over the image as a mask that has the same size as the image. Is there any modules or functions I can refer to? Thanks.</p>
",,2016-08-26 12:41:55,Fit Points With a Smooth Curve,<python><opencv><matplotlib><computer-vision><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
46109,39046011,2016-08-19 18:55:51,,"<p>I have a bunch of scans of forms that all look like this:</p>

<p><a href=""https://i.stack.imgur.com/PqyPq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PqyPq.png"" alt=""enter image description here""></a></p>

<p>I'm trying to take out each row to make each on its own image (a row being the box with 10 all the way to the right of the form). I've written a function (in python) that will find all the boxes, OCR each box on its own (using tesseract), determine whether the ID label is present (in this blank form, just the 10), and use the height of the box and width of the whole table to pull out the row. </p>

<p>The problem with this process is the OCR; some of the tables are so pixelated that no text is detected at all, so that row doesn't get taken out of the table. I used the row rectangle boundaries of one form that had a good OCR result to take out the rows from all the forms, but, for some reason, some of the forms have differently sized headers, or the row height is larger or smaller than 'normal' (I've resized every table to be the same resolution). One thing that does not change is the general layout of the text windows within each row, though one form's rows might be taller or shorter relative to another table. </p>

<p>My question: how can I identify each row as a feature using one (or a set of) example(s), while accounting for the slight variation in the row position in various examples? I would appreciate any ideas you might have. </p>

<p>I'm working with Python 2.7, OpenCV 3.1.0 (on windows), and the same with scikit-image and scikit-learn on an ubuntu VM. </p>
",,2016-08-19 19:03:37,OpenCV Identify rows in Table of variable size,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
46229,39269499,2016-09-01 10:47:46,,"<p>I have a tiff image stack that I've used to reconstruct into a 3D volume. The stack contains 'slices' of the object which is a cylinder. I'm attempting to extract information concerning the surface of the cylinder (roughness, local gradients) using scikit image now and am struggling to find the correct function which would allow me to extract said information.</p>

<p>Does anyone have any experience with this type of experiment?</p>
",,2016-09-01 10:47:46,Extracting information concerning a the surface of a 3D volume from an imagestack in python,<python><image-processing><3d><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46386,39554476,2016-09-18 05:36:18,,"<p>I tried to follow an example <a href=""https://stackoverflow.com/questions/24240039/save-numpy-array-as-image-with-high-precision-16-bits-with-scikit-image"">given here</a>. The code segment is</p>

<pre><code>import numpy as np
from skimage import io, exposure, img_as_uint, img_as_float
im = np.array([[1., 2.], [3., 4.]], dtype='float64')
im = exposure.rescale_intensity(im, out_range='float')
im = img_as_uint(im)
io.imsave('test_16bit.png', im)
im2 = io.imread('test_16bit.png')
</code></pre>

<p>However, compiling this program gives the following warning message. What might cause the problem, and how to fix it?</p>

<p><strong>/devl/lib/python3.4/site-packages/scikit_image-0.12.3-py3.4-linux-x86_64.egg/skimage/util/dtype.py:110: UserWarni: Possible precision loss when converting from float64 to uint16
  ""%s to %s"" % (dtypeobj_in, dtypeobj))</strong></p>
",2017-05-23 10:27:25,2018-06-29 13:09:59,possible precision loss when using skimage,<python><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46401,39367655,2016-09-07 10:40:16,,"<p>I have problems storing my image after watershed segmentation as a binary image. When I plot the segmentation with cmap=plt.cm.gray it shows a binary image but I don't know how to store the image (without to display it).</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt
from skimage.morphology import watershed
from scipy import ndimage as ndi
from skimage import morphology
from skimage.filters import sobel
from skimage.io import imread, imsave, imshow
import scipy.misc

img = cv2.imread('07.png')
img = cv2.medianBlur(img,5)
b,g,r = cv2.split(img)

elevation_map = sobel(r)
markers = np.zeros_like(r)
markers[s &lt; 140] = 1
markers[s &gt; 200] = 2
segmentation = morphology.watershed(elevation_map, markers)

fig, ax = plt.subplots(figsize=(4, 3))
ax.imshow(segmentation, cmap=plt.cm.gray, interpolation='nearest')
ax.axis('off')
plt.show()
</code></pre>
",,2016-09-07 13:20:07,Save binary image after watershed,<python><image-segmentation><watershed><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46423,39557205,2016-09-18 11:34:01,,"<p>I want to remove some contour from an image but I don't know how to achieve it using <code>skimage</code>? I do something like this in <code>OpenCV</code> using <code>drawContour</code> but I can't find the equivalent in <code>skimage</code>.</p>

<p>Assume I have a simple image like:</p>

<pre><code>0 0 0 0 0 0 0 0

0 0 1 1 1 1 0 0

0 0 1 0 0 1 0 0

0 0 1 1 1 1 0 0

0 0 0 0 0 0 0 0
</code></pre>

<p>that has only one connected component. </p>

<p>I need to remove it by masking it. </p>

<p>The final result will be a 8 * 5 zero matrix!</p>

<pre><code>a = '''0 0 0 0 0 0 0 0                                             
0 0 1 1 1 1 0 0
0 0 1 0 0 1 0 0
0 0 1 1 1 1 0 0
0 0 0 0 0 0 0 0'''
np.array([int(i) for i in a.split()], dtype=bool).reshape(5, 8)
cc = measure.regionprops(measure.label(a))[0]
# here is what I do for removing cc
</code></pre>

<p>What should I do to remove <code>cc</code> connected component using <code>skimage</code>?</p>
",2016-09-18 13:24:23,2019-05-31 17:51:47,remove a contour from image using skimage,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46479,39453168,2016-09-12 14:52:37,,"<p>I have used <em>skimage.measure.label</em> to get labels of my image but i was wondering if there was a function or a best way to group the labels with a distance condition on their outline.</p>

<p>Currently i use <em>skimage.measure.regionprops</em> to analyse each label then <em>skimage.segmentation.find_boundaries</em> to get the outline of each label then i get the coordinates, i check the distance between each points, i update the label if the distance match the condition and then i reuse <em>regionprops</em> to get right labels after grouping (i will post my code soon).</p>

<p>Currently work with this code:</p>

<pre><code>import math
import matplotlib.pyplot as plt
import numpy as np

from skimage.draw import ellipse
from skimage.measure import label, regionprops
from skimage.transform import rotate


image = np.zeros((600, 600))

rr, cc = ellipse(300, 350, 100, 220)
rr2, cc2 = ellipse(100, 100, 20, 50)
image[rr, cc] = 1
image[rr2, cc2] = 1

image = rotate(image, angle=15, order=0)

label_img = label(image)
regions = regionprops(label_img)

fig, ax = plt.subplots()
ax.imshow(image, cmap=plt.cm.gray)

for props in regions:
    y0, x0 = props.centroid
    orientation = props.orientation
    x1 = x0 + math.cos(orientation) * 0.5 * props.major_axis_length
    y1 = y0 - math.sin(orientation) * 0.5 * props.major_axis_length
    x2 = x0 - math.sin(orientation) * 0.5 * props.minor_axis_length
    y2 = y0 - math.cos(orientation) * 0.5 * props.minor_axis_length

    ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5)
    ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5)
    ax.plot(x0, y0, '.g', markersize=15)

    minr, minc, maxr, maxc = props.bbox
    bx = (minc, maxc, maxc, minc, minc)
    by = (minr, minr, maxr, maxr, minr)
    ax.plot(bx, by, '-b', linewidth=2.5)

ax.axis((0, 600, 600, 0))
plt.show()
</code></pre>
",,2016-09-12 16:32:16,Skimage: group labels with distance condition on outline,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46592,39677462,2016-09-24 14:30:13,,"<p>I m trying to fill holes for a chessboard for stereo application. The chessboard is at micro scale thus it is complicated to avoid dust... as you can see :</p>

<p><a href=""https://i.stack.imgur.com/mCOFl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mCOFl.png"" alt=""enter image description here""></a></p>

<p>Thus, the corners detection is impossible. I tried with SciPy's binary_fill_holes or similar approaches but i have a full black image, i dont understand.</p>
",2016-09-24 14:47:32,2018-06-27 07:04:54,Filling holes in image with OpenCV or Skimage,<python><opencv><numpy><scipy><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
46697,39805697,2016-10-01 10:48:57,,"<p>When I tried to convert the image to gray scale using:</p>

<pre><code>from skimage.io import imread
from skimage.color import rgb2gray
mountain_r = rgb2gray(imread(os.getcwd() + '/mountain.jpg'))

#Plot
import matplotlib.pyplot as plt
plt.figure(0)
plt.imshow(mountain_r)
plt.show()
</code></pre>

<p>I got a weird colored image instead of a gray scale.</p>

<p>Manually implementing the function also gives me the same result. The custom function is:</p>

<pre><code>def rgb2grey(rgb):
    if len(rgb.shape) is 3:
        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])

    else:
        print 'Current image is already in grayscale.'
        return rgb
</code></pre>

<p><a href=""https://i.stack.imgur.com/mqgPr.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mqgPr.jpg"" alt=""Original""></a></p>

<p>Coloured image that is not in greyscale.
<a href=""https://i.stack.imgur.com/2X7lL.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2X7lL.jpg"" alt=""gray""></a></p>

<p>Why doesn't the function convert the image to greyscale?</p>
",2016-10-01 14:16:46,2019-09-30 13:57:04,skimage: Why does rgb2gray from skimage.color result in a colored image?,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46773,39615632,2016-09-21 11:43:02,,"<p>I have a set of images I'm using to perform a 3D reconstruction using mayavi in python, unfortunately there is a bright streak through most images over the features of interest that is screwing my reconstruction up, as shown in the figure below. The features of interest in this case are the 'holes' in the structure.</p>

<p><a href=""https://i.stack.imgur.com/XldCX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XldCX.png"" alt=""Reconstruction slice with bright streak through the middle""></a></p>

<p>I've attempted to use scikit-image erode and dilate functions to remove the feature, unfortunately the distorts the rest of the image a little too much, whilst failing to remove the feature completely, as shown below.</p>

<p><a href=""https://i.stack.imgur.com/bhcZG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bhcZG.png"" alt=""enter image description here""></a></p>

<p>Does anyone know how I can simply remove this bright streak, without distorting the rest of the image?</p>
",,2016-09-21 11:43:02,Remove bright streak from image using python,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
46968,39900964,2016-10-06 16:14:51,,"<p>I am on windows using Anaconda with python 3.5, I want to install opencv3.1, so I found a channel called conda-forge has the opencv3.2 for my python version, but when I try to install it by</p>

<blockquote>
  <p>conda install -c conda-forge opencv</p>
</blockquote>

<p>I got error message:</p>

<pre><code>Fetching package metadata .............
Solving package specifications: ....
UnsatisfiableError: The following specifications were found to be in conflict:
  - opencv -&gt; jpeg 9*
  - opencv -&gt; numpy 1.9*
  - opencv -&gt; python 2.7*|3.4*
Use ""conda info &lt;package&gt;"" to see the dependencies for each package.
</code></pre>

<p>but when I do:</p>

<blockquote>
  <p>conda info opencv</p>
</blockquote>

<p>there is an opencv for python3.5, why conda cannot find it?</p>

<pre><code>opencv 3.1.0 np111py35_1
------------------------
file name   : conda-forge::opencv-3.1.0-np111py35_1.tar.bz2
name        : opencv
version     : 3.1.0
build number: 1
build string: np111py35_1
channel     : conda-forge
size        : 84.4 MB
arch        : x86_64
binstar     : {'owner_id': '5528f42ce1dad12974506e8d', 'package_id': '56feac4e676dfa6a0572551e', 'channel': 'main'}
fn          : opencv-3.1.0-np111py35_1.tar.bz2
has_prefix  : True
license     : BSD 3-clause
machine     : x86_64
md5         : 9ff21915905c36894059eaf8d6d554cc
operatingsystem: win32
platform    : win
priority    : 1
schannel    : conda-forge
subdir      : win-64
target-triplet: x86_64-any-win32
url         : https://conda.anaconda.org/conda-forge/win-64/opencv-3.1.0-np111py35_1.tar.bz2
dependencies:
    jpeg 9*
    libpng &gt;=1.6.21,&lt;1.7
    libtiff 4.0.*
    numpy 1.11*
    python 3.5*
    zlib 1.2.*
</code></pre>

<p>so I checked the env by <code>conda list</code> and I have:</p>

<pre><code> conda list
# packages in environment at C:\Users\MPNV38\AppData\Local\Continuum\Anaconda3:
#
_license                  1.1                      py35_1
_nb_ext_conf              0.3.0                    py35_0
alabaster                 0.7.9                    py35_0
anaconda                  4.2.0               np111py35_0
anaconda-clean            1.0.0                    py35_0
anaconda-client           1.5.1                    py35_0
anaconda-navigator        1.3.1                    py35_0
argcomplete               1.0.0                    py35_1
astroid                   1.4.7                    py35_0
astropy                   1.2.1               np111py35_0
babel                     2.3.4                    py35_0
backports                 1.0                      py35_0
beautifulsoup4            4.5.1                    py35_0
bitarray                  0.8.1                    py35_1
blaze                     0.10.1                   py35_0
bokeh                     0.12.2                   py35_0
boto                      2.42.0                   py35_0
bottleneck                1.1.0               np111py35_0
bzip2                     1.0.6                    vc14_3  [vc14]
cffi                      1.7.0                    py35_0
chest                     0.2.3                    py35_0
click                     6.6                      py35_0
cloudpickle               0.2.1                    py35_0
clyent                    1.2.2                    py35_0
colorama                  0.3.7                    py35_0
comtypes                  1.1.2                    py35_0
conda                     4.2.9                    py35_0
conda-build               2.0.3                    py35_0
conda-env                 2.6.0                         0
configobj                 5.0.6                    py35_0
console_shortcut          0.1.1                    py35_1
contextlib2               0.5.3                    py35_0
cryptography              1.5                      py35_0
curl                      7.49.0                   vc14_0  [vc14]
cycler                    0.10.0                   py35_0
cython                    0.24.1                   py35_0
cytoolz                   0.8.0                    py35_0
dask                      0.11.0                   py35_0
datashape                 0.5.2                    py35_0
decorator                 4.0.10                   py35_0
dill                      0.2.5                    py35_0
docutils                  0.12                     py35_2
dynd-python               0.7.2                    py35_0
entrypoints               0.2.2                    py35_0
et_xmlfile                1.0.1                    py35_0
fastcache                 1.0.2                    py35_1
filelock                  2.0.6                    py35_0
flask                     0.11.1                   py35_0
flask-cors                2.1.2                    py35_0
freetype                  2.5.5                    vc14_1  [vc14]
get_terminal_size         1.0.0                    py35_0
gevent                    1.1.2                    py35_0
greenlet                  0.4.10                   py35_0
h5py                      2.6.0               np111py35_2
hdf5                      1.8.15.1                 vc14_4  [vc14]
heapdict                  1.0.0                    py35_1
icu                       57.1                     vc14_0  [vc14]
idna                      2.1                      py35_0
imagesize                 0.7.1                    py35_0
ipykernel                 4.5.0                    py35_0
ipython                   5.1.0                    py35_0
ipython_genutils          0.1.0                    py35_0
ipywidgets                5.2.2                    py35_0
itsdangerous              0.24                     py35_0
jdcal                     1.2                      py35_1
jedi                      0.9.0                    py35_1
jinja2                    2.8                      py35_1
jpeg                      8d                       vc14_2  [vc14]
jsonschema                2.5.1                    py35_0
jupyter                   1.0.0                    py35_3
jupyter_client            4.4.0                    py35_0
jupyter_console           5.0.0                    py35_0
jupyter_core              4.2.0                    py35_0
lazy-object-proxy         1.2.1                    py35_0
libdynd                   0.7.2                         0
libpng                    1.6.22                   vc14_0  [vc14]
libtiff                   4.0.6                    vc14_2  [vc14]
llvmlite                  0.13.0                   py35_0
locket                    0.2.0                    py35_1
lxml                      3.6.4                    py35_0
markupsafe                0.23                     py35_2
matplotlib                1.5.3               np111py35_0
menuinst                  1.4.1                    py35_0
mistune                   0.7.3                    py35_0
mkl                       11.3.3                        1
mkl-service               1.1.2                    py35_2
mpmath                    0.19                     py35_1
multipledispatch          0.4.8                    py35_0
nb_anacondacloud          1.2.0                    py35_0
nb_conda                  2.0.0                    py35_0
nb_conda_kernels          2.0.0                    py35_0
nbconvert                 4.2.0                    py35_0
nbformat                  4.1.0                    py35_0
nbpresent                 3.0.2                    py35_0
networkx                  1.11                     py35_0
nltk                      3.2.1                    py35_0
nose                      1.3.7                    py35_1
notebook                  4.2.3                    py35_0
numba                     0.28.1              np111py35_0
numexpr                   2.6.1               np111py35_0
numpy                     1.11.1                   py35_1
odo                       0.5.0                    py35_1
openpyxl                  2.3.2                    py35_0
openssl                   1.0.2j                   vc14_0  [vc14]
pandas                    0.18.1              np111py35_0
partd                     0.3.6                    py35_0
path.py                   8.2.1                    py35_0
pathlib2                  2.1.0                    py35_0
patsy                     0.4.1                    py35_0
pep8                      1.7.0                    py35_0
pickleshare               0.7.4                    py35_0
pillow                    3.3.1                    py35_0
pip                       8.1.2                    py35_0
pkginfo                   1.3.2                    py35_0
ply                       3.9                      py35_0
prompt_toolkit            1.0.3                    py35_0
psutil                    4.3.1                    py35_0
py                        1.4.31                   py35_0
pyasn1                    0.1.9                    py35_0
pycosat                   0.6.1                    py35_1
pycparser                 2.14                     py35_1
pycrypto                  2.6.1                    py35_4
pycurl                    7.43.0                   py35_0
pyflakes                  1.3.0                    py35_0
pygments                  2.1.3                    py35_0
pylint                    1.5.4                    py35_1
pyopenssl                 16.0.0                   py35_0
pyparsing                 2.1.4                    py35_0
pyqt                      5.6.0                    py35_0
pytables                  3.2.2               np111py35_4
pytest                    2.9.2                    py35_0
python                    3.5.2                         0
python-dateutil           2.5.3                    py35_0
pytz                      2016.6.1                 py35_0
pywin32                   220                      py35_1
pyyaml                    3.12                     py35_0
pyzmq                     15.4.0                   py35_0
qt                        5.6.0                    vc14_0  [vc14]
qtawesome                 0.3.3                    py35_0
qtconsole                 4.2.1                    py35_2
qtpy                      1.1.2                    py35_0
requests                  2.11.1                   py35_0
rope                      0.9.4                    py35_1
ruamel_yaml               0.11.14                  py35_0
scikit-image              0.12.3              np111py35_1
scikit-learn              0.17.1              np111py35_1
scipy                     0.18.1              np111py35_0
setuptools                27.2.0                   py35_1
simplegeneric             0.8.1                    py35_1
singledispatch            3.4.0.3                  py35_0
sip                       4.18                     py35_0
six                       1.10.0                   py35_0
snowballstemmer           1.2.1                    py35_0
sockjs-tornado            1.0.3                    py35_0
sphinx                    1.4.6                    py35_0
spyder                    3.0.0                    py35_0
sqlalchemy                1.0.13                   py35_0
statsmodels               0.6.1               np111py35_1
sympy                     1.0                      py35_0
tk                        8.5.18                   vc14_0  [vc14]
toolz                     0.8.0                    py35_0
tornado                   4.4.1                    py35_0
traitlets                 4.3.0                    py35_0
unicodecsv                0.14.1                   py35_0
vs2015_runtime            14.0.25123                    0
wcwidth                   0.1.7                    py35_0
werkzeug                  0.11.11                  py35_0
wheel                     0.29.0                   py35_0
widgetsnbextension        1.2.6                    py35_0
win_unicode_console       0.5                      py35_0
wrapt                     1.10.6                   py35_0
xlrd                      1.0.0                    py35_0
xlsxwriter                0.9.3                    py35_0
xlwings                   0.10.0                   py35_0
xlwt                      1.1.2                    py35_0
zlib                      1.2.8                    vc14_3  [vc14]
</code></pre>
",2016-10-06 17:00:57,2016-10-17 15:22:32,conda install fails even i found the package by conda info package,<python><opencv><anaconda><conda>,,,CC BY-SA 3.0,True,False,False,True,False
47375,40154110,2016-10-20 12:12:47,,"<p><strong>The problem:</strong> 
I have a binary image, with multiple objects. I need to find a way to fit the largest possible Square in these irregular objects. <em>see image attached below</em></p>

<p>I tried to formulate this as an optimization problem using boundingbox  of each labelled object as initial guess and a cost function as shown below. Bounding Box rows are assumed as initial guess for colums as well since its a square.</p>

<p>Using the same approach as Stefan's answer on stackoverflow: <a href=""https://stackoverflow.com/questions/28281742/fitting-a-circle-to-a-binary-image"">fitting a circle to a binary image</a>
For my parameters I cannot figure out how to optimize the parameters against this simple cost function</p>

<pre><code>import scipy.optimize as optimize
import numpy as np
import skimage.draw as draw
import skimage.measure as measure

def cost(params):
   baser,basec,deltar = params
   ycords = range(int(baser),int(baser + deltar))
   xcords = range(int(basec),int(basec + deltar))
   coords = draw.polygon(ycords,xcords,shape=region.image.shape)    
   template = np.zeros_like(region.image)
   template[coords] = 1
return -np.sum(template == region.image)
labels , nlabels = measure.label(img,neighbors=4,return_num=True)
regions = measure.regionprops(labels.astype(int))

for region in regions:
   minr, minc, maxr, _ = region.bbox
   baser = minr
   basec = minc
   deltar = maxr-minr   # One parameter in case of square

   print ""intial :"", baser,basec,deltar
   OptimizeResult = optimize.fmin(cost,(baser,basec,deltar),disp=True)

    baser,basec,deltar = OptimizeResult 
    print ""final : "" , OptimizeResult
    # the above 2 are same. and so the results don't change
</code></pre>

<p>The params do not change, optimization stops at iteration 0. and cost does not change as well.
I have tried different solvers, and optimize.minimize. the params don't change.</p>

<p>Ideally I would want a rectangle in each of the regions. each region(binary object) color coded in different color <a href=""https://i.stack.imgur.com/fUhRA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fUhRA.png"" alt=""Labelled Image""></a></p>

<p>Please also comment if optimization is a good way to solve these problems. </p>

<p>The Annotated image as requested is posted below.The rectangles(assume squares after edit) on a few objects are in BLACK (sorry for poor color choice) <a href=""https://i.stack.imgur.com/BfWIE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BfWIE.png"" alt=""Annotated Image""></a></p>
",2017-05-23 12:16:38,2016-10-21 05:47:07,"Finding largest Rectangles in Images, through Optimization",<python><image-processing><scipy><convex-optimization><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
47425,40235643,2016-10-25 08:58:54,,"<p>I'm trying to blur around specific regions in a 2D image (the data is an array of size <em>m</em> x <em>n</em>).</p>

<p>The points are specified by an <em>m</em> x <em>n</em> mask. <code>cv2</code> and <code>scikit</code> avaiable.</p>

<p>I tried:</p>

<ol>
<li><p>Simply applying blur filters to the masked image. But that isn't not working.</p></li>
<li><p>Extracting the points to blur by np.nan the rest, blurring and reassembling. Also not working, because the blur obviously needs the surrounding points to work correctly.</p></li>
</ol>

<p>Any ideas?</p>

<p>Cheers</p>
",2017-01-12 10:48:08,2017-01-12 10:48:08,Python: Blur specific region in an image,<python><opencv><image-processing><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
47458,40159603,2016-10-20 16:19:15,,"<p>I am using the following code to read a set of tiff files from a folder</p>

<pre><code>from PIL import image
from skimage import io
io.use_plugin('pil')
images = os.listdir(train_data_path)
for image_name in images:
    img = io.imread(os.path.join(train_data_path, image_name))
</code></pre>

<p>When running the above code, most of the files were reading smoothly. But I found the program will generate some warning message for some specific file</p>

<pre><code>/devl/lib/python3.4/site-packages/scikit_image-0.12.3-py3.4-linux-x86_64.egg/skimage/external/tifffile/tifffile.py:1794: RuntimeWarning: py_decodelzw encountered unexpected end of stream                                  
strip = decompress(strip)      
</code></pre>

<p>When opening that file, I cannot see any explicit difference with others. What can the reason underlying this?</p>
",,2018-07-20 22:44:36,on reading tiff file using skimage,<python><image-processing><scipy><tiff><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
47522,40162891,2016-10-20 19:25:06,,"<p>I am trying to customize an existing code to suit my own need. Originally, the code use <code>imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)</code> to store a list of image files in an numpy array format. Iterating the folder, each image file is read as follows <code>img = skimage.io.imread(os.path.join(train_data_path, image_name))</code> It works just fine.
The code is as follows:</p>

<pre><code> image_rows = 420
 image_cols = 580
 imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)
 i=0
 for image_name in images:
     img = skimage.io.imread(os.path.join(train_data_path, image_name))
     img = np.array([img])
     imgs[i]=img
     i+=1
</code></pre>

<p>In order to suit my own need, I tend to have image file array with the shape <code>[total, image_rows,image_cols,1]</code>. In other words, I modified it as <code>imgs = np.ndarray((total,image_rows, image_cols,1), dtype=np.uint8)</code>  However, running the code causes the following error</p>

<pre><code> imgs[i] = img
 ValueError: could not broadcast input array from shape (1,420,580) into shape         
(420,580,1)
</code></pre>

<p>Are there any way to change the shape of <code>img</code>, which originally has shape of <code>[1,420,580]</code> after reading from file. How can I change it to <code>[420,580,1]</code> without affecting the corresponding pixel values in the image.</p>
",,2016-10-20 20:14:18,on modifying the shape of numpy array resulting from input image,<python><numpy><scipy><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
47538,40085314,2016-10-17 11:33:26,,"<p>Sobel Edge detection:<br>
<img src=""https://i.stack.imgur.com/bbgpD.png"" alt=""Sobel Edge detection""></p>

<p>Original Image:<br>
<img src=""https://i.stack.imgur.com/L8yvq.jpg"" alt=""Original Image""></p>

<p>I have used sobel edge detection technique to identify the boundaries of each object in the given image. How can I extract the objects in the original image using these boundaries. We can ignore the objects with smaller pixel count.</p>
",2016-10-17 20:20:34,2016-10-17 20:20:34,Extract each object after edge detection using sobel filter,<python><opencv><image-processing><image-segmentation><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
47578,40087362,2016-10-17 13:13:04,,"<p>I want to use <strong>skimage.restoration.denoise_wavelet</strong> to denoise a image. But problem occurs on importing.</p>

<p><code>from skimage.restoration import denoise_nl_means,denoise_wavelet</code>  </p>

<p><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-2-161a32d32528&gt; in &lt;module&gt;()
----&gt; 1 from skimage.restoration import denoise_nl_means,denoise_wavelet
ImportError: cannot import name denoise_wavelet</code></p>

<p>There is no problem in importing <strong>denoise_nl_means</strong> which is in the same category with <strong>denoise_wavelet</strong>. It doesn't make sense. </p>

<p>I used <strong>pip</strong> for installing and updating the skimage package and <strong>jupyter notebook</strong> for coding.
I installed all requirements before installing scikit-image 0.12.3. The requirement items I installed were:</p>

<p><code>matplotlib 1.5.1,numpy 1.11.1,scipy 0.18.1,six 1.10.0,networkx 1.11,pillow 3.4.1,dask 0.10.0,PyWavelets 0.4.0</code>.</p>
",,2018-09-23 03:20:45,Importing error in python package skimage,<python><image-processing><filter><wavelet><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
47840,40407723,2016-11-03 17:25:08,,"<p>I am using skimage and sklearn to train a classifier of images of the dataset food101</p>

<pre><code>def process_image(image_fp):
image_ = imread(image_fp)
resize(image_, (400, 350)).shape
image=rgb2gray(image_)
descs=skimage.feature.daisy(image, step=180, radius=58, rings=2, histograms=6, orientations=8)
if descs.shape[0]!=3:
    descs=descs.transpose(1, 0, 2)
return descs.reshape(descs.size).tolist()
</code></pre>

<p>When it comes to:</p>

<pre><code>clf = grid_search.GridSearchCV(svm.SVC(), parameters).fit(x_train, y_train)
</code></pre>

<p>It appears an error because of different sizes of the return of the function 'process_image'.</p>

<p>I can solve the problem by selecting only the number of elements of the list with less elements, but I think it may have a more correct way to do it.</p>
",2016-11-04 12:42:55,2016-11-04 12:42:55,Python skimage daisy different sizes of feature vectors,<python><image-processing><scikit-learn><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48060,40586276,2016-11-14 10:14:37,,"<p>I am trying to extract the LAB a-channel of a 32-bit RGB image. However I fail to read the image correctly and I get unexpected results.</p>

<pre><code>import cv2
org = cv2.imread('42.png', -1)
print org.dtype
# print uint8
lab_image = cv2.cvtColor(org, cv2.COLOR_RGB2LAB)
l,a,b = cv2.split(lab_image)
cv2.imshow('', a)
cv2.waitKey(0)
</code></pre>

<p>Original image:
<a href=""http://labtools.ipk-gatersleben.de/images/42.png"" rel=""nofollow noreferrer"">http://labtools.ipk-gatersleben.de/images/42.png</a></p>

<p>Expected output (ImageJ):
<a href=""http://labtools.ipk-gatersleben.de/images/imagej_out.png"" rel=""nofollow noreferrer"">http://labtools.ipk-gatersleben.de/images/imagej_out.png</a></p>

<p>OpenCV output:
<a href=""http://labtools.ipk-gatersleben.de/images/python_out.png"" rel=""nofollow noreferrer"">http://labtools.ipk-gatersleben.de/images/python_out.png</a></p>

<p>I also tried to read/convert the image with skimage but the result is the same...</p>
",2017-01-12 11:07:36,2017-05-17 20:21:32,Unexpected output while converting RGB image to LAB image,<python><opencv><color-space><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
48184,40730877,2016-11-21 22:53:25,,"<p>In one code segment, open cv uses</p>

<pre><code>import cv2
img = cv2.threshold(img, 0.5, 1., cv2.THRESH_BINARY)[1].astype(np.uint8)
</code></pre>

<p>In skimage or pure Python, are there any efficient ways or existing functions that achieve the same goal as the above open cv (cv2) usage?</p>
",2016-11-22 06:24:57,2019-09-17 03:53:31,image threshold in skimage or pure python,<python><image-processing><scipy><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48205,40773267,2016-11-23 20:05:10,,"<p>I am using the following function to resize an image set having shape <code>(samples, 1, image_row, image_column)</code>. I am using the <code>skimage</code> library.</p>

<pre><code>from skimage import io
from skimage.transform import resize   
def preprocess(imgs):
imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)
for i in range(imgs.shape[0]):
    imgs_p[i, 0]  = resize(imgs[i, 0], (img_rows, img_cols))
return imgs_p
</code></pre>

<p>However, I noticed that the resized images kind of becomes 0-1 array. Here are some testing results. We can see the resized image consists of only 0-1 value. I ma not sure what's wrong with my resize function.</p>

<pre><code>   print(image[0,0].shape)

   (420, 580)
   print(image[0,0])
[[  0 155 152 ...,  87  91  90]
  [  0 255 255 ..., 140 141 141]
  [  0 255 255 ..., 157 156 158]
    ...,
  [  0  77  63 ..., 137 133 122]
  [  0  77  63 ..., 139 136 127]
  [  0  77  64 ..., 149 144 137]]

  print(resized_image[0,0].shape)
  (96, 128)
  print(resized_image[0,0])
  [[1 1 0 ..., 0 0 0]
   [0 0 0 ..., 0 0 0]
   [0 0 0 ..., 0 0 0]
     ...,
   [0 0 0 ..., 0 0 0]
   [0 0 0 ..., 0 0 0]
   [0 0 0 ..., 0 0 0]]
</code></pre>
",2017-01-05 18:59:01,2017-01-05 18:59:01,Loss of information after resizing the image using skimage python library,<python><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48226,40815273,2016-11-26 05:29:13,,"<p>I am having problem with importing <code>skimage.color</code> module. Although I can import and call <code>skimage.color.rgb2gray</code> from python shell, I cannot do the same thing from my application. </p>

<p>I checked <code>skimage</code> lib places on my PC. They all seems to be fine. But when I try to call skimage.color. from my code it always gives me this</p>

<pre><code>Traceback (most recent call last):
File ""main-video2.py"", line 44, in &lt;module&gt;
image = color.rgb2gray(image)
</code></pre>

<p>I checked the module like this </p>

<pre><code>username@ubuntu:~/dev/computer_vision$ python 
Python 2.7.12 (default, Nov 19 2016, 06:48:10) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 &gt;&gt;&gt; 
 KeyboardInterrupt
 &gt;&gt;&gt; from skimage import color 
 &gt;&gt;&gt; color 
 &lt;module 'skimage.color' from '/usr/local/lib/python2.7/dist-packages/skimage/color/__init__.pyc'&gt;
</code></pre>

<p>Anybody can help me to understand why would be the reason while I'm able to call it from shell but not from my code ?</p>
",2017-01-12 10:24:39,2019-04-25 10:44:50,skimage.color.rgb2gray import trouble,<python><image-processing><computer-vision><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48269,40703086,2016-11-20 10:43:05,,"<p>I have been using the SLIC implementation of skimage to segment images in superpixels. I would like to use GLCMs to extract additional features from these superpixels for a classification problem. These superpixels are not rectangular. In MATLAB you can set pixels to NaN and they will be ignored by the algorithm (<a href=""https://stackoverflow.com/questions/8237000/the-method-of-calculate-the-glcm-of-a-specific-point-in-a-image"">link</a>). I could use this to make bounding boxes around the superpixels and then just setting the unused pixels to NaN.</p>

<p>The <a href=""http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.greycomatrix"" rel=""nofollow noreferrer"">greycomatrix</a> function in skimage does not work entirely the same as the MATLAB implementation however. When setting pixels to NaN the function fails on an assert to check if all values are larger than 0.</p>

<p>Is there a Python implementation available which is able to work with nonrectangular ROIs?</p>
",2017-05-23 11:46:18,2018-04-13 11:33:55,Python: taking the GLCM of a non-rectangular region,<python><image-processing><scikit-image><mahotas><glcm>,,,CC BY-SA 3.0,False,True,False,True,False
48584,41019222,2016-12-07 13:51:41,,"<p>I am trying to use <a href=""http://scikit-image.org/docs/dev/api/skimage.restoration.html#skimage.restoration.wiener"" rel=""nofollow noreferrer"">skimage.restoration.wiener</a>, but I always end up with an image with a bunch of 1 (or -1), what am I doing wrong? The original image comes from <a href=""http://links.uwaterloo.ca/Repository.html"" rel=""nofollow noreferrer"">Uni of Waterloo</a>.</p>

<pre><code>import numpy as np
from scipy.misc import imread
from skimage import color, data, restoration
from scipy.signal import convolve2d as conv2

def main():
  image = imread(""/Users/gsamaras/Downloads/boat.tif"")
  psf = np.ones((5, 5)) / 25
  image = conv2(image, psf, 'same')
  image += 0.1 * image.std() * np.random.standard_normal(image.shape)

  deconvolved = restoration.wiener(image, psf, 0.00001)
  print deconvolved
  print image

if __name__ == ""__main__"":
    main()
</code></pre>

<p>Output:</p>

<pre><code>[[ 1. -1.  1. ...,  1. -1. -1.]
 [-1. -1.  1. ..., -1.  1.  1.]
 [ 1.  1.  1. ...,  1.  1.  1.]
 ..., 
 [ 1.  1.  1. ...,  1. -1.  1.]
 [ 1.  1.  1. ..., -1.  1. -1.]
 [ 1.  1.  1. ..., -1.  1.  1.]]
[[  62.73526298   77.84202199   94.1563234  ...,   85.12442365
    69.80579057   48.74330501]
 [  74.79638704  101.6248559   143.09978769 ...,  100.07197414
    94.34431216   59.72199141]
 [  96.41589893  132.53865314  161.8286996  ...,  137.17602535
   117.72691238   80.38638741]
 ..., 
 [  82.87641732  122.23168689  146.14129645 ...,  102.01214025
    75.03217549   59.78417916]
 [  74.25240964  100.64285679  127.38475015 ...,   88.04694654
    66.34568789   46.72457454]
 [  42.53382524   79.48377311   88.65000364 ...,   50.84624022
    36.45044106   33.22771889]]
</code></pre>

<p>And I tried several values. What am I missing?</p>
",2016-12-07 14:32:45,2016-12-08 20:03:28,Deblur an image using scikit-image,<python><numpy><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48892,41005044,2016-12-06 21:15:03,,"<p>Its <a href=""http://scikit-image.org/download.html"" rel=""nofollow noreferrer"">scikit-image Download</a> says:</p>

<pre><code>pip install -U scikit-image
</code></pre>

<p>or</p>

<pre><code>easy_install -U scikit-image
</code></pre>

<p>but both fail, regardless of the flag U, as shown below:</p>

<pre><code>Georgioss-MacBook-Pro:Downloads gsamaras$ sudo pip install scikit-image
The directory '/Users/gsamaras/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/gsamaras/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting scikit-image
  Downloading scikit_image-0.12.3-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (22.2MB)
    100% || 22.2MB 53kB/s 
Requirement already satisfied: dask[array]&gt;=0.5.0 in /Library/Python/2.7/site-packages (from scikit-image)
Collecting six&gt;=1.7.3 (from scikit-image)
  Downloading six-1.10.0-py2.py3-none-any.whl
Collecting networkx&gt;=1.8 (from scikit-image)
  Downloading networkx-1.11-py2.py3-none-any.whl (1.3MB)
    100% || 1.3MB 133kB/s 
Requirement already satisfied: pillow&gt;=2.1.0 in /Library/Python/2.7/site-packages (from scikit-image)
Requirement already satisfied: numpy; extra == ""array"" in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from dask[array]&gt;=0.5.0-&gt;scikit-image)
Requirement already satisfied: toolz&gt;=0.7.2; extra == ""array"" in /Library/Python/2.7/site-packages (from dask[array]&gt;=0.5.0-&gt;scikit-image)
Collecting decorator&gt;=3.4.0 (from networkx&gt;=1.8-&gt;scikit-image)
  Downloading decorator-4.0.10-py2.py3-none-any.whl
Installing collected packages: six, decorator, networkx, scikit-image
  Found existing installation: six 1.4.1
    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.
    Uninstalling six-1.4.1:
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py"", line 778, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 754, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_uninstall.py"", line 115, in remove
    renames(path, new_path)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py"", line 267, in renames
    shutil.move(old, new)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 302, in move
    copy2(src, real_dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 131, in copy2
    copystat(src, dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 103, in copystat
    os.chflags(dst, st.st_flags)
OSError: [Errno 1] Operation not permitted: '/tmp/pip-qlMJKP-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'
Georgioss-MacBook-Pro:Downloads gsamaras$ 
</code></pre>

<p>and:</p>

<pre><code>Georgioss-MacBook-Pro:Downloads gsamaras$ sudo easy_install scikit-image
Searching for scikit-image
Reading https://pypi.python.org/simple/scikit-image/
Best match: scikit-image 0.12.3
Downloading https://pypi.python.org/packages/86/d0/b0192dc9a544da90f2d9150bcd84b981c6873e42a1f752b6affb89180ad8/scikit-image-0.12.3.tar.gz#md5=04ea833383e0b6ad5f65da21292c25e1
Processing scikit-image-0.12.3.tar.gz
Writing /tmp/easy_install-X6Pjoh/scikit-image-0.12.3/setup.cfg
Running scikit-image-0.12.3/setup.py -q bdist_egg --dist-dir /tmp/easy_install-X6Pjoh/scikit-image-0.12.3/egg-dist-tmp-lHJxkL
Killed: 9
</code></pre>

<p>Notice that <code>brew</code> also fails, it cannot find it.</p>

<p>I have Python 2.7.10, Matplotlib 1.3.1, PIL 3.4.2 and Scipy '0.18.1'. What to do?</p>
",2017-09-08 08:12:34,2019-09-19 21:40:57,How to install scikit-image?,<python><macos><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48920,41008973,2016-12-07 03:36:32,,"<p>Currently, I use skimage in python to extract the skeleton of open space from some a binarized map as following pictures, </p>

<p><a href=""https://i.stack.imgur.com/UJPlf.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UJPlf.gif"" alt=""Original BW map image""></a><a href=""https://i.stack.imgur.com/aCJ7T.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aCJ7T.gif"" alt=""Skeleton-ized open area""></a></p>

<p>With following python codes:</p>

<pre><code>from skimage.morphology import skeletonize
from skimage import draw
from skimage.io import imread, imshow
from skimage.color import rgb2gray

# load image from file
img_fname=os.path.join('images','mall1_2F_schema.png') 
image=imread(img_fname)

# Change RGB color to gray 
image=rgb2gray(image)

# Change gray image to binary
image=np.where(image&gt;np.mean(image),1.0,0.0)

# perform skeletonization
skeleton = skeletonize(image)
</code></pre>

<p>Now I would like to extract the end points and cross points from the skeleton as nodes for Networkx graph object while calculating the distance between the neighboring nodes as edges for Networkx Graph object. Currently, I have to manually get the point coordinates and input to NetworkX object initialization process, do we have any smarter way to do everything automatically?</p>

<p>By the way, I found <a href=""https://stackoverflow.com/users/1811651/yaron-kahanovitch"">yaron kahanovitch</a>'s answer to <a href=""https://stackoverflow.com/questions/27648195/how-to-find-the-length-of-a-path-curve-in-a-picture#"">the question on stackoverflow</a> proposes similar approaches. However, no more realization suggestions was given, and I think NetworkX could be one approach.</p>

<p>Any suggestions from you are greatly appreciated.</p>
",,2016-12-07 03:36:32,How to extract skimage skeleton information to NetworkX nodes and edges in python for further advanced analysis,<python><image-processing><networkx><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
48979,41048953,2016-12-08 21:16:45,,"<p>My program generates some numpy array during the running process, I save them using</p>

<pre><code>   from skimage import io
   io.imsave(""img.tif"",imgs[0,0])
</code></pre>

<p>The <code>imgs</code> is of type <code>float32</code>, shape <code>(1,1,128,128)</code>; while the maximum value is <code>1.0</code> and minimum value is <code>0.0</code>. The image is correctly saved. However, when I open it, I got the following error message. What can the reason?</p>

<p><a href=""https://i.stack.imgur.com/68FIi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/68FIi.jpg"" alt=""enter image description here""></a></p>
",,2016-12-08 21:38:07,the saved image cannot be opened,<python><numpy><image-processing><scipy><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
49070,41135679,2016-12-14 05:47:46,,"<p><a href=""http://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.load_sift"" rel=""nofollow noreferrer"">The documentation for load_sift</a></p>

<p><code>from skimage import io
img = open('g.png')
rv = io.load_sift(img)</code></p>

<p>This code is not working. It seems that this is not how I'm supposed to open the image file.I could not understand the documentation.</p>
",,2016-12-15 20:30:45,Opening image files for load_sift in scikit-image,<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
49121,41311565,2016-12-24 08:13:57,,"<p>I am using skimage learn in python to extract HOG features from an image.</p>

<p>skimage.features.hog returns a tuple (fd,hog_arr) where fd stands for HOG feature descriptors and 'hog_arr' is for visual representation of HOG features. (This is what I understood after looking at <a href=""http://scikit-image.org/docs/dev/auto_examples/plot_hog.html"" rel=""nofollow noreferrer"">http://scikit-image.org/docs/dev/auto_examples/plot_hog.html</a>)</p>

<p>I want to apply HOG on the entire image and extract the specific local HOG features out of it. For example, if the image is a face, I want to apply HOG on face and extract the hog features corresponding to 'eye' of the facial image.</p>

<p>As fd variable stores the actual HOG features of the image, I am not understanding how to extract such specific HOG features corresponding to a specific location inside the image ( I am able to visualize the HOG features corresponding to the image through 'hog_arr' through cv2.imshow)</p>

<p>How to go about this? Any help?</p>
",2016-12-24 14:13:34,2016-12-24 14:13:34,Extracting local HOG features after applying to an image in Python,<python><image><opencv><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
49147,41312644,2016-12-24 10:56:56,,"<p>I'm new to Tensorflow and I'm trying to design a Loss Function to account for the difference in the 3-Channel Histogram of the Input image and the Output image.</p>

<p>I know the L1 loss in Tensorflow is like <code>tf.reduce_mean(tf.abs(Output- Input)</code> but don't know how to write it to compare Histograms.</p>

<p>Thanks...</p>
",2016-12-26 02:47:07,2016-12-26 02:47:07,Tensorflow - Histogram Loss Function,<python><opencv><tensorflow><histogram><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
49226,41550022,2017-01-09 14:20:12,,"<p>The following code gives me the error present in the title : </p>

<pre><code>from skimage.feature import peak_local_max
local_maxi = peak_local_max(imd,labels=iml, 
                            indices=False,num_peaks_per_label=2)
</code></pre>

<p>Where <code>imd</code> is a ""distance transformed image"" which was obtained with :</p>

<pre><code>from scipy import ndimage
imd = ndimage.distance_transform_edt(im) 
</code></pre>

<p><code>im</code> is the input binary image that I would like to later on segment with the watershed function of scikit-image. But to use this function properly, I first need to find the markers which will serve as the starting flooding points : that's what I'm trying to do with the 'peak_local_max' function.</p>

<p>Also, <code>iml</code> is the labeled version of <code>im</code>, that I got with :</p>

<pre><code>from skimage.measure import label
iml = label(im)
</code></pre>

<p>I don't know what I've been doing wrong. Also, I've noticed that, the function seems to totally ignore its <code>num_peaks</code> argument. For instance, when I do :</p>

<pre><code>local_maxi = peak_local_max(imd,labels=iml,
                            indices=True,num_peaks=1)
</code></pre>

<p>I always get the same number of peaks detected as when I set <code>num_peaks=500</code> or <code>num_peaks=np.inf</code>. What am I missing here please ?</p>
",2017-01-10 12:53:06,2017-01-10 16:03:44,skimage - TypeError: peak_local_max() got an unexpected keyword argument 'num_peaks_per_label',<python><image-processing><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
49245,41399193,2016-12-30 16:08:56,,"<p>I have two images: </p>

<p><a href=""https://i.stack.imgur.com/c0DcR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c0DcR.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/l3Hf7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3Hf7.png"" alt=""enter image description here""></a></p>

<p>Exuse the different resolution but that's not the point. On the left I have a ""large"" blob due to a camera reflection. I want to get rid of that blob, so closing the blob. But on the right I have smaller blobs that are valuable information that I need to keep. 
Both of these image need to undergo the same algorithm.
If I use a simple opening the smaller blobs will be gone, too. Is there an easy way to implement this in Python with skimage or/and PIL?</p>

<p>In a perfect world the left image should just create a white circle, where the right image should have the black dots within the white circle. It is okay to change the size of the black dots on the right image.</p>

<p>Here is an image which should describe the problem at the image directly <a href=""https://i.stack.imgur.com/kgoNL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kgoNL.png"" alt=""enter image description here""></a></p>
",2016-12-30 16:55:12,2017-03-11 20:42:21,Get rid of a large blob in a binary image but keep smaller blobs in python with PIL or skimage,<python><image-processing><python-imaging-library><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
49304,41482179,2017-01-05 10:08:49,,"<p>Fingerprint sensor (Persona) is being used to get a fingerprint image. I am trying to enhance this image. I am using OpenCV for this purpose. Here is my original image:</p>

<p><a href=""https://i.stack.imgur.com/gXOPp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gXOPp.jpg"" alt=""""></a></p>

<p>I have applied otsu transform on it and got this image:</p>

<p>Now I have applied Gabor filter from OpenCV on orientations of <code>0, 45, 90, 135</code>. I have got this result:</p>

<p>Here is my code in Python OpenCV for application of gabor filter:</p>

<pre><code>import numpy as np
import cv2

from matplotlib import pyplot as plt

//cv2.getGaborKernel(ksize, sigma, theta, lambda, gamma, psi, ktype)
// ksize - size of gabor filter (n, n)
// sigma - standard deviation of the gaussian function
// theta - orientation of the normal to the parallel stripes
// lambda - wavelength of the sunusoidal factor
// gamma - spatial aspect ratio
// psi - phase offset
// ktype - type and range of values that each pixel in the gabor kernel 
//canhold

g_kernel = cv2.getGaborKernel((25, 25), 6.0, np.pi/4, 8.0, 0.5, 0, ktype=cv2.CV_32F)
g_kernel1 = cv2.getGaborKernel((30, 30), 6.0, (3*np.pi)/4, 8.0, 0.5, 0, ktype=cv2.CV_32F)
g_kernel2 = cv2.getGaborKernel((30, 30),4 , 0, 8, 0.5, 0, ktype=cv2.CV_32F)
g_kernel3 = cv2.getGaborKernel((30, 30),4 , np.pi, 8, 0.5, 0, ktype=cv2.CV_32F)

print np.pi/4
img = cv2.imread('C:/Users/admin123/Desktop/p.png')
img1 = cv2.imread('C:/Users/admin123/Desktop/p.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)

// Otsu thresholding
ret2,img1 = cv2.threshold(img1,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
cv2.imshow('otsu', img1)

filtered_img = cv2.filter2D(img, cv2.CV_8UC3, g_kernel)
filtered_img1 = cv2.filter2D(img, cv2.CV_8UC3, g_kernel1)
filtered_img2 = cv2.filter2D(img, cv2.CV_8UC3, g_kernel2)
filtered_img3 = cv2.filter2D(img, cv2.CV_8UC3, g_kernel3)

cv2.imshow('0', filtered_img)
cv2.imshow('1', filtered_img1)
cv2.imshow('2', filtered_img2)
cv2.imshow('image', img)

cv2.addWeighted(filtered_img2,0.4,filtered_img1,0.8,0,img) #0 degree and 90
cv2.addWeighted(img,0.4,filtered_img,0.6,0,img) #0 degree and 90
cv2.addWeighted(img,0.4,filtered_img3,0.6,0,img)
cv2.addWeighted(img,0.4,img1,0.6,0.3,img)

cv2.imshow('per',img)

//threshold will convert it plain zero and white image
ret,thresh1 = cv2.threshold(img,150,255,cv2.THRESH_BINARY)#127 instead of 200

cv2.imshow('per1',thresh1)

h, w = g_kernel.shape[:2]
g_kernel = cv2.resize(g_kernel, (3*w, 3*h), interpolation=cv2.INTER_CUBIC)
g_kernel1 = cv2.resize(g_kernel1, (3*w, 3*h), interpolation=cv2.INTER_CUBIC)

cv2.imshow('gabor kernel (resized)', g_kernel)
cv2.imshow('gabor kernel1 (resized)', g_kernel1)

cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>

<p>I want robust fingerprint recognition. For this I want image of this level to get accurate Minutiae points:</p>

<p><a href=""https://i.stack.imgur.com/JnxBP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JnxBP.png"" alt=""enter image description here""></a></p>

<p>How can I get this much result from enhancement? What changes are required in code to get the enhanced result? </p>
",2017-01-05 13:40:19,2019-11-01 22:06:51,Fingerprint enhancement,<python-2.7><image-processing><computer-vision><opencv3.0><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
49551,41578473,2017-01-10 21:10:43,,"<p>I have segmented my image using the quickshift method found in the scikit image libary. How can I calculate the average color and the area of a superpixel? How can I interpret the return value of the quickshift() method? The documentation says the return value is ""Integer mask indicating segment labels"" but this is not clear for me. How can I make a boolean array in the shape of the original image, filled with ones, where the particular superpixel is present, in this presentation my life would be easier(I used to work with this kind of masks in OpenCV). Can you help me with this? My code (simplified example from scikit-image website):</p>

<pre><code>from skimage.data import astronaut
from skimage.segmentation import felzenszwalb, slic, quickshift
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

img = img_as_float(astronaut()[::2, ::2])
segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)

print(""Quickshift number of segments: %d"" % len(np.unique(segments_quick)))
plt.imshow(mark_boundaries(img, segments_quick))

plt.show()
</code></pre>
",,2019-09-01 15:15:34,How to calculate average color of a superpixel in scikit-image,<python><image-processing><computer-vision><scikit-image><superpixels>,,,CC BY-SA 3.0,True,False,False,True,False
49644,41674036,2017-01-16 10:16:49,,"<p>I am trying to preform a simple contrast stretch with python <code>skimage</code>, on the image opened with <code>gdal</code> as array of type <code>float32</code>. I first calculate the percentile with:</p>

<pre><code>p2, p98 = np.percentile(arrayF, (P1, P2))
</code></pre>

<p>and then try to perform the stretch with:</p>

<pre><code>img_rescale = exposure.rescale_intensity(arrayF, in_range=(p2, p98))
</code></pre>

<p>The returned image written to <code>.tiff</code> with GDAL contains only 'ones' and no data.</p>

<p>The cause of the problem might be in data range. For this arrayF it is between 0,0352989 and 1,03559. The script works fine when stretching the array with values 0 - 255.</p>

<p>Here is the function:</p>

<pre><code>def contrastStrecher(Raster1, p1, p2, OutDir, OutName1):
    fileNameR1 = Raster1
    P1 = p1
    P2 =p2

    outputPath = OutDir
    outputName = OutName1

    extension = os.path.splitext(fileNameR1)[1]

    raster1 = gdal.Open(fileNameR1, GA_ReadOnly)

    colsR1 =  raster1.RasterXSize
    rowsR1 =  raster1.RasterYSize
    bandsR1 = raster1.RasterCount
    driverR1 = raster1.GetDriver().ShortName

    geotransformR1 = raster1.GetGeoTransform()
    proj1 = raster1.GetProjection()

    bandF = raster1.GetRasterBand(1)
    nodataF = bandF.GetNoDataValue()
    newnodata = -1.
    arrayF = bandF.ReadAsArray().astype(""float32"")

    nodatamaskF = arrayF == nodataF

    arrayF[nodatamaskF] = newnodata

    p2, p98 = np.percentile(arrayF, (P1, P2))

    img_rescale = exposure.rescale_intensity(arrayF, in_range=(p2, p98))

    del arrayF

    img_rescale[nodatamaskF] = newnodata

    driver = gdal.GetDriverByName(driverR1)
    outraster = driver.Create(outputPath + outputName + extension, colsR1, rowsR1, 1, gdal.GDT_Float32)

    outraster.SetGeoTransform(geotransformR1)
    outraster.SetProjection(proj1)

    outband = outraster.GetRasterBand(1)
    outband.WriteArray(img_rescale)
    del img_rescale
    outband.FlushCache()
    outband.SetNoDataValue(newnodata)
    del outraster, outband
</code></pre>

<p>I figured out that value of <code>newnodata</code> interferes with the calculation. Previously I assigned a velue of <code>-9999.9</code> to it and the results were as described above. Now with <code>-1.</code> it seems that the function outputs correct results however I'm not entirely sure of that as the <code>nodata</code> or <code>newnodata</code> value should not be included in calculation.</p>

<p><a href=""https://i.stack.imgur.com/iTmgs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iTmgs.png"" alt=""Sample of the image:""></a></p>
",2017-01-19 18:31:09,2019-11-21 04:25:06,skimage - Contrast stretch using python,<python-2.7><image-processing><contrast><scikit-image>,,,CC BY-SA 3.0,False,False,False,True,False
49751,41682313,2017-01-16 17:55:48,,"<p>I need to do a face tracking with a sequence ID.
Example: John will be the first face with <code>ID=1</code>, Mark will be the second face to appears with <code>ID=2</code>, if John disappear and appear again in the video will be <code>ID=3</code>. I think that is simple, but I can't get anything like this to work.</p>

<p>I have this code to face recognition:</p>

<pre><code>import cv2
import sys

cascPath = sys.argv[1]
faceCascade = cv2.CascadeClassifier(cascPath)

video_capture = cv2.VideoCapture(0)

while True:
    ret, frame = video_capture.read()

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    faces = faceCascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.cv.CV_HAAR_SCALE_IMAGE
    )
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)


    cv2.imshow('Video', frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

video_capture.release()
cv2.destroyAllWindows()
</code></pre>

<p>But I don't know how to generate these ID's.</p>
",2017-01-16 20:01:55,2017-01-16 21:30:57,Face tracking with a sequence ID,<python><opencv><scikit-learn><scikit-image>,,,CC BY-SA 3.0,True,False,False,True,False
