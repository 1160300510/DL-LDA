{"number": 2637, "owner": "neldivad", "title": "SentimentIntensityAnalyzer() from nltk.sentiment.vader does not respond to hashtags. ", "body": "Example: \r\n```\r\nSentimentIntensityAnalyzer.polarity_scores( 'Strings with hashtag #stupid #useless #BAD' )  \r\n... compound: 0.0, \r\n... neg: 0.0, \r\n... neu: 1.0, \r\n... pos: 0.0,\r\n```"}
{"number": 2635, "owner": "prtpydv", "title": "Fix file handling of CrubadanCorpusReader", "body": "Fixes  [#2633](https://github.com/nltk/nltk/issues/2633)\r\n\r\nHello!\r\n## Pull request overview\r\nThis PR is about solving file handling issue in the CrubadanCorpusReader class. \r\n\r\nTwo non-public methods ( `_load_lang_mapping_data` and `_load_lang_ngrams`) in the CrubadanCorpusReader class open txt files in the Crubadan corpus without closing them. \r\nRunning some public methods of this class (like `crubadan_to_iso` ) that call either of these non-public methods, issues the following warning:\r\n```\r\nResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/user/nltk_data/corpora/crubadan/ab-3grams.txt' mode='r' encoding='utf-8'>\r\n```\r\nThis issue is reported in [#2633](https://github.com/nltk/nltk/issues/2633)\r\n## Bug\r\n### Why does this bug exist?\r\nThe two non-public methods `_load_lang_mapping_data` and `_load_lang_ngrams` open and use corpus files without closing them:\r\nhttps://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/crubadan.py#L68-81\r\nand \r\nhttps://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/crubadan.py#L83-105\r\n\r\n### How to reproduce?\r\n```python\r\nimport unittest\r\n\r\nfrom nltk.classify import textcat\r\nfrom nltk.corpus import crubadan\r\n\r\n\r\ndef predict_language(text):\r\n    cls = textcat.TextCat()\r\n    lang = cls.guess_language(text)\r\n    return crubadan.iso_to_crubadan(lang)\r\n\r\n\r\nclass Test(unittest.TestCase):\r\n    def test(self):\r\n        print(predict_language('test'))\r\n```\r\nRunning this unit test issues ResourceWarning.\r\n```\r\n$ python -m unittest nltk_test.py\r\n/Users/xxx/venv/lib/python3.7/site-packages/nltk/corpus/reader/crubadan.py:79: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/xxx/nltk_data/corpora/crubadan/table.txt' mode='r' encoding='utf-8'>\r\n  raw = open(mapper_file, \"r\", encoding=\"utf-8\").read().strip()\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n/Users/xxx/venv/lib/python3.7/site-packages/nltk/corpus/reader/crubadan.py:48: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/xxx/nltk_data/corpora/crubadan/ab-3grams.txt' mode='r' encoding='utf-8'>\r\n  self._all_lang_freq[lang] = self._load_lang_ngrams(lang)\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n/Users/xxx/venv/lib/python3.7/site-packages/nltk/corpus/reader/crubadan.py:48: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/xxx/nltk_data/corpora/crubadan/abn-3grams.txt' mode='r' encoding='utf-8'>\r\n  self._all_lang_freq[lang] = self._load_lang_ngrams(lang)\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n```\r\n\r\n### A fix\r\nWrapping the file handling statements under a `with` statement as follows ensures the accessed files are closed after use:\r\n```python\r\n    def _load_lang_mapping_data(self):\r\n        \"\"\" Load language mappings between codes and description from table.txt \"\"\"\r\n        if isinstance(self.root, ZipFilePathPointer):\r\n            raise RuntimeError(\r\n                \"Please install the 'crubadan' corpus first, use nltk.download()\"\r\n            )\r\n\r\n        mapper_file = path.join(self.root, self._LANG_MAPPER_FILE)\r\n        if self._LANG_MAPPER_FILE not in self.fileids():\r\n            raise RuntimeError(\"Could not find language mapper file: \" + mapper_file)\r\n\r\n        with open(mapper_file, \"r\", encoding=\"utf-8\") as raw:\r\n            strip_raw = raw.read().strip()\r\n\r\n            self._lang_mapping_data = [row.split(\"\\t\") for row in strip_raw.split(\"\\n\")]\r\n```\r\n```python\r\n    def _load_lang_ngrams(self, lang):\r\n        \"\"\" Load single n-gram language file given the ISO 639-3 language code\r\n            and return its FreqDist \"\"\"\r\n\r\n        if lang not in self.langs():\r\n            raise RuntimeError(\"Unsupported language.\")\r\n\r\n        crubadan_code = self.iso_to_crubadan(lang)\r\n        ngram_file = path.join(self.root, crubadan_code + \"-3grams.txt\")\r\n\r\n        if not path.isfile(ngram_file):\r\n            raise RuntimeError(\"No N-gram file found for requested language.\")\r\n\r\n        counts = FreqDist()\r\n        with open(ngram_file, \"r\", encoding=\"utf-8\") as f:\r\n            for line in f:\r\n                data = line.split(\" \")\r\n\r\n                ngram = data[1].strip(\"\\n\")\r\n                freq = int(data[0])\r\n\r\n                counts[ngram] = freq\r\n\r\n        return counts\r\n```\r\n"}
{"number": 2634, "owner": "stefanDeveloper", "title": "DRT is not correctly resolved with nltk.sem.drt.resolve_anaphore()", "body": "I created a small example to visualize my findings. If I set my discourse referent with character `e` like here:\r\n\r\n```\r\ndrs_a = dexpr(r'([a], [zzz(a)])')\r\ndrs_not_b = dexpr(r'([], [-([b],[yyy(b,a)])])')\r\ndrs_and = dexpr(r'([], [([c],[xxx(c), -([d],[www(d)])]) -> ([e], [e=Entity, ([f], [vvv(f)]) | ([g], [vvv(g), PRO(g)])])])')\r\ndrs = drs_a + drs_not_b + drs_and\r\nprint(drs.simplify().resolve_anaphora().pretty_format())\r\n```\r\n\r\nAnd simplifying and resolving my anaphoras, I get this following result:\r\n\r\n```\r\n ____________________________________________________________ \r\n| a                                                          |\r\n|------------------------------------------------------------|\r\n| zzz(a)                                                     |\r\n|      __________                                            |\r\n|     | b        |                                           |\r\n| __  |----------|                                           |\r\n|   | | yyy(b,a) |                                           |\r\n|     |__________|                                           |\r\n|   ________________      ________________________________   |\r\n|  | c              |    | e                              |  |\r\n| (|----------------| -> |--------------------------------|) |\r\n|  | xxx(c)         |    | (e = Entity)                   |  |\r\n|  |      ________  |    |   ________     _____________   |  |\r\n|  |     | d      | |    |  | f      |   | g           |  |  |\r\n|  | __  |--------| |    | (|--------| | |-------------|) |  |\r\n|  |   | | www(d) | |    |  | vvv(f) |   | vvv(g)      |  |  |\r\n|  |     |________| |    |  |________|   | (g = [a,c]) |  |  |\r\n|  |________________|    |               |_____________|  |  |\r\n|                        |________________________________|  |\r\n|____________________________________________________________|\r\n```\r\n\r\nSomehow, my `g` is missing my `e`. I wonder if `e` is a restricted character? I've checked [NLTK DRT How to](http://www.nltk.org/howto/drt.html) and couldn't find any information about it. Nevermind, after changing my discourse referent to `p`, like here:\r\n\r\n```\r\ndrs_and = dexpr(r'([], [([c],[xxx(c), -([d],[www(d)])]) -> ([p], [p=Entity, ([f], [vvv(f)]) | ([g], [vvv(g), PRO(g)])])])')\r\n```\r\n\r\nMy `g` has now my missing `p`:\r\n\r\n```\r\n ______________________________________________________________ \r\n| a                                                            |\r\n|--------------------------------------------------------------|\r\n| zzz(a)                                                       |\r\n|      __________                                              |\r\n|     | b        |                                             |\r\n| __  |----------|                                             |\r\n|   | | yyy(b,a) |                                             |\r\n|     |__________|                                             |\r\n|   ________________      __________________________________   |\r\n|  | c              |    | p                                |  |\r\n| (|----------------| -> |----------------------------------|) |\r\n|  | xxx(c)         |    | (p = Entity)                     |  |\r\n|  |      ________  |    |   ________     _______________   |  |\r\n|  |     | d      | |    |  | f      |   | g             |  |  |\r\n|  | __  |--------| |    | (|--------| | |---------------|) |  |\r\n|  |   | | www(d) | |    |  | vvv(f) |   | vvv(g)        |  |  |\r\n|  |     |________| |    |  |________|   | (g = [a,c,p]) |  |  |\r\n|  |________________|    |               |_______________|  |  |\r\n|                        |__________________________________|  |\r\n|______________________________________________________________|\r\n```\r\n\r\nMaybe I overlook something, why I can not use `e` as a discourse referent. Does someone have an idea why I get this behavior?\r\n\r\nBest regards\r\nStefan"}
{"number": 2633, "owner": "Wundark", "title": "CrubadanCorpusReader doesn't close open file handle", "body": "In the code below, for the function `_load_lang_ngrams`, a file is opened but is never closed.\r\n\r\nhttps://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/crubadan.py#L97\r\n\r\nThis raises a warning\r\n\r\n```\r\nResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/user/nltk_data/corpora/crubadan/ab-3grams.txt' mode='r' encoding='utf-8'>\r\n  self._all_lang_freq[lang] = self._load_lang_ngrams(lang)\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n```"}
{"number": 2631, "owner": "dannysepler", "title": "Can we delete CoreNLP test code? (It's currently skipped)", "body": "This originally was part of https://github.com/nltk/nltk/pull/2629, but it's probably best to wait and let folks chat about it.\r\n\r\n- https://github.com/nltk/nltk/pull/1942 first marked it as skipped, and here's the relevant comment: `skipped all corenlp tests since we're dropping official support after stanford release their official python wrapper`\r\n- Original implementation: https://github.com/nltk/nltk/pull/1249\r\n\r\ncc @alvations @dimazest "}
{"number": 2630, "owner": "dannysepler", "title": "Minor improvements to the CI build", "body": "One of @iliakur's comments on my original diff, which I really liked.\r\n\r\nThe alternative is to add this to the `pytest.ini` so that tests always run parallel -- but I don't like the experience of `pytest path/to/file.py` still running parallel."}
{"number": 2629, "owner": "dannysepler", "title": "Fully remove nose in favor of pytest", "body": "The most immediate followup of https://github.com/nltk/nltk/pull/2595 is to stop using nose in the repo entirely.\r\n\r\nAlso cleaned up:\r\n- A few pytest warnings (there's a few more)\r\n- A few random test code things (like unneeded / unreachable code)"}
{"number": 2628, "owner": "kovvalsky", "title": "Feature-based CFG overgenerates (apparently ignores features)", "body": "**MWE**\r\n\r\n```\r\nfrom nltk.parse.generate import generate\r\nfrom nltk.grammar import FeatStructNonterminal, FeatureGrammar\r\n\r\ngrammar = '''\r\n## NLTK-style feature-based CFG\r\n\r\n% start NP\r\n\r\n#####################\r\n# Grammar Rules\r\n\r\nNP[NUM=?n] -> DT[NUM=?n] N[NUM=?n]\r\n\r\n# ###################\r\n# Lexical Rules\r\n\r\nDT[NUM=sg]  -> 'this'\r\nDT[NUM=pl]  -> 'these'\r\n\r\nN[NUM=sg] -> 'dog'\r\nN[NUM=pl] -> 'dogs'\r\n'''\r\n\r\ngr = FeatureGrammar.fromstring(grammar) \r\n\r\nfor s in generate(gr, n=200):\r\n    print(s)\r\n```\r\n\r\nOutputs all 4 possibilities instead of only two grammatical NPs.\r\n```\r\n['this', 'dog']\r\n['this', 'dogs'] # this should be blocked\r\n['these', 'dog'] # this should be blocked\r\n['these', 'dogs']\r\n```\r\n\r\nIt seems the features are not taken into account during generation.\r\nThough parsing does take features into account.\r\n\r\nOne workaround is to generate and then parse.\r\n\r\nMaybe at least mention in http://www.nltk.org/howto or https://www.nltk.org/book/ch09.html that generation ignores features.\r\n\r\n"}
{"number": 2626, "owner": "manujosephv", "title": "Fixing Witten-Bell", "body": "@iliakur \r\n\r\nThe Witten-Bell smoothing is a recursive interpolation method, which takes the below form:\r\n\r\n![image](https://user-images.githubusercontent.com/10508493/99868793-be99ee00-2beb-11eb-9f69-defbde6c081f.png)\r\n\r\n- [Source](http://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x207.html)\r\n- Page 42 [Source](https://www.statmt.org/book/slides/07-language-models.pdf)\r\n- Page 2[Source](https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe6.pdf)\r\n- Page 24[Source](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)"}
{"number": 2623, "owner": "AmenRa", "title": "Potential bug in Porter stemmer", "body": "Hi, I accidentally noticed that at line [504](https://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/stem/porter.py#L504) of porter.py there is a potential bug:\r\n```python\r\nrules.append(\r\n    (\"logi\", \"log\", lambda stem: self._has_positive_measure(word[:-3]))\r\n)\r\n```\r\nThe `lambda` function's body refers to `word` and not to `stem`.\r\nIs it correct or should it refer to `stem` instead?\r\n"}
{"number": 2622, "owner": "ermik", "title": "[wiki] SENNA binary link is outdated", "body": "On this page: \r\nhttps://github.com/nltk/nltk/wiki/Installing-Third-Party-Software \r\n\r\nThe link to SENNA toolkit is no longer accessible. It was most likely taken from [here](https://www.nec-labs.com/research-departments/machine-learning/machine-learning-software/Senna) where it also leads nowhere.\r\n\r\nThe only other place I found this distribution author's website: https://ronan.collobert.com/senna/"}
{"number": 2614, "owner": "anasselhoud", "title": "Raising a non implemented error", "body": "Hello,\r\n\r\nI was implementing some code for text analysis. Created the following class (everything is well defined):\r\n\r\n```\r\nclass NGramTagChunker(ChunkParserI):\r\n  def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\r\n    train_sent_tags=conll_tag_chunks(train_sentences)\r\n    self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)\r\ndef parse(self,tagged_sentence):\r\n    if not tagged_sentence:\r\n      return None\r\n    pos_tags=[tag for word, tag in tagged_sentence]\r\n    chunk_pos_tags=self.chunk_tagger.tag(pos_tags)\r\n    chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\r\n    wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]\r\n    return conlltags2tree(wpc_tags)\r\n\r\n#train chunker model\r\nntc=NGramTagChunker(train_data)\r\n#evaluate chunker model performance\r\nprint(ntc.evaluate(test_data))\r\n```\r\n\r\nThe problem is specifically in the last line of the code (ntc.evaluate). It raises a not implemented error as follow:\r\n```\r\n     32         :rtype: Tree\r\n     33         \"\"\"\r\n---> 34         raise NotImplementedError()\r\n     35 \r\n     36     def evaluate(self, gold):\r\n\r\nNotImplementedError: \r\n```"}
{"number": 2612, "owner": "ekaf", "title": "Fixes issue #2314 (Infinite loop in Wordnet closure and tree)", "body": "This fixes issue #2314 (Infinite loop in Wordnet closure and tree)\r\n\r\n## 1. The Problem\r\n\r\nThe origin of the bugs mentioned in issue #2314  is that the \"nltk_data/corpora/wordnet/data.verb\" database file contains two endless cycles: one hyponyms() cycle between 'restrain.v.01' and 'inhibit.v.04', and one topic_domains() cycle between 'computer.n.01' and 'computer_science.n.01'.\r\n\r\nAs a consequence of these cycles in the database, the closure() function in the WordNet module loops forever on the hyponyms of both 'restrain.v.01' and 'inhibit.v.04', and the topic_domains of both 'computer.n.01' and 'computer_science.n.01', these combinations yielding 4 failing calls:\r\n\r\n```\r\n>>> wn=nltk.corpus.wordnet\r\n>>> print(list(wn.synset('restrain.v.01').closure(lambda s:s.hyponyms())))\r\n>>> print(list(wn.synset('inhibit.v.04').closure(lambda s:s.hyponyms())))\r\n>>> print(list(wn.synset('computer.n.01').closure(lambda s:s.topic_domains())))\r\n>>> print(list(wn.synset('computer_science.n.01').closure(lambda s:s.topic_domains())))\r\n```\r\nLikewise, the tree() function throws a RecursionError for any of the 4 corresponding calls. \r\n\r\n```\r\n>>> from pprint import pprint\r\n>>> pprint(wn.synset('restrain.v.01').tree(lambda s:s.hyponyms()))\r\n>>> pprint(wn.synset('inhibit.v.04').tree(lambda s:s.hyponyms()))\r\n>>> pprint(wn.synset('computer.n.01').tree(lambda s:s.topic_domains()))\r\n>>> pprint(wn.synset('computer_science.n.01').tree(lambda s:s.topic_domains()))\r\n```\r\n\r\nSo 8 calls fail in total, in addition to a few thousand symmetric links from the verb_groups, attributes and also_sees relations.\r\n\r\nOne solution is to patch the database (as was already done in the past), but this does not prevent similar bugs in future db versions.\r\n\r\nPR #2610 by @CubieDev proposes to handle the errors entirely within the WordNet module.\r\n\r\nOn the other hand, the present PR proposes to handle the problem more generally in nltk/util.py, considering that the source of the problem with the current \"closure\" function is that it calls a \"breadth_first\" function imported from nltk.util, where the comment (\"No need to check for cycles\") is unfortunate, since there really is a need to check for cycles, in order to avoid eventual endless loops. Cyclicity  also occurs in other graphs than Wordnet, so a general-purpose solution outside of the Wordnet module could be useful to check other graphs as well. \r\n\r\nAlso, due to the potential severity of this bug, it is preferrable to raise a warning than just to ignore the cycles silently.\r\n\r\n## 2. Fixing Closure\r\n\r\nThis fix implements an \"acyclic_breadth_first\" function in nltk/util.py, that detects and discards cycles. It is just a clone of the already existing \"breadth_first\" function, with a few additional lines to handle the cycles. Then wordnet.py needs only a simple wrapper to implement relation closure:\r\n\r\n```\r\n    def closure(self, rel, depth=-1):\r\n        \"\"\"\r\n        Return the transitive closure of source under the rel\r\n        relationship, breadth-first, discarding cycles:\r\n\r\n        >>> from nltk.corpus import wordnet as wn\r\n        >>> computer = wn.synset('computer.n.01')\r\n        >>> topic = lambda s:s.topic_domains()\r\n        >>> print(list(computer.closure(topic)))\r\n        [Synset('computer_science.n.01')]\r\n\r\n        UserWarning: Discarded redundant search ([(Synset('computer.n.01'), 2)])\r\n\r\n\r\n        Include redundant pathes (but only once), avoiding duplicate searches\r\n        (from 'animal.n.01' to 'entity.n.01'):\r\n\r\n        >>> dog = wn.synset('dog.n.01')\r\n        >>> hyp = lambda s:s.hypernyms()\r\n        >>> print(list(dog.closure(hyp)))\r\n        [Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'),\r\n        Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'),\r\n        Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'),\r\n        Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'),\r\n        Synset('physical_entity.n.01'), Synset('entity.n.01')]\r\n\r\n        UserWarning: Discarded redundant search ([(Synset('animal.n.01'), 7)])\r\n        \"\"\"\r\n\r\n        from nltk.util import acyclic_breadth_first\r\n        for synset in acyclic_breadth_first(self, rel, depth):\r\n            if synset != self:\r\n                yield synset\r\n\r\n```\r\n\r\nThe current implementation of Wordnet closure does not include the caller synset in the output (unlike the 'tree' fumction), and this behaviour is maintained here.\r\n\r\nPlease note the  duplicate path (from 'animal.n.01' to 'entity.n.01') in the hypernyms of 'dog.n.01', which has not received much attention previously. Pruning this path indiscriminately would be incorrect, since it occurs legitimately in two different branches of the tree, so we want to keep both instances of it in the output of \"tree\" (but search it just once in \"closure\").\r\n\r\n## 3. Fixing Tree\r\n\r\nMaybe the case is weaker for implementing an \"acyclic_depth_first\" search function that detects cycles in nltk/util.py,\r\nand the proposal in PR #2610 by @CubieDev to keep it within the WordNet module is a good alternative, although the coding style should be sufficiently explicit, in order to be able to report the eventual cycles, instead of ignoring them silently.\r\n\r\nIn the present PR, the Wordnet \"tree\" wrapper can simply be:\r\n\r\n```\r\n    def tree(self, rel, depth=-1, cut_mark=None):\r\n        \"\"\"\r\n        Return the full relation tree, including self,\r\n        discarding cycles:\r\n\r\n        >>> from nltk.corpus import wordnet as wn\r\n        >>> from pprint import pprint\r\n        >>> computer = wn.synset('computer.n.01')\r\n        >>> topic = lambda s:s.topic_domains()\r\n        >>> pprint(computer.tree(topic))\r\n        [Synset('computer.n.01'), [Synset('computer_science.n.01')]]\r\n\r\n        UserWarning: Discarded redundant search ([(Synset('computer.n.01'), -3)])\r\n\r\n\r\n        But keep duplicate branches (from 'animal.n.01' to 'entity.n.01'):\r\n\r\n        >>> dog = wn.synset('dog.n.01')\r\n        >>> hyp = lambda s:s.hypernyms()\r\n        >>> pprint(dog.tree(hyp))\r\n        [Synset('dog.n.01'),\r\n         [Synset('canine.n.02'),\r\n          [Synset('carnivore.n.01'),\r\n           [Synset('placental.n.01'),\r\n            [Synset('mammal.n.01'),\r\n             [Synset('vertebrate.n.01'),\r\n              [Synset('chordate.n.01'),\r\n               [Synset('animal.n.01'),\r\n                [Synset('organism.n.01'),\r\n                 [Synset('living_thing.n.01'),\r\n                  [Synset('whole.n.02'),\r\n                   [Synset('object.n.01'),\r\n                    [Synset('physical_entity.n.01'),\r\n                     [Synset('entity.n.01')]]]]]]]]]]]]],\r\n         [Synset('domestic_animal.n.01'),\r\n          [Synset('animal.n.01'),\r\n           [Synset('organism.n.01'),\r\n            [Synset('living_thing.n.01'),\r\n             [Synset('whole.n.02'),\r\n              [Synset('object.n.01'),\r\n               [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]\r\n        \"\"\"\r\n\r\n        from nltk.util import acyclic_branches_depth_first\r\n        return acyclic_branches_depth_first(self, rel, depth, cut_mark)\r\n```\r\n\r\nThe new function acyclic_branches_depth_first() traverses the nodes of a tree in depth-first order, discarding eventual cycles within the same branch, but keeping duplicate pathes in different branches. The branches are kept separate by recursing with a different \"traversed\" set for each child.\r\n\r\n## 4. Tree-size  Limitation\r\n\r\nBut the worst problem by far, is the also_sees() relation (2692 links between Synsets), which has both 2490 symmetric links, and 202 non-symmetric links that involve Synsets that also have at least a symmetric link. This combination is particularly toxic, because it produces a huge Strongly Connected Component (SCC), counting 758 synsets, where any member of the SCC is transitively connected to all other members. I am not sure how to estimate the size of such a tree, but only guessing that it could approximately be the average number of children per node, raised to the 758 power. Then the full also_sees() tree, starting at any member of this SCC, for ex. 'concrete.a.01', could not fit on a single computer, no matter how efficiently trees are represented or computed.\r\n\r\nSo the proposed approach obviously cannot handle such harder cases, where the trees grow to an unmanageable size, and  the computation only terminates in reasonable time if the \"depth\" parameter is limited:\r\n\r\n```\r\n>>> import nltk\r\n>>> wn=nltk.corpus.wordnet\r\n>>> from pprint import pprint\r\n>>> pprint(wn.synset('certified.a.01').tree(lambda s:s.also_sees(),cut_mark='...',depth=4))\r\n[Synset('certified.a.01'),\r\n [Synset('authorized.a.01'),\r\n  [Synset('lawful.a.01'),\r\n   [Synset('legal.a.01'),\r\n    \"Cycle(Synset('lawful.a.01'),0,...)\",\r\n    [Synset('legitimate.a.01'), '...']],\r\n   [Synset('straight.a.06'),\r\n    [Synset('honest.a.01'), '...'],\r\n    \"Cycle(Synset('lawful.a.01'),0,...)\"]],\r\n  [Synset('legitimate.a.01'),\r\n   \"Cycle(Synset('authorized.a.01'),1,...)\",\r\n   [Synset('legal.a.01'),\r\n    [Synset('lawful.a.01'), '...'],\r\n    \"Cycle(Synset('legitimate.a.01'),0,...)\"],\r\n   [Synset('valid.a.01'),\r\n    \"Cycle(Synset('legitimate.a.01'),0,...)\",\r\n    [Synset('reasonable.a.01'), '...']]],\r\n  [Synset('official.a.01'), \"Cycle(Synset('authorized.a.01'),1,...)\"]],\r\n [Synset('documented.a.01')]]\r\n```\r\n\r\nTo handle such cases, global pruning is needed, which is now implemented in a new \"acyclic_tree\" function. This uses depth first search to compute an arbitrary spanning tree. Alternatively, breadth-first search could compute a minimum spanning tree (MST), which may not necessarily be worthwile, since it would require some overhead in order to assemble a tree structure. The proposed solution can cope with all cycles, but has the drawback that it truncates some legitimate duplicate pathes:\r\n\r\n```\r\n>>> import nltk\r\n>>> wn=nltk.corpus.wordnet\r\n>>> from pprint import pprint\r\n>>> pprint(wn.synset('dog.n.01').acyclic_tree(lambda s:s.hypernyms(),cut_mark='...'))\r\n[Synset('dog.n.01'),\r\n [Synset('canine.n.02'),\r\n  [Synset('carnivore.n.01'),\r\n   [Synset('placental.n.01'),\r\n    [Synset('mammal.n.01'),\r\n     [Synset('vertebrate.n.01'),\r\n      [Synset('chordate.n.01'),\r\n       [Synset('animal.n.01'),\r\n        [Synset('organism.n.01'),\r\n         [Synset('living_thing.n.01'),\r\n          [Synset('whole.n.02'),\r\n           [Synset('object.n.01'),\r\n            [Synset('physical_entity.n.01'),\r\n             [Synset('entity.n.01')]]]]]]]]]]]]],\r\n [Synset('domestic_animal.n.01'), \"Cycle(Synset('animal.n.01'),-3,...)\"]]\r\n```\r\n"}
{"number": 2611, "owner": "mchanitlu", "title": "download issue of nltk", "body": "pip3 install nltk\r\nProcessing /Users/chanlu/Library/Caches/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155/nltk-3.5-py3-none-any.whl\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (0.17.0)\r\nCollecting regex\r\n  Using cached regex-2020.9.27.tar.gz (690 kB)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.50.2)\r\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (7.1.2)\r\nBuilding wheels for collected packages: regex\r\n  Building wheel for regex (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-wheel-r8jsu98t\r\n       cwd: /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/\r\n  Complete output (17 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.macosx-10.15-x86_64-3.8\r\n  creating build/lib.macosx-10.15-x86_64-3.8/regex\r\n  copying regex_3/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n  copying regex_3/regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n  copying regex_3/_regex_core.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n  copying regex_3/test_regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n  running build_ext\r\n  building 'regex._regex' extension\r\n  creating build/temp.macosx-10.15-x86_64-3.8\r\n  creating build/temp.macosx-10.15-x86_64-3.8/regex_3\r\n  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c regex_3/_regex.c -o build/temp.macosx-10.15-x86_64-3.8/regex_3/_regex.o\r\n  xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\r\n  error: command 'clang' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for regex\r\n  Running setup.py clean for regex\r\nFailed to build regex\r\nInstalling collected packages: regex, nltk\r\n    Running setup.py install for regex ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-record-04g_xr2i/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/regex\r\n         cwd: /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/\r\n    Complete output (17 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.macosx-10.15-x86_64-3.8\r\n    creating build/lib.macosx-10.15-x86_64-3.8/regex\r\n    copying regex_3/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n    copying regex_3/regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n    copying regex_3/_regex_core.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n    copying regex_3/test_regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex\r\n    running build_ext\r\n    building 'regex._regex' extension\r\n    creating build/temp.macosx-10.15-x86_64-3.8\r\n    creating build/temp.macosx-10.15-x86_64-3.8/regex_3\r\n    clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c regex_3/_regex.c -o build/temp.macosx-10.15-x86_64-3.8/regex_3/_regex.o\r\n    xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\r\n    error: command 'clang' failed with exit status 1\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-record-04g_xr2i/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/regex Check the logs for full command output.\r\n\r\n*****I don't know why I cannot download nltk. Thank you for your help***"}
{"number": 2610, "owner": "CubieDev", "title": "Wordnet infinite looping for closure and tree", "body": "Fixes #2314, Fixes #2565.\r\n\r\nHello!\r\n\r\n---\r\n## Pull request overview\r\nThis PR is about solving infinite loops from wordnet. In particular:\r\n- Remove cycles from `closure`\r\n- Remove infinite loops from `tree`\r\n- Added tests from #2314 and #2565 that failed previously.\r\n\r\n@alvations I know you are working on moving Wordnet to a separate repository. I checked, and these issues exist there too. As such, you might be interested in these changes too. If you'd like, I can add a PR there too.\r\n\r\n---\r\n\r\n## Bug 1: Infinite cycle in closure\r\n### How to reproduce:\r\n\r\n```python\r\nlist(wn.synset('restrain.v.01').closure(lambda s:s.hyponyms()))\r\n```\r\nThis will never terminate. \r\n\r\n### Why does this bug exist?\r\nLet's look at the implementation of `closure`:\r\nhttps://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/corpus/reader/wordnet.py#L550-L574\r\n\r\nFrom lines 571 and 572 it is clear that we want to avoid duplicates. However, the used `breadth_first` implementation says [here](https://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/util.py#L215) that it does not check for cycles.\r\nSo, if a `rel` is passed to `closure` that is cyclic starting from the given `Synset`, then the loop using `breadth_first` on line 570 will loop forever. After all elements of the loop have been yielded once, lines 571 or 572 will prevent these repeats from being yielded, but the loop will go on. That is why there is non-termination in `closure`.\r\n\r\n### A fix\r\nI implemented a version of BFS very similar to `breadth_first`, but one that discards repeats. The implementation is pretty straightforward, using a deque. The deque is popped left, extracting the first synset and depth. If this synset isn't the current or some previous synset, we yield it. Then, if we have reached the desired max depth, we don't extend the deque. Otherwise, we do. The deque is extended by the outputs of `rel(synset)`, but for each output is checked whether it has been yielded before.\r\n\r\n---\r\n\r\n## Bug 2: Infinite cycle in tree\r\n### How to reproduce:\r\n\r\n```python\r\nlist(wn.synset('restrain.v.01').tree(lambda s:s.hyponyms()))\r\n```\r\nThis will throw a RecursionError.\r\n\r\n### Why does this bug exist?\r\nLet's look at the implementation of `tree`:\r\nhttps://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/corpus/reader/wordnet.py#L756-L791\r\n\r\nOn line 788 we recurse using outputs of `rel(self)`. There is no check for cycles at all. So, we need to check whether synsets have already appeared higher up on the same branch. As @ekaf mentioned in his comment [here](https://github.com/nltk/nltk/issues/2314#issuecomment-702528702), we cannot simply prune a branch if a synset is found that has been placed anywhere in the tree, as we will truncate the non-cyclic result of the following line improperly:\r\n```python\r\nwn.synset('dog.n.01').tree(lambda s: s.hypernyms())\r\n```\r\nShould result in:\r\n```python\r\n    [Synset('dog.n.01'),\r\n     [Synset('canine.n.02'),\r\n      [Synset('carnivore.n.01'),\r\n       [Synset('placental.n.01'),\r\n        [Synset('mammal.n.01'),\r\n         [Synset('vertebrate.n.01'),\r\n          [Synset('chordate.n.01'),\r\n           [Synset('animal.n.01'),\r\n            [Synset('organism.n.01'),\r\n             [Synset('living_thing.n.01'),\r\n              [Synset('whole.n.02'),\r\n               [Synset('object.n.01'),\r\n                [Synset('physical_entity.n.01'),\r\n                 [Synset('entity.n.01')]]]]]]]]]]]]],\r\n     [Synset('domestic_animal.n.01'),\r\n      [Synset('animal.n.01'),\r\n       [Synset('organism.n.01'),\r\n        [Synset('living_thing.n.01'),\r\n         [Synset('whole.n.02'),\r\n          [Synset('object.n.01'),\r\n           [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]\r\n```\r\nbut would then result in:\r\n```python\r\n    [Synset('dog.n.01'),\r\n     [Synset('canine.n.02'),\r\n      [Synset('carnivore.n.01'),\r\n       [Synset('placental.n.01'),\r\n        [Synset('mammal.n.01'),\r\n         [Synset('vertebrate.n.01'),\r\n          [Synset('chordate.n.01'),\r\n           [Synset('animal.n.01'),\r\n            [Synset('organism.n.01'),\r\n             [Synset('living_thing.n.01'),\r\n              [Synset('whole.n.02'),\r\n               [Synset('object.n.01'),\r\n                [Synset('physical_entity.n.01'),\r\n                 [Synset('entity.n.01')]]]]]]]]]]]]],\r\n     [Synset('domestic_animal.n.01'),\r\n      [Synset('animal.n.01')]]]\r\n```\r\n\r\n### A fix\r\nSo, we need to prune only if a synset is found that has already been placed on the **same** branch in the tree. We can easily do this by adding another parameter `traverse`, which is a set of already traversed synsets on the same branch. Then, we can recurse with\r\n```python\r\ntree += [x.tree(rel, depth - 1, cut_mark, {*traversed, x}) for x in rel(self) if x not in traversed]\r\n```\r\ninstead of \r\n```python\r\ntree += [x.tree(rel, depth - 1, cut_mark) for x in rel(self)]\r\n```\r\n(Note for those unaware, `{*traversed, x}` creates a new set with the contents of `traversed` and `x`)\r\n\r\nBecause `traversed` is an implementation detail, I hid it by moving it to `_tree`, which only exists to be called by `tree` and by itself recursively.\r\n\r\n---\r\n\r\n## Tests\r\nI added 5 tests, 3 of which failed previously, and 2 of which failed at some point during my development process.\r\nThe 3 tests are from comments in #2314, and one is a duplicate from #2565. These issues can be closed if this is merged and passes the tests.\r\n\r\n---\r\n\r\nThanks to @ekaf for his many helpful comments and inspiration in #2314. \r\n\r\n- Tom Aarsen\r\n"}
{"number": 2609, "owner": "comidan", "title": "Possible issue while parsing a specific sentence with EarleyChartParser and InsideChartParser", "body": "While trying to parse a specific sentence from the Treebank corpus these two parsers behave strangely.\r\nThis is the sentence from treebank:\r\n`['A', 'form', 'of', 'asbestos', 'once', 'used', '*', '*', 'to', 'make', 'Kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', '*', 'to', 'it', 'more', 'than', '30', 'years', 'ago', ',', 'researchers', 'reported', '0', '*T*-1', '.']`\r\n\r\nThe EarleyChartParser, while using a simple CFG grammar, will produce a very big forest of 24284 trees finding the correct one.\r\nInstead InsideChartParser seems to go in an infinite loop while sorting a queue, one of its internal data structures.\r\n\r\nFor the parser of the PCFG I am using this function:\r\n\r\n```python\r\ndef pchart(parser, sentence, gold_tree): #here the parser is a InsideChartParser\r\n    test_trees = list(parser.parse(sentence))\r\n    print(\"PCHART PARSER - TREES FOUND: \", len(test_trees))\r\n    best_prob = 0.0\r\n    for idx, test_tree in enumerate(test_trees):\r\n        print(\"TREE: %d\" %idx)\r\n        print(test_tree)\r\n        curr_prob = test_tree.prob()\r\n        if curr_prob > best_prob:\r\n            best_prob = curr_prob\r\n            best_tree = test_tree\r\n        if test_tree.productions() == gold_tree.productions(): #check if the tree iterated now is the correct one\r\n            print(\"CORRECT TREE\")\r\n        else:\r\n            print(\"WRONG TREE\")\r\n    return best_tree\r\n```\r\n\r\nWhile for the Early parser I am using this one:\r\n\r\n```python\r\ndef earley(parser, sentence, gold_tree): #here the parser is a EarleyChartParser\r\n    test_trees = list(parser.parse(sentence)) #creates a forest of trees, with every trees being able to parse that sentence\r\n    print(\"EARLEY PARSER - TREES FOUND: \", len(test_trees))\r\n    for idx, test_tree in enumerate(test_trees):\r\n        print(\"TREE: %d\" %idx)\r\n        print(test_tree)\r\n        if test_tree.productions() == gold_tree.productions():\r\n            print(\"CORRECT TREE\")\r\n        else:\r\n            print(\"WRONG TREE\")`\r\n```\r\n\r\nAfter enumerating those lots of trees Early stops and the InsideChartParser starts but goes in a very long loop, I think due to the enormous amount of trees or productions at this point, and does not terminate any time soon:\r\n\r\n```Traceback (most recent call last)\r\n<ipython-input-9-641b936c1306> in <module>\r\n     24 \r\n     25 if __name__ == '__main__':\r\n---> 26     main()\r\n\r\n<ipython-input-9-641b936c1306> in main()\r\n     18     gold_tree = treebank.parsed_sents()[3]\r\n     19     earley(cfg_earley_parser, sentence, gold_tree) #run earley parser and compare it with gold tree and shows the various trees\r\n---> 20     tree = pchart(pcfg_pchart_parser, sentence, gold_tree) #shows the possible trees and returns the one with maximum probability\r\n     21     print(\"BEST TREE WITH PROBABILITY: %.12e\" %tree.prob())\r\n     22     tree.draw()\r\n\r\n<ipython-input-8-5e94e1748d29> in pchart(parser, sentence, gold_tree)\r\n      1 def pchart(parser, sentence, gold_tree):\r\n----> 2     test_trees = list(parser.parse(sentence))\r\n      3     print(\"PCHART PARSER - TREES FOUND: \", len(test_trees))\r\n      4 \r\n      5     best_prob = 0.0\r\n\r\nF:\\Development\\anaconda3\\lib\\site-packages\\nltk\\parse\\pchart.py in parse(self, tokens)\r\n    244         while len(queue) > 0:\r\n    245             # Re-sort the queue.\r\n--> 246             self.sort_queue(queue, chart)\r\n    247 \r\n    248             # Prune the queue to the correct size if a beam was defined\r\n\r\nF:\\Development\\anaconda3\\lib\\site-packages\\nltk\\parse\\pchart.py in sort_queue(self, queue, chart)\r\n    359         :rtype: None\r\n    360         \"\"\"\r\n--> 361         queue.sort(key=lambda edge: edge.prob())\r\n    362 \r\n    363 \r\n\r\nF:\\Development\\anaconda3\\lib\\site-packages\\nltk\\parse\\pchart.py in <lambda>(edge)\r\n    359         :rtype: None\r\n    360         \"\"\"\r\n--> 361         queue.sort(key=lambda edge: edge.prob())\r\n    362 \r\n    363\r\n```\r\n\r\nFor a grammar so simple, is it a right behaviour?\r\nBecause this does not happen for other sentences I used from the treebank corpus.\r\n\r\nThank you for your time!"}
{"number": 2608, "owner": "CubieDev", "title": "Fixed incorrect MacIntyre contraction", "body": "Fixes #2607.\r\n\r\nHello!\r\n\r\n---\r\n\r\n### Pull Request overview\r\nA MacIntyre contraction used for NLTKWordTokenizer and TreebankWordTokenizer was incorrectly adapted from the original. I modified two regexes that solve this issue, but this change has some other (potentially positive, potentially negative) consequences for the NLTKWordTokenizer. (In short, the NLTKWordTokenizer looks slightly more like TreebankWordTokenizer for specific cases ending with `'n`)\r\n\r\n---\r\n\r\n### Intro\r\nAs I described in https://github.com/nltk/nltk/issues/2607#issuecomment-701323755, and as @nmfzone pointed out, NLTKWordTokenizer and TreebankWordTokenizer incorrectly handle `more'n`. According to the original tokenizer by Robert MacIntyre, `more'n` should be split into `more` and `'n`. However, it was split into `more`, `'` and `n`. Read that issue and the comments before continuing to read this PR.\r\n\r\nIn case you're interested, this is the original `tokenizer.sed` by Robert MacIntyre here (converted to .txt so I could upload it here):\r\n[tokenizer.sed.txt](https://github.com/nltk/nltk/files/5305250/tokenizer.sed.txt)\r\n\r\n### The issue\r\nThe contraction that handles `more'n` accidentally handles `mor'n` instead (line 25): \r\nhttps://github.com/nltk/nltk/blob/5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4/nltk/tokenize/destructive.py#L13-L29\r\n\r\nI have resolved this issue by making the following change:\r\n```python\r\nr\"(?i)\\b(more)(?#X)('n)\\b\",\r\n            ^\r\n```\r\n\r\nHowever, though this solved the issue for the TreebankWordTokenizer, the issue persisted in NLTKWordTokenizer.\r\nThis is because of line 49 here:\r\nhttps://github.com/nltk/nltk/blob/5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4/nltk/tokenize/destructive.py#L44-L50\r\n\r\nThis regex will place a space inbetween any `'` and what comes behind, *unless* what comes behind is whitespace or any item in the following list: `re`, `ve`, `ll`, `m`, `t`, `s` and `d`. So, `would've` or `she'll` stay the same, so that they can be handled by `ENDING_QUOTES`. However, the `n` from `more'n` is not in that list, and is converted to `more' n`. Then, one of the `PUNCTUATION` regexes will convert that further to `more ' n`, and the contraction regex will no longer catch it to convert it to `more` and `'n`. \r\nOne fix for this is adding `n` to the list of characters for which to not add spaces. This is what I've done. This has some consequences for inputs such as `\"word'n\"`:\r\n\r\n### Consequences for NLTKWordTokenizer\r\nOld: \r\n```python\r\n>>> from nltk.tokenize import word_tokenize\r\n>>> word_tokenize(\"It's more'n enough.\")\r\n['It', \"'s\", 'more', \"'\", 'n', 'enough', '.'] # wrong, `'` and `n` should be grouped together\r\n>>> word_tokenize(\"It's ore'n enough.\")\r\n['It', \"'s\", 'mor', \"'\", 'n', 'enough', '.'] # different from new, debatably better\r\n>>> word_tokenize(\"It's wrong'n enough.\")\r\n['It', \"'s\", 'wrong', \"'\", 'n', 'enough', '.'] # different from new, debatably better\r\n>>> word_tokenize(\"It's 'n enough.\")\r\n['It', \"'s\", \"'\", 'n', 'enough', '.'] # different from new, debatably better\r\n>>> word_tokenize(\"It's more'd enough.\")\r\n['It', \"'s\", 'more', \"'d\", 'enough', '.'] # unaffected by this PR\r\n>>> word_tokenize(\"It's mor's enough.\")\r\n['It', \"'s\", 'mor', \"'s\", 'enough', '.'] # unaffected by this PR\r\n>>> word_tokenize(\"It's 'd enough.\")\r\n['It', \"'s\", \"'d\", 'enough', '.'] # unaffected by this PR\r\n```\r\nNew:\r\n```python\r\n>>> from nltk.tokenize import word_tokenize\r\n>>> word_tokenize(\"It's more'n enough.\")\r\n['It', \"'s\", 'more', \"'n\", 'enough', '.'] # correct\r\n>>> word_tokenize(\"It's ore'n enough.\")\r\n['It', \"'s\", \"mor'n\", 'enough', '.'] # different from old, debatably better\r\n>>> word_tokenize(\"It's wrong'n enough.\")\r\n['It', \"'s\", \"wrong'n\", 'enough', '.'] # different from old, debatably better\r\n>>> word_tokenize(\"It's 'n enough.\")\r\n['It', \"'s\", \"'n\", 'enough', '.'] # different from old, debatably better\r\n>>> word_tokenize(\"It's more'd enough.\")\r\n['It', \"'s\", 'more', \"'d\", 'enough', '.'] # unaffected by this PR\r\n>>> word_tokenize(\"It's mor's enough.\")\r\n['It', \"'s\", 'mor', \"'s\", 'enough', '.'] # unaffected by this PR\r\n>>> word_tokenize(\"It's 'd enough.\")\r\n['It', \"'s\", \"'d\", 'enough', '.'] # unaffected by this PR\r\n```\r\nIn words, `'n` is now preserved and left untouched, just like `'s` and `'d`, etc. (see the last three examples)\r\ni.e. `(.*)'n` will stay as `[\"$1'n\"]`, with one exception: `more'n -> [\"more\", \"'n\"]`. \r\nThese new results are identical to the `TreebankWordTokenizer.tokenize()` results with the same inputs.\r\n\r\n### Consequences for TreebankWordTokenizer\r\nOld: \r\n```python\r\n>>> from nltk.tokenize import TreebankWordTokenizer as word_tokenizer\r\n>>> wt = word_tokenizer()\r\n>>> wt.tokenize(\"It's more'n enough.\")\r\n['It', \"'s\", \"more'n\", 'enough', '.'] # wrong, direct consequence of wrong contraction rule\r\n>>> wt.tokenize(\"It's ore'n enough.\")\r\n['It', \"'s\", 'mor', \"'n\", 'enough', '.'] # wrong, direct consequence of wrong contraction rule\r\n>>> wt.tokenize(\"It's wrong'n enough.\")\r\n['It', \"'s\", 'wrong', \"'\", 'n', 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's 'n enough.\")\r\n['It', \"'s\", \"'\", 'n', 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's more'd enough.\")\r\n['It', \"'s\", 'more', \"'d\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's mor's enough.\")\r\n['It', \"'s\", 'mor', \"'s\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's 'd enough.\")\r\n['It', \"'s\", \"'d\", 'enough', '.'] # unaffected by this PR\r\n```\r\nNew:\r\n```python\r\n>>> from nltk.tokenize import TreebankWordTokenizer as word_tokenizer\r\n>>> wt = word_tokenizer()\r\n>>> wt.tokenize(\"It's more'n enough.\")\r\n['It', \"'s\", 'more', \"'n\", 'enough', '.'] # correct\r\n>>> wt.tokenize(\"It's ore'n enough.\")\r\n['It', \"'s\", \"mor'n\", 'enough', '.'] # correct\r\n>>> wt.tokenize(\"It's wrong'n enough.\")\r\n['It', \"'s\", \"wrong'n\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's 'n enough.\")\r\n['It', \"'s\", \"'n\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's more'd enough.\")\r\n['It', \"'s\", 'more', \"'d\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's mor's enough.\")\r\n['It', \"'s\", 'mor', \"'s\", 'enough', '.'] # unaffected by this PR\r\n>>> wt.tokenize(\"It's 'd enough.\")\r\n['It', \"'s\", \"'d\", 'enough', '.'] # unaffected by this PR\r\n```\r\nUnlike for `NLTKWordTokenizer`, this PR has no sideeffects beyond the intended effect of fixing the one issue for `more'n`. \r\n\r\n---\r\n\r\n### Tests\r\nI added the following test, which previously failed, but passes with these changes.\r\n```python\r\n    >>> s11 = \"It's more'n enough.\"\r\n    >>> word_tokenize(s11)\r\n    ['It', \"'s\", 'more', \"'n\", 'enough', '.']\r\n```\r\n\r\n---\r\n\r\nMerging this PR should close issue #2607\r\n\r\nWhether this PR should be merged depends on your preferences regarding the consequences for `NLTKWordTokenizer`. Perhaps alternative solutions without these sideeffects are possible too, if the sideeffects are deemed undesirable. However, the sideeffects might be a happy accident, depending on your preferences on the matter.\r\n\r\nIf the cure is deemed worse than the sickness, then you should still merge the changes to the contraction regex on line 25 of `nltk/tokenize/destructive.py`, as this will fix the `TreebankWordTokenizer` with no other sideeffects. (As some of the aforementioned regexes will make it impossible for either the old or the new contraction regex to be matched, for the `NLTKWordTokenizer`)\r\n\r\n- Tom Aarsen"}
{"number": 2607, "owner": "nmfzone", "title": "Wrong result for word more'n in Word Tokenizer and PTB Tokenizer", "body": "Let sentence: `It's more'n enough`\r\n\r\nIf I'm not mistaken, the PTB Tokenizer should result something like this:\r\n\r\n`[\"It\", \"'s\", \"more\", \"'n\", \"enough\"]`\r\n\r\nBut, it's not. It returns:\r\n\r\n`[\"It\", \"'s\", \"more\", \"'\", \"n\", \"enough\"]`\r\n\r\nSince Word Tokenizer trying to implement PTB contraction, the result should be like that, right?\r\n\r\n**PS**: Word Tokenizer contraction is `mor'n`, while original PTB contraction is `more'n`. Need clarification."}
{"number": 2602, "owner": "yyz159756", "title": "Exception in thread \"main\" java.lang.NullPointerException when training a MaltParser", "body": "I follow the guide in this url:https://cl.lingfil.uu.se/~nivre/research/parseme_lab.html\r\n\r\nI want to try to train a parser soI ran a command: java -jar -Xmx2g D:\\env\\maltParser\\maltparser-1.8.1\\maltparser-1.8.1.jar  -c en-parser -m learn -i E:\\data\\UD_English-GUM-master\\en_gum-ud-train.conllu.\r\n\r\nbut then cmd output like this following:\r\n-----------------------------------------------------------------------------\r\n                          MaltParser 1.8.1\r\n-----------------------------------------------------------------------------\r\n         MALT (Models and Algorithms for Language Technology) Group\r\n             Vaxjo University and Uppsala University\r\n                             Sweden\r\n-----------------------------------------------------------------------------\r\n\r\nStarted: Sun Sep 13 17:19:25 CST 2020\r\n  Transition system    : Arc-Eager\r\n  Parser configuration : Nivre with allow_root=true, allow_reduce=false and enforce_tree=false\r\n  Oracle               : Arc-Eager\r\n  Data Format          : file:/E:/data/en-parser/conllx.xml\r\nException in thread \"main\" java.lang.NullPointerException\r\n        at org.maltparser.parser.algorithm.nivre.ArcEagerOracle.predict(ArcEagerOracle.java:30)\r\n        at org.maltparser.parser.BatchTrainer.parse(BatchTrainer.java:47)\r\n        at org.maltparser.parser.SingleMalt.oracleParse(SingleMalt.java:222)\r\n        at org.maltparser.parser.SingleMaltChartItem.process(SingleMaltChartItem.java:128)\r\n        at org.maltparser.core.flow.FlowChartInstance.process(FlowChartInstance.java:200)\r\n        at org.maltparser.Engine.process(Engine.java:63)\r\n        at org.maltparser.MaltConsoleEngine.maltParser(MaltConsoleEngine.java:99)\r\n        at org.maltparser.MaltConsoleEngine.startEngine(MaltConsoleEngine.java:76)\r\n        at org.maltparser.Malt.main(Malt.java:18)\r\n\r\nIt remainded me there is a NullPointerExceptionin thread.\r\n\r\nBut I ran a command: java -jar D:\\env\\maltParser\\maltparser-1.8.1\\maltparser-1.8.1.jar. It can produce correct output:\r\n-----------------------------------------------------------------------------\r\n                          MaltParser 1.8.1\r\n-----------------------------------------------------------------------------\r\n         MALT (Models and Algorithms for Language Technology) Group\r\n             Vaxjo University and Uppsala University\r\n                             Sweden\r\n-----------------------------------------------------------------------------\r\n\r\nUsage:\r\n   java -jar maltparser-1.8.1.jar -f <path to option file> <options>\r\n   java -jar maltparser-1.8.1.jar -h for more help and options\r\n\r\nhelp                  (  -h) : Show options\r\n-----------------------------------------------------------------------------\r\noption_file           (  -f) : Path to option file\r\n-----------------------------------------------------------------------------\r\nverbosity            *(  -v) : Verbosity level\r\n  debug      - Logging of debugging messages\r\n  error      - Logging of error events\r\n  fatal      - Logging of very severe error events\r\n  info       - Logging of informational messages\r\n  off        - Logging turned off\r\n  warn       - Logging of harmful situations\r\n-----------------------------------------------------------------------------\r\n\r\nDocumentation: docs/index.html\r\n\r\n\r\nCan someone give me a direction to solve this problem?\r\n\r\n\r\n"}
{"number": 2599, "owner": "osmanio2", "title": "Unigram scores incorrectly computed for Kneser-Ney smoothing causing high perplexities ", "body": "The unigram scores in KneserNey smoothing introduced in this commit https://github.com/nltk/nltk/commit/2759b03d85983fedd29b646380c713c3f4ccff66 are computed as a uniform distribution over the vocabulary, which is clearly not how any variant of KneserNey computes it. \r\n\r\nhttps://github.com/nltk/nltk/blob/5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4/nltk/lm/smoothing.py#L47"}
{"number": 2598, "owner": "stevenluda", "title": "raw_freq score does not sum to 1", "body": "Noticed that when calculating bigram frequency using raw_freq. The result score does not sum to 1.\r\nChecked the source code, finding that it is using the total number of words as TOTAL for the denominator instead of total number of bigrams.\r\n\r\nWondering if this is a bug or it has some rationale behind it?"}
{"number": 2597, "owner": "yetingli", "title": "BUG: fix regex to avoid DoS", "body": "The regex which can match over long parts of strings but then be rejected by nonmatching at the end can be used for\r\ndenial-of-service. This addresses #2596"}
{"number": 2596, "owner": "yetingli", "title": "Potential Regex Denial of Service (ReDoS) in xmldocs", "body": "Type of Issue\r\nPotential Regex Denial of Service (ReDoS)\r\n\r\nDescription\r\nThe vulnerable regular expression is located in\r\n\r\nhttps://github.com/nltk/nltk/blob/ca357e5cdcdb137f40c45346bb8bfea618dd863f/nltk/corpus/reader/xmldocs.py#L225\r\n\r\nThe ReDOS vulnerability of the regex is mainly due to the sub-pattern `\\s*/?\\s*` and can be exploited with the following string\r\n`\"<\"+\" \" * 5000`\r\n\r\nI think you can limit the input length or modify the regex."}
{"number": 2593, "owner": "oceangiruis", "title": "Missing ccg.pdf", "body": "In help(nltk.ccg) it is written:\r\n\r\nDESCRIPTION\r\n    For more information see nltk/doc/contrib/ccg/ccg.pdf\r\n\r\nHowever I could not find this pdf document anywhere in my computer (nor anywhere online). "}
{"number": 2592, "owner": "dersuchendee", "title": "NLTK vader seems to not be updated with Vader's Github repo", "body": "It seems like some Vader issues have been fixed by the founder, but have not been implemented in NLTK. Comparison of the two vader.py files clearly shows different codes, this one (https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py) being more complete. Why?"}
{"number": 2591, "owner": "keer0204", "title": "Error in downloading ", "body": "![WhatsApp Image 2020-08-31 at 12 46 26 PM](https://user-images.githubusercontent.com/65659281/91697813-5c52c380-eb8f-11ea-8e2b-1986ab534cfa.jpeg)\r\nTrying to download stopwards from nltk , I used\r\nimport nltk\r\nnltk.download()\r\nI'm getting this error how to resolve this?"}
{"number": 2588, "owner": "zluvsand", "title": "Possible additions to english stopwords", "body": "This is a proposal to improve current list of english stop words. \r\n\r\n**Code used to access stop words:**\r\n\r\n```\r\nfrom nltk.corpus import stopwords\r\nprint(stopwords.words(\"english\"))\r\n```\r\n**Output:**\r\n_['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]_\r\n\r\n**Proposal:**\r\nWhile inspecting stop words, I have noticed that these variations of stop words could potentially be added to the corpus: _\"cannot\", \"could\", \"done\", \"let\", \"may\" \"mayn\",  \"might\",  \"must\", \"need\", \"ought\", \"oughtn\", \"shall\",  \"would\"_. "}
{"number": 2584, "owner": "weissercn", "title": "Cannot import all translation modules", "body": "Some submodules under translate are implemented and described in the documentation, but cannot be imported and used. \r\n\r\nimport nltk\r\nnltk.translate.gleu_score.corpus_gleu\r\n\r\nfails with this error:\r\nAttributeError: module 'nltk.translate' has no attribute 'gleu_score'"}
{"number": 2580, "owner": "shsarv", "title": "warning", "body": "'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))."}
{"number": 2578, "owner": "danielafe7-usp", "title": "TweetTokenizer add new emoticons characters", "body": "Hello! There is any way to add new characters to the emoticons tokenization ?\r\nFor example:\r\nfrom nltk.tokenize import TweetTokenizer\r\ntknzr = TweetTokenizer()\r\ns1 = \"This is a cooool :-*\"\r\ntknzr.tokenize(s1)\r\n\r\nIt gives as result:\r\n['This', 'is', 'a', 'cooool', ':', '-', '*']\r\n\r\nBut I need to preserve these specific emotion \":-*\".\r\nThanks for advance!"}
{"number": 2577, "owner": "al45tair", "title": "Averaged perceptron tagger cannot load data from paths with %-encoded characters in their names", "body": "If you use a non-default data directory that happens to have something that looks like a URL-encoded character in its name, you can't use `PerceptronTagger`, because both in `__init__.py` (for Russian) and in `perceptron.py`, it does\r\n\r\n    url = \"file:\" + str(find(NAME_OF_PICKLE))\r\n    tagger.load(url)\r\n\r\n(You can see this pattern in the `_get_tagger()` function on line 100 of `__init__.py`, as well as in the `__init__()` method of `PerceptronTagger` on line 167.)\r\n\r\nThe problem is that `find()` returns a path, not a URL fragment. For this code to be valid, it needs to url-encode the result of the `find()` call before prepending \"file\". As it stands, what will happen is that the `load()` call will eventually call `find()` again, which will url-decode the path even though it wasn't actually url-encoded. And then it will fail to find the file, because it won't be using the correct path name any more."}
{"number": 2575, "owner": "orsharir", "title": "Fix treebank detokenizer", "body": "Fix a few issues with how the Treebank's word detokenizer mishandled quotes, punctuation marks, and parentheses, including #2295 and #2220."}
{"number": 2573, "owner": "amirdaaee", "title": "Hack to keep NLTK's \"tokenize\" module fails ", "body": "in decorators.py file line 24:\r\n`sys.path = [p for p in sys.path if  \"nltk\" not in p]`\r\ncauses an error while importing nltk when there are other objects than strings in the path variable(such as pathlib.Posixpath)\r\nthis issue can be solved simply just by replacing the above code with:\r\n`sys.path = [p for p in sys.path if (type(p) == str and \"nltk\" not in p)]`\r\n"}
{"number": 2572, "owner": "shvms", "title": "Fixes #2571", "body": ""}
{"number": 2571, "owner": "shvms", "title": "Method2 of smoothing function in nltk.translate.bleu_score needs to ignore unigram precision score", "body": "According to [Lin & Och, 2004](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf), second smoothing function should 1 to both numerator & denominator of precision score for all n-grams where n >= 2. However, we can see from the code that it adds 1 to precision score corresponding to every n-gram including unigrams.\r\n\r\n```python\r\ndef method2(self, p_n, *args, **kwargs):\r\n        \"\"\"\r\n        Smoothing method 2: Add 1 to both numerator and denominator from\r\n        Chin-Yew Lin and Franz Josef Och (2004) Automatic evaluation of\r\n        machine translation quality using longest common subsequence and\r\n        skip-bigram statistics. In ACL04.\r\n        \"\"\"\r\n        return [\r\n            Fraction(p_i.numerator + 1, p_i.denominator + 1, _normalize=False)\r\n            for p_i in p_n\r\n        ]\r\n```"}
{"number": 2567, "owner": "gorj-tessella", "title": "WordNetLemmatizer not properly lemmatizing some words", "body": "Some words are lemmatized improperly, due to picking the smallest possible lemma: \r\n\r\n```py\r\nlemmatizer = WordNetLemmatizer()\r\nlemmatizer.lemmatize('dose', 'n') # returns \"dose\"\r\nlemmatizer.lemmatize('doses', 'n') # returns \"dos\"\r\nwordnet._morphy('doses', 'n') # returns [\"dose\", \"dos\"]\r\nwordnet.morphy('doses', 'n') # returns \"dose\"\r\n```\r\n"}
{"number": 2565, "owner": "GiovanniGabbolini", "title": "WordNet: Loop in hyponyms hierarchy", "body": "Hello,\r\n\r\nnavigating the hierarchy of hyponyms I end up in a loop:\r\n\r\n```\r\n>>> from nltk.corpus import wordnet as wn\r\n>>> synset=wn.synset('restrain.v.01'); print(synset)\r\nSynset('restrain.v.01')\r\n>>> synset=synset.hyponyms()[3]; print(synset)\r\nSynset('inhibit.v.04')\r\n>>> synset=synset.hyponyms()[2]; print(synset)\r\nSynset('restrain.v.01')\r\n>>> synset=synset.hyponyms()[3]; print(synset)\r\nSynset('inhibit.v.04')\r\n>>> synset=synset.hyponyms()[2]; print(synset)\r\nSynset('restrain.v.01')\r\n....\r\n```\r\n\r\nIs this meant to be?\r\n\r\n\r\nnltk Version: 3.4.5 on Python 3.7\r\n\r\nGiovanni"}
{"number": 2554, "owner": "AhmadAbdallah13", "title": "AttributeError: 'Tree' object has no attribute 'tree'. The class is 'nltk.tree.Tree'", "body": "I'm getting into the NLP world by reading and coding along with this https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72. Everything was working fine until I wanted to do Dependency Parsing. I want to leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree, the code is\r\n\r\n    from nltk.parse.stanford import StanfordDependencyParser\r\n    sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar', path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-        parser-3.5.2-models.jar')    \r\n\r\n    result = list(sdp.raw_parse(sentence))  \r\n\r\n    #print the dependency tree\r\n    dep_tree = [parse.tree() for parse in result][0]\r\n    print(dep_tree)\r\n\r\nThe variable \"sentence\" is defined earlier, and the paths are correct because I used them earlier. When I run this code in Jupyter notebook the output should be **\"(beats (unveils US (supercomputer (world 's) (powerful most)))\r\nChina)\"** but instead I get this error \r\n\r\n    AttributeError                            Traceback (most recent call last)\r\n    <ipython-input-66-c3b0e2efe04f> in <module>\r\n          6 \r\n          7 # print the dependency tree\r\n    ----> 8 dep_tree = [parse.tree() for parse in result][0]\r\n          9 print(dep_tree)\r\n    \r\n    <ipython-input-66-c3b0e2efe04f> in <listcomp>(.0)\r\n          6 \r\n          7 # print the dependency tree\r\n    ----> 8 dep_tree = [parse.tree() for parse in result][0]\r\n          9 print(dep_tree)\r\n    \r\n    AttributeError: 'Tree' object has no attribute 'tree'\r\nAny piece of information on this would be extremely helpful, thank you."}
{"number": 2551, "owner": "guizonatto", "title": "'RegexpTagger' object has no attribute '_regexps'", "body": "Is the issue https://github.com/nltk/nltk/issues/287 back in the 3.5 version?\r\n\r\nI test the same code in 3.4.5 and it works fine.\r\n\r\n"}
{"number": 2549, "owner": "BS211561", "title": "CoreNLP server connection issue", "body": "For some reason, I can't connect to the server. Can anyone help me out? Here's the code I'm running.\r\n\r\nfrom nltk.parse.corenlp import CoreNLPServer\r\nserver = CoreNLPServer(\"stanford-corenlp-4.0.0.jar\",\"stanford-corenlp-4.0.0-models.jar\",verbose=True)\r\nserver.start()\r\n\r\nfrom nltk.parse.corenlpnltk.pa import CoreNLPParser\r\nparser = CoreNLPParser()\r\nparse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\r\n\r\nserver.stop()\r\n\r\nI am running it in a file inside the folder that I downloaded for CoreNLP so that I don't have to worry about abs paths or anything.\r\nHere's what it outputs:\r\n\r\n[Found java: /usr/bin/java]\r\n[Found java: /usr/bin/java]\r\nTraceback (most recent call last):\r\nFile \"/Users/benstevens/Desktop/Bernstein2/Ben/stanford-corenlp-4.0.0/BenTest.py\", line 4, in\r\nserver.start()\r\nFile \"/Users/benstevens/Library/Python/3.8/lib/python/site-packages/nltk/parse/corenlp.py\", line 153, in start\r\nraise CoreNLPServerError(\"Could not connect to the server.\")\r\nnltk.parse.corenlp.CoreNLPServerError: Could not connect to the server.\r\n\r\nI would greatly appreciate any help."}
{"number": 2545, "owner": "HarikrishnanBalagopal", "title": "Add a button to hide doctest arrows", "body": "# THIS IS A FEATURE REQUEST\r\n\r\nIn the examples shown in the documentation the >>> arrows are annoying.\r\nThey and the output of the code have to manually removed after copy pasting the examples in order to run them.\r\n\r\nExample of annoying arrows in documentation: [https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)\r\n\r\n# Solution:\r\n\r\n[https://z4r.github.io/python/2011/12/02/hides-the-prompts-and-output/](https://z4r.github.io/python/2011/12/02/hides-the-prompts-and-output/)\r\n\r\nThe above link shows how to configure Sphinx to add a button that can hide the arrows\r\nSee the examples in numpy docs to see how it works: [https://numpy.org/doc/stable/reference/generated/numpy.mean.html](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\r\n\r\nThe button hides the arrows as well as the output of each line to allow easy copy pasting.\r\n\r\n![p1](https://user-images.githubusercontent.com/20921177/82733203-e5a45f80-9d2f-11ea-97e7-2caf576e57d5.png)\r\n![p2](https://user-images.githubusercontent.com/20921177/82733204-e937e680-9d2f-11ea-814e-ee278c8a8a37.png)\r\n"}
{"number": 2543, "owner": "ZohaibRamzan", "title": "NLTK Sentence tokenizer does not tokenize properly if there exists 'e.g.' or 'i.e.' in sentence.", "body": "Like i have sentence:\r\n'The first approach, single-molecule simulation, taken by the StochSim simulator, tracks individual molecules and their state (e.g., what other molecules they are bound to) so that only the complexes formed at any given time are enumerated (and not all possible complexes) [11].'\r\n\r\nThe sentence tokenizer splits this sentence and gives me following two sentences;\r\nThe first approach , single-molecule simulation , taken by the StochSim simulator , tracks individual molecules and their state ( e.g.\r\n\r\nand\r\n\r\n, what other molecules they are bound to ) so that only the complexes formed at any given time are enumerated ( and not all possible complexes ) [ 11 ] .\r\n\r\nHow can this be resolved?\r\n\r\n"}
{"number": 2542, "owner": "jayralencar", "title": "synset_from_sense_key: searching for sense key in index.sense", "body": "I found a little mistake on the method ```synset_from_sense_key```:\r\n\r\nOn searching for the sense key ```bank%1:06:00::```, for example, it returns the synset ```Synset('bank.n.10')``` (offset: 00169305)  instead of ```Synset('bank.n.10')``` (offset: 02787772), which is the correct. See index.sense excerpt:\r\n\r\n...\r\nbanjul%1:15:00:: 08946042 1 0\r\nbank%1:04:00:: 00169305 10 0\r\n**bank%1:06:00:: 02787772 9 0**\r\nbank%1:06:01:: 04139859 8 0\r\nbank%1:14:00:: 08420278 2 20\r\n...\r\n\r\nTo fix it I used the same logic of the method ```lemma_from_key```, using ```_binary_search_file```."}
{"number": 2538, "owner": "bsolomon1124", "title": "Add wheel distribution(s) to PyPI", "body": "Has nltk considered the feasibility of adding wheels to PyPI?\r\n\r\nAs of now it is one of ~10% of packages listed on https://pythonwheels.com/ that [does not provide wheels](https://pypi.org/project/nltk/#files).\r\n\r\nIt looks like nltk is pure-Python with no dependencies on shared libraries or the like.  That seems like it would make building the wheel itself pretty painless."}
{"number": 2536, "owner": "mariokostelac", "title": "Fix punkt tokenizer complexity for very long tokens", "body": "This change prevents N^2 complexity for inputs with very long tokens (N being the length of the longest token). Instead of using `finditer` with regex that starts from any point in the string, we use match method that starts only at the string beginning and moves from one opportunity to another (either end of next blank sequence of previous break).\r\n\r\n**Why should we bother changing complexity**\r\nI've discovered that while processing some real world conversational data. The current implementation choked on some strings having tokens 60k characters long (yes, over time, users will find all sorts of bottlenecks in the real system \ud83e\udd37\u200d\u2642\ufe0f), taking > 8s to tokenise that that string. Reducing to complexity to this brings the execution below few millis. I can't remember exact numbers, but can create some benchmark if it's bringing extra value here.\r\n\r\n**What was N^2 exactly?**\r\nUsed regex can start at any position in the string. If we have a token\r\nlike 01234567890123...9, \\S* starts matching 0 and matches everything till\r\nthe last 9 and then figures out that it does not have potential sentence\r\nending character.\r\n\r\nSince finditer returns matches starting at any position, it tries matching with character 1, matches everything till the end and then does not find potential sentence ending, ... It does that matching N times, each time going through N/2 characters (on average).\r\n\r\n**Complexity test**\r\nI've found regression tests like the one I wrote useful. If you are ok with it, I can improve the robustness."}
{"number": 2531, "owner": "fievelk", "title": "[WIP] Create Docker container for tests", "body": "# WIP\r\n\r\nDocker container for tests"}
{"number": 2529, "owner": "tibnb545", "title": "corpus ribes scorer throws division by zero error", "body": "Division by zero error is constantly thrown for ````corpus_ribes()```` function. \r\n\r\nTo replicate: \r\nTake the example from the docs (https://www.nltk.org/api/nltk.translate.html) and shorten some\r\nof the input sentences. In this case I only shortened hyp2 and hyp2a. \r\n````\r\nfrom nltk.translate.ribes_score import corpus_ribes\r\n\r\nhyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']\r\nref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',  'ensures', 'that', 'the', 'military', 'will', 'forever',  'heed', 'Party', 'commands']\r\nref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which', 'guarantees', 'the', 'military', 'forces', 'always', 'being', 'under', 'the', 'command', 'of', 'the', 'Party']\r\nref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',  'army', 'always', 'to', 'heed', 'the', 'directions',  'of', 'the', 'party']\r\n\r\nhyp2 = ['he', 'read', 'the']\r\nref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because', 'he']\r\n\r\nlist_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\r\nhypotheses = [hyp1, hyp2]\r\ncorpus_ribes(list_of_references, hypotheses)\r\n````\r\nOutput: \r\n````\r\n--------------------------------------------------------------------------\r\nZeroDivisionError                         Traceback (most recent call last)\r\n<ipython-input-175-b02599af69e7> in <module>\r\n      1 list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\r\n      2 hypotheses = [hyp1, hyp2]\r\n----> 3 corpus_ribes(list_of_references, hypotheses)\r\n\r\n/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in corpus_ribes(list_of_references, hypotheses, alpha, beta)\r\n    117     # Iterate through each hypothesis and their corresponding references.\r\n    118     for references, hypothesis in zip(list_of_references, hypotheses):\r\n--> 119         corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta)\r\n    120     return corpus_best_ribes / len(hypotheses)\r\n    121 \r\n\r\n/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in sentence_ribes(references, hypothesis, alpha, beta)\r\n     53         # Collects the *worder* from the ranked correlation alignments.\r\n     54         worder = word_rank_alignment(reference, hypothesis)\r\n---> 55         nkt = kendall_tau(worder)\r\n     56 \r\n     57         # Calculates the brevity penalty\r\n\r\n/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in kendall_tau(worder, normalize)\r\n    288     num_possible_pairs = choose(worder_len, 2)\r\n    289     # Kendall's Tau computation.\r\n--> 290     tau = 2 * num_increasing_pairs / num_possible_pairs - 1\r\n    291     if normalize:  # If normalized, the tau output falls between 0.0 to 1.0\r\n    292         return (tau + 1) / 2\r\n\r\nZeroDivisionError: division by zero\r\n````"}
{"number": 2528, "owner": "ParthS28", "title": "TypeError: attrib() got an unexpected keyword argument 'convert'", "body": "I've been getting this error recently. I have used nltk before without any problem. Why is this happening? \r\nI am using jupyter (python3) notebook. However, if I import nltk on terminal, there is no error.\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-53ff8bda7639> in <module>()\r\n      1 import numpy as np\r\n      2 import pandas as pd\r\n----> 3 from nltk.tokenize import word_tokenize\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/nltk/__init__.pyc in <module>()\r\n    141 ###########################################################\r\n    142 \r\n--> 143 from nltk.chunk import *\r\n    144 from nltk.classify import *\r\n    145 from nltk.inference import *\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/nltk/chunk/__init__.py in <module>()\r\n    155 from nltk.data import load\r\n    156 \r\n--> 157 from nltk.chunk.api import ChunkParserI\r\n    158 from nltk.chunk.util import (\r\n    159     ChunkScore,\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/nltk/chunk/api.py in <module>()\r\n     11 ##//////////////////////////////////////////////////////\r\n     12 \r\n---> 13 from nltk.parse import ParserI\r\n     14 \r\n     15 from nltk.chunk.util import ChunkScore\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/nltk/parse/__init__.py in <module>()\r\n     98 from nltk.parse.malt import MaltParser\r\n     99 from nltk.parse.evaluate import DependencyEvaluator\r\n--> 100 from nltk.parse.transitionparser import TransitionParser\r\n    101 from nltk.parse.bllip import BllipParser\r\n    102 from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/nltk/parse/transitionparser.py in <module>()\r\n     20     from numpy import array\r\n     21     from scipy import sparse\r\n---> 22     from sklearn.datasets import load_svmlight_file\r\n     23     from sklearn import svm\r\n     24 except ImportError:\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/datasets/__init__.py in <module>()\r\n     21 from .lfw import fetch_lfw_pairs\r\n     22 from .lfw import fetch_lfw_people\r\n---> 23 from .twenty_newsgroups import fetch_20newsgroups\r\n     24 from .twenty_newsgroups import fetch_20newsgroups_vectorized\r\n     25 from .mldata import fetch_mldata, mldata_filename\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/datasets/twenty_newsgroups.py in <module>()\r\n     42 from .base import _fetch_remote\r\n     43 from .base import RemoteFileMetadata\r\n---> 44 from ..feature_extraction.text import CountVectorizer\r\n     45 from ..preprocessing import normalize\r\n     46 from ..utils import deprecated\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/feature_extraction/__init__.py in <module>()\r\n      8 from .hashing import FeatureHasher\r\n      9 from .image import img_to_graph, grid_to_graph\r\n---> 10 from . import text\r\n     11 \r\n     12 __all__ = ['DictVectorizer', 'image', 'img_to_graph', 'grid_to_graph', 'text',\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py in <module>()\r\n     28 from ..externals import six\r\n     29 from ..externals.six.moves import xrange\r\n---> 30 from ..preprocessing import normalize\r\n     31 from .hashing import FeatureHasher\r\n     32 from .stop_words import ENGLISH_STOP_WORDS\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/preprocessing/__init__.py in <module>()\r\n      4 \"\"\"\r\n      5 \r\n----> 6 from ._function_transformer import FunctionTransformer\r\n      7 \r\n      8 from .data import Binarizer\r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/preprocessing/_function_transformer.py in <module>()\r\n      3 from ..base import BaseEstimator, TransformerMixin\r\n      4 from ..utils import check_array\r\n----> 5 from ..utils.testing import assert_allclose_dense_sparse\r\n      6 from ..externals.six import string_types\r\n      7 \r\n\r\n/home/parth/.local/lib/python2.7/site-packages/sklearn/utils/testing.py in <module>()\r\n    749 \r\n    750 try:\r\n--> 751     import pytest\r\n    752 \r\n    753     skip_if_32bit = pytest.mark.skipif(_IS_32BIT,\r\n\r\n/usr/lib/python2.7/dist-packages/pytest.py in <module>()\r\n     11     hookspec, hookimpl\r\n     12 )\r\n---> 13 from _pytest.fixtures import fixture, yield_fixture\r\n     14 from _pytest.assertion import register_assert_rewrite\r\n     15 from _pytest.freeze_support import freeze_includes\r\n\r\n/usr/lib/python2.7/dist-packages/_pytest/fixtures.py in <module>()\r\n    840 \r\n    841 @attr.s(frozen=True)\r\n--> 842 class FixtureFunctionMarker(object):\r\n    843     scope = attr.ib()\r\n    844     params = attr.ib(convert=attr.converters.optional(tuple))\r\n\r\n/usr/lib/python2.7/dist-packages/_pytest/fixtures.py in FixtureFunctionMarker()\r\n    842 class FixtureFunctionMarker(object):\r\n    843     scope = attr.ib()\r\n--> 844     params = attr.ib(convert=attr.converters.optional(tuple))\r\n    845     autouse = attr.ib(default=False)\r\n    846     ids = attr.ib(default=None, convert=_ensure_immutable_ids)\r\n\r\nTypeError: attrib() got an unexpected keyword argument 'convert'\r\n```"}
{"number": 2527, "owner": "TMiguelT", "title": "Quote author names mixed up in wordnet definitions", "body": "If I run the following code:\r\n\r\n```python\r\nfrom nltk.corpus import wordnet\r\n\r\nfor ss in wordnet.all_synsets():\r\n    if ' - ' in ss.definition():\r\n        print(ss, ss.definition())\r\n```\r\nI get a list of definitions like this:\r\n```\r\nSynset('abstemious.a.01') sparing in consumption of especially food and drink; - John Galsworthy\r\nSynset('ascetic.s.02') practicing great self-denial; - William James\r\nSynset('dead-on.s.01') accurate and to the point; ; - Peter S.Prescott\r\nSynset('used_to.s.01') in the habit; ; ; - Henry David Thoreau\r\nSynset('predaceous.s.02') living by or given to victimizing others for personal gain; ; - Peter S. Prescott; - W.E.Swinton\r\nSynset('passive.a.01') lacking in energy or will; - George Meredith\r\nSynset('resistless.s.02') offering no resistance; ; - Theodore Roosevelt\r\nSynset('alcoholic.s.02') addicted to alcohol; - Carl Van Doren\r\nSynset('reductive.s.01') characterized by or causing diminution or curtailment;  - R.H.Rovere\r\nSynset('mounted.s.02') decorated with applied ornamentation; often used in combination; - F.V.W.Mason\r\nSynset('coordinated.s.02') being dexterous in the use of more than one set of muscle movements; - Mary McCarthy\r\nSynset('light-fingered.s.01') having nimble fingers literally or figuratively; especially for stealing or picking pockets; - Harry Hansen; - Time\r\nSynset('bumbling.s.01') lacking physical movement skills, especially with the hands; ; ; ; - Mary H. Vorse\r\nSynset('uninfluenced.s.01') not influenced or affected; - V.L.Parrington\r\n```\r\n\r\nI'm concerned that these authors (such as `- Theodore Roosevelt`) possibly shouldn't be in the definition? I think these are the authors of the last example in the `ss.examples()` list, that haven't been parsed as part of the example because they aren't within the double quotes. "}
{"number": 2523, "owner": "atlijas", "title": "Deprecation Warnings", "body": "When importing sent_tokenize from nltk.tokenize:\r\n\r\npath/to/python3.7/dist-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\r\n  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\r\npath/to/python3.7/dist-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  from collections import Sequence, defaultdict\r\npath/to/python3.7/dist-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  from collections import Counter, Iterable"}
{"number": 2522, "owner": "KOLANICH", "title": "Security issues with pickles", "body": "Pickles security nightmare because they alow arbitrary code execution and because this code is not explicitly visible, to extract and analyse it tools are needed that currently don't exist. They are good places to plant hardly discovered backdoors. But this lib relies heavily on them. It downloads some pickled pretrained stuff and doesn't work without it.\r\n\r\nWe need to solve this issue. There are several issues here:\r\n1. Pickles are used. They should be replaced. The replacements can be some custom code and either a feature-specific binary format, or general purpose binary format, such as CBOR.\r\n2. I haven't found the recepies to build the pretrained models. I mean for each pretrained model should be\r\n    * a python file that:\r\n        * fetches the needed datasets\r\n        * preprocesses the data and trains the model\r\n        * evaluating its performance\r\n        * intentionally written the way to be easily auditable\r\n    * and a JSON config file storing\r\n        * hyperparams\r\n        * datasets locations\r\n        * previously achieved performance\r\n    So, if using project- or thirdpparty devs-provided pickles is inacceptible because I cannot trust them, I should be able to recreate own pickles from scratch. Anyway, even if we replace pickles by something else, we still need the way to improve the pretrained models, i.e. by retraining them on better datasets or using better hyperparams (I have a lib for hyperparams tuning, BTW). So, IMHO for every pretrained model there should be the code reproducing its creation."}
{"number": 2521, "owner": "007fred50", "title": "import nltk eror, No module named '_sqlite3'", "body": "I'm in ubuntu 16.04\r\n\r\nhere what i do:\r\n```\r\nsudo apt-get install sqlite-devel\r\nsudo apt-get install libsqlite3-dev\r\nsudo pip3 install pysqlite\r\n```\r\n\r\nI try to import nltk library.\r\n\r\nline (1): import nltk\r\n```\r\nTraceback (most recent call last):\r\n  File \"chat.py\", line 1, in <module>\r\n    import nltk\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/__init__.py\", line 150, in <module>\r\n    from nltk.translate import *\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/translate/__init__.py\", line 23, in <module>\r\n    from nltk.translate.meteor_score import meteor_score as meteor\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/translate/meteor_score.py\", line 10, in <module>\r\n    from nltk.stem.porter import PorterStemmer\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/stem/__init__.py\", line 29, in <module>\r\n    from nltk.stem.snowball import SnowballStemmer\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/stem/snowball.py\", line 32, in <module>\r\n    from nltk.corpus import stopwords\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/corpus/__init__.py\", line 66, in <module>\r\n    from nltk.corpus.reader import *\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/__init__.py\", line 105, in <module>\r\n    from nltk.corpus.reader.panlex_lite import *\r\n  File \"/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/panlex_lite.py\", line 15, in <module>\r\n    import sqlite3\r\n  File \"/usr/local/lib/python3.7/sqlite3/__init__.py\", line 23, in <module>\r\n    from sqlite3.dbapi2 import *\r\n  File \"/usr/local/lib/python3.7/sqlite3/dbapi2.py\", line 27, in <module>\r\n    from _sqlite3 import *\r\nModuleNotFoundError: No module named '_sqlite3'\r\n```\r\n\r\ncanyou help me with this?\r\n\r\nThanks for your time !"}
{"number": 2519, "owner": "kkkppp", "title": "PunktSentenceTokenizer doesn't work with custom end sentence character", "body": "Example:\r\n\r\n```\r\nimport nltk\r\nfrom nltk.tokenize.punkt import PunktLanguageVars\r\n\r\nclass ExtLangVars(PunktLanguageVars):\r\n    sent_end_chars = ('.', '?', '!', '^')\r\n\r\npunkt_tk = nltk.data.load('tokenizers/punkt/english.pickle')\r\npunkt_tk._lang_vars = ExtLangVars()\r\n\r\ntxt1 = 'Subject: Some subject. Attachments: Some attachemnts'\r\ntxt2 = 'Subject: Some subject^ Attachments: Some attachemnts'\r\n\r\nr = punkt_tk.tokenize(txt1)\r\nprint(r)\r\n\r\nr = punkt_tk.tokenize(txt2)\r\nprint(r)\r\n\r\nd = punkt_tk.debug_decisions(txt1)\r\nfor x in d:\r\n    print(nltk.tokenize.punkt.format_debug_decision(x))\r\n\r\n#d = punkt_tk.debug_decisions(txt2)\r\n#for x in d:\r\n#    print(nltk.tokenize.punkt.format_debug_decision(x))\r\n\r\n```\r\nAlso if you uncomment last lines, it will fail with index out of bounds. I think it's because of hardcoded \r\n\r\n`        self.period_final = tok.endswith('.')`\r\n\r\nin PunktToken. Tried many ways to work around this, but no luck. Any suggestions would be appreciated."}
{"number": 2516, "owner": "jayvdb", "title": "https://nltk.org SSL_ERROR_BAD_CERT_DOMAIN", "body": "https://nltk.org (https) is a redirect to https://www.nltk.org/ , however it shows a cert error.\r\n\r\nThe cert is only valid for \"shortener.secureserver.net, www.shortener.secureserver.net\""}
{"number": 2509, "owner": "stevenbird", "title": "Remove outdated names in init.py", "body": "Cf #2508 \r\n"}
{"number": 2500, "owner": "qiray", "title": "sent_tokenize error in Russian", "body": "I've got an error using NLTK parser for some Rusian classic books. The text below isn't one sentence:\r\n\r\n``` python\r\nimport nltk\r\ntext = \"\u041d\u043e \u0432\u0435\u0434\u044c \u0432\u043e\u0442 \u0447\u0442\u043e \u043f\u0440\u0438 \u044d\u0442\u043e\u043c, \u0434\u043e\u0431\u0440\u0435\u0439\u0448\u0438\u0439 \u0420\u043e\u0434\u0438\u043e\u043d \u0420\u043e\u043c\u0430\u043d\u043e\u0432\u0438\u0447, \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u0435\u0442: \u0432\u0435\u0434\u044c \u043e\u0431\u0449\u0435\u0433\u043e-\u0442\u043e \u0441\u043b\u0443\u0447\u0430\u044f-\u0441, \u0442\u043e\u0433\u043e \u0441\u0430\u043c\u043e\u0433\u043e, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u0441\u0435 \u044e\u0440\u0438\u0434\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0444\u043e\u0440\u043c\u044b \u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435\u043d\u044b \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043e\u043d\u0438 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u044b \u0438 \u0432 \u043a\u043d\u0438\u0436\u043a\u0438 \u0437\u0430\u043f\u0438\u0441\u0430\u043d\u044b, \u0432\u043e\u0432\u0441\u0435 \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442-\u0441 \u043f\u043e \u0442\u043e\u043c\u0443 \u0441\u0430\u043c\u043e\u043c\u0443, \u0447\u0442\u043e \u0432\u0441\u044f\u043a\u043e\u0435 \u0434\u0435\u043b\u043e, \u0432\u0441\u044f\u043a\u043e\u0435, \u0445\u043e\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0435, \u043a\u0430\u043a \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u043e \u0441\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u0432 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u0442\u043e\u0442\u0447\u0430\u0441 \u0436\u0435 \u0438 \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0432 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0447\u0430\u0441\u0442\u043d\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439-\u0441; \u0434\u0430 \u0438\u043d\u043e\u0433\u0434\u0430 \u0432\u0435\u0434\u044c \u0432 \u043a\u0430\u043a\u043e\u0439: \u0442\u0430\u043a-\u0442\u0430\u043a\u0438 \u043d\u0438 \u043d\u0430 \u0447\u0442\u043e \u043f\u0440\u0435\u0436\u043d\u0435\u0435 \u043d\u0435 \u043f\u043e\u0445\u043e\u0436\u0438\u0439-\u0441. \u041f\u0440\u0435\u043a\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043d\u043e\u0433\u0434\u0430 \u0441\u043b\u0443\u0447\u0430\u0438 \u0441\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u0440\u043e\u0434\u0435-\u0441. \u0414\u0430 \u043e\u0441\u0442\u0430\u0432\u044c \u044f \u0438\u043d\u043e\u0433\u043e-\u0442\u043e \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d\u0430 \u0441\u043e\u0432\u0441\u0435\u043c \u043e\u0434\u043d\u043e\u0433\u043e: \u043d\u0435 \u0431\u0435\u0440\u0438 \u044f \u0435\u0433\u043e \u0438 \u043d\u0435 \u0431\u0435\u0441\u043f\u043e\u043a\u043e\u0439, \u043d\u043e \u0447\u0442\u043e\u0431 \u0437\u043d\u0430\u043b \u043e\u043d \u043a\u0430\u0436\u0434\u044b\u0439 \u0447\u0430\u0441 \u0438 \u043a\u0430\u0436\u0434\u0443\u044e \u043c\u0438\u043d\u0443\u0442\u0443, \u0438\u043b\u0438 \u043f\u043e \u043a\u0440\u0430\u0439\u043d\u0435\u0439 \u043c\u0435\u0440\u0435 \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u043b, \u0447\u0442\u043e \u044f \u0432\u0441\u0451 \u0437\u043d\u0430\u044e, \u0432\u0441\u044e \u043f\u043e\u0434\u043d\u043e\u0433\u043e\u0442\u043d\u0443\u044e, \u0438 \u0434\u0435\u043d\u043d\u043e \u0438 \u043d\u043e\u0449\u043d\u043e \u0441\u043b\u0435\u0436\u0443 \u0437\u0430 \u043d\u0438\u043c, \u043d\u0435\u0443\u0441\u044b\u043f\u043d\u043e \u0435\u0433\u043e \u0441\u0442\u043e\u0440\u043e\u0436\u0443, \u0438 \u0431\u0443\u0434\u044c \u043e\u043d \u0443 \u043c\u0435\u043d\u044f \u0441\u043e\u0437\u043d\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434 \u0432\u0435\u0447\u043d\u044b\u043c \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u043d\u0438\u0435\u043c \u0438 \u0441\u0442\u0440\u0430\u0445\u043e\u043c, \u0442\u0430\u043a \u0432\u0435\u0434\u044c, \u0435\u0439-\u0431\u043e\u0433\u0443, \u0437\u0430\u043a\u0440\u0443\u0436\u0438\u0442\u0441\u044f, \u043f\u0440\u0430\u0432\u043e-\u0441, \u0441\u0430\u043c \u043f\u0440\u0438\u0434\u0435\u0442 \u0434\u0430, \u043f\u043e\u0436\u0430\u043b\u0443\u0439, \u0435\u0449\u0435 \u0438 \u043d\u0430\u0434\u0435\u043b\u0430\u0435\u0442 \u0447\u0435\u0433\u043e-\u043d\u0438\u0431\u0443\u0434\u044c, \u0447\u0442\u043e \u0443\u0436\u0435 \u043d\u0430 \u0434\u0432\u0430\u0436\u0434\u044b \u0434\u0432\u0430 \u043f\u043e\u0445\u043e\u0434\u0438\u0442\u044c \u0431\u0443\u0434\u0435\u0442, \u0442\u0430\u043a \u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u0438\u0434 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c,\u00a0\u2014 \u043e\u043d\u043e \u0438 \u043f\u0440\u0438\u044f\u0442\u043d\u043e-\u0441. \u042d\u0442\u043e \u0438 \u0441 \u043c\u0443\u0436\u0438\u043a\u043e\u043c \u0441\u0438\u0432\u043e\u043b\u0430\u043f\u044b\u043c \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438, \u0430 \u0443\u0436 \u0441 \u043d\u0430\u0448\u0438\u043c \u0431\u0440\u0430\u0442\u043e\u043c, \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0443\u043c\u043d\u044b\u043c \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c, \u0434\u0430 \u0435\u0449\u0435 \u0432 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u0443\u044e \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0440\u0430\u0437\u0432\u0438\u0442\u044b\u043c, \u0438 \u043f\u043e\u0434\u0430\u0432\u043d\u043e!\"\r\nprint(nltk.sent_tokenize(text, language=\"russian\"))\r\n```\r\n\r\nThis text should be separated into several sentences:\r\n\r\n```\r\n\u041d\u043e \u0432\u0435\u0434\u044c \u0432\u043e\u0442 \u0447\u0442\u043e \u043f\u0440\u0438 \u044d\u0442\u043e\u043c, \u0434\u043e\u0431\u0440\u0435\u0439\u0448\u0438\u0439 \u0420\u043e\u0434\u0438\u043e\u043d \u0420\u043e\u043c\u0430\u043d\u043e\u0432\u0438\u0447, \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u0435\u0442: \u0432\u0435\u0434\u044c \u043e\u0431\u0449\u0435\u0433\u043e-\u0442\u043e \u0441\u043b\u0443\u0447\u0430\u044f-\u0441, \u0442\u043e\u0433\u043e \u0441\u0430\u043c\u043e\u0433\u043e, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u0441\u0435 \u044e\u0440\u0438\u0434\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0444\u043e\u0440\u043c\u044b \u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435\u043d\u044b \u0438 \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043e\u043d\u0438 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u044b \u0438 \u0432 \u043a\u043d\u0438\u0436\u043a\u0438 \u0437\u0430\u043f\u0438\u0441\u0430\u043d\u044b, \u0432\u043e\u0432\u0441\u0435 \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442-\u0441 \u043f\u043e \u0442\u043e\u043c\u0443 \u0441\u0430\u043c\u043e\u043c\u0443, \u0447\u0442\u043e \u0432\u0441\u044f\u043a\u043e\u0435 \u0434\u0435\u043b\u043e, \u0432\u0441\u044f\u043a\u043e\u0435, \u0445\u043e\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0435, \u043a\u0430\u043a \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u043e \u0441\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u0432 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u0442\u043e\u0442\u0447\u0430\u0441 \u0436\u0435 \u0438 \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0432 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0447\u0430\u0441\u0442\u043d\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439-\u0441; \u0434\u0430 \u0438\u043d\u043e\u0433\u0434\u0430 \u0432\u0435\u0434\u044c \u0432 \u043a\u0430\u043a\u043e\u0439: \u0442\u0430\u043a-\u0442\u0430\u043a\u0438 \u043d\u0438 \u043d\u0430 \u0447\u0442\u043e \u043f\u0440\u0435\u0436\u043d\u0435\u0435 \u043d\u0435 \u043f\u043e\u0445\u043e\u0436\u0438\u0439-\u0441. \r\n\r\n\u041f\u0440\u0435\u043a\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043d\u043e\u0433\u0434\u0430 \u0441\u043b\u0443\u0447\u0430\u0438 \u0441\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u0440\u043e\u0434\u0435-\u0441. \r\n\r\n\u0414\u0430 \u043e\u0441\u0442\u0430\u0432\u044c \u044f \u0438\u043d\u043e\u0433\u043e-\u0442\u043e \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d\u0430 \u0441\u043e\u0432\u0441\u0435\u043c \u043e\u0434\u043d\u043e\u0433\u043e: \u043d\u0435 \u0431\u0435\u0440\u0438 \u044f \u0435\u0433\u043e \u0438 \u043d\u0435 \u0431\u0435\u0441\u043f\u043e\u043a\u043e\u0439, \u043d\u043e \u0447\u0442\u043e\u0431 \u0437\u043d\u0430\u043b \u043e\u043d \u043a\u0430\u0436\u0434\u044b\u0439 \u0447\u0430\u0441 \u0438 \u043a\u0430\u0436\u0434\u0443\u044e \u043c\u0438\u043d\u0443\u0442\u0443, \u0438\u043b\u0438 \u043f\u043e \u043a\u0440\u0430\u0439\u043d\u0435\u0439 \u043c\u0435\u0440\u0435 \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u043b, \u0447\u0442\u043e \u044f \u0432\u0441\u0451 \u0437\u043d\u0430\u044e, \u0432\u0441\u044e \u043f\u043e\u0434\u043d\u043e\u0433\u043e\u0442\u043d\u0443\u044e, \u0438 \u0434\u0435\u043d\u043d\u043e \u0438 \u043d\u043e\u0449\u043d\u043e \u0441\u043b\u0435\u0436\u0443 \u0437\u0430 \u043d\u0438\u043c, \u043d\u0435\u0443\u0441\u044b\u043f\u043d\u043e \u0435\u0433\u043e \u0441\u0442\u043e\u0440\u043e\u0436\u0443, \u0438 \u0431\u0443\u0434\u044c \u043e\u043d \u0443 \u043c\u0435\u043d\u044f \u0441\u043e\u0437\u043d\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434 \u0432\u0435\u0447\u043d\u044b\u043c \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u043d\u0438\u0435\u043c \u0438 \u0441\u0442\u0440\u0430\u0445\u043e\u043c, \u0442\u0430\u043a \u0432\u0435\u0434\u044c, \u0435\u0439-\u0431\u043e\u0433\u0443, \u0437\u0430\u043a\u0440\u0443\u0436\u0438\u0442\u0441\u044f, \u043f\u0440\u0430\u0432\u043e-\u0441, \u0441\u0430\u043c \u043f\u0440\u0438\u0434\u0435\u0442 \u0434\u0430, \u043f\u043e\u0436\u0430\u043b\u0443\u0439, \u0435\u0449\u0435 \u0438 \u043d\u0430\u0434\u0435\u043b\u0430\u0435\u0442 \u0447\u0435\u0433\u043e-\u043d\u0438\u0431\u0443\u0434\u044c, \u0447\u0442\u043e \u0443\u0436\u0435 \u043d\u0430 \u0434\u0432\u0430\u0436\u0434\u044b \u0434\u0432\u0430 \u043f\u043e\u0445\u043e\u0434\u0438\u0442\u044c \u0431\u0443\u0434\u0435\u0442, \u0442\u0430\u043a \u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u0438\u0434 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c, \u2014 \u043e\u043d\u043e \u0438 \u043f\u0440\u0438\u044f\u0442\u043d\u043e-\u0441. \r\n\r\n\u042d\u0442\u043e \u0438 \u0441 \u043c\u0443\u0436\u0438\u043a\u043e\u043c \u0441\u0438\u0432\u043e\u043b\u0430\u043f\u044b\u043c \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438, \u0430 \u0443\u0436 \u0441 \u043d\u0430\u0448\u0438\u043c \u0431\u0440\u0430\u0442\u043e\u043c, \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0443\u043c\u043d\u044b\u043c \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c, \u0434\u0430 \u0435\u0449\u0435 \u0432 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u0443\u044e \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0440\u0430\u0437\u0432\u0438\u0442\u044b\u043c, \u0438 \u043f\u043e\u0434\u0430\u0432\u043d\u043e!\r\n```"}
{"number": 2498, "owner": "nimning", "title": "nltk bleu_score return 0", "body": "I am using nltk's bleu_score package to compute blue score. However, I always got zero blue score for any sentence. Even if I use the cases mentioned in this post, I still got zero bleu score.\r\nhttps://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score \r\n\r\nI run scripts on a fresh installed anaconda. Python 3.6 reproduce the same error.\r\n\r\n```\r\n>>> import sys\r\n>>> import nltk\r\n>>> print(sys.version)\r\n3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\r\n>>> print(nltk.__version__)\r\n3.4.5\r\n>>> hypothesis = ['This', 'is', 'cat']\r\n>>> reference = ['This', 'is', 'a', 'cat']\r\n>>> references = [reference]\r\n>>> list_of_references = [references]\r\n>>> list_of_hypotheses = [hypothesis]\r\n>>> nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)\r\n\r\nC:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning:\r\nThe hypothesis contains 0 counts of 3-gram overlaps.\r\nTherefore the BLEU score evaluates to 0, independently of\r\nhow many N-gram overlaps of lower order it contains.\r\nConsider using lower n-gram order or use SmoothingFunction()\r\n  warnings.warn(_msg)\r\nC:\\Users\\user\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning:\r\nThe hypothesis contains 0 counts of 4-gram overlaps.\r\nTherefore the BLEU score evaluates to 0, independently of\r\nhow many N-gram overlaps of lower order it contains.\r\nConsider using lower n-gram order or use SmoothingFunction()\r\n  warnings.warn(_msg)\r\n**8.987727354491445e-155**\r\n```\r\n\r\n"}
{"number": 2497, "owner": "mohannad57", "title": "TextTiling cutoff_policy (HC is not set properly) ", "body": "The TextTiling code Line 297 - 300 shows:\r\n297:        if self.cutoff_policy == LC:\r\n298:            cutoff = avg - stdev / 2.0\r\n299:        else:\r\n300:            cutoff = avg - stdev / 2.0\r\n\r\nLine 300 should be\r\n300:            cutoff = avg + stdev / 2.0"}
{"number": 2495, "owner": "palasso", "title": "Redundant operations on ngram language models cause high performance impact", "body": "### General Info\r\n**Module:** `nltk.lm`\r\n**Type:** Performance\r\n**Impact:** High\r\n### Issue\r\nMost ngram language models perform redundant operations causing a high performance impact\r\n### Details\r\n#### Language models affected\r\n`Lidstone`, `Laplace`, `KneserNeyInterpolated`, `WittenBellInterpolated`\r\n#### Root cause\r\nRe-calculating `len(self.vocab)` and `self.counts[len(context) + 1].N()` instead of calculating them only once\r\n#### Performance impact\r\n100-1000x speed difference\r\n### Background\r\nWhile I was developing a noisy channel model I used NLTK's language models for the noisy channel model's language model. I observed a big difference in performance between `MLE` and the other language models provided by NLTK. After examining the code more thoroughly I identified the root cause of that difference to be some re-calculations being done unecessary every time the language model would calculate the probability of a ngram. I made a minimal set of changes to resolve the performance issue and observed a 100-1000x times increase in performance after I run some [tests](https://github.com/palasso/nltk/blob/9a3fa65cdf3b7175e2cda1eb704b7d57de1c2003/nltk.ipynb).\r\n### Update\r\nI updated the [tests](https://github.com/palasso/nltk/blob/30617c100ff07a889491fdb4ab2725359f20d1a8/nltk.ipynb) to include a different implementation that modifies only `Vocabulary` (based on discussion in PR #2496 ). This new implementation has the benefit of changing smaller parts of the `lm` module with the drawback that it doesn't resolve performance issues for `WittenBellInterpolated`."}
{"number": 2493, "owner": "ab-10", "title": "Fix escape sequences", "body": "Pull request in response to #2378 "}
{"number": 2492, "owner": "orbennatan", "title": "Install NTLK did not work on clean Anaconda python 3.7", "body": "To reproduce\r\nOn a Windows 10 machine uninstall old versions of Anaconda and Pycharm. Delete by hand everything python or anaconda I could find (including the registry).\r\nInstall Anaconda\r\npip install ntlk.\r\nYou get an error \"could not find version to satisfy the requirement\". It doesn't say version for what.\r\nWhat fixes the problem is\r\npip install ntlk=3.3\r\nWhich means I can't use the newest version of NTLK.\r\nThis used to work before. I noticed there is a new version now."}
{"number": 2491, "owner": "sajal2692", "title": "Reading a CFG from string with escaped single and double quotes in terminals results in failure", "body": "As mentioned in the topic title, reading a grammar string which contains productions with escaped single and/or double quotes in the terminals on the RHS results in different ValueErrors.\r\n\r\nExample A:\r\nThe following code:\r\n```\r\ngrammar = CFG.fromstring(\"\"\"\r\n            S -> A\r\n            A -> 'manager\\'s, discount'\r\n            \"\"\")\r\n```\r\nraises:\r\n```\r\n   raise ValueError('Expected a nonterminal, found: ' + string[pos:])\r\nValueError: Expected a nonterminal, found: , discount'\r\n```\r\n\r\nExample B:\r\nThe following code:\r\n```\r\ngrammar = CFG.fromstring(\"\"\"\r\n            S -> A\r\n            A -> \"\\\"manager\\'s discount\\\"\"\r\n            \"\"\")\r\n```\r\nraises:\r\n```\r\n    raise ValueError('Unterminated string')\r\nValueError: Unterminated string\r\n```\r\n\r\nThe reason for this is as follows:\r\nThe regex pattern `_TERMINAL_RE = re.compile(r'( \"[^\"]+\" | \\'[^\\']+\\' ) \\s*', re.VERBOSE)` used by the helper function `def _read_production` in https://www.nltk.org/_modules/nltk/grammar.html does not account for escaped single or double quote characters.\r\n\r\nProposed solution:\r\nFor the case when a terminal is enclosed with single quotes, modifying the regex to ` r'( \"[^\"]+\" | \\'([^\\']|[\\\\\\'])+\\' ) \\s*'` should enable it to account for escaped single quotes. Something similar can be done for the case with double quotes in the regex."}
{"number": 2478, "owner": "movabo", "title": "ValueError when unpickling ParentedTree with Python 3.7 or higher", "body": "While working in Python 3.6, when unpickling a ParentedTree with Python 3.7 or Python 3.8 following ValueError occurs:\r\n`ValueError: Can not insert a subtree that already has a parent.`\r\n\r\nFollowing example script produces the Error:\r\n\r\n```python\r\nimport pickle\r\nfrom nltk.tree import ParentedTree\r\n\r\n\r\ntree = ParentedTree.fromstring('(S (NN x) (NP x) (NN x))')\r\npickled = pickle.dumps(tree)\r\ntree_2 = pickle.loads(pickled)\r\n\r\nprint(tree)\r\nprint(tree_2)\r\n```\r\n\r\nOutput of Python 3.6 (working):\r\n```\r\n(S (NN x) (NP x) (NN x))\r\n(S (NN x) (NP x) (NN x))\r\n```\r\n\r\nOutput in Python 3.7 / 3.8:\r\n```\r\nTraceback (most recent call last):\r\n  File \"nltk_pickle_test.py\", line 7, in <module>\r\n    tree_2 = pickle.loads(pickled)\r\n  File \"pickletest/venv3.8/lib/python3.8/site-packages/nltk/tree.py\", line 1192, in extend\r\n    self._setparent(child, len(self))\r\n  File \"pickletest/venv3.8/lib/python3.8/site-packages/nltk/tree.py\", line 1358, in _setparent\r\n    raise ValueError('Can not insert a subtree that already ' 'has a parent.')\r\nValueError: Can not insert a subtree that already has a parent.\r\n```\r\n\r\nThe error also occurs when saving and loading via a file. Tested with nltk 3.4.5."}
{"number": 2477, "owner": "staticdev", "title": "Installation of nltk 3.4.5 problem with pip on Ubuntu and python 3.6.8", "body": "I am using latest pip (19.3.1) and when I try to install nltk (following the installation documentation) I get:\r\n\r\n```\r\nCollecting nltk\r\n  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5MB 2.0MB/s \r\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.13.0)\r\nBuilding wheels for collected packages: nltk\r\n  Building wheel for nltk (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-wd9ue65u --python-tag cp36\r\n       cwd: /tmp/pip-install-nnqqbh5b/nltk/\r\n  Complete output (405 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib\r\n  creating build/lib/nltk\r\n  copying nltk/grammar.py -> build/lib/nltk\r\n  copying nltk/book.py -> build/lib/nltk\r\n  copying nltk/jsontags.py -> build/lib/nltk\r\n  copying nltk/toolbox.py -> build/lib/nltk\r\n  copying nltk/treeprettyprinter.py -> build/lib/nltk\r\n  copying nltk/tree.py -> build/lib/nltk\r\n  copying nltk/lazyimport.py -> build/lib/nltk\r\n  copying nltk/featstruct.py -> build/lib/nltk\r\n  copying nltk/collections.py -> build/lib/nltk\r\n  copying nltk/treetransforms.py -> build/lib/nltk\r\n  copying nltk/__init__.py -> build/lib/nltk\r\n  copying nltk/data.py -> build/lib/nltk\r\n  copying nltk/compat.py -> build/lib/nltk\r\n  copying nltk/tgrep.py -> build/lib/nltk\r\n  copying nltk/wsd.py -> build/lib/nltk\r\n  copying nltk/util.py -> build/lib/nltk\r\n  copying nltk/text.py -> build/lib/nltk\r\n  copying nltk/decorators.py -> build/lib/nltk\r\n  copying nltk/collocations.py -> build/lib/nltk\r\n  copying nltk/downloader.py -> build/lib/nltk\r\n  copying nltk/help.py -> build/lib/nltk\r\n  copying nltk/internals.py -> build/lib/nltk\r\n  copying nltk/probability.py -> build/lib/nltk\r\n  creating build/lib/nltk/sentiment\r\n  copying nltk/sentiment/__init__.py -> build/lib/nltk/sentiment\r\n  copying nltk/sentiment/util.py -> build/lib/nltk/sentiment\r\n  copying nltk/sentiment/vader.py -> build/lib/nltk/sentiment\r\n  copying nltk/sentiment/sentiment_analyzer.py -> build/lib/nltk/sentiment\r\n  creating build/lib/nltk/cluster\r\n  copying nltk/cluster/kmeans.py -> build/lib/nltk/cluster\r\n  copying nltk/cluster/gaac.py -> build/lib/nltk/cluster\r\n  copying nltk/cluster/__init__.py -> build/lib/nltk/cluster\r\n  copying nltk/cluster/api.py -> build/lib/nltk/cluster\r\n  copying nltk/cluster/util.py -> build/lib/nltk/cluster\r\n  copying nltk/cluster/em.py -> build/lib/nltk/cluster\r\n  creating build/lib/nltk/chat\r\n  copying nltk/chat/eliza.py -> build/lib/nltk/chat\r\n  copying nltk/chat/rude.py -> build/lib/nltk/chat\r\n  copying nltk/chat/__init__.py -> build/lib/nltk/chat\r\n  copying nltk/chat/zen.py -> build/lib/nltk/chat\r\n  copying nltk/chat/util.py -> build/lib/nltk/chat\r\n  copying nltk/chat/iesha.py -> build/lib/nltk/chat\r\n  copying nltk/chat/suntsu.py -> build/lib/nltk/chat\r\n  creating build/lib/nltk/tag\r\n  copying nltk/tag/brill.py -> build/lib/nltk/tag\r\n  copying nltk/tag/stanford.py -> build/lib/nltk/tag\r\n  copying nltk/tag/mapping.py -> build/lib/nltk/tag\r\n  copying nltk/tag/__init__.py -> build/lib/nltk/tag\r\n  copying nltk/tag/senna.py -> build/lib/nltk/tag\r\n  copying nltk/tag/hunpos.py -> build/lib/nltk/tag\r\n  copying nltk/tag/api.py -> build/lib/nltk/tag\r\n  copying nltk/tag/hmm.py -> build/lib/nltk/tag\r\n  copying nltk/tag/util.py -> build/lib/nltk/tag\r\n  copying nltk/tag/perceptron.py -> build/lib/nltk/tag\r\n  copying nltk/tag/sequential.py -> build/lib/nltk/tag\r\n  copying nltk/tag/brill_trainer.py -> build/lib/nltk/tag\r\n  copying nltk/tag/crf.py -> build/lib/nltk/tag\r\n  copying nltk/tag/tnt.py -> build/lib/nltk/tag\r\n  creating build/lib/nltk/ccg\r\n  copying nltk/ccg/combinator.py -> build/lib/nltk/ccg\r\n  copying nltk/ccg/__init__.py -> build/lib/nltk/ccg\r\n  copying nltk/ccg/api.py -> build/lib/nltk/ccg\r\n  copying nltk/ccg/logic.py -> build/lib/nltk/ccg\r\n  copying nltk/ccg/lexicon.py -> build/lib/nltk/ccg\r\n  copying nltk/ccg/chart.py -> build/lib/nltk/ccg\r\n  creating build/lib/nltk/twitter\r\n  copying nltk/twitter/common.py -> build/lib/nltk/twitter\r\n  copying nltk/twitter/twitter_demo.py -> build/lib/nltk/twitter\r\n  copying nltk/twitter/__init__.py -> build/lib/nltk/twitter\r\n  copying nltk/twitter/api.py -> build/lib/nltk/twitter\r\n  copying nltk/twitter/util.py -> build/lib/nltk/twitter\r\n  copying nltk/twitter/twitterclient.py -> build/lib/nltk/twitter\r\n  creating build/lib/nltk/metrics\r\n  copying nltk/metrics/association.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/paice.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/__init__.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/agreement.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/distance.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/scores.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/spearman.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/confusionmatrix.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/aline.py -> build/lib/nltk/metrics\r\n  copying nltk/metrics/segmentation.py -> build/lib/nltk/metrics\r\n  creating build/lib/nltk/lm\r\n  copying nltk/lm/smoothing.py -> build/lib/nltk/lm\r\n  copying nltk/lm/preprocessing.py -> build/lib/nltk/lm\r\n  copying nltk/lm/models.py -> build/lib/nltk/lm\r\n  copying nltk/lm/counter.py -> build/lib/nltk/lm\r\n  copying nltk/lm/__init__.py -> build/lib/nltk/lm\r\n  copying nltk/lm/api.py -> build/lib/nltk/lm\r\n  copying nltk/lm/util.py -> build/lib/nltk/lm\r\n  copying nltk/lm/vocabulary.py -> build/lib/nltk/lm\r\n  creating build/lib/nltk/inference\r\n  copying nltk/inference/resolution.py -> build/lib/nltk/inference\r\n  copying nltk/inference/prover9.py -> build/lib/nltk/inference\r\n  copying nltk/inference/nonmonotonic.py -> build/lib/nltk/inference\r\n  copying nltk/inference/__init__.py -> build/lib/nltk/inference\r\n  copying nltk/inference/tableau.py -> build/lib/nltk/inference\r\n  copying nltk/inference/api.py -> build/lib/nltk/inference\r\n  copying nltk/inference/discourse.py -> build/lib/nltk/inference\r\n  copying nltk/inference/mace.py -> build/lib/nltk/inference\r\n  creating build/lib/nltk/translate\r\n  copying nltk/translate/ibm3.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ribes_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/chrf_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ibm4.py -> build/lib/nltk/translate\r\n  copying nltk/translate/metrics.py -> build/lib/nltk/translate\r\n  copying nltk/translate/phrase_based.py -> build/lib/nltk/translate\r\n  copying nltk/translate/gale_church.py -> build/lib/nltk/translate\r\n  copying nltk/translate/nist_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/__init__.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ibm5.py -> build/lib/nltk/translate\r\n  copying nltk/translate/api.py -> build/lib/nltk/translate\r\n  copying nltk/translate/gleu_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/meteor_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/stack_decoder.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ibm2.py -> build/lib/nltk/translate\r\n  copying nltk/translate/bleu_score.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ibm1.py -> build/lib/nltk/translate\r\n  copying nltk/translate/ibm_model.py -> build/lib/nltk/translate\r\n  copying nltk/translate/gdfa.py -> build/lib/nltk/translate\r\n  creating build/lib/nltk/corpus\r\n  copying nltk/corpus/europarl_raw.py -> build/lib/nltk/corpus\r\n  copying nltk/corpus/__init__.py -> build/lib/nltk/corpus\r\n  copying nltk/corpus/util.py -> build/lib/nltk/corpus\r\n  creating build/lib/nltk/tokenize\r\n  copying nltk/tokenize/repp.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/sexpr.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/stanford.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/toktok.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/regexp.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/casual.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/__init__.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/treebank.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/api.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/simple.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/stanford_segmenter.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/util.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/punkt.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/mwe.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/texttiling.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/sonority_sequencing.py -> build/lib/nltk/tokenize\r\n  copying nltk/tokenize/nist.py -> build/lib/nltk/tokenize\r\n  creating build/lib/nltk/app\r\n  copying nltk/app/chartparser_app.py -> build/lib/nltk/app\r\n  copying nltk/app/chunkparser_app.py -> build/lib/nltk/app\r\n  copying nltk/app/nemo_app.py -> build/lib/nltk/app\r\n  copying nltk/app/__init__.py -> build/lib/nltk/app\r\n  copying nltk/app/rdparser_app.py -> build/lib/nltk/app\r\n  copying nltk/app/wordnet_app.py -> build/lib/nltk/app\r\n  copying nltk/app/concordance_app.py -> build/lib/nltk/app\r\n  copying nltk/app/collocations_app.py -> build/lib/nltk/app\r\n  copying nltk/app/srparser_app.py -> build/lib/nltk/app\r\n  copying nltk/app/wordfreq_app.py -> build/lib/nltk/app\r\n  creating build/lib/nltk/sem\r\n  copying nltk/sem/relextract.py -> build/lib/nltk/sem\r\n  copying nltk/sem/lfg.py -> build/lib/nltk/sem\r\n  copying nltk/sem/chat80.py -> build/lib/nltk/sem\r\n  copying nltk/sem/__init__.py -> build/lib/nltk/sem\r\n  copying nltk/sem/linearlogic.py -> build/lib/nltk/sem\r\n  copying nltk/sem/hole.py -> build/lib/nltk/sem\r\n  copying nltk/sem/drt_glue_demo.py -> build/lib/nltk/sem\r\n  copying nltk/sem/boxer.py -> build/lib/nltk/sem\r\n  copying nltk/sem/cooper_storage.py -> build/lib/nltk/sem\r\n  copying nltk/sem/util.py -> build/lib/nltk/sem\r\n  copying nltk/sem/glue.py -> build/lib/nltk/sem\r\n  copying nltk/sem/logic.py -> build/lib/nltk/sem\r\n  copying nltk/sem/drt.py -> build/lib/nltk/sem\r\n  copying nltk/sem/skolemize.py -> build/lib/nltk/sem\r\n  copying nltk/sem/evaluate.py -> build/lib/nltk/sem\r\n  creating build/lib/nltk/stem\r\n  copying nltk/stem/isri.py -> build/lib/nltk/stem\r\n  copying nltk/stem/wordnet.py -> build/lib/nltk/stem\r\n  copying nltk/stem/regexp.py -> build/lib/nltk/stem\r\n  copying nltk/stem/__init__.py -> build/lib/nltk/stem\r\n  copying nltk/stem/api.py -> build/lib/nltk/stem\r\n  copying nltk/stem/porter.py -> build/lib/nltk/stem\r\n  copying nltk/stem/lancaster.py -> build/lib/nltk/stem\r\n  copying nltk/stem/util.py -> build/lib/nltk/stem\r\n  copying nltk/stem/snowball.py -> build/lib/nltk/stem\r\n  copying nltk/stem/cistem.py -> build/lib/nltk/stem\r\n  copying nltk/stem/rslp.py -> build/lib/nltk/stem\r\n  copying nltk/stem/arlstem.py -> build/lib/nltk/stem\r\n  creating build/lib/nltk/parse\r\n  copying nltk/parse/bllip.py -> build/lib/nltk/parse\r\n  copying nltk/parse/corenlp.py -> build/lib/nltk/parse\r\n  copying nltk/parse/stanford.py -> build/lib/nltk/parse\r\n  copying nltk/parse/recursivedescent.py -> build/lib/nltk/parse\r\n  copying nltk/parse/generate.py -> build/lib/nltk/parse\r\n  copying nltk/parse/__init__.py -> build/lib/nltk/parse\r\n  copying nltk/parse/pchart.py -> build/lib/nltk/parse\r\n  copying nltk/parse/api.py -> build/lib/nltk/parse\r\n  copying nltk/parse/shiftreduce.py -> build/lib/nltk/parse\r\n  copying nltk/parse/util.py -> build/lib/nltk/parse\r\n  copying nltk/parse/viterbi.py -> build/lib/nltk/parse\r\n  copying nltk/parse/malt.py -> build/lib/nltk/parse\r\n  copying nltk/parse/transitionparser.py -> build/lib/nltk/parse\r\n  copying nltk/parse/featurechart.py -> build/lib/nltk/parse\r\n  copying nltk/parse/dependencygraph.py -> build/lib/nltk/parse\r\n  copying nltk/parse/projectivedependencyparser.py -> build/lib/nltk/parse\r\n  copying nltk/parse/earleychart.py -> build/lib/nltk/parse\r\n  copying nltk/parse/evaluate.py -> build/lib/nltk/parse\r\n  copying nltk/parse/nonprojectivedependencyparser.py -> build/lib/nltk/parse\r\n  copying nltk/parse/chart.py -> build/lib/nltk/parse\r\n  creating build/lib/nltk/chunk\r\n  copying nltk/chunk/named_entity.py -> build/lib/nltk/chunk\r\n  copying nltk/chunk/regexp.py -> build/lib/nltk/chunk\r\n  copying nltk/chunk/__init__.py -> build/lib/nltk/chunk\r\n  copying nltk/chunk/api.py -> build/lib/nltk/chunk\r\n  copying nltk/chunk/util.py -> build/lib/nltk/chunk\r\n  creating build/lib/nltk/classify\r\n  copying nltk/classify/decisiontree.py -> build/lib/nltk/classify\r\n  copying nltk/classify/positivenaivebayes.py -> build/lib/nltk/classify\r\n  copying nltk/classify/__init__.py -> build/lib/nltk/classify\r\n  copying nltk/classify/senna.py -> build/lib/nltk/classify\r\n  copying nltk/classify/api.py -> build/lib/nltk/classify\r\n  copying nltk/classify/svm.py -> build/lib/nltk/classify\r\n  copying nltk/classify/naivebayes.py -> build/lib/nltk/classify\r\n  copying nltk/classify/textcat.py -> build/lib/nltk/classify\r\n  copying nltk/classify/util.py -> build/lib/nltk/classify\r\n  copying nltk/classify/tadm.py -> build/lib/nltk/classify\r\n  copying nltk/classify/weka.py -> build/lib/nltk/classify\r\n  copying nltk/classify/rte_classify.py -> build/lib/nltk/classify\r\n  copying nltk/classify/maxent.py -> build/lib/nltk/classify\r\n  copying nltk/classify/scikitlearn.py -> build/lib/nltk/classify\r\n  copying nltk/classify/megam.py -> build/lib/nltk/classify\r\n  creating build/lib/nltk/test\r\n  copying nltk/test/segmentation_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/semantics_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/translate_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/inference_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/discourse_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/doctest_nose_plugin.py -> build/lib/nltk/test\r\n  copying nltk/test/__init__.py -> build/lib/nltk/test\r\n  copying nltk/test/all.py -> build/lib/nltk/test\r\n  copying nltk/test/classify_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/gluesemantics_malt_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/wordnet_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/compat_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/probability_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/gensim_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/nonmonotonic_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/corpus_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/childes_fixt.py -> build/lib/nltk/test\r\n  copying nltk/test/runtests.py -> build/lib/nltk/test\r\n  copying nltk/test/portuguese_en_fixt.py -> build/lib/nltk/test\r\n  creating build/lib/nltk/tbl\r\n  copying nltk/tbl/rule.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/demo.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/__init__.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/api.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/template.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/erroranalysis.py -> build/lib/nltk/tbl\r\n  copying nltk/tbl/feature.py -> build/lib/nltk/tbl\r\n  creating build/lib/nltk/draw\r\n  copying nltk/draw/tree.py -> build/lib/nltk/draw\r\n  copying nltk/draw/__init__.py -> build/lib/nltk/draw\r\n  copying nltk/draw/util.py -> build/lib/nltk/draw\r\n  copying nltk/draw/table.py -> build/lib/nltk/draw\r\n  copying nltk/draw/dispersion.py -> build/lib/nltk/draw\r\n  copying nltk/draw/cfg.py -> build/lib/nltk/draw\r\n  creating build/lib/nltk/misc\r\n  copying nltk/misc/chomsky.py -> build/lib/nltk/misc\r\n  copying nltk/misc/sort.py -> build/lib/nltk/misc\r\n  copying nltk/misc/__init__.py -> build/lib/nltk/misc\r\n  copying nltk/misc/minimalset.py -> build/lib/nltk/misc\r\n  copying nltk/misc/wordfinder.py -> build/lib/nltk/misc\r\n  copying nltk/misc/babelfish.py -> build/lib/nltk/misc\r\n  creating build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/ycoe.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/sentiwordnet.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/panlex_lite.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/pros_cons.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/switchboard.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/chunked.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/toolbox.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/nkjp.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/categorized_sents.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/reviews.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/childes.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/knbc.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/aligned.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/wordnet.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/pl196x.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/comparative_sents.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/__init__.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/senseval.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/rte.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/plaintext.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/sinica_treebank.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/api.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/dependency.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/crubadan.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/verbnet.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/panlex_swadesh.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/nps_chat.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/util.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/opinion_lexicon.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/mte.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/lin.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/cmudict.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/udhr.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/indian.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/twitter.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/bnc.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/conll.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/string_category.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/tagged.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/framenet.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/chasen.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/ipipan.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/wordlist.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/timit.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/nombank.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/xmldocs.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/propbank.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/ieer.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/semcor.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/ppattach.py -> build/lib/nltk/corpus/reader\r\n  copying nltk/corpus/reader/bracket_parse.py -> build/lib/nltk/corpus/reader\r\n  creating build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_collocations.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_naivebayes.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_brill.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_2x_compat.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_corpus_views.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_twitter_auth.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_tgrep.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_nombank.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_tokenize.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/__init__.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_json2csv_corpus.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_wordnet.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_corpora.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_rte_classify.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_tag.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/utils.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_classify.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_cfg2chomsky.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_concordance.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_seekable_unicode_stream_reader.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_senna.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_chunk.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_stem.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_pos_tag.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_hmm.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_aline.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_disagreement.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_corenlp.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_data.py -> build/lib/nltk/test/unit\r\n  copying nltk/test/unit/test_cfd_mutation.py -> build/lib/nltk/test/unit\r\n  creating build/lib/nltk/test/unit/lm\r\n  copying nltk/test/unit/lm/__init__.py -> build/lib/nltk/test/unit/lm\r\n  copying nltk/test/unit/lm/test_preprocessing.py -> build/lib/nltk/test/unit/lm\r\n  copying nltk/test/unit/lm/test_counter.py -> build/lib/nltk/test/unit/lm\r\n  copying nltk/test/unit/lm/test_models.py -> build/lib/nltk/test/unit/lm\r\n  copying nltk/test/unit/lm/test_vocabulary.py -> build/lib/nltk/test/unit/lm\r\n  creating build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_gdfa.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_stack_decoder.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_nist.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/__init__.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm3.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm1.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm2.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm4.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_bleu.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm_model.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/unit/translate/test_ibm5.py -> build/lib/nltk/test/unit/translate\r\n  copying nltk/test/stem.doctest -> build/lib/nltk/test\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 1, in <module>\r\n    File \"/tmp/pip-install-nnqqbh5b/nltk/setup.py\", line 103, in <module>\r\n      zip_safe=False,  # since normal files will be present too?\r\n    File \"/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py\", line 145, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\r\n      dist.run_commands()\r\n    File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/home/01677387637/.local/lib/python3.6/site-packages/wheel/bdist_wheel.py\", line 192, in run\r\n      self.run_command('build')\r\n    File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/lib/python3.6/distutils/command/build.py\", line 135, in run\r\n      self.run_command(cmd_name)\r\n    File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py\", line 53, in run\r\n      self.build_package_data()\r\n    File \"/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py\", line 126, in build_package_data\r\n      srcfile in self.distribution.convert_2to3_doctests):\r\n  TypeError: argument of type 'NoneType' is not iterable\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for nltk\r\n  Running setup.py clean for nltk\r\nFailed to build nltk\r\nInstalling collected packages: nltk\r\n    Running setup.py install for nltk ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-hlea11gz/install-record.txt --single-version-externally-managed --compile\r\n         cwd: /tmp/pip-install-nnqqbh5b/nltk/\r\n    Complete output (407 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib\r\n    creating build/lib/nltk\r\n    copying nltk/grammar.py -> build/lib/nltk\r\n    copying nltk/book.py -> build/lib/nltk\r\n    copying nltk/jsontags.py -> build/lib/nltk\r\n    copying nltk/toolbox.py -> build/lib/nltk\r\n    copying nltk/treeprettyprinter.py -> build/lib/nltk\r\n    copying nltk/tree.py -> build/lib/nltk\r\n    copying nltk/lazyimport.py -> build/lib/nltk\r\n    copying nltk/featstruct.py -> build/lib/nltk\r\n    copying nltk/collections.py -> build/lib/nltk\r\n    copying nltk/treetransforms.py -> build/lib/nltk\r\n    copying nltk/__init__.py -> build/lib/nltk\r\n    copying nltk/data.py -> build/lib/nltk\r\n    copying nltk/compat.py -> build/lib/nltk\r\n    copying nltk/tgrep.py -> build/lib/nltk\r\n    copying nltk/wsd.py -> build/lib/nltk\r\n    copying nltk/util.py -> build/lib/nltk\r\n    copying nltk/text.py -> build/lib/nltk\r\n    copying nltk/decorators.py -> build/lib/nltk\r\n    copying nltk/collocations.py -> build/lib/nltk\r\n    copying nltk/downloader.py -> build/lib/nltk\r\n    copying nltk/help.py -> build/lib/nltk\r\n    copying nltk/internals.py -> build/lib/nltk\r\n    copying nltk/probability.py -> build/lib/nltk\r\n    creating build/lib/nltk/sentiment\r\n    copying nltk/sentiment/__init__.py -> build/lib/nltk/sentiment\r\n    copying nltk/sentiment/util.py -> build/lib/nltk/sentiment\r\n    copying nltk/sentiment/vader.py -> build/lib/nltk/sentiment\r\n    copying nltk/sentiment/sentiment_analyzer.py -> build/lib/nltk/sentiment\r\n    creating build/lib/nltk/cluster\r\n    copying nltk/cluster/kmeans.py -> build/lib/nltk/cluster\r\n    copying nltk/cluster/gaac.py -> build/lib/nltk/cluster\r\n    copying nltk/cluster/__init__.py -> build/lib/nltk/cluster\r\n    copying nltk/cluster/api.py -> build/lib/nltk/cluster\r\n    copying nltk/cluster/util.py -> build/lib/nltk/cluster\r\n    copying nltk/cluster/em.py -> build/lib/nltk/cluster\r\n    creating build/lib/nltk/chat\r\n    copying nltk/chat/eliza.py -> build/lib/nltk/chat\r\n    copying nltk/chat/rude.py -> build/lib/nltk/chat\r\n    copying nltk/chat/__init__.py -> build/lib/nltk/chat\r\n    copying nltk/chat/zen.py -> build/lib/nltk/chat\r\n    copying nltk/chat/util.py -> build/lib/nltk/chat\r\n    copying nltk/chat/iesha.py -> build/lib/nltk/chat\r\n    copying nltk/chat/suntsu.py -> build/lib/nltk/chat\r\n    creating build/lib/nltk/tag\r\n    copying nltk/tag/brill.py -> build/lib/nltk/tag\r\n    copying nltk/tag/stanford.py -> build/lib/nltk/tag\r\n    copying nltk/tag/mapping.py -> build/lib/nltk/tag\r\n    copying nltk/tag/__init__.py -> build/lib/nltk/tag\r\n    copying nltk/tag/senna.py -> build/lib/nltk/tag\r\n    copying nltk/tag/hunpos.py -> build/lib/nltk/tag\r\n    copying nltk/tag/api.py -> build/lib/nltk/tag\r\n    copying nltk/tag/hmm.py -> build/lib/nltk/tag\r\n    copying nltk/tag/util.py -> build/lib/nltk/tag\r\n    copying nltk/tag/perceptron.py -> build/lib/nltk/tag\r\n    copying nltk/tag/sequential.py -> build/lib/nltk/tag\r\n    copying nltk/tag/brill_trainer.py -> build/lib/nltk/tag\r\n    copying nltk/tag/crf.py -> build/lib/nltk/tag\r\n    copying nltk/tag/tnt.py -> build/lib/nltk/tag\r\n    creating build/lib/nltk/ccg\r\n    copying nltk/ccg/combinator.py -> build/lib/nltk/ccg\r\n    copying nltk/ccg/__init__.py -> build/lib/nltk/ccg\r\n    copying nltk/ccg/api.py -> build/lib/nltk/ccg\r\n    copying nltk/ccg/logic.py -> build/lib/nltk/ccg\r\n    copying nltk/ccg/lexicon.py -> build/lib/nltk/ccg\r\n    copying nltk/ccg/chart.py -> build/lib/nltk/ccg\r\n    creating build/lib/nltk/twitter\r\n    copying nltk/twitter/common.py -> build/lib/nltk/twitter\r\n    copying nltk/twitter/twitter_demo.py -> build/lib/nltk/twitter\r\n    copying nltk/twitter/__init__.py -> build/lib/nltk/twitter\r\n    copying nltk/twitter/api.py -> build/lib/nltk/twitter\r\n    copying nltk/twitter/util.py -> build/lib/nltk/twitter\r\n    copying nltk/twitter/twitterclient.py -> build/lib/nltk/twitter\r\n    creating build/lib/nltk/metrics\r\n    copying nltk/metrics/association.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/paice.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/__init__.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/agreement.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/distance.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/scores.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/spearman.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/confusionmatrix.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/aline.py -> build/lib/nltk/metrics\r\n    copying nltk/metrics/segmentation.py -> build/lib/nltk/metrics\r\n    creating build/lib/nltk/lm\r\n    copying nltk/lm/smoothing.py -> build/lib/nltk/lm\r\n    copying nltk/lm/preprocessing.py -> build/lib/nltk/lm\r\n    copying nltk/lm/models.py -> build/lib/nltk/lm\r\n    copying nltk/lm/counter.py -> build/lib/nltk/lm\r\n    copying nltk/lm/__init__.py -> build/lib/nltk/lm\r\n    copying nltk/lm/api.py -> build/lib/nltk/lm\r\n    copying nltk/lm/util.py -> build/lib/nltk/lm\r\n    copying nltk/lm/vocabulary.py -> build/lib/nltk/lm\r\n    creating build/lib/nltk/inference\r\n    copying nltk/inference/resolution.py -> build/lib/nltk/inference\r\n    copying nltk/inference/prover9.py -> build/lib/nltk/inference\r\n    copying nltk/inference/nonmonotonic.py -> build/lib/nltk/inference\r\n    copying nltk/inference/__init__.py -> build/lib/nltk/inference\r\n    copying nltk/inference/tableau.py -> build/lib/nltk/inference\r\n    copying nltk/inference/api.py -> build/lib/nltk/inference\r\n    copying nltk/inference/discourse.py -> build/lib/nltk/inference\r\n    copying nltk/inference/mace.py -> build/lib/nltk/inference\r\n    creating build/lib/nltk/translate\r\n    copying nltk/translate/ibm3.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ribes_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/chrf_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ibm4.py -> build/lib/nltk/translate\r\n    copying nltk/translate/metrics.py -> build/lib/nltk/translate\r\n    copying nltk/translate/phrase_based.py -> build/lib/nltk/translate\r\n    copying nltk/translate/gale_church.py -> build/lib/nltk/translate\r\n    copying nltk/translate/nist_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/__init__.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ibm5.py -> build/lib/nltk/translate\r\n    copying nltk/translate/api.py -> build/lib/nltk/translate\r\n    copying nltk/translate/gleu_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/meteor_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/stack_decoder.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ibm2.py -> build/lib/nltk/translate\r\n    copying nltk/translate/bleu_score.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ibm1.py -> build/lib/nltk/translate\r\n    copying nltk/translate/ibm_model.py -> build/lib/nltk/translate\r\n    copying nltk/translate/gdfa.py -> build/lib/nltk/translate\r\n    creating build/lib/nltk/corpus\r\n    copying nltk/corpus/europarl_raw.py -> build/lib/nltk/corpus\r\n    copying nltk/corpus/__init__.py -> build/lib/nltk/corpus\r\n    copying nltk/corpus/util.py -> build/lib/nltk/corpus\r\n    creating build/lib/nltk/tokenize\r\n    copying nltk/tokenize/repp.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/sexpr.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/stanford.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/toktok.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/regexp.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/casual.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/__init__.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/treebank.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/api.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/simple.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/stanford_segmenter.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/util.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/punkt.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/mwe.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/texttiling.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/sonority_sequencing.py -> build/lib/nltk/tokenize\r\n    copying nltk/tokenize/nist.py -> build/lib/nltk/tokenize\r\n    creating build/lib/nltk/app\r\n    copying nltk/app/chartparser_app.py -> build/lib/nltk/app\r\n    copying nltk/app/chunkparser_app.py -> build/lib/nltk/app\r\n    copying nltk/app/nemo_app.py -> build/lib/nltk/app\r\n    copying nltk/app/__init__.py -> build/lib/nltk/app\r\n    copying nltk/app/rdparser_app.py -> build/lib/nltk/app\r\n    copying nltk/app/wordnet_app.py -> build/lib/nltk/app\r\n    copying nltk/app/concordance_app.py -> build/lib/nltk/app\r\n    copying nltk/app/collocations_app.py -> build/lib/nltk/app\r\n    copying nltk/app/srparser_app.py -> build/lib/nltk/app\r\n    copying nltk/app/wordfreq_app.py -> build/lib/nltk/app\r\n    creating build/lib/nltk/sem\r\n    copying nltk/sem/relextract.py -> build/lib/nltk/sem\r\n    copying nltk/sem/lfg.py -> build/lib/nltk/sem\r\n    copying nltk/sem/chat80.py -> build/lib/nltk/sem\r\n    copying nltk/sem/__init__.py -> build/lib/nltk/sem\r\n    copying nltk/sem/linearlogic.py -> build/lib/nltk/sem\r\n    copying nltk/sem/hole.py -> build/lib/nltk/sem\r\n    copying nltk/sem/drt_glue_demo.py -> build/lib/nltk/sem\r\n    copying nltk/sem/boxer.py -> build/lib/nltk/sem\r\n    copying nltk/sem/cooper_storage.py -> build/lib/nltk/sem\r\n    copying nltk/sem/util.py -> build/lib/nltk/sem\r\n    copying nltk/sem/glue.py -> build/lib/nltk/sem\r\n    copying nltk/sem/logic.py -> build/lib/nltk/sem\r\n    copying nltk/sem/drt.py -> build/lib/nltk/sem\r\n    copying nltk/sem/skolemize.py -> build/lib/nltk/sem\r\n    copying nltk/sem/evaluate.py -> build/lib/nltk/sem\r\n    creating build/lib/nltk/stem\r\n    copying nltk/stem/isri.py -> build/lib/nltk/stem\r\n    copying nltk/stem/wordnet.py -> build/lib/nltk/stem\r\n    copying nltk/stem/regexp.py -> build/lib/nltk/stem\r\n    copying nltk/stem/__init__.py -> build/lib/nltk/stem\r\n    copying nltk/stem/api.py -> build/lib/nltk/stem\r\n    copying nltk/stem/porter.py -> build/lib/nltk/stem\r\n    copying nltk/stem/lancaster.py -> build/lib/nltk/stem\r\n    copying nltk/stem/util.py -> build/lib/nltk/stem\r\n    copying nltk/stem/snowball.py -> build/lib/nltk/stem\r\n    copying nltk/stem/cistem.py -> build/lib/nltk/stem\r\n    copying nltk/stem/rslp.py -> build/lib/nltk/stem\r\n    copying nltk/stem/arlstem.py -> build/lib/nltk/stem\r\n    creating build/lib/nltk/parse\r\n    copying nltk/parse/bllip.py -> build/lib/nltk/parse\r\n    copying nltk/parse/corenlp.py -> build/lib/nltk/parse\r\n    copying nltk/parse/stanford.py -> build/lib/nltk/parse\r\n    copying nltk/parse/recursivedescent.py -> build/lib/nltk/parse\r\n    copying nltk/parse/generate.py -> build/lib/nltk/parse\r\n    copying nltk/parse/__init__.py -> build/lib/nltk/parse\r\n    copying nltk/parse/pchart.py -> build/lib/nltk/parse\r\n    copying nltk/parse/api.py -> build/lib/nltk/parse\r\n    copying nltk/parse/shiftreduce.py -> build/lib/nltk/parse\r\n    copying nltk/parse/util.py -> build/lib/nltk/parse\r\n    copying nltk/parse/viterbi.py -> build/lib/nltk/parse\r\n    copying nltk/parse/malt.py -> build/lib/nltk/parse\r\n    copying nltk/parse/transitionparser.py -> build/lib/nltk/parse\r\n    copying nltk/parse/featurechart.py -> build/lib/nltk/parse\r\n    copying nltk/parse/dependencygraph.py -> build/lib/nltk/parse\r\n    copying nltk/parse/projectivedependencyparser.py -> build/lib/nltk/parse\r\n    copying nltk/parse/earleychart.py -> build/lib/nltk/parse\r\n    copying nltk/parse/evaluate.py -> build/lib/nltk/parse\r\n    copying nltk/parse/nonprojectivedependencyparser.py -> build/lib/nltk/parse\r\n    copying nltk/parse/chart.py -> build/lib/nltk/parse\r\n    creating build/lib/nltk/chunk\r\n    copying nltk/chunk/named_entity.py -> build/lib/nltk/chunk\r\n    copying nltk/chunk/regexp.py -> build/lib/nltk/chunk\r\n    copying nltk/chunk/__init__.py -> build/lib/nltk/chunk\r\n    copying nltk/chunk/api.py -> build/lib/nltk/chunk\r\n    copying nltk/chunk/util.py -> build/lib/nltk/chunk\r\n    creating build/lib/nltk/classify\r\n    copying nltk/classify/decisiontree.py -> build/lib/nltk/classify\r\n    copying nltk/classify/positivenaivebayes.py -> build/lib/nltk/classify\r\n    copying nltk/classify/__init__.py -> build/lib/nltk/classify\r\n    copying nltk/classify/senna.py -> build/lib/nltk/classify\r\n    copying nltk/classify/api.py -> build/lib/nltk/classify\r\n    copying nltk/classify/svm.py -> build/lib/nltk/classify\r\n    copying nltk/classify/naivebayes.py -> build/lib/nltk/classify\r\n    copying nltk/classify/textcat.py -> build/lib/nltk/classify\r\n    copying nltk/classify/util.py -> build/lib/nltk/classify\r\n    copying nltk/classify/tadm.py -> build/lib/nltk/classify\r\n    copying nltk/classify/weka.py -> build/lib/nltk/classify\r\n    copying nltk/classify/rte_classify.py -> build/lib/nltk/classify\r\n    copying nltk/classify/maxent.py -> build/lib/nltk/classify\r\n    copying nltk/classify/scikitlearn.py -> build/lib/nltk/classify\r\n    copying nltk/classify/megam.py -> build/lib/nltk/classify\r\n    creating build/lib/nltk/test\r\n    copying nltk/test/segmentation_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/semantics_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/translate_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/inference_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/discourse_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/doctest_nose_plugin.py -> build/lib/nltk/test\r\n    copying nltk/test/__init__.py -> build/lib/nltk/test\r\n    copying nltk/test/all.py -> build/lib/nltk/test\r\n    copying nltk/test/classify_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/gluesemantics_malt_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/wordnet_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/compat_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/probability_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/gensim_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/nonmonotonic_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/corpus_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/childes_fixt.py -> build/lib/nltk/test\r\n    copying nltk/test/runtests.py -> build/lib/nltk/test\r\n    copying nltk/test/portuguese_en_fixt.py -> build/lib/nltk/test\r\n    creating build/lib/nltk/tbl\r\n    copying nltk/tbl/rule.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/demo.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/__init__.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/api.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/template.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/erroranalysis.py -> build/lib/nltk/tbl\r\n    copying nltk/tbl/feature.py -> build/lib/nltk/tbl\r\n    creating build/lib/nltk/draw\r\n    copying nltk/draw/tree.py -> build/lib/nltk/draw\r\n    copying nltk/draw/__init__.py -> build/lib/nltk/draw\r\n    copying nltk/draw/util.py -> build/lib/nltk/draw\r\n    copying nltk/draw/table.py -> build/lib/nltk/draw\r\n    copying nltk/draw/dispersion.py -> build/lib/nltk/draw\r\n    copying nltk/draw/cfg.py -> build/lib/nltk/draw\r\n    creating build/lib/nltk/misc\r\n    copying nltk/misc/chomsky.py -> build/lib/nltk/misc\r\n    copying nltk/misc/sort.py -> build/lib/nltk/misc\r\n    copying nltk/misc/__init__.py -> build/lib/nltk/misc\r\n    copying nltk/misc/minimalset.py -> build/lib/nltk/misc\r\n    copying nltk/misc/wordfinder.py -> build/lib/nltk/misc\r\n    copying nltk/misc/babelfish.py -> build/lib/nltk/misc\r\n    creating build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/ycoe.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/sentiwordnet.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/panlex_lite.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/pros_cons.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/switchboard.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/chunked.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/toolbox.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/nkjp.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/categorized_sents.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/reviews.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/childes.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/knbc.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/aligned.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/wordnet.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/pl196x.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/comparative_sents.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/__init__.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/senseval.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/rte.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/plaintext.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/sinica_treebank.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/api.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/dependency.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/crubadan.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/verbnet.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/panlex_swadesh.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/nps_chat.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/util.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/opinion_lexicon.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/mte.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/lin.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/cmudict.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/udhr.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/indian.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/twitter.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/bnc.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/conll.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/string_category.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/tagged.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/framenet.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/chasen.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/ipipan.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/wordlist.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/timit.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/nombank.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/xmldocs.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/propbank.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/ieer.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/semcor.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/ppattach.py -> build/lib/nltk/corpus/reader\r\n    copying nltk/corpus/reader/bracket_parse.py -> build/lib/nltk/corpus/reader\r\n    creating build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_collocations.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_naivebayes.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_brill.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_2x_compat.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_corpus_views.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_twitter_auth.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_tgrep.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_nombank.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_tokenize.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/__init__.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_json2csv_corpus.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_wordnet.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_corpora.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_rte_classify.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_tag.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/utils.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_classify.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_cfg2chomsky.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_concordance.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_seekable_unicode_stream_reader.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_senna.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_chunk.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_stem.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_pos_tag.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_hmm.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_aline.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_disagreement.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_corenlp.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_data.py -> build/lib/nltk/test/unit\r\n    copying nltk/test/unit/test_cfd_mutation.py -> build/lib/nltk/test/unit\r\n    creating build/lib/nltk/test/unit/lm\r\n    copying nltk/test/unit/lm/__init__.py -> build/lib/nltk/test/unit/lm\r\n    copying nltk/test/unit/lm/test_preprocessing.py -> build/lib/nltk/test/unit/lm\r\n    copying nltk/test/unit/lm/test_counter.py -> build/lib/nltk/test/unit/lm\r\n    copying nltk/test/unit/lm/test_models.py -> build/lib/nltk/test/unit/lm\r\n    copying nltk/test/unit/lm/test_vocabulary.py -> build/lib/nltk/test/unit/lm\r\n    creating build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_gdfa.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_stack_decoder.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_nist.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/__init__.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm3.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm1.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm2.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm4.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_bleu.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm_model.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/unit/translate/test_ibm5.py -> build/lib/nltk/test/unit/translate\r\n    copying nltk/test/stem.doctest -> build/lib/nltk/test\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-nnqqbh5b/nltk/setup.py\", line 103, in <module>\r\n        zip_safe=False,  # since normal files will be present too?\r\n      File \"/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py\", line 145, in setup\r\n        return distutils.core.setup(**attrs)\r\n      File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\r\n        dist.run_commands()\r\n      File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n        self.run_command(cmd)\r\n      File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/usr/local/lib/python3.6/dist-packages/setuptools/command/install.py\", line 61, in run\r\n        return orig.install.run(self)\r\n      File \"/usr/lib/python3.6/distutils/command/install.py\", line 589, in run\r\n        self.run_command('build')\r\n      File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/usr/lib/python3.6/distutils/command/build.py\", line 135, in run\r\n        self.run_command(cmd_name)\r\n      File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py\", line 53, in run\r\n        self.build_package_data()\r\n      File \"/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py\", line 126, in build_package_data\r\n        srcfile in self.distribution.convert_2to3_doctests):\r\n    TypeError: argument of type 'NoneType' is not iterable\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-nnqqbh5b/nltk/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-hlea11gz/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\r\n```\r\n\r\nAny idea of what to do?"}
{"number": 2475, "owner": "liubo12", "title": "jaccard_distance does not match the data formula", "body": "from nltk.metrics.distance import jaccard_distance\r\nI think the jaccard_distance  does not match the data formula, the jaccard_distance might be:\r\n\r\n    return len(label1.intersection(label2)) / len(\r\n        label1.union(label2)\r\n    )\r\n\r\nbut the source code is :\r\n\r\n    return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(\r\n        label1.union(label2)\r\n    )\r\n"}
{"number": 2474, "owner": "xprogramer", "title": "Add a new Arabic light stemmer (ARLSTem2)", "body": "A new Arabic light stemmer that is an improvement of a previous algorithm (ARLSTem). The new algorithm considerably minimises the under-stemming errors produced by light stemmers."}
{"number": 2467, "owner": "kimballXD", "title": "corpus formatting error in sinica_treebank", "body": "### Issue\r\n\r\nI bounced to an error when I try to use the parsed sentences provided in  `sinica_treebank`.\r\n\r\nCode to reproduce the error:\r\n```python\r\nimport nltk\r\nfrom nltk.corpus import sinica_treebank\r\nnltk.download('sinica_treebank')\r\nraw_treebank = sinica_treebank.parsed_sents()\r\nfor i in range(10000):\r\n    try:\r\n        tree = raw_treebank[i]\r\n    except Exception as e:\r\n        print(i, e)\r\n```\r\nAs you can see from the output message, the parser failed to parse the corpus starting from the line index `5347`.\r\n\r\n###  Root Cause\r\nThere is missing a space in the line `5347` in `{central_installation_folder}/corpora/sinica_treebank/parsed` file. \r\n\r\nCurrently, the line `5437` starts with the text \r\n```#963:00963..[0]VP(evaluation:Dbb:\u4ecd\u7136|...``` \r\n\r\nChange it to \r\n```#963:00963..[0] VP(evaluation:Dbb:\u4ecd\u7136|```\r\n\r\nwill fix the parser error, i.e., makes `sinica_treebank.parsed_sents()` works again.\r\n\r\nPlease help to corect the corpus file. Thank you!\r\n\r\n\r\n"}
{"number": 2464, "owner": "alvations", "title": "[WIP] Testing new Github workflow feature", "body": "- Create pythonpackage.yml"}
{"number": 2460, "owner": "grofte", "title": "Danish sentence tokenizer fails to split on newline", "body": "When sentences are terminated with a newline rather than .!? the tokenizer fails to split. If it's wrong in Danish, it is probably wrong in a bunch of other languages. \r\n\r\nnltk version 3.4.5\r\nPython version 3.6.9\r\nUbuntu Linux 18.04\r\n\r\n```python\r\nimport nltk \r\nsent_tokenizer = nltk.data.load('tokenizers/punkt/danish.pickle')\r\ntext = \"\"\"Den normale kropstemperatur er 37,0\u00baC, n\u00e5r den m\u00e5les i endetarmen; temperaturen er lavere, n\u00e5r den m\u00e5les i f.eks. \u00f8ret eller munden\r\nFeber defineres som kernetemperatur over 38\u00b0C m\u00e5lt i endetarmen\r\nDe allerfleste sygdomme med feber er harml\u00f8se og helbreder sig selv. Det g\u00e6lder f.eks. fork\u00f8lelser og andre virussygdomme\r\nI nogle tilf\u00e6lde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebet\u00e6ndelse (meningitis), blodforgiftning (sepsis) og nyreb\u00e6kkenbet\u00e6ndelse (pyelonefritis)\r\n\"\"\"\r\nprint(sent_tokenizer.tokenize(text))\r\n```\r\n\r\n```\r\n['Den normale kropstemperatur er 37,0\u00baC, n\u00e5r den m\u00e5les i endetarmen; temperaturen er lavere, n\u00e5r den m\u00e5les i f.eks. \u00f8ret eller munden\\nFeber defineres som kernetemperatur over 38\u00b0C m\u00e5lt i endetarmen\\nDe allerfleste sygdomme med feber er harml\u00f8se og helbreder sig selv.', 'Det g\u00e6lder f.eks. fork\u00f8lelser og andre virussygdomme\\nI nogle tilf\u00e6lde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebet\u00e6ndelse (meningitis), blodforgiftning (sepsis) og nyreb\u00e6kkenbet\u00e6ndelse (pyelonefritis)']\r\n```"}
{"number": 2459, "owner": "michael-veksler", "title": "nltk.metrics.distance.jaro_similarity returns lower values than it should", "body": "Jaro similarity is supposed to give the same results if the strings are reversed:\r\n\r\n```\r\nfrom nltk.metrics import distance as dist\r\na='rureiter'\r\nb='enreiter'\r\nprint(\"regular={}, reversed={}\".format(dist.jaro_similarity(a, b), dist.jaro_similarity(a[::-1], b[::-1])))\r\n```\r\nThe code above prints `regular=0.722222222222, reversed=0.833333333333`.\r\n\r\nIn fact, manual pen-and-paper examination shows that the correct similarity metric, in both cases \r\nis 0.833333333333:\r\n\r\nSeparate the strings into prefix and suffix:\r\n```\r\na = 'ru' + 'reiter'\r\nb = 'en' + 'reiter'\r\n```\r\nThe suffixes of `a` and `b` are equal. Because the suffixes are equal, and the prefixes have nothing in common between them, then the expected values are `matches == 6`  and `transpositions == 0`. With these values:\r\n\r\n```\r\na = 'ru' + 'reiter'\r\nb = 'en' + 'reiter'\r\nmatches = 6\r\ntranspositions = 0\r\nprint(\r\n            1\r\n            / 3\r\n            * (\r\n                matches / len(a)\r\n                + matches / len(b)\r\n                + (matches - transpositions // 2) / matches\r\n            )\r\n        )\r\n```\r\nThe above code, unlike nltk, gives the correct answer of 0.8333333333333333 .\r\n\r\nThe issue lies in the fact that the current implementation does not try to minimize the number of transpositions in its algorithm, contrary to its documentation:\r\n\r\n> The Jaro distance between is the min no. of single-character transpositions\r\n>     required to change one word into another\r\n\r\nThe implementation simply finds the first occurrence of each character of the first string (`a`) in the second string (`b`). This order of matching does not guarantee that the match will be optimal.  In this example, the match is:\r\nThe first character of `a`  is \"r\", and is matched against the third character of `b`. From that point, the suffix \"reiter\" can't be matched in full. Worse, the next match of `a` is character \"e\" which is matched against the first character of `b`. This matching makes a transposition:\r\n\r\n```\r\nr u r e i t e r\r\n \\  _/    \r\n  \\/    \r\n  /\\   \r\n /  | \r\ne n r e i t e r\r\n```\r\nLater, things get even worse. The last \"e\" of `a` gets matched against the middle \"e\" of `b`:\r\n```\r\nr u r e i t e r\r\n \\  _/     /\r\n  \\/    __/\r\n  /\\   /\r\n /  | /\r\ne n r e i t e r\r\n```\r\nThis matching causes more transpositions, for no reason.\r\n\r\nA correct Jaro algorithm should find the minimum value of transposition possible.\r\n\r\nWith `matches=6`, and `transpositions=4` the result is 0.722222222222 .\r\n\r\nNote that `transpositions=4` because the list of matched indices of the second string is sorted, while the first is not. Before sorting:\r\n```\r\nflagged_1 = [0, 3, 4, 5, 6] \r\nflagged_2 = [2, 0, 4, 5, 3] \r\n```\r\nAfter sorting:\r\n```\r\nflagged_1 = [0, 3, 4, 5, 6] \r\nflagged_2 = [0, 2, 3, 4, 5]  # 4 different entries\r\n```\r\n"}
{"number": 2457, "owner": "katrinahuayu", "title": "Issue with averaged_perceptron_tagger", "body": "Faced with issue when using averaged_perceptron_tagger.\r\n\r\nLookupError: \r\n**********************************************************************\r\n  Resource C: not found.\r\n  Please use the NLTK Downloader to obtain the resource:\r\n\r\n  >>> import nltk\r\n  >>> nltk.download('C:')\r\n  \r\n  For more information see: https://www.nltk.org/data.html\r\n\r\n  Attempted to load /C:/Users/YW291553/Desktop/Caleres/Sentiment/VDIDFS/ProfileData/FolderRedirection/YW291553/Application Data/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\r\n\r\n  Searched in:\r\n    - ''\r\n"}
{"number": 2456, "owner": "jacobdweightman", "title": "grammar.CFG.is_chomsky_normal_form returns True even if start symbol is produced by some production", "body": "In a Chomsky normal form grammar, the start symbol must not occur on the right side of a production by definition. However, the current implementation does not check for this. As a result, NLTK indicates that the grammar `S -> S S` is in normal form, even though it does not comply with the definition.\r\n```\r\n>>> import nltk.grammar\r\n>>> G = nltk.grammar.CFG.fromstring(\"S -> S S\")\r\n>>> G.is_chomsky_normal_form()\r\nTrue\r\n```"}
{"number": 2442, "owner": "Izorar", "title": "Issue with synset_from_sense_key function to access adjective satellites", "body": "I tried to use the synset_from_sense_key function and here is the error:\r\n\r\nFile \"/home/izorar/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\", line 1356, in synset raise WordNetError(message % lemma)\r\n\r\nWordNetError: adjective satellite requested but only plain adjective found for lemma 'first'\r\n\r\nAny idea on how to fix the error?"}
{"number": 2441, "owner": "ndvbd", "title": "Wordnet synsets query problem ", "body": "Doing:\r\n`print wn.synsets(\"tablesssssssssssssssss\")`\r\nreturns results (it shouldn't, as there is no such word in English),\r\nBut if we use the Wordnet online API here:\r\n`http://wordnetweb.princeton.edu/perl/webwn`\r\nfor the same word, we get no results (correct).\r\nWhy is this problem?"}
{"number": 2436, "owner": "tybrs", "title": "Syntax error in parse string of higher-order expressions", "body": "https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/inference/tableau.py#L681-L694\r\n\r\n```\r\nfrom nltk.inference.tableau import *\r\ntestHigherOrderTableauProver()\r\n```\r\nGives the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nUnexpectedTokenException                  Traceback (most recent call last)\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in parse(self, data, signature)\r\n    155         try:\r\n--> 156             result = self.process_next_expression(None)\r\n    157             if self.inRange(0):\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in process_next_expression(self, context)\r\n    291 \r\n--> 292         accum = self.handle(tok, context)\r\n    293 \r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in handle(self, tok, context)\r\n    304         if self.isvariable(tok):\r\n--> 305             return self.handle_variable(tok, context)\r\n    306 \r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in handle_variable(self, tok, context)\r\n    360                 )\r\n--> 361             self.assertNextToken(Tokens.CLOSE)\r\n    362         return accum\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in assertNextToken(self, expected)\r\n    572             if tok != expected:\r\n--> 573                 raise UnexpectedTokenException(self._currentIndex, tok, expected)\r\n    574 \r\n\r\nUnexpectedTokenException: Unexpected token: '&'.  Expected token ')'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nLogicalExpressionException                Traceback (most recent call last)\r\n<ipython-input-13-36a31f27b2ca> in <module>\r\n----> 1 testHigherOrderTableauProver()\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in testHigherOrderTableauProver()\r\n    681 \r\n    682 def testHigherOrderTableauProver():\r\n--> 683     tableau_test('believe(j, -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])\r\n    684     tableau_test('believe(j, lie(b) & cheat(b))', ['believe(j, lie(b))'])\r\n    685     tableau_test(\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in tableau_test(c, ps, verbose)\r\n    698 def tableau_test(c, ps=None, verbose=False):\r\n    699     pc = Expression.fromstring(c)\r\n--> 700     pps = [Expression.fromstring(p) for p in ps] if ps else []\r\n    701     if not ps:\r\n    702         ps = []\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in <listcomp>(.0)\r\n    698 def tableau_test(c, ps=None, verbose=False):\r\n    699     pc = Expression.fromstring(c)\r\n--> 700     pps = [Expression.fromstring(p) for p in ps] if ps else []\r\n    701     if not ps:\r\n    702         ps = []\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in fromstring(cls, s, type_check, signature)\r\n    961             return cls._type_checking_logic_parser.parse(s, signature)\r\n    962         else:\r\n--> 963             return cls._logic_parser.parse(s, signature)\r\n    964 \r\n    965     def __call__(self, other, *additional):\r\n\r\n~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in parse(self, data, signature)\r\n    159         except LogicalExpressionException as e:\r\n    160             msg = '%s\\n%s\\n%s^' % (e, data, ' ' * mapping[e.index - 1])\r\n--> 161             raise LogicalExpressionException(None, msg)\r\n    162 \r\n    163         if self.type_check:\r\n\r\nLogicalExpressionException: Unexpected token: '&'.  Expected token ')'.\r\nbelieve(j, -lie(b) & -cheat(b))\r\n```\r\nIt appears the `Expression.fromstring` method is getting hung up on the lack of parenthesis around the propositional argument for the higher-order predicate. The following seems to work:\r\n```\r\nfrom nltk.inference.tableau import *\r\ntableau_test(\"believe(j, -lie(b))\", [\"believe(j, (-lie(b) & -cheat(b)))\"])\r\ntableau_test(\"believe(j, (lie(b) & cheat(b)))\", [\"believe(j, lie(b))\"])\r\ntableau_test(\r\n    \"believe(j, lie(b))\", [\"lie(b)\"]\r\n)  # how do we capture that John believes all things that are true\r\ntableau_test(\r\n    \"believe(j, know(b, cheat(b)))\",\r\n    [\"believe(j, (know(b, lie(b)) & know(b, (steals(b) & cheat(b)))))\"],\r\n)\r\ntableau_test(\"P(Q(y), (R(y) & R(z)))\", [\"P((Q(x) & Q(y)), (R(y) & R(z)))\"])\r\n\r\ntableau_test(\"believe(j, (cheat(b) & lie(b)))\", [\"believe(j, (lie(b) & cheat(b)))\"])\r\ntableau_test(\"believe(j, (-cheat(b) & -lie(b)))\", [\"believe(j, (-lie(b) & -cheat(b)))\"])\r\n```\r\nWilling to do further testing or PR, if needed. Cheers!\r\n\r\n[Edit: removed unintended \"not\"; fixed typo \"to\"]"}
{"number": 2435, "owner": "Daksh", "title": "Remove duplicate productions in CNF for CFGs", "body": "Related Issue #1884 \r\n\r\nI was not sure if I should add my name (in AUTHORS.md) for such a small change. Did not know what is the protocol followed in nltk community, if maintainers believe it is a mistake, please accept my apologies and let me know, I will amend the commit :)"}
{"number": 2434, "owner": "mmmm1998", "title": "Bad solving of issue #2151", "body": "https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/tag/mapping.py#L90-L112\r\n\r\nThis patch from #2151 just don't work, because `source == 'ru-rnc-new'` failed on line\r\nhttps://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/tag/mapping.py#L90\r\nwith LookupError for file 'ru-rnc-new.map'\r\n\r\nSo, why don't change 'ru-rnc-new' to `ru-rnc.map`, or just create `ru-rnc-new.map`?\r\n\r\nP.S. this is a @alvations patch, so requesting the author"}
{"number": 2433, "owner": "fuzihaofzh", "title": "corpus_ribes divide 0 error", "body": "In ribes_score.py line 290. I encounter the divide 0 error. We should prevent num_possible_pairs from being 0."}
{"number": 2429, "owner": "shinkeika", "title": " Chinese characters not  show in tree.draw. Maybe there is a deeper problem.", "body": "nltk tree draw cannot plot the syntax tree of Chinese Characters\r\nI can use the pretty_print to draw.\r\nbut in tree.draw() Chinese Characters has gone\r\n\r\n![image](https://user-images.githubusercontent.com/29203912/66900019-988c3c80-f02e-11e9-9261-814b33b2b9d6.png)\r\n\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/29203912/66899499-a5f4f700-f02d-11e9-99fb-f0cdffccab51.png)\r\n\r\n"}
{"number": 2428, "owner": "antran22", "title": "Implemented nltk.lm.LanguageModel.suggest", "body": "Implemented a `suggest` method for LanguageModel, which gives out a list of words paired with their score that can follow a given context.\r\nIn order for this method to work, another method `_weighted_sample` must be implemented. This method sample k elements without replacement in a given population.\r\nAlso I've rewritten `_weighted_choice` to accept another `k` parameter, which is the number of the sampled results."}
{"number": 2427, "owner": "tianfrank", "title": "How to use the user/custom dictionary when tokenizing Chinese texts using Stanford CoreNLP API in NLTK?", "body": "When I try to do dependency parsing using the stanford parser, it tokenizes the texts first. However, the results are not always satisfactory concerning Chinese texts. Can I add my own words to the dictionary so that it will not split one Chinese word into two tokens?\r\nThanks very much!"}
{"number": 2425, "owner": "bsolomon1124", "title": "Docstring for word_tokenize \"preserve_line\" parameter is unclear", "body": "From the `word_tokenize()` docstring:\r\n\r\n```\r\n:param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\r\n```\r\n\r\nHuh?  It seems there's at least an extra \"the\" in there.\r\n\r\nOn top of that, it does not actually tell a new reader _what this does_.  I'm assuming this is a reference to the fact that `TreebankWordTokenizer` expects sentence-tokenized inputs.  From\r\n\r\n```\r\nsentences = [text] if preserve_line else sent_tokenize(text, language)\r\n```\r\n\r\nIt _looks_ like what this parameter does is to say, \"if you're telling `word_tokenize()` that the input is a single sentence, then skip the sent tokenization step.\"  But that's entirely unclear from the docstring."}
{"number": 2424, "owner": "agannon", "title": "Feature/multiple ngram bleu", "body": ""}
{"number": 2423, "owner": "stevenbird", "title": "Extract wordnet into a separate package", "body": "@alvations has created https://github.com/nltk/wordnet\r\nWe need to deprecate the existing NLTK wordnet corpus reader\r\n"}
{"number": 2421, "owner": "goodmami", "title": "Cannot get WordNet synsets for English without lemmatization", "body": "Querying wordnet with `wordnet.synsets()` will lemmatize the query word, but only for English. While this is useful for many applications, sometimes I do not want such lemmatization. For instance, I have dictionary forms for multiple languages (from, e.g., Swadesh lists) and I want to detect differences in polysemy between languages, but the lemmatization inflates the apparent polysemy for English. There appears to be no way (in the public API) to do an English query without lemmatization.\r\n\r\nFor example:\r\n\r\n```python\r\n>>> wn.synsets('eyeglasses')\r\n[Synset('spectacles.n.01'), Synset('monocle.n.01')]\r\n>>> wn.synsets('eyeglasses')[0].lemma_names()\r\n['spectacles', 'specs', 'eyeglasses', 'glasses']\r\n>>> wn.synsets('eyeglasses')[1].lemma_names()\r\n['monocle', 'eyeglass']\r\n```\r\n\r\nThe second synset (`monocle.n.01`) was found because 'eyeglass' appears in its lemmas, but not 'eyeglasses', which is only in the first synset. Sometimes specifying the POS can help, as with 'scissors' and 'scissor.v.01', but not always (as with 'eyeglasses' above, both are 'n'). I end up needing to write a wrapper like this:\r\n\r\n```python\r\ndef synsets(lemma, pos=None, lang='eng', check_exceptions=True):\r\n    results = wn.synsets(lemma,\r\n                         pos=pos,\r\n                         lang=lang,\r\n                         check_exceptions=check_exceptions)\r\n    if lang == 'eng':\r\n        results = [ss for ss in results if lemma in ss.lemma_names()]\r\n    return results\r\n```\r\n\r\nAm I missing something or is this currently the best way around the issue?"}
{"number": 2420, "owner": "etienne-v", "title": "WordNet sense keys mismatch", "body": "Hi,\r\n\r\nI'm having problems with retrieving the correct definitions from the nltk wordnet corpus for a given sense key.  Please see [this](https://github.com/getalp/UFSAC/issues/2) related issue from the UFSAC repo.\r\n\r\nThe current ntlk WordNet version is 3.0:\r\n```\r\nimport nltk\r\nnltk.download('wordnet')\r\nfrom nltk.corpus import wordnet as wn\r\n\r\nprint(wn.get_version())\r\n```\r\ngives:\r\n```\r\n3.0\r\n```\r\n\r\nLooking at the official WordNet 3.0 database (http://wordnetcode.princeton.edu/3.0/WordNet-3.0.tar.gz), the key \"drive%1:04:03::\" has a synset ID of 00572489 (according to the `dict/index.sense` file), and the definition related to this synset ID is \"hitting a golf ball off of a tee with a driver\" (according to the `dict/data.noun` file).\r\n\r\nThe key associated with the above definition from the official WordNet online search is also \"drive%1:04:03::\" (along with \"driving%1:04:03::\"), even though the online search version is 3.1.\r\n\r\nFirstly, using the key \"drive%1:04:03::\" to look up the definition in ntlk as follows\r\n```\r\nprint(wn.synset_from_sense_key(\"drive%1:04:03::\"))\r\nprint(wn.synset_from_sense_key(\"drive%1:04:03::\").definition())\r\nprint(wn.synset_from_sense_key(\"drive%1:04:03::\").offset())\r\n```\r\ngives:\r\n```\r\nSynset('campaign.n.02')\r\na series of actions advancing a principle or tending toward a particular end\r\n798245\r\n```\r\nThis definition and ID doesn't correspond to either the WordNet online search or the database.  However, querying nltk with the key \"drive%1:04:06::\" (replacing 03 with 06)\r\n```\r\nprint(wn.synset_from_sense_key(\"drive%1:04:06::\"))\r\nprint(wn.synset_from_sense_key(\"drive%1:04:06::\").definition())\r\nprint(wn.synset_from_sense_key(\"drive%1:04:06::\").offset())\r\n```\r\ngives the expected definition and ID:\r\n```\r\nSynset('drive.n.06')\r\nhitting a golf ball off of a tee with a driver\r\n572489\r\n```\r\nFor this key, why doesn't ntlk give the definition as recorded in the database and the online search?\r\n\r\nAlso, when using the key \"driving%1:04:03::\" (referring to the second synonym for that sense), to obtain the synset with\r\n```\r\nprint(wn.synset_from_sense_key(\"driving%1:04:03::\"))\r\n```\r\nI get the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset(self, name)\r\n   1333         try:\r\n-> 1334             offset = self._lemma_pos_offset_map[lemma][pos][synset_index]\r\n   1335         except KeyError:\r\n\r\nIndexError: list index out of range\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nWordNetError                              Traceback (most recent call last)\r\n<ipython-input-83-1778759d53e3> in <module>\r\n----> 1 print(wn.synset_from_sense_key(\"driving%1:04:03::\"))\r\n\r\n~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset_from_sense_key(self, sense_key)\r\n   1563 \r\n   1564         synset_id = '.'.join([lemma, synset_types[int(ss_type)], lex_id])\r\n-> 1565         return self.synset(synset_id)\r\n   1566 \r\n   1567     #############################################################\r\n\r\n~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset(self, name)\r\n   1343             else:\r\n   1344                 tup = lemma, pos, n_senses, \"senses\"\r\n-> 1345             raise WordNetError(message % tup)\r\n   1346 \r\n   1347         # load synset information from the appropriate file\r\n\r\nWordNetError: lemma 'driving' with part of speech 'n' has only 2 senses\r\n```\r\n\r\nAny advice on the above issues will be much appreciated... thanks!"}
{"number": 2416, "owner": "bieniek", "title": "nltk.corpus.reader.NKJPCorpusReader fails on Python 3.x", "body": "NKJPCorpusReader object inside is using tempfile library which is different in v. 2.7 and 3.5. In Python v.2.7:\r\n```\r\nself.write_file = tempfile.NamedTemporaryFile(delete=False)\r\n```\r\ncreates file with mode='w'\r\n\r\nIn python v3.x\r\n\r\ncreates file with mode='w+b' which makes line 274 of '~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/nltk/corpus/reader/nkjp.py' failing with error:\r\n\r\nTypeError: a bytes-like object is required, not 'str'\r\n\r\nAddition of explicit mode is required to fix it."}
{"number": 2415, "owner": "tianfrank", "title": "How to parse a (Chinese) paragraph/passage using Stanford CoreNLP API in NLTK", "body": "Dear all,\r\nI'm new to natural language processing. Now I'm trying to do dependency parsing for a text. The examples given are all single sentences. Can anybody help me out about the depdendency parsing of a paragraph or even a passage? And it would be very helpful if the example is given for Chinese multiple sentences parsing.\r\nThank you very much!"}
{"number": 2412, "owner": "stevenbird", "title": "CI instructions are out of date", "body": "http://www.nltk.org/dev/jenkins.html should now refer to https://www.travis-ci.org/nltk/nltk\r\n"}
{"number": 2378, "owner": "pombredanne", "title": "Update various regex escape sequences", "body": "The latest versions of Python are more strict wrt. escape in regex.\r\nFor instance with 3.6.8, there are 10+ warnings like this one:\r\n```\r\n...\r\nlib/python3.6/site-packages/nltk/featstruct.py:2092: DeprecationWarning: invalid escape sequence \\d\r\n    RANGE_RE = re.compile('(-?\\d+):(-?\\d+)')\r\n```\r\n\r\nThe regex(es) should be updated to silence these warnings."}
{"number": 2376, "owner": "vezeli", "title": "Splitting sentences fails on some corner cases", "body": "I understand how difficult it is to split sentences that contain abbreviations and that adding abbreviations can have pitfalls, as it is nicely explained in #2154. However, I have stumbled upon some corner cases that I would like to ask about. It looks like using any of the following\r\n- e.g.\r\n- i.e.\r\n- et al.\r\n\r\nin the sentence will split the sentence in a wrong way.\r\n\r\nExample for i.e. and e.g.\r\n\r\n```python\r\n>>> sentence = (\"Even though exempli gratia and id est are both Latin \"\r\n                \"(and therefore italicized), no need to put e.g. or i.e. in \"\r\n                \"italics when they\u2019re in abbreviated form.\")\r\n>>> sent_tokenize_list = sent_tokenize(sentence)                                                                                                                           \r\n\r\n>>> sent_tokenize_list                                                                                                                                            \r\n['Even though exempli gratia and id est are both Latin (and therefore italicized), no need to put e.g.',\r\n 'or i.e.',\r\n 'in italics when they\u2019re in abbreviated form.']\r\n```\r\n\r\nExample for et al.\r\n\r\n```python                                                 \r\n>>> from nltk.tokenize import sent_tokenize\r\n>>> sentence = (\"If David et al. get the financing, we can move forward \"\r\n                \"with the prototype. However, this is very unlikely be cause \"\r\n                \"they did not publish sufficiently last year.\")\r\n>>> sent_tokenize_list = sent_tokenize(sentence)\r\n>>> sent_tokenize_list\r\n['If David et al.',\r\n 'get the financing, we can move forward with the prototype.',\r\n 'However, this is very unlikely because they did not publish sufficiently last year.']\r\n```\r\n\r\nOn my laptop I am using  `nltk.__version__`  3.4.5.\r\n\r\nAs I see it this issue is different than #2154 because these are well known and commonly used abbreviations (especially in academic circles). \r\n"}
{"number": 2374, "owner": "behunter957", "title": "problem with pos_tag and the cardinal \"zero\"", "body": "in the following code, first sentence tagging unexpectedly tags \"zero\" as 'NN', second sentence tagging correctly tags \"zero\" as 'CD'\r\n\r\ndef preprocess(sent):\r\n\tsent = nltk.word_tokenize(sent)\r\n\tsent = nltk.pos_tag(sent)\r\n\treturn sent\r\n\r\nsentence = \"\"\"my social security number is nine one three seven four four six five zero definitely.\"\"\"\r\nsent = preprocess(sentence)\r\nprint(sent)\r\nsentence = \"\"\"my social security number is nine one three seven four four six zero five definitely.\"\"\"\r\nsent = preprocess(sentence)\r\nprint(sent)\r\n"}
{"number": 2373, "owner": "waltbai", "title": "MaltParser bug for parse_tagged_sents", "body": "maltparser version 1.9.2.\r\nI've input several sentences but the parser only output the result of the first sentence.\r\nThen I read the examples in maltparser and I found that the input and output .conll files only use one blank line to split two sentences.\r\nIt seems that nltk.parse.util.taggedsents_to_conll(sentences) should deal with this problem."}
{"number": 2371, "owner": "alvations", "title": "Replace nltk.decorators.py with decorator library", "body": "From @Copper-Head, \r\n\r\nCan we get rid of `decorators.py` by using the [decorator](https://pypi.org/project/decorator/) library?"}
{"number": 2365, "owner": "Zhaofeng-Wu", "title": "What is the proper way to compute a corpus level METEOR score?", "body": "For BLEU, there is a `corpus_bleu` function, but there is no such function for METEOR. I am reading the METEOR paper and code, and it looks like it's more complicated than a simple average of sentence METEOR scores. How should I do it then? Should I concatenate all the hypothesis & reference sentences, and then compute the METEOR score?"}
{"number": 2360, "owner": "chengafni", "title": "nltk.app.wordnet() error", "body": "Hi,\r\nI get an error message when trying to run nltk.app.wordnet() (see below).\r\nSystem specifications: Win 7 64 bit, Enthought Canopy environment version 2.1.9.3717 (64 bit) , Python version 3.5.2, NLTK version: 3.2.4-5 (I know it's not the latest released version, but for some reason Canopy insists that there aren't newer versions).\r\nAny suggestions?\r\nThanks,\r\nChen\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nGetoptError                               Traceback (most recent call last)\r\n<ipython-input-43-a9319fd796ca> in <module>()\r\n----> 1 nltk.app.wordnet()\r\n\r\nC:\\Users\\Chen G\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\nltk\\app\\wordnet_app.py in app()\r\n    945     # Parse and interpret options.\r\n    946     (opts, _) = getopt.getopt(argv[1:], \"l:p:sh\",\r\n--> 947                               [\"logfile=\", \"port=\", \"server-mode\", \"help\"])\r\n    948     port = 8000\r\n    949     server_mode = False\r\nC:\\Users\\Chen G\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\getopt.py in getopt(args, shortopts, longopts)\r\n     93             opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\r\n     94         else:\r\n---> 95             opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\r\n     96 \r\n     97     return opts, args\r\nC:\\Users\\Chen G\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\getopt.py in do_shorts(opts, optstring, shortopts, args)\r\n    193     while optstring != '':\r\n    194         opt, optstring = optstring[0], optstring[1:]\r\n--> 195         if short_has_arg(opt, shortopts):\r\n    196             if optstring == '':\r\n    197                 if not args:\r\nC:\\Users\\Chen G\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\getopt.py in short_has_arg(opt, shortopts)\r\n    209         if opt == shortopts[i] != ':':\r\n    210             return shortopts.startswith(':', i+1)\r\n--> 211     raise GetoptError(_('option -%s not recognized') % opt, opt)\r\n    212 \r\n    213 if __name__ == '__main__':\r\nGetoptError: option -f not recognized\r\n```"}
{"number": 2357, "owner": "mi-kon", "title": "nltk.download() logs out on OS X", "body": "Hello,\r\n\r\nI have a macbook running OS X 10.14.6, Python 3.7.3, conda 4.7.5 .\r\n\r\nWhen I try run interactive installer from NLTK python package, following the steps from: http://www.nltk.org/data.html in my terminal, after\r\n\r\n`>> nltk.download()`\r\n\r\nInstead of opening NLTK Downloader in a new window, I get logged out (ok, i assume something related to GUI crashes at this point). `nltk.download_shell()`works instead, as it will not try to open GUI installer, but I still want to figure out why GUI won't work for me. Any suggestions would be appreciated!\r\n"}
{"number": 2352, "owner": "BramVanroy", "title": "Escape parentheses in NLTK parse tree", "body": "In NLTK we can convert a parentheses tree into an actual Tree object. However, when a token contains parentheses, the parsing is not what you would expect since NLTK parses those parentheses as a new node.\r\n\r\nAs an example, take the sentence\r\n\r\n> They like(d) it a lot\r\n\r\nThis could be parsed as \r\n\r\n```\r\n(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))\r\n```\r\n\r\nBut if you parse this with NLTK into a tree, and output it - it is clear that the `(d)` is parsed as a new node, which is no surprise.\r\n\r\n\r\n```python\r\nfrom nltk import Tree\r\n\r\ns = '(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))'\r\n\r\ntree = Tree.fromstring(s)\r\nprint(tree)\r\n```\r\n\r\nThe result is\r\n\r\n```\r\n(S\r\n  (NP (PRP They))\r\n  (VP like (d ) (NP (PRP it)) (NP (DT a) (NN lot)))\r\n  (. .))\r\n```\r\n\r\nSo `(d )` is a node inside the VP rather than part of the token `like`. Is there a way in the tree parser to escape parentheses?\r\n\r\n[[cross post from SO](https://stackoverflow.com/questions/57293069/escape-parentheses-in-nltk-parse-tree)]"}
{"number": 2351, "owner": "ndo3", "title": "WordNet lemmas do not contain non-zero causes() and entailments()", "body": "Hey! I have been trying to look for the causes() and entailments() relationship between verbs like in the WordNet documentation by Christiane Fellbaum, but those functions return no non-empty lists given any lemma. Based on Christiane's documentation, Lemma('kill').causes() should return a list that contains at least one element, 'die'. Is this an unimplemented feature of NLTK WordNet, or is it a bug?\r\n\r\nThanks! :)"}
{"number": 2346, "owner": "KyleRaney", "title": "Possible Install Solve", "body": "Possible Solve. no errors so far. working with 64 bit\r\ninstalled pip, numpy, wheel, pipenv, setuptools, virtualenv, virtualwrapper, NLTK, etc.\r\n\r\npip needs to be 19+ version and python 3+ version\r\n\r\nsystem-> Advanced system settings -> Environment Variables -> EDIT path -> NEW paths\r\n\r\nC:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\r\nC:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\r\nC:\\Users\\USERNAME\\AppData\\Roaming\\Python\\Python37\\Scripts\r\nC:\\Users\\USERNAME\\AppData\\Roaming\\Python\\Python37\r\n\r\nof course these will differ for where ever you installed pip & python and your username\r\nIMPORTANT you do Local and Roaming paths\r\n\r\nHope this helps! seems to work for me\r\n\r\n_Originally posted by @KyleRaney in https://github.com/nltk/nltk/issues/1079#issuecomment-516202186_"}
{"number": 2341, "owner": "hzhwcmhf", "title": "A wrong implementation in bleu_score.SmoothingFunction", "body": "I found the bug in the condition like this:\r\n\r\n```\r\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\r\n\r\nhyp = ['a', 'a', 'a', 'a', 'a', 'a', 'a']\r\nref = [['a', 'a', 'a', 'd', 'a', 'a', 'a']]\r\nprint(corpus_bleu([ref], [hyp], (0, 0, 0, 1), smoothing_function=SmoothingFunction().method4))\r\n\r\nhyp = ['a', 'b', 'b', 'b', 'b', 'b', 'b']\r\nref = [['a', 'a', 'a', 'd', 'a', 'a', 'a']]\r\nprint(corpus_bleu([ref], [hyp], (0, 0, 0, 1), smoothing_function=SmoothingFunction().method4))\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\n0.17954959837224665\r\n0.17954959837224665\r\n```\r\n\r\nIt's quite strange why they can have same smoothing 4-gram precision, so I went to the code at https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L576-L590\r\n\r\n```\r\ndef method4(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):\r\n        \"\"\"\r\n        Smoothing method 4:\r\n        Shorter translations may have inflated precision values due to having\r\n        smaller denominators; therefore, we give them proportionally\r\n        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry\r\n        suggests dividing by 1/ln(len(T)), where T is the length of the translation.\r\n        \"\"\"\r\n        for i, p_i in enumerate(p_n):\r\n            if p_i.numerator == 0 and hyp_len != 0:\r\n                incvnt = i + 1 * self.k / math.log(\r\n                    hyp_len\r\n                )  # Note that this K is different from the K from NIST.\r\n                p_n[i] = 1 / incvnt\r\n        return p_n\r\n```\r\n\r\n`p_n[i]` is a fraction object, once `p_n[i].numberator == 0`, it will turn to a constant of float that `p_n[i] = 1 / (i + self.k / math.log(hyp_len))`.\r\n\r\nI find the original paper and it says:\r\n\r\n**the numerator (to be precise, the matched n-gram count) should be `1 / (i + self.k / math.log(hyp_len))`, but the denominator should remain the same !!**\r\n\r\nI go through the other smoothing function roughly, it seems Smoothing 4-7 have the same problem... I think it should be fixed immediately, because I have already find some papers' results are wrong because of ``nltk``....\r\n\r\n"}
{"number": 2339, "owner": "BernierCR", "title": "svd_dimensions in VectorSpaceClusterer is broken again. ", "body": "So I would like to try using svd_dimensions in KMeansCluster. So you set it to what you want when you instantiate the class. Then, when you run kclusterer.cluster, it's supposed to pick up that setting and apply it to your input. \r\n\r\nBut then it complains about mismatched dimensions, so it must not be doing that part correctly. \r\n\r\nException has occurred: ValueError\r\nshapes (500,1124) and (500,) not aligned: 1124 (dim 1) != 500 (dim 0)\r\n\r\nAny idea what's going on? I have 1124 dimensions, so SVD would be helpful. \r\n\r\nThanks"}
{"number": 2334, "owner": "danuker", "title": "TypeError in FreqDist.tabulate for bigrams", "body": "I use the following code:\r\n\r\n```python\r\nwith open(corpus_filename) as corpus_f:\r\n    tokens = preprocess(corpus_f.read())\r\n    bgs = nltk.bigrams(tokens)\r\n    fdist =  nltk.FreqDist(bgs)\r\n    print(fdist.tabulate())\r\n```\r\n\r\nWhat happens is the `tabulate` method can't format the tuples generated by `bigrams`:\r\n`width = max(len(\"%s\" % s) for s in samples)`\r\n\r\nWhen it gets to any key like `('hello', 'world)`, it breaks:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"learner.py\", line 32, in <module>\r\n    print(fdist.tabulate())\r\n  File \"<edited>/env/lib/python3.7/site-packages/nltk/probability.py\", line 333, in tabulate\r\n    width = max(len(\"%s\" % s) for s in samples)\r\n  File \"<edited>/env/lib/python3.7/site-packages/nltk/probability.py\", line 333, in <genexpr>\r\n    width = max(len(\"%s\" % s) for s in samples)\r\nTypeError: not all arguments converted during string formatting\r\n```\r\n\r\nThis is because the string formatting doesn't work:\r\n```bash\r\n$ ipython\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: \"%s\" % ('hello', 'world')                                                                                                                                                             \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-60c0688ae365> in <module>\r\n----> 1 \"%s\" % ('hello', 'world')\r\n\r\nTypeError: not all arguments converted during string formatting\r\n\r\nIn [2]: \"{}\".format(('hello', 'world'))                                                                                                                                                       \r\nOut[2]: \"('hello', 'world')\"\r\n```\r\n\r\nWould it be OK to replace the `%s` formatting with the [\"new style\" string formatting operator](https://pyformat.info/)?"}
{"number": 2333, "owner": "StefRe", "title": "Sentence and word tokenizers fail for unicode double quotes u\"\\u201C\" and u\"\\u201D\"", "body": "This is related to #1675 and #1682.\r\n\r\n```\r\nimport nltk\r\n\r\ntext = '\u201cFirst sentence.\u201d Next one.'\r\n\r\nsentences = nltk.tokenize.sent_tokenize(text)\r\nprint(sentences)\r\nprint(nltk.tokenize.word_tokenize(sentences[0]))\r\n```\r\ndoesn't split the two sentences and doesn't tokenize `sentence.` into `sentence` and `.`:\r\n```\r\n['\u201cFirst sentence.\u201d Next one.']\r\n['\u201c', 'First', 'sentence.', '\u201d', 'Next', 'one', '.']\r\n```\r\nI'm wondering why it **works correctly if you leave out the second sentence**:\r\n`text = '\u201cFirst sentence.\u201d'` is correctly tokenized into: `['\u201c', 'First', 'sentence', '.', '\u201d']`\r\n \r\n(nltk 3.4.4 / Python 3.4.3 (default, Nov 12 2018, 22:25:49) [GCC 4.8.4] on linux)"}
{"number": 2329, "owner": "loretoparisi", "title": "sklearn DeprecationWarning:  the imp module is deprecated in favour of importlib", "body": "```\r\n/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n```\r\n\r\nI have on macOS\r\n\r\n```\r\n$ python3 --version\r\nPython 3.7.3\r\n```\r\n\r\nand latest nltk from pip3.\r\nThis issue should be related to https://github.com/scikit-learn/scikit-learn/issues/12434"}
{"number": 2328, "owner": "amadanmath", "title": "TreebankWordDetokenizer fails to reattach comma", "body": "For tokenisation, the commit https://github.com/nltk/nltk/pull/223/commits/2414f9f2d99402e64e6cb5b2b3a26df94f6b8837 introduced an exception for commas and colons, stating that a comma can be a token in its own right (insert a space before and after it) if it is not followed by a digit, presumably in order to ensure 30,000,000 and 12:59 remain a single token.\r\n\r\nHowever, when _detokenisation_ was added, I don't think the reverse of that rule makes sense:\r\n\r\n```\r\n(\r\n    re.compile(r'\\s([:,])\\s([^\\d])'),\r\n    r'\\1 \\2',\r\n)  # Keep right pad after comma/colon before non-digits.\r\n```\r\n\r\n(\"Remove a space before the comma, as long as it is not followed by a space and a digit.\")\r\n\r\nIf it were followed by a digit in the tokenisation, it would not have gained an intervening space: 30,000,000 remained 30,000,000, so `\\s([:,])\\s` already doesn't match (no need to restrict `[^\\d]`).\r\n\r\nOn the other hand, if the pre-tokenised original had a comma, space and a digit, then a space would have been inserted before the comma, because the space between the comma and a digit qualified as a non-digit. However, in reverse, that space does not get deleted:\r\n\r\n```\r\nfrom nltk.tokenize.treebank import *\r\n\"From those 100, 10 were selected,\"      # => 'From those 100, 10 were selected.'\r\nTreebankWordTokenizer().tokenize(_)      # => ['From', 'those', '100', ',', '10', 'were', 'selected', '.']\r\nTreebankWordDetokenizer().detokenize(_)  # => 'From those 100 , 10 were selected.'\r\n```\r\n\r\nNotice the comma not being restored to the normal position. I believe the reverse rule should simply say that if a comma is all by its lonesome, smush it with the previous token:\r\n\r\n```\r\n(re.compile(r'\\s([:,])\\s'), r'\\1 ')\r\n```\r\n\r\nor maybe even this, in case the comma is the last token:\r\n\r\n```\r\n(re.compile(r'\\s([:,])'), r'\\1')\r\n```\r\n"}
{"number": 2320, "owner": "marco-roberti", "title": "One-shot BLEU-[2, 3, 4] computation", "body": "Hello everyone,\r\n\r\nI need to compute the BLEU score with more than one ngram length (ideally, BLEU2, BLEU3, BLEU4, and BLEU5). In my case, this is a very long task, as every hypothesis has some thousand references.\r\n\r\nReading the implementation of the corpus_bleu function, which takes weights:Tuple between its parameters - and thus calculating BLEU-[len(weights)] - , I found out that it gets all the information to compute BLEU-m s.t. 2 <= m < len(weights).\r\nWouldn't it be nice to have a more general function that can compute BLEU with different ngram lengths at the same time? A possible implementation would be the weights parameter accepting a list of weight tuples as value, computing precisions that are useful for the longest tuple and using those precisions to get all the desired BLEU scores using the weights list.\r\n\r\nThis would result in a more general implementation and it would avoid the computation waste of calculating the same values more than one time."}
{"number": 2317, "owner": "dbalgley", "title": "Import Error/Variable Error in probability.ConditionalFreqDist.plot()", "body": "It seems like changes going back to 42ffffd36cec0fa10418e7d19294d5b91c2b05b6 (2/23/2019) are causing me issues.\r\n\r\n1. On attempting to run example from Chapter 2 of the nltk book (below) I get an ImportError.  Seems to be related to the change made on Feb 23 specifically related to the imports:\r\n`cfd = nltk.ConditionalFreqDist( ( genre, word ) for genre in brown.categories( ) for word in brown.words( categories=genre ) )\r\n    cfd.plot()`\r\n\r\n> Traceback (most recent call last):\r\n  File \"C:\\XXX\\PycharmProjects\\ciExp\\venv\\lib\\site-packages\\nltk\\probability.py\", line 1907, in plot\r\n    from matplotlib import plt\r\nImportError: cannot import name 'plt' from 'matplotlib' (C:\\XXX\\PycharmProjects\\ciExp\\venv\\lib\\site-packages\\matplotlib\\__init__.py)\r\n\r\n2. Correcting the import statement to `import matplotlib.pyplot as plt` as it was before Feb 23 results in an UnboundedLocalError in the same plot() function.  There was a change on May 7 to that line where it removed a comment from `if v in self`: \r\n\r\n> File \"C:\\XXX\\PycharmProjects\\ciExp\\venv\\lib\\site-packages\\nltk\\probability.py\", line 1919, in plot\r\n    kwargs, 'samples', sorted(set(v for c in conditions\r\n  File \"C:\\XXX\\PycharmProjects\\ciExp\\venv\\lib\\site-packages\\nltk\\probability.py\", line 1920, in <genexpr>\r\n    if v in self\r\nUnboundLocalError: local variable 'v' referenced before assignment\r\n\r\nCode snippet from probability.py:\r\n`samples = _get_kwarg(\r\n            kwargs, 'samples', sorted(set(v for c in conditions\r\n                                          if v in self\r\n                                          for v in self[c]))\r\n        )  # this computation could be wasted`\r\n\r\n3. Commenting out the `if v in self` line (probably ill advised but I wanted to walk this back) results in a NameError:\r\n\r\n>   File \"C:\\XXX\\PycharmProjects\\ciExp\\venv\\lib\\site-packages\\nltk\\probability.py\", line 1940, in plot\r\n    ax.plot(freqs, *args, **kwargs)\r\nNameError: name 'ax' is not defined\r\n\r\nI can basically continue to walk this back until the code just hangs infinitely (as far as I can tell).  By reverting the probability.py file back before Feb 23, the matplotlib window will show up but (once again, as far as I can tell) it hangs infinitely without showing my data.  Other matplotlib function calls seem to work just fine (like those in chapter 1).  In my py 3.7 virtenv the version of nltk: 3.4.3 and matplotlib: 3.1.0 so I'm pretty sure it's not an installation issue.  Any help would be appreciated!"}
{"number": 2315, "owner": "TechProofreader", "title": "Inaccuracies with Perceptron Tagger", "body": "I'm using NLTK to tokenize and tag text pulled from webpages (similar to how the perceptron tagger was trained via the WSJ text) and I'm finding that it is suffering from some inaccuracies. \r\nBasically, if I filter text to exclude everything but nouns and foreign words, it still outputs symbols that it tags as nouns and/or foreign words, which is obviously an error. \r\nIt seems to be accurate with everything else, but for whatever reason, it keeps tagging symbols as nouns and/or foreign words, and thus improperly filtering the text.\r\nIf you're looking for a code example, you could run the script I wrote that includes this function, which you can find in my repositories (it's the Narrative Web Scraper). Foxnews.com and Politico.com work well with the script, so I would recommend inserting those URLs into the script where it says 'url goes here'. \r\nTo get around this error, I'm considering just writing some python code for my project that will filter out any symbols that slip past NLTK, but before I do so, if anyone has any ideas as to why this is happening and how it can be fixed, please let me know.\r\nI have read that I could possibly re-train the tagger for my own project, but that seems excessive right now. I know I could also use another tagger, such as the Stanford tagger, but with the deprecating coming up and the fact that I would have to wrap it into my program, I feel like this could risk further bugs, so I'm holding off on that right now until the transition with the Stanford code is complete. \r\nThanks"}
{"number": 2314, "owner": "basaldella", "title": "Infinite loop in Wordnet closure and tree", "body": "When I try to compute the hyponymic closure of `restrain.v.01` I get stuck in an infinite loop. Example code:\r\n```python\r\n>>> import nltk\r\n>>> nltk.__version__\r\n'3.4.3'\r\n>>> from nltk.corpus import wordnet as wn\r\n>>> ss = wn.synset('restrain.v.01')\r\n>>> list(ss.closure(lambda s:s.hyponyms()))\r\n```\r\nAnd from here it just hangs. I guess that the reason is that the method is based on [`breadth_first`](http://www.nltk.org/api/nltk.html#nltk.util.breadth_first), which explicitly states in the documentation that is not designed to handle loops.\r\n\r\nThe same happens when running\r\n```python\r\n>>> list(ss.tree(lambda s:s.hyponyms()))\r\n```\r\n\r\nWhich fails with a `RecursionError`. \r\n\r\nThis seems to be closely related to #52, which is now closed, but the bug seems to be still present nonetheless. \r\n\r\n"}
{"number": 2309, "owner": "Vonisoa", "title": "wordnet morphy features with french words", "body": "Hi , I saw this from nltk wordnet documentation:\r\n\r\n```\r\nfrom nltk.corpus import wordnet as wn \r\nprint(wn.morphy('dogs'))\r\n```\r\nAnd we get `dog`.\r\n\r\nIf I want to use this feature for french words, how should I do it?"}
{"number": 2303, "owner": "stefanondisponibile", "title": "TreebankWordTokenizer [BUG]", "body": "Looks like there's a broken regexp on Treebank's `PUNCTUATION` patterns:\r\n\r\n```\r\n([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$\r\n```\r\nshould `\"` be escaped?\r\n"}
{"number": 2302, "owner": "inmoonlight", "title": "metrics.agreement.AnnotationTask kappa result seems wrong", "body": "I got strange kappa result from `AnnotationTask`\r\n\r\nThis is how I calculated kappa.\r\n\r\n```python\r\nfrom nltk.metrics.agreement import AnnotationTask\r\n\r\nevaluator1 = [3, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5]\r\nevaluator2 = [0, 4, 5, 2, 2, 1, 0, 0, 5, 0, 5, 0, 5, 2, 4, 0, 0, 4, 0, 2]\r\n\r\ndata = []\r\nfor i in range(len(evaluator1)):\r\n    for j, k in zip(evaluator1, evaluator2):\r\n        data.append(('1', str(i), str(j)))\r\n        data.append(('2', str(i), str(k)))\r\n\r\ntask = AnnotationTask(data)\r\n\r\nprint(task.kappa()) # 1.0158730158730158\r\n```\r\n\r\nSince kappa is bigger than 1, it seems wrong enough.\r\n\r\nIn addition to this computation, I checked kappa from `sklearn`.\r\n\r\n```python\r\nfrom sklearn.metrics import cohen_kappa_score\r\n\r\nprint(cohen_kappa_score(evaluator1, evaluator2)) # -0.011904761904761862\r\n```"}
{"number": 2299, "owner": "martinevanschouwenburg", "title": "collocations function returns error", "body": "I was going through chapter 1 of the book and the collocations function returns an error. It seems like line 440 in text.py is redundant, since the collocation_list function has been introduced. I fixed the issue by rewriting the current line 440 and line 441 in text.py.\r\n\r\nold code:\r\n collocation_strings = [w1 + ' ' + w2 for w1, w2 in self.collocation_list(num, window_size)]*\r\n print(tokenwrap(collocation_strings, separator=\"; \"))\r\n\r\nnew code:\r\nprint(tokenwrap(self.collocation_list(), separator=\"; \"))\r\n "}
{"number": 2296, "owner": "alvations", "title": "Dropping Python 2.7 Support", "body": "From https://travis-ci.org/nltk/nltk/jobs/530566954, some of the dependencies NLTK is dependent on no longer supports Python 2.7. I think it's good timing to also drop support for Python 2.7 so that our CI continues to work and move the library forward."}
{"number": 2295, "owner": "Dobatymo", "title": "Treebank detokenizer fails to undo quote conversion", "body": "Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\r\nNLTK version: 3.4.1\r\n\r\nQuotes `\"` are converted to ` `` ` or `''` by the treebank tokenizer. The detokenizer is supposed to revert its regexes but fails to do so for these quotes. The space is lost also.\r\n\r\n```python\r\n>>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\r\n>>> tmp = TreebankWordTokenizer().tokenize('How \"are\" you?')\r\n>>> tmp\r\n['How', '``', 'are', \"''\", 'you', '?']\r\n>>> TreebankWordDetokenizer().tokenize(tmp)\r\n'How``are\"you?'\r\n```"}
{"number": 2290, "owner": "alvations", "title": "Is it alright if we automatically install the resource if it's not found?", "body": "It has been quite some time where new users are still getting stymied with `resource_not_found` errors, e.g. https://stackoverflow.com/questions/55955060/resource-punkt-not-found \r\n\r\n**Would it be a good thing if we did what more modern libraries are doing by automatically installing them the first them if users don't already have them?** E.g. when using [tensorflow hub ](https://www.tensorflow.org/hub) or [pytorch pretrained bert](https://github.com/huggingface/pytorch-pretrained-BERT), or [sklearn datasets](https://scikit-learn.org/stable/datasets/index.html#id5)"}
{"number": 2284, "owner": "ssantichaivekin", "title": "ChartParser cannot parse grammar with inverted order", "body": "The ChartParser can parse this grammar:\r\n```python\r\ngr = nltk.CFG.fromstring(\"\"\"\r\n        S -> L\r\n        S -> S '+' S | S '-' S\r\n        L -> '1' | '2' \r\n        \"\"\")\r\n        parser = nltk.ChartParser(gr)\r\n        parse_obj = parser.parse('1 + 2'.split())\r\n        tree = next(parse_obj)\r\n```\r\nBut not this one:\r\n```python\r\ngr = nltk.CFG.fromstring(\"\"\"\r\n        L -> '1' | '2' \r\n        S -> L\r\n        S -> S '+' S | S '-' S\r\n        \"\"\")\r\n        parser = nltk.ChartParser(gr)\r\n        parse_obj = parser.parse('1 + 2'.split())\r\n        tree = next(parse_obj) # breaks (StopIteration)\r\n```\r\nIs this the expected behavior?"}
{"number": 2278, "owner": "alvations", "title": "Wordnet similarity quirky when only one synsets needs root", "body": "Sometimes when only one synset needs root, the similarity between the synsets are not commutative:\r\n\r\n```python\r\nfrom nltk.corpus import wordnet as nltk_wn\r\n\r\nncat = nltk_wn.synset('cat.n.01')\r\nnbuy = nltk_wn.synset('buy.v.01')\r\n\r\nprint(nltk_wn.path_similarity(nbuy, ncat), nltk_wn.wup_similarity(nbuy, ncat))\r\nprint(nltk_wn.path_similarity(ncat, nbuy), nltk_wn.wup_similarity(ncat, nbuy))\r\n```\r\n\r\n[out]:\r\n\r\n```\r\n0.058823529411764705 0.1111111111111111\r\nNone None\r\n```\r\n\r\nDetails on https://stackoverflow.com/q/20075335/610569 "}
{"number": 2277, "owner": "www008", "title": "If only one  relationship , nltk.sem.relextract.extract_rels() can't work ", "body": "If only one  relationship appears in the sentence, nltk.sem.relextract.extract_rels() outputs an empty list. Suggest change relextract.py:\r\n\r\ndef semi_rel2reldict(pairs, window=5, trace=False):\r\n   ... \r\n    **while len(pairs) > 2:  # line 184:**\r\n        reldict = defaultdict(str)\r\n        reldict['lcon'] = _join(pairs[0][0][-window:])\r\n        reldict['subjclass'] = pairs[0][1].label()\r\n        reldict['subjtext'] = _join(pairs[0][1].leaves())\r\n        reldict['subjsym'] = list2sym(pairs[0][1].leaves())\r\n        reldict['filler'] = _join(pairs[1][0])\r\n        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)\r\n        reldict['objclass'] = pairs[1][1].label()\r\n        reldict['objtext'] = _join(pairs[1][1].leaves())\r\n        reldict['objsym'] = list2sym(pairs[1][1].leaves())\r\n        **reldict['rcon'] = _join(pairs[2][0][:window])   # line 195**\r\n       ...\r\nSuggest : \r\ndef semi_rel2reldict(pairs, window=5, trace=False):\r\n   ... \r\n **while len(pairs) >= 2:**\r\n        reldict = defaultdict(str)\r\n        reldict['lcon'] = _join(pairs[0][0][-window:])\r\n        reldict['subjclass'] = pairs[0][1].label()\r\n        reldict['subjtext'] = _join(pairs[0][1].leaves())\r\n        reldict['subjsym'] = list2sym(pairs[0][1].leaves())\r\n        reldict['filler'] = _join(pairs[1][0])\r\n        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)\r\n        reldict['objclass'] = pairs[1][1].label()\r\n        reldict['objtext'] = _join(pairs[1][1].leaves())\r\n        reldict['objsym'] = list2sym(pairs[1][1].leaves())\r\n        **reldict['rcon'] = _join(pairs[2][0][:window]) if len(pairs) > 2 else ''**\r\n       ..."}
{"number": 2273, "owner": "alvations", "title": "Max depth of the all wordnet POS should be returned and kept as static", "body": "The `_compute_max_depth()` used for `lch_similarity()` returns `None` regardless of POS :\r\n\r\n```python\r\nfrom nltk.corpus import wordnet as nltk_wn\r\nfrom nltk.corpus.reader.wordnet import POS_LIST\r\n\r\nfor pos in POS_LIST:\r\n    print(pos, \r\n          nltk_wn._compute_max_depth(pos, simulate_root=True), \r\n          nltk_wn._compute_max_depth(pos, simulate_root=False))\r\n          print(pos, nltk_wn._max_depth)\r\n```\r\n\r\n[out]:\r\n\r\n```\r\nn None None\r\nn defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})\r\nv None None\r\nv defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})\r\na None None\r\na defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})\r\nr None None\r\nr defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})\r\n```\r\n\r\nThe `self._compute_max_depth` doesn't return anything, my suggestion is to expose it as a public function and let it return the depth while setting the max depth.\r\n\r\n----\r\n\r\nAnother point is that the `max_depth` will call the `all_synsets()` once and this is costly, it could be saved as static value from the start and access to the integer would be much simpler."}
{"number": 2270, "owner": "alvations", "title": "Hypernyms related functions in Wordnets can be unified", "body": "These functions in wordnet's `Synset()` object could be unified:\r\n \r\n - hypernym_paths\r\n - min_depth\r\n - max_depth\r\n - root_hypernyms\r\n\r\nThe while loops / recursion achieve the same purpose of reaching the root hypernyms, while doing so, the min and max depth should be logged and so should the hypernym paths. \r\n\r\nWe can derive the other three attributes given the hypernym_paths, e.g. \r\n\r\n```python\r\nfrom nltk.corpus import wordnet as nltk_wn\r\nss = nltk_wn.synset('sweet.n.1')\r\nhypernym_paths = ss.hypernym_paths()\r\nassert ss.max_depth() == max(len(path) for path in hypernym_paths) - 1\r\nassert ss.min_depth() == min(len(path) for path in hypernym_paths) - 1\r\nassert list(set([path[0] for path in hypernym_paths])) == ss.root_hypernyms()\r\n```\r\n\r\n----\r\n\r\nSomething like this:\r\n\r\n```python\r\n    def init_hypernym_paths(self):\r\n        \"\"\"\r\n        Get the path(s) from this synset to the root, where each path is a\r\n        list of the synset nodes traversed on the way to the root.\r\n        :return: A list of lists, where each list gives the node sequence\r\n        connecting the initial ``Synset`` node and a root node.\r\n        \"\"\"\r\n        self._hyperpaths = []\r\n        hypernyms = self.hypernyms() + self.instance_hypernyms()\r\n        if len(hypernyms) == 0:\r\n            paths = [[self]]\r\n        for hypernym in hypernyms:\r\n            for ancestor_list in hypernym.hypernym_paths():\r\n                ancestor_list.append(self)\r\n                self._hyperpaths.append(ancestor_list)\r\n        # Compute the path related statistics.\r\n        self._min_depth = min(len(path) for path in self._hyperpaths)\r\n        self._max_depth = max(len(path) for path in self._hyperpaths)\r\n        # Compute the store the root hypernyms.\r\n        self._root_hypernyms = list(set([path[0] for path in self._hyperpaths]))\r\n\r\n    def hypernym_paths():\r\n        return self._hyperpaths\r\n\r\n    def min_depth():\r\n        return self._min_depth\r\n\r\n    def max_depth():\r\n        return self._max_depth\r\n\r\n    def root_hypernyms():\r\n        return self._root_hypernyms\r\n```"}
{"number": 2266, "owner": "alvations", "title": "Cyclic initialization between Lemma, Synset and WordNetCorpusReader", "body": "The cyclic initialization between Lemma, Synset and WordNetCorpusReader is sort of \"un-wanted\" which actually makes #1548 quite impossible\r\n\r\nLooking at the definition, \r\n\r\n - `WordNetCorpusReader` creates `Synset` and `Lemma` objects on the fly, keeping them in caches https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1121 \r\n\r\n - To create a `Synset`, the class is so tied to the `WordNetCorpusReader` such that creation of empty synsets becomes meaningless https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1121 and only created at [`_synset_from_pos_and_line()`](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1396) function\r\n\r\n - To create a `Lemma`, it needs to be tied to the synsets, https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L236, by definition of the lemma. \r\n\r\nA suggestion would be\r\n\r\n -  first to decouple lemma vs synset such that the initialization is not dependent on each other, then link them with some optional kwargs. \r\n\r\n - unload the loaded functions in the WordNetCorpusReader, such that it reads only the lines from the wordnet files. \r\n\r\n - from the output of the wordnet files, \r\n    - create the synset independent of the lemmas and \r\n    - create the lemma independent of the synset\r\n    - create an attribute that links the synset to lemma without having them as attributes of each other.\r\n\r\n----\r\n\r\nFor lemmas, the only uses of the synset are:\r\n\r\n - `__repr__` accessing the synset name, this should be easy to fetch or initialize\r\n - `_related` accesing synset's `_lemma_pointers` which could be been added to initialize the lemma. "}
{"number": 2264, "owner": "hamukazu", "title": "Different behavior of download for Jupyter and usual REPL", "body": "In the usual REPL of Python, the following works.\r\n\r\n```python\r\n>>> import nltk\r\n>>> nltk.download(\"reuters\")\r\n>>> paras = nltk.corpus.reuters.paras()\r\n\r\n```\r\nHowever, in Jupyter Notebook it fails with the following message\r\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-27-49a61c024eed> in <module>()\r\n      2 nltk.download(\"reuters\")\r\n      3 reuters = nltk.corpus.reuters\r\n----> 4 paras=reuters.paras()\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py in paras(self, fileids, categories)\r\n    172     def paras(self, fileids=None, categories=None):\r\n    173         return PlaintextCorpusReader.paras(\r\n--> 174             self, self._resolve(fileids, categories))\r\n    175 \r\n    176 # is there a better way?\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py in paras(self, fileids)\r\n    115         return concat([self.CorpusView(path, self._read_para_block, encoding=enc)\r\n    116                        for (path, enc, fileid)\r\n--> 117                        in self.abspaths(fileids, True, True)])\r\n    118 \r\n    119     def _read_word_block(self, stream):\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/api.py in abspaths(self, fileids, include_encoding, include_fileid)\r\n    191             fileids = [fileids]\r\n    192 \r\n--> 193         paths = [self._root.join(f) for f in fileids]\r\n    194 \r\n    195         if include_encoding and include_fileid:\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/api.py in <listcomp>(.0)\r\n    191             fileids = [fileids]\r\n    192 \r\n--> 193         paths = [self._root.join(f) for f in fileids]\r\n    194 \r\n    195         if include_encoding and include_fileid:\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/data.py in join(self, fileid)\r\n    338     def join(self, fileid):\r\n    339         _path = os.path.join(self._path, fileid)\r\n--> 340         return FileSystemPathPointer(_path)\r\n    341 \r\n    342     def __repr__(self):\r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/compat.py in _decorator(*args, **kwargs)\r\n    219     def _decorator(*args, **kwargs):\r\n    220         args = (args[0], add_py3_data(args[1])) + args[2:]\r\n--> 221         return init_func(*args, **kwargs)\r\n    222     return wraps(init_func)(_decorator)\r\n    223 \r\n\r\n~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/data.py in __init__(self, _path)\r\n    316         _path = os.path.abspath(_path)\r\n    317         if not os.path.exists(_path):\r\n--> 318             raise IOError('No such file or directory: %r' % _path)\r\n    319         self._path = _path\r\n    320 \r\n\r\nOSError: No such file or directory: '/home/kato/nltk_data/corpora/reuters/test/14826'\r\n```\r\n\r\nThese results are for the same environment of the same machine. I am using Anaconca 5.3.1 (Python 3.7.3, nltk 3.4). \r\n\r\nAfter that, I tried to unzip the data file as follows:\r\n```\r\n$ cd ~/nltk_data/corpora\r\n$ unzip reuters.zip\r\n```\r\nThen it started to work even in Jupyter Notebook.\r\n\r\nTo conclude, it look that the usual Python REPL reads zipped file directly but Jupyter Notebook only works if there is unzipped data."}
{"number": 2263, "owner": "island99", "title": "Why the same tokens have different tags using \u2018chi_tagger.tag\u2019\uff1f", "body": "Hi guys, I want to tag my tokens using nltk for Chinese, but it became the same in the end, why?\r\n![image](https://user-images.githubusercontent.com/18048381/55732724-130c4c00-5a4f-11e9-8445-62cccea52165.png)\r\n\r\nThanks a lot!"}
{"number": 2259, "owner": "rajeshkumargp", "title": "Stanford POS tags are missing'_' in nltk.tag.StanfordPOSTagger", "body": "I am using the stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar. \r\n'_' is tagged as 'CD' by Stanford POS Tagger when running using Java jar program, and the same is getting missed when receiving in Python Nltk StanfordPOSTagger.\r\n\r\n**Input Sentence:**  \r\n\"_ computer is made to implement simulation\" \r\n\r\n**Stanford Jar result:** \r\n__SYM computer_NN is_VBZ made_VBN to_TO implement_VB simulation_NN \r\n\r\n**NLTK TAG StanfordPOSTagger result:** \r\n\r\n[('', 'CD'), ('computer', 'NN'), ('is', 'VBZ'), ('made', 'VBN'), ('to', 'TO'), ('implement', 'VB'), ('simulation', 'NN')]\r\n\r\n\r\n**Code Snippet:** \r\n`from nltk.tag import StanfordPOSTagger`\r\n`TAGGER_MODEL = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'`\r\n`TAGGER_JAR = 'stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar' `\r\n`stanford_tagger = StanfordPOSTagger(TAGGER_MODEL,TAGGER_JAR)`\r\n`t ='_ computer is made to implement simulation'`\r\n`ttk = nltk.tokenize.word_tokenize(t)`\r\n`sfttk = stanford_tagger.tag(ttk)`\r\n`print(sfttk)`\r\n\r\nThis is similar to #1632\r\n\r\n\r\n\r\n"}
{"number": 2255, "owner": "briemadu", "title": "update prior in hmm unsupervised", "body": "In relation to the comment in line 1074 in hmm.py:\r\n\r\n> \"Rabiner says the priors don't need to be updated. I don't believe him. FIXME\r\n\r\nI suppose Rabiner is referring to left-to-right HMMs only (see equation 46 in his paper). For general HMMs we do have to update priors, don't we?"}
{"number": 2250, "owner": "BLKSerene", "title": "PunktTokenizer does not use the correct version of the pickled model on Python 3.x", "body": "Hi, I'm trying to package my program with NLTK and nltk_data using PyInstaller. So to minimize the size of the data file, I removed the zip file and all models for Python 2.x in `nltk_data/tokenizers/punkt` (only the `PY3` folder is left).\r\n\r\nBut it seems that the `PunktTokenizer` always uses the Python 2.x version of the pickled model regardless of Python version I'm using. And the error message says that it can't find `tokenizers/punkt/english.pickle` instead of `tokenizers/punkt/PY3/english.pickle`.\r\n\r\nRemoving the `PY3` folder is okay, so it seems that the Python 3 version of the pickled model is never used.\r\n\r\nOS: Windows 10 64-bit\r\nPython version: 3.7.2 64-bit\r\nNLTK version: 3.4"}
{"number": 2248, "owner": "tenuretech", "title": "Enhancement: K means clusterer random seed initialization", "body": "I would like to have the option to initialize the random seed of the K-means clusterer in an easier manner, to allow reproducibility of the results. "}
{"number": 2244, "owner": "amadanmath", "title": "CoreNLPServer on a non-default port", "body": "If port is specified for `CoreNLPServer`, it is tested, put into the URL, but never put into `corenlp_options` ([here](https://github.com/nltk/nltk/blob/ac6a2394800138e4638279abc47a2394093cca72/nltk/parse/corenlp.py#L84)). Thus, `CoreNLPServer(port=9001).url` will be `http://localhost:9001`, but it will actually listen at `9000`. Should there not be another line there, to the effect of\r\n\r\n    corenlp_options.extend(['-port', str(port)])\r\n\r\nSimilarly, two lines above, in case `None` was passed and the default `9000` is not available, `try_port`  tries to discover an available port; it is appended to `corenlp_options`, but without the `-port` option, and thus AFAIK does not have an effect. That line, too, should likely be changed to the line quoted above."}
{"number": 2241, "owner": "erickrf", "title": "Add words to the Portuguese stopword list", "body": "A few very common words in Portuguese are not included in the stopword list. At least `\u00e9`, `ser` and `ter` should be included for consistency, since these are verbs whose other inflected forms are already included."}
{"number": 2239, "owner": "nick-ulle", "title": "Plot methods don't return Axes object", "body": "The `.plot()` methods on `FreqDist` and `ConditionalFreqDist` currently return `None`. This makes it difficult to customize the resulting plot.\r\n\r\nIn addition, returning `None` here is counterintuitive to users coming from other scientific computing packages. As an example, Matplotlib, Pandas, and Seaborn, all typically return an `Axes` object from their plotting methods.\r\n\r\nI propose changing the `.plot()` methods to return an `Axes` object."}
{"number": 2235, "owner": "shuraluberetsky", "title": "Uniform interface for similarity measures in Wordnet", "body": "Hello,\r\n\r\nplayed around a little with NLTK and noticed that similarity measures in Wordnet behave differently when comparing similarity of different parts-of-speech - say, a noun and a verb. Some quietly return None, some throw an exception.\r\n\r\nFor example - having\r\n\r\nnoun = nltk.corpus.wordnet.synset('table.n.01')\r\nverb = nltk.corpus.wordnet.synset('go.v.01')\r\n\r\nand\r\n\r\nic = nltk.corpus.wordnet_ic.ic(\"ic_brown.dat\")\r\n\r\nthe different methods to calculate similarity will behave as follows:\r\n\r\nnoun.jcn_similarity(verb, ic) # throws a WordNetError exception\r\nnoun.lch_similarity(verb) # throws a WordNetError exception\r\nnoun.lin_similarity(verb, ic) # throws a WordNetError exception\r\nnoun.path_similarity(verb) # returns None\r\nnoun.res_similarity(verb, ic) # throws a WordNetError exception\r\nnoun.wup_similarity(verb) # returns None\r\n\r\nSo, 2 out of 6 similarity measures return None, and 4 of 6 throw an exception in this case. What is the preferred behaviour for this case and why should not it be uniform for all the similarity measures?"}
{"number": 2222, "owner": "rmalouf", "title": "TweetTokenizer and punctuation inside URLs", "body": "The twitter tokenizer exhibits (what I think is) undesirable behavior when tokenizing URLs. For example:\r\n```\r\ntok.tokenize('http://t.co/LYsklSmIVS \u201chttp://t.co/LYsklSmIVS\u201d \u201chttp://t.co/LYsklSmIVS\u201dxxx')\r\n```\r\nyields\r\n```\r\n['http://t.co/LYsklSmIVS',  '\u201c',  'http://t.co/LYsklSmIVS',  '\u201d',  '\u201c',  'http://t.co/LYsklSmIVS\u201dxxx']\r\n```\r\nwhere\r\n```\r\n['http://t.co/LYsklSmIVS',  '\u201c',  'http://t.co/LYsklSmIVS',  '\u201d',  '\u201c',  'http://t.co/LYsklSmIVS',  '\u201d',  'xxx']```\r\nis what I would prefer.\r\n\r\nThe issue is that the regular expression used to tokenize URLs looks for the longest substring that could be an URL, and technically most punctuation marks can occur inside an URL. The regex does make an exception for a single punctuation mark at the end of an URL before a word break, but that doesn't help if there's a space missing before the next token.\r\n\r\nThe current URL matcher is, in my opinion, too greedy for working with casual online texts.  While it's legally possible for an URL to have a character like `\u201d` in the middle of it, it's much more likely that `\u201d` ought to be split off as a separate token.\r\n\r\nIn a way, parsing URLs in tweets should be trivial (because they all take the form `http://t.co/...`) and there's no real need for any fancy URL-matching regex.  But, on the other hand, twitter might change their URL format at any time and people might be using this tagger for parsing texts from other sites, so we don't want to make too many assumptions.\r\n\r\nAny thoughts about the best way to handle this?  "}
{"number": 2220, "owner": "Dobatymo", "title": "Treebank detokenizer parenthesis issue", "body": "Python version: Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32\r\nNLTK version: 3.4\r\n\r\nParentheses in the input are not correctly detokenized.\r\n\r\n```python\r\nfrom nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\r\nt = TreebankWordTokenizer()\r\nd = TreebankWordDetokenizer()\r\nd.detokenize(t.tokenize(\"Hello (world)\"))\r\n```\r\nOutput: `'Hello (world )'` (notice the space)"}
{"number": 2219, "owner": "rj-7", "title": "Wordnet : synset not found for many existing sense keys", "body": "The function nltk.corpus.wordnet.lemma_from_key(key) does not return synsets for many valid sensekeys.\r\n`print wn.lemma_from_key(hot%3:00:00:tasty:00).synset()`\r\nprints out \r\n`WordNetError: No synset found for key 'hot%3:00:00:tasty:00'`\r\nThere are many similar keys resulting the same outcome.\r\nAlso I am using WordNet 3.1 princeton web search to verify the validity of keys. But nltk has a lower wordnet version (3.0). I wonder if this is causing the problem. If I am right can we get the 3.1 upgrade on nltk soon? Desperately need this for research project.\r\nSome other keys are:\r\nneed%2:37:00::\r\nfair%3:00:00:beautiful:00\r\nclarify%2:30:00::\r\nprevious%5:00:00:preceding(a):00\r\nold%3:00:00:familiar:00\r\nAnd so on..."}
{"number": 2215, "owner": "alvations", "title": "Deprecating REPP wrapper and using pydelphin", "body": "The original C++ REPP is nice but it doesn't have continuous support for different OS, c.f. #1985 #2192 \r\n\r\nIt would be nice to deprecate the wrapper and incorporate REPP from `pydelphin` into NLTK to replace the current wrapper we have now: https://github.com/delph-in/pydelphin/issues/202 "}
{"number": 2214, "owner": "sapphire008", "title": "String to tree: ValueError: range() arg 3 must not be zero", "body": "```\r\nnltk.tree.Tree.fromstring('''(S\r\n  (NP (Det the) (N dog))\r\n  (VP\r\n      (V saw)\r\n      (NP (Det a) (N man))\r\n      (PP (P in) (NP (Det the) (N park)))\r\n  ))''')\r\n```\r\n\r\nRaises ValueError: range() arg 3 must not be zero.\r\n\r\nThis is associated with the example taken from http://www.nltk.org/book/ch08.html. \r\n\r\n```\r\ngrammar1 = nltk.CFG.fromstring(\"\"\"\r\n  S -> NP VP\r\n  VP -> V NP | V NP PP\r\n  PP -> P NP\r\n  V -> \"saw\" | \"ate\" | \"walked\"\r\n  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\r\n  Det -> \"a\" | \"an\" | \"the\" | \"my\"\r\n  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\r\n  P -> \"in\" | \"on\" | \"by\" | \"with\"\r\n  \"\"\")\r\n\r\nsent = \"the dog saw a man in the park\".split()\r\nrd_parser = nltk.RecursiveDescentParser(grammar1)\r\nfor tree in rd_parser.parse(sent):\r\n    print(tree)\r\n```\r\n\r\nPrinting the tree can be displayed correctly. However, when calling \r\n\r\n```\r\ndisplay.display(tree) # IPython.display\r\n```\r\n\r\nSame error was raised.\r\n\r\nNLTK version 3.3. Python version 3.6.5 Anaconda distribution. MacOSX 10.14.2"}
{"number": 2212, "owner": "coranholmes", "title": "Error loading home: Package 'home' not found in index and nltk.data.path becomes empty", "body": "I want to use `nltk.pos_tag`, and I have downloaded `punkt` and `averaged_perceptron_tagger`. My codes work fine on MacOS. Then I copy my `nltk_data` folder to another linux and configure the nltk.data.path. When I run the same codes again, the `nltk.word_tokenize` can work fine but `nltk.post_tag` triggers an error \r\n```\r\nLookupError:\r\n**********************************************************************\r\n  Resource \\u001b[93mhome\\u001b[0m not found.\r\n  Please use the NLTK Downloader to obtain the resource:\r\n  \\u001b[31m>>> import nltk\r\n  >>> nltk.download('home')\r\n  \\u001b[0m\r\n  Attempted to load \\u001b[93m/home/admin/work/nltk_data.zip/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\\u001b[0m\r\n  Searched in:\r\n    - u''\r\n**********************************************************************\r\n```\r\nHowever, when I try to run `nltk.download('home')`, I have another error:\r\n```\r\nError loading home: Package 'home' not found in index\r\n```\r\nAnd I feel very strange the search path becomes empty.\r\nAnyone can give me some suggestions?"}
{"number": 2210, "owner": "tarskiandhutch", "title": "Adding Constraint to Parser in nltk.parse.corenlp", "body": "When using CoreNLP's standard Java interface, constraints can be used to increase the accuracy of parse results based on known characteristics of a given data set. For instance, if a user knows that all sentences have a finite matrix clause, the parser can be constrained to produce an `S` label at root level, and then rank the most probable parses accordingly.\r\n\r\nSee [this test file](https://github.com/stanfordnlp/CoreNLP/blob/master/itest/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserITest.java) hosted on CoreNLP's Github, for the shift reduce parser in particular.\r\n\r\nI'd like to impose similar constraints on my parser output, but retain the functionality of NLTK and Python codebase kindly made possible by the nltk.parse.corenlp module. Currently, I establish a connection to the parser as follows\r\n\r\n```\r\n$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"parse\" -port 9000 -timeout 30000`\r\n```\r\n\r\nand then set up the NLTK interface with\r\n\r\n```\r\nfrom nltk.parse.corenlp import CoreNLPParser\r\nparser = CoreNLPParser(url='http://localhost:9000')\r\n```\r\n\r\nSince my only interaction with CoreNLP's Java underpinnings is the command line call to the server, I do not see any way to modify the input to the nltk.parse.corenlp interface such that a constraint could be added to the parser. Is there a way to add a contraint from within the NLTK interface? If not, is there a way to pass a constraint to the Java side, for instance, when establishing the CoreNLP server connection?\r\n\r\nThank you very much!"}
{"number": 2209, "owner": "devikasondhi", "title": "Lemmatizer on contractions and some pronouns", "body": "Hello,\r\n\r\nThe WordNetLemmatizer does not seem to account for contractions like \"'ll\" or pronouns like \"her\".\r\n\r\nUsage:\r\n`lemmatizer.lemmatize(\"'ll\")` returns `\"'ll\"` when expected `\"will\"`\r\n`lemmatizer.lemmatize(\"her\")` returns `\"her\"` when expected `\"she\"`\r\n"}
{"number": 2204, "owner": "betterenvi", "title": "ZeroDivisionError when computing bleu_score", "body": "Python version: 3.5\r\nNLTK version: 3.2.5\r\n```txt\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py\", line 89, in sentence_bleu\r\n    emulate_multibleu)\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py\", line 199, in corpus_bleu\r\n    hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py\", line 544, in method4\r\n    incvnt = i+1 * self.k / math.log(hyp_len) # Note that this K is different from the K from NIST.\r\nZeroDivisionError: float division by zero\r\n```"}
{"number": 2203, "owner": "no-identd", "title": "Include/port SyllabiPy (which includes LegaliPy) for Syllable Tokenization", "body": "See here for the project by @henchc:\r\n\r\nhttps://github.com/henchc/syllabipy\r\n\r\nHere for the project homepage:\r\n\r\nhttp://syllabipy.com/\r\n\r\nAnd here for the research paper both originate from:\r\nhttps://escholarship.org/uc/item/13c6h2z2\r\n\r\nSee PDF page 44 (document page 31) for a schematic of how it works.\r\n\r\nIncorporating this would definitely require some refactoring (throwing out util.py to make the tokenizer freely chosable, for example) and properly abstracting the current hardcoding. @Yomguithereal seems to have already done this for their port of it into their Talisman library:\r\n\r\nhttps://github.com/Yomguithereal/talisman/tree/master/src/tokenizers/syllables\r\n\r\nHowever, Talisman uses Javascript - otherwise I'd just suggest including most of Talisman (albeit not all, since Talisman itself uses NLTK, so that'd become a tad bit too recursive.\ud83d\ude09) in NLTK. \ud83d\ude09\r\n\r\nBoth Talisman & SyllabiPy/LegaliPy use the MIT license, which should have compatibility.\r\n"}
{"number": 2202, "owner": "no-identd", "title": "Improve tokenization of Multi Word Expressions by including \"python partitioner\"", "body": "I suspect that @jakerylandwilliams & @andyreagan's https://github.com/jakerylandwilliams/partitioner could significantly improve the tokenization quality of NLTK, specifically when it comes to MWEs (Multi Word Expressions).\r\n\r\n@NeelShah18 recently ported it to Python 3:\r\n\r\nhttps://github.com/jakerylandwilliams/partitioner/pull/7\r\n\r\nSo, including it in NLTK should seem easy enough.\r\n\r\nFor more information on the approach used there, see here:\r\n\r\nhttps://noisy-text.github.io/2017/pdf/WNUT01.pdf\r\n\r\nAnd here:\r\nhttps://arxiv.org/abs/1710.07729\r\n\r\nIt's Apache 2.0 licensed, so the licenses seem compatible as well.\r\n"}
{"number": 2200, "owner": "Querela", "title": "BigramCollocationFinder.score_ngrams with BigramAssocMeasure.likelihood_ratio raises ValueError: math domain error", "body": "I want to compute bigram collocations per sentence and tried to output my demo results in a jupyter notebook. Only for those four/five sentence combinations and a window size of larger than 4, the ordering with loglikelihood-ratio fails. Probably because of a negative log computation.  \r\nI'm not sure what to do at this point. Below my demo code to recreate the error.\r\n\r\n```python\r\nimport collections\r\nimport nltk\r\nfrom nltk.collocations import BigramCollocationFinder\r\nfrom nltk.metrics import BigramAssocMeasures\r\nfrom somajo import Tokenizer\r\n\r\nsentences = [\r\n    \"Ich gehe gerne nach Hause.\",\r\n    \"Arbeit an der Uni macht Spa\u00df.\",\r\n    \"Ich esse gerne Eis.\",\r\n    #\"Warum ist die Erde rund?\",\r\n    \"Jemand anderes isst gerne Eis.\"\r\n]\r\n\r\nsentences_tok = [tokenizer.tokenize(s) for s in sentences]\r\nall_tok = [word for sentence in sentences_tok for word in sentence]\r\n\r\ndef compute_neighbor_collocations(sentences_tok, window_size=2):\r\n    sentences_tok = iter(sentences_tok)\r\n    \r\n    sentence_tok = next(sentences_tok)\r\n    first_finder = BigramCollocationFinder.from_words(sentence_tok, window_size=window_size)\r\n    for sentence_tok in sentences_tok:\r\n        finder = BigramCollocationFinder.from_words(sentence_tok, window_size=window_size)\r\n        first_finder.ngram_fd.update(finder.ngram_fd)\r\n        first_finder.word_fd.update(finder.word_fd)\r\n    \r\n    return first_finder\r\n\r\n# window_size=2 raises no error\r\nfinder_sentence = compute_neighbor_collocations(sentences_tok, window_size=255)\r\nscored_sentence = finder_sentence.score_ngrams(BigramAssocMeasures.likelihood_ratio)\r\n```\r\n```python\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-39-3af87a5a33c6> in <module>\r\n      1 #finder_sentence.apply_freq_filter(3)\r\n      2 #finder_sentence.nbest(BigramAssocMeasures.raw_freq, 100)\r\n----> 3 scored_sentence = finder_sentence.score_ngrams(BigramAssocMeasures.likelihood_ratio)\r\n      4 scored_sentence\r\n\r\n/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in score_ngrams(self, score_fn)\r\n    119         lowest score, as determined by the scoring function provided.\r\n    120         \"\"\"\r\n--> 121         return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))\r\n    122 \r\n    123     def nbest(self, score_fn, n):\r\n\r\n/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in _score_ngrams(self, score_fn)\r\n    111         \"\"\"\r\n    112         for tup in self.ngram_fd:\r\n--> 113             score = self.score_ngram(score_fn, *tup)\r\n    114             if score is not None:\r\n    115                 yield tup, score\r\n\r\n/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in score_ngram(self, score_fn, w1, w2)\r\n    183         n_ix = self.word_fd[w1]\r\n    184         n_xi = self.word_fd[w2]\r\n--> 185         return score_fn(n_ii, (n_ix, n_xi), n_all)\r\n    186 \r\n    187 \r\n\r\n/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/metrics/association.py in likelihood_ratio(cls, *marginals)\r\n    141         return (cls._n *\r\n    142                 sum(obs * _ln(obs / (exp + _SMALL) + _SMALL)\r\n--> 143                     for obs, exp in zip(cont, cls._expected_values(cont))))\r\n    144 \r\n    145     @classmethod\r\n\r\n/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/metrics/association.py in <genexpr>(.0)\r\n    141         return (cls._n *\r\n    142                 sum(obs * _ln(obs / (exp + _SMALL) + _SMALL)\r\n--> 143                     for obs, exp in zip(cont, cls._expected_values(cont))))\r\n    144 \r\n    145     @classmethod\r\n\r\nValueError: math domain error\r\n```"}
{"number": 2162, "owner": "alvations", "title": "Runtime warning when using nltk.downloader from CLI", "body": "I'm not sure it's because my environment wasn't set up wrongly or because the `nltk.downloader` code is somehow violating Python causing the `RuntimeWarning`\r\n\r\n```python\r\n$ python3 -m nltk.downloader reuters\r\n    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n      warn(RuntimeWarning(msg))\r\n    [nltk_data] Downloading package reuters to\r\n    [nltk_data]     /Users/liling.tan/nltk_data...\r\n    [nltk_data]   Package reuters is already up-to-date!\r\n```"}
{"number": 2154, "owner": "AmitMY", "title": "Sentence tokenizer fails on simple case", "body": "Every time there is \"no.\" meaning \"number\", the sentence tokenizer fails.\r\n\r\nI have to replace \"no.\" with \"shorthand_number\" and replace back after sentence splitting. (My rule is a bit more complex, but in general)"}
{"number": 2139, "owner": "Vimos", "title": "Tree pretty print branch position is wrong", "body": "When I represent the equation `(16 * 2) / 2` into the tree format `(Div (Mult 16 4) 2)` and  `2 / (16 * 2) ` into the tree format `(Div 2 (Mult 16 4))`, the `pretty_print` is the same. \r\n\r\nThe behavior with `pretty_print` is different from `draw`, `draw` will give the right picture.\r\n\r\n```\r\n\u279c  coqa_rnet git:(rc) \u2717 ipython                                                    [18/10/5| 3:38PM]\r\nPython 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import nltk\r\n\r\nIn [17]: nltk.__version__\r\nOut[17]: '3.2.5'\r\n\r\nIn [2]: a = \"(Div (Mult 16 4) 2)\"\r\n\r\nIn [3]: t = nltk.Tree(a)\r\n\r\nIn [4]: t = nltk.Tree.fromstring(a)\r\n\r\nIn [5]: t\r\nOut[5]: Tree('Div', [Tree('Mult', ['16', '4']), '2'])\r\n\r\nIn [6]: t.pretty_print()\r\n    Div         \r\n  ___|___        \r\n |      Mult    \r\n |    ___|____   \r\n 2   16       4 \r\n\r\nIn [8]: a = \"(Div 2 (Mult 16 4))\"\r\n\r\nIn [9]: t = nltk.Tree.fromstring(a)\r\n\r\nIn [10]: t.pretty_print()\r\n    Div         \r\n  ___|___        \r\n |      Mult    \r\n |    ___|____   \r\n 2   16       4 \r\n```"}
{"number": 2128, "owner": "alvations", "title": "Replacing Tkinter", "body": "Tkinter had served us well for a while. Esp. when `nltk.download()` is getting heated issues esp. with Jupyter notebook and Mac/Windows users.\r\n\r\nI think it's time to move to a more modern GUI framework, perhaps https://electronjs.org/ or http://kivy.org?\r\n\r\nAnyone good at GUI wants to take a stab at it?"}
{"number": 2127, "owner": "priyaananthasankar", "title": "Loading NLTK corpus like Wordnet from blobs", "body": "nltk.data.load(\"URL\") works fine if there are specific items to be loaded like a pickle file or stopwords, Wordnet is a compiled library and there needs to be some way to load this from a blob storage.\r\n\r\nIn AWS Lambda, the set of zipped files can contain the wordnet compiled binaries and can be loaded.\r\nBut if wordnet needs to be imported on runtime from NLTK from the cloud, the best way to do it is from a blob, (without using nltk.download() and dealing with system paths)\r\n\r\nThis issue is raised to support corpora loading from blobs."}
{"number": 2121, "owner": "alvations", "title": "Dangerous default values in arguments", "body": "There are quite some dangerous default dicts and lists in class/functions arguments. \r\n\r\nBut in some cases it's unclear whether the purpose is really to keep the values in the list in subsequent calls. See https://stackoverflow.com/questions/26320899/why-is-the-empty-dictionary-a-dangerous-default-value-in-python \r\n\r\nIt'll be good to look through each of them and convert the to default to `None` instead of `{}` or `[]` but I'm not exactly sure which of these cases the items in list should be persistent and when it shouldn't.\r\n\r\n\r\nA list of them are:\r\n\r\n```\r\nnltk/tgrep.py:757: [W0102(dangerous-default-value), _tgrep_exprs_action.top_level_pred] Dangerous default value macro_dict (builtins.dict) as argument\r\nnltk/util.py:579: [W0102(dangerous-default-value), binary_search_file] Dangerous default value {} as argument\r\nnltk/cluster/util.py:188: [W0102(dangerous-default-value), Dendrogram.__init__] Dangerous default value [] as argument\r\nnltk/cluster/util.py:225: [W0102(dangerous-default-value), Dendrogram.show] Dangerous default value [] as argument\r\nnltk/app/wordnet_app.py:565: [W0102(dangerous-default-value), _collect_all_synsets] Dangerous default value dict() (builtins.dict) as argument\r\nnltk/app/wordnet_app.py:632: [W0102(dangerous-default-value), Reference.__init__] Dangerous default value dict() (builtins.dict) as argument\r\nnltk/classify/weka.py:180: [W0102(dangerous-default-value), WekaClassifier.train] Dangerous default value [] as argument\r\nnltk/chunk/util.py:521: [W0102(dangerous-default-value), ieerstr2tree] Dangerous default value [] as argument\r\nnltk/chat/util.py:38: [W0102(dangerous-default-value), Chat.__init__] Dangerous default value {} as argument\r\nnltk/sem/drt.py:850: [W0102(dangerous-default-value), resolve_anaphora] Dangerous default value [] as argument\r\nnltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value [] as argument\r\nnltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value [] as argument\r\nnltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value set() (builtins.set) as argument\r\nnltk/sem/chat80.py:334: [W0102(dangerous-default-value), clause2concepts] Dangerous default value [] as argument\r\nnltk/sem/boxer.py:195: [W0102(dangerous-default-value), Boxer._call] Dangerous default value [] as argument\r\nnltk/sem/relextract.py:83: [W0102(dangerous-default-value), descape_entity] Dangerous default value html_entities.entitydefs (builtins.dict) as argument\r\nnltk/corpus/reader/framenet.py:1229: [W0102(dangerous-default-value), FramenetCorpusReader.frame_by_id] Dangerous default value [] as argument\r\nnltk/corpus/reader/framenet.py:1271: [W0102(dangerous-default-value), FramenetCorpusReader.frame_by_name] Dangerous default value [] as argument\r\nnltk/corpus/reader/framenet.py:1336: [W0102(dangerous-default-value), FramenetCorpusReader.frame] Dangerous default value [] as argument\r\nnltk/corpus/reader/framenet.py:1468: [W0102(dangerous-default-value), FramenetCorpusReader.lu] Dangerous default value [] as argument\r\nnltk/corpus/reader/framenet.py:1621: [W0102(dangerous-default-value), FramenetCorpusReader._lu_file] Dangerous default value [] as argument\r\nnltk/corpus/reader/framenet.py:2520: [W0102(dangerous-default-value), FramenetCorpusReader._handle_frame_elt] Dangerous default value [] as argument\r\nnltk/parse/chart.py:1240: [W0102(dangerous-default-value), ChartParser.__init__] Dangerous default value BU_LC_STRATEGY (builtins.list) as argument\r\nnltk/parse/chart.py:1410: [W0102(dangerous-default-value), SteppingChartParser.__init__] Dangerous default value [] as argument\r\nnltk/parse/featurechart.py:430: [W0102(dangerous-default-value), FeatureChartParser.__init__] Dangerous default value BU_LC_FEATURE_STRATEGY (builtins.list) as argument\r\nnltk/parse/earleychart.py:265: [W0102(dangerous-default-value), IncrementalChartParser.__init__] Dangerous default value BU_LC_INCREMENTAL_STRATEGY (builtins.list) as argument\r\nnltk/parse/earleychart.py:381: [W0102(dangerous-default-value), FeatureIncrementalChartParser.__init__] Dangerous default value BU_LC_INCREMENTAL_FEATURE_STRATEGY (builtins.list) as argument\r\nnltk/inference/mace.py:166: [W0102(dangerous-default-value), MaceCommand._call_interpformat] Dangerous default value [] as argument\r\nnltk/inference/mace.py:205: [W0102(dangerous-default-value), Mace._call_mace4] Dangerous default value [] as argument\r\nnltk/inference/prover9.py:168: [W0102(dangerous-default-value), Prover9Parent._call] Dangerous default value [] as argument\r\nnltk/inference/prover9.py:282: [W0102(dangerous-default-value), Prover9._call_prover9] Dangerous default value [] as argument\r\nnltk/inference/prover9.py:315: [W0102(dangerous-default-value), Prover9._call_prooftrans] Dangerous default value [] as argument\r\nnltk/draw/util.py:1814: [W0102(dangerous-default-value), CanvasFrame.pack] Dangerous default value {} as argument\r\nnltk/draw/util.py:2011: [W0102(dangerous-default-value), ColorizedList.__init__] Dangerous default value [] as argument\r\nnltk/draw/util.py:2177: [W0102(dangerous-default-value), ColorizedList.pack] Dangerous default value {} as argument\r\nnltk/draw/util.py:2181: [W0102(dangerous-default-value), ColorizedList.grid] Dangerous default value {} as argument\r\nnltk/draw/table.py:61: [W0102(dangerous-default-value), MultiListbox.__init__] Dangerous default value {} as argument\r\nnltk/draw/table.py:295: [W0102(dangerous-default-value), MultiListbox.configure] Dangerous default value {} as argument\r\nnltk/draw/table.py:322: [W0102(dangerous-default-value), MultiListbox.rowconfigure] Dangerous default value {} as argument\r\nnltk/draw/table.py:330: [W0102(dangerous-default-value), MultiListbox.columnconfigure] Dangerous default value {} as argument\r\nnltk/draw/table.py:575: [W0102(dangerous-default-value), Table.__init__] Dangerous default value {} as argument\r\nnltk/draw/table.py:670: [W0102(dangerous-default-value), Table.rowconfigure] Dangerous default value {} as argument\r\nnltk/draw/table.py:674: [W0102(dangerous-default-value), Table.columnconfigure] Dangerous default value {} as argument\r\nnltk/tag/crf.py:50: [W0102(dangerous-default-value), CRFTagger.__init__] Dangerous default value {} as argument\r\nnltk/tokenize/texttiling.py:64: [W0102(dangerous-default-value), TextTilingTokenizer.__init__] Dangerous default value DEFAULT_SMOOTHING (builtins.list) as argument\r\nnltk/ccg/api.py:257: [W0102(dangerous-default-value), PrimitiveCategory.__init__] Dangerous default value [] as argument\r\n```"}
{"number": 2118, "owner": "alvations", "title": "Non parents initialization ", "body": "There are several classes that inherits from `SyntaxCorpusReader` but are calling `CorpusReader` for the `__init__()`:\r\n\r\n- KNBCorpusReader\r\n- BracketParseCorpusReader\r\n- DependencyCorpusReader\r\n\r\n**Why are they inheritting from `SyntaxCorpusReader` but initializing from `CorpusReader`?** Should they be fixed and initialize using SyntaxCorpusReader's init?\r\n\r\n"}
{"number": 2116, "owner": "alvations", "title": "How to handle demos in individual modules?", "body": "Some modules have `demo()` methods that sort of act like doctest / unittest for the respective modules but it's hard to track which module has `demo()` and which doesn't, esp. in the `nltk.corpus.__init__.py`, i.e. https://github.com/nltk/nltk/blob/develop/nltk/corpus/__init__.py#L302\r\n\r\n```python\r\ndef demo():\r\n    # This is out-of-date:\r\n    abc.demo()\r\n    brown.demo()\r\n#    chat80.demo()\r\n    cmudict.demo()\r\n    conll2000.demo()\r\n    conll2002.demo()\r\n    genesis.demo()\r\n    gutenberg.demo()\r\n    ieer.demo()\r\n```\r\n\r\nIt also leads to an unfortunate hack to override `demo()` function at the top-most `nltk.__init__.py`, i.e. https://github.com/nltk/nltk/blob/develop/nltk/__init__.py#L183\r\n\r\n```python\r\n# override any accidentally imported demo\r\ndef demo():\r\n    print(\"To run the demo code for a module, type nltk.module.demo()\")\r\n```\r\n\r\nThe above in `nltk.__init__.py` isn't a good thing. Possibly, we should either:\r\n\r\n  - each module should be self contained and only propagate a list of `__all__` up or\r\n  - imports at the top-most `__init__` should be specific or\r\n  - move `demo()` codes from each module into somewhere else, e.g. `nltk.test.demos` "}
{"number": 2113, "owner": "alvations", "title": "Cyclic imports in nltk.ccg.lexicon", "body": "Pylint seems to be complaining about `nltk.ccg.lexicon` having cyclic imports but I'm not familiar with R0401 to understand what is the error.\r\n\r\nAnyone knows how to resolve this issue? \r\n\r\n```\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.corpus -> nltk.tokenize -> nltk.tokenize.texttiling)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.draw.tree -> nltk.tree)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tree -> nltk.treeprettyprinter)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.grammar -> nltk.parse.pchart)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.stem -> nltk.stem.porter)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.classify.maxent -> nltk.classify.tadm)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tag -> nltk.tag.brill)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.shiftreduce)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.cluster -> nltk.cluster.gaac)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.chunk -> nltk.chunk.regexp)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm5)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.mwe -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.draw.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.treeprettyprinter -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tree -> nltk.treetransforms)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.draw.table -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.collocations -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.corpus -> nltk.tokenize -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.cfg -> nltk.draw.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.cfg -> nltk.tree -> nltk.treetransforms -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.corenlp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.wsd -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.help -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.featstruct -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.misc -> nltk.misc.wordfinder -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.confusionmatrix -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.agreement -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.scores -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.gaac -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.cluster -> nltk.cluster.kmeans)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.em -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.kmeans -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.tnt -> nltk.tag.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.senna -> nltk.tag.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.classify.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.weka -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.corpus.util -> nltk.corpus.reader.api -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.classify.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.scikitlearn -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.classify.megam -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.textcat)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.positivenaivebayes -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.decisiontree -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.classify.tadm -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.chunk.api -> nltk.chunk.util)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.tree -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.chunk.api -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.featstruct -> nltk.sem.logic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.malt -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.inference.mace -> nltk.sem.logic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tree -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.tnt -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.corenlp -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.rslp -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.hmm -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.translate -> nltk.translate.ribes_score -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.translate -> nltk.translate.bleu_score -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm2)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm3)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm4)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm1)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.boxer -> nltk.sem.drt -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.skolemize -> nltk.sem.logic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.util -> nltk.grammar -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.boxer -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.lfg -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem.glue -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.relextract)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem.glue -> nltk.sem.linearlogic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.evaluate -> nltk.sem.logic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.featurechart -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.corpus.reader.util -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.parse.dependencygraph -> nltk.tree -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.bllip -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.parse.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.recursivedescent -> nltk.parse.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.util -> nltk.parse.featurechart -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.parse.chart -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.shiftreduce -> nltk.parse.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.recursivedescent)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.nonprojectivedependencyparser -> nltk.grammar -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.transitionparser)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.earleychart -> nltk.parse.featurechart -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.viterbi -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.stem -> nltk.stem.snowball -> nltk.stem.porter)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.tableau -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem -> nltk.sem.lfg -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.inference.mace -> nltk.inference.prover9)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.resolution -> nltk.sem.skolemize -> nltk.sem.logic -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.sentiment -> nltk.sentiment.sentiment_analyzer -> nltk.sentiment.util)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.sentiment -> nltk.sentiment.vader -> nltk.sentiment.util)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tree -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.table -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.dispersion -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.perceptron -> nltk.tag.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.hunpos -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.crf -> nltk.tag.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.stanford -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.nonprojectivedependencyparser -> nltk.classify -> nltk.classify.naivebayes -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tag -> nltk.tag.brill_trainer)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.mapping -> nltk.data)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.stanford_segmenter -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.texttiling -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.mwe -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.repp -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.sexpr -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.treebank -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.simple -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.toktok -> nltk.tokenize.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.lexicon -> nltk.ccg.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.combinator -> nltk.ccg.api -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.lexicon -> nltk.internals)\r\nnltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.lexicon -> nltk.internals)\r\n```"}
{"number": 2112, "owner": "alvations", "title": "CoreNLPParser tag() should allow properties overloading", "body": "With the current `CoreNLPParser.tag()`, the \"retokenization\" by Stanford CoreNLP is unexpected:\r\n\r\n```python\r\n>>> from nltk.parse.corenlp import CoreNLPParser\r\n>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\r\n>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']\r\n>>> ner_tagger.tag(sent)\r\n[('my', 'O'),\r\n ('phone', 'O'),\r\n ('number', 'O'),\r\n ('is', 'O'),\r\n ('1111\\xa01111\\xa01111', 'NUMBER')]\r\n```\r\n\r\nThe expected behavior should be:\r\n\r\n```python\r\n>>> from nltk.parse.corenlp import CoreNLPParser\r\n>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\r\n>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']\r\n>>> ner_tagger.tag(sent)\r\n[('my', 'O'), ('phone', 'O'), ('number', 'O'), ('is', 'O'), ('1111', 'DATE'), ('1111', 'DATE'), ('1111', 'DATE')]\r\n```\r\n\r\nProposed solution is to allow `properties` arguments overloading for `.tag()` and `.tag_sents()`, i.e. at https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L348 and by default use `properties = {'tokenize.whitespace':'true'}` because we are concatenating the tokens by spaces in `tag_sents()`.\r\n\r\n```python\r\n\r\n    def tag_sents(self, sentences, properties=None):\r\n        \"\"\"\r\n        Tag multiple sentences.\r\n\r\n        Takes multiple sentences as a list where each sentence is a list of\r\n        tokens.\r\n\r\n        :param sentences: Input sentences to tag\r\n        :type sentences: list(list(str))\r\n        :rtype: list(list(tuple(str, str))\r\n        \"\"\"\r\n        # Converting list(list(str)) -> list(str)\r\n        sentences = (' '.join(words) for words in sentences)\r\n        if properties == None:\r\n            properties = {'tokenize.whitespace':'true'}\r\n        return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]\r\n\r\n    def tag(self, sentence, properties=None):\r\n        \"\"\"\r\n        Tag a list of tokens.\r\n\r\n        :rtype: list(tuple(str, str))\r\n\r\n        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\r\n        >>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()\r\n        >>> parser.tag(tokens)\r\n        [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),\r\n        ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]\r\n\r\n        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\r\n        >>> tokens = \"What is the airspeed of an unladen swallow ?\".split()\r\n        >>> parser.tag(tokens)\r\n        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),\r\n        ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),\r\n        ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\r\n        \"\"\"\r\n        return self.tag_sents([sentence], properties)[0]\r\n\r\n    def raw_tag_sents(self, sentences, properties=None):\r\n        \"\"\"\r\n        Tag multiple sentences.\r\n\r\n        Takes multiple sentences as a list where each sentence is a string.\r\n\r\n        :param sentences: Input sentences to tag\r\n        :type sentences: list(str)\r\n        :rtype: list(list(list(tuple(str, str)))\r\n        \"\"\"\r\n        default_properties = {'ssplit.isOneSentence': 'true',\r\n                              'annotators': 'tokenize,ssplit,' }\r\n\r\n        default_properties.update(properties or {})\r\n\r\n        # Supports only 'pos' or 'ner' tags.\r\n        assert self.tagtype in ['pos', 'ner']\r\n        default_properties['annotators'] += self.tagtype\r\n        for sentence in sentences:\r\n            tagged_data = self.api_call(sentence, properties=default_properties)\r\n            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]\r\n                    for tagged_sentence in tagged_data['sentences']]\r\n```\r\n\r\nThat should enforce the list of string tokens input by the users. \r\n\r\nDetails on https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together\r\n\r\nIf we allow the `.tag()` to overload the properties before the `raw_tag_sents`, that'll also allow users to easily handle cases like #1876 "}
{"number": 2097, "owner": "alvations", "title": "How to stabilize NaiveBayesClassifier outputs?", "body": "On sometimes we see that the continuous integration test fails because of some non-deterministic results from the `NgramTagger` i.e. are `NaiveBayesClassifer`, e.g. https://nltk.ci.cloudbees.com/job/pull_request_tests/822/PYV=3.6.4,jdk=jdk8latestOnlineInstall/testReport/junit/(root)/nltk/tag/\r\n\r\nIs there someway to set a random seed or something such that classifier's output is consistent and make sure that the model is easily replicable?"}
{"number": 2093, "owner": "scoder", "title": "Compile some processing intensive modules", "body": "We are using NLTK 3.3 for large scale text processing with custom grammars, and found that compiling two of the NLTK modules with Cython speeds up our processing by about 30%, essentially for free. All we had to do was\r\n```\r\ncythonize -i -3 nltk/parse/chart.py nltk/grammar.py\r\n```\r\nafter the NLTK installation. Note that this compilation remains entirely optional, we didn't change any code.\r\n\r\nObviously, the little downside for us is now that we need to keep our own binary wheel build of NLTK around in order to keep the faster package pip installable for us, but that is easily worth the hours of processing time that we save every day.\r\n\r\nSo, would this be something that the NLTK project would want to do as well?\r\n\r\nBasically, it would be an option in the `setup.py` script to compile some modules (specifically for CPython), and the NLTK project could start uploading binary wheels for different Python versions to PyPI, which, when used, would speed up the processing on end-user side. Since no code change is involved, choosing to use the wheels or not is entirely up to the users in that case. Specifically, Linux binary wheels would certainly be appreciated for all the CI test runners out there."}
{"number": 2091, "owner": "HaelChan", "title": "nltk.app.concordance() crashes on macOS terminal", "body": "macOS: 10.12.6\r\nPython: 3.6.3\r\nNLTK: 3.3\r\n\r\nWhen calling `nltk.app.concordance()` on Terminal, Python would crash with the exception message:\r\n```\r\n>>> nltk.app.concordance()\r\n2018-08-22 16:13:05.134 Python[11374:5908786] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fc147d6d460\r\n2018-08-22 16:13:05.139 Python[11374:5908786] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fc147d6d460'\r\n*** First throw call stack:\r\n(\r\n\t0   CoreFoundation                      0x00007fff945cd2cb __exceptionPreprocess + 171\r\n\t1   libobjc.A.dylib                     0x00007fffa93e448d objc_exception_throw + 48\r\n\t2   CoreFoundation                      0x00007fff9464ef04 -[NSObject(NSObject) doesNotRecognizeSelector:] + 132\r\n\t3   CoreFoundation                      0x00007fff9453f755 ___forwarding___ + 1061\r\n\t4   CoreFoundation                      0x00007fff9453f2a8 _CF_forwarding_prep_0 + 120\r\n\t5   Tk                                  0x0000000102731c02 TkpInit + 471\r\n\t6   Tk                                  0x00000001026ad2a9 Tk_Init + 1794\r\n\t7   _tkinter.cpython-36m-darwin.so      0x0000000102588ddf Tcl_AppInit + 82\r\n\t8   _tkinter.cpython-36m-darwin.so      0x0000000102584422 _tkinter_create + 1047\r\n\t9   Python                              0x0000000100b208b5 _PyCFunction_FastCallDict + 166\r\n\t10  Python                              0x0000000100b88a00 call_function + 511\r\n\t11  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177\r\n\t12  Python                              0x0000000100b8921b _PyEval_EvalCodeWithName + 1903\r\n\t13  Python                              0x0000000100b89b0f _PyFunction_FastCallDict + 448\r\n\t14  Python                              0x0000000100ae729c _PyObject_FastCallDict + 214\r\n\t15  Python                              0x0000000100ae73c0 _PyObject_Call_Prepend + 156\r\n\t16  Python                              0x0000000100ae7109 PyObject_Call + 102\r\n\t17  Python                              0x0000000100b344b9 slot_tp_init + 61\r\n\t18  Python                              0x0000000100b3121b type_call + 184\r\n\t19  Python                              0x0000000100ae7260 _PyObject_FastCallDict + 154\r\n\t20  Python                              0x0000000100b889c9 call_function + 456\r\n\t21  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177\r\n\t22  Python                              0x0000000100b89be7 _PyFunction_FastCall + 122\r\n\t23  Python                              0x0000000100ae729c _PyObject_FastCallDict + 214\r\n\t24  Python                              0x0000000100ae73c0 _PyObject_Call_Prepend + 156\r\n\t25  Python                              0x0000000100ae7109 PyObject_Call + 102\r\n\t26  Python                              0x0000000100b344b9 slot_tp_init + 61\r\n\t27  Python                              0x0000000100b3121b type_call + 184\r\n\t28  Python                              0x0000000100ae7260 _PyObject_FastCallDict + 154\r\n\t29  Python                              0x0000000100b889c9 call_function + 456\r\n\t30  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177\r\n\t31  Python                              0x0000000100b89be7 _PyFunction_FastCall + 122\r\n\t32  Python                              0x0000000100b889d0 call_function + 463\r\n\t33  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177\r\n\t34  Python                              0x0000000100b8921b _PyEval_EvalCodeWithName + 1903\r\n\t35  Python                              0x0000000100b7fc53 PyEval_EvalCode + 42\r\n\t36  Python                              0x0000000100ba8b84 run_mod + 54\r\n\t37  Python                              0x0000000100ba8934 PyRun_InteractiveOneObject + 578\r\n\t38  Python                              0x0000000100ba81a4 PyRun_InteractiveLoopFlags + 105\r\n\t39  Python                              0x0000000100ba8108 PyRun_AnyFileExFlags + 60\r\n\t40  Python                              0x0000000100bbcb8a Py_Main + 3754\r\n\t41  Python                              0x0000000100ad7e1b Python + 7707\r\n\t42  libdyld.dylib                       0x00007fffa9cca235 start + 1\r\n)\r\nlibc++abi.dylib: terminating with uncaught exception of type NSException\r\nAbort trap: 6\r\n```\r\n\r\nThe same problem seems to apply to IDLE as well: [nltk.app.concordance() crashes IDLE on Mac OS\r\n](https://stackoverflow.com/questions/42882127/nltk-app-concordance-crashes-idle-on-mac-os).\r\nHowever, nltk.app.concordance() works well on Jupyter Notebook and PyCharm."}
{"number": 2076, "owner": "gkucsko", "title": "TreebankWordTokenizer span_tokenize still throwing exception for some corner-cases", "body": "very similar to #1865, there are still unhandled inputs to TreebankWordTokenizer that will cause span_tokenize to throw:\r\n```\r\n>>> from nltk.tokenize import TreebankWordTokenizer\r\n>>> s = u\"\\'\\'good muffins\\\"\"\r\n>>> list(TreebankWordTokenizer().span_tokenize(s))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"nltk/tokenize/treebank.py\", line 198, in span_tokenize\r\n    for tok in align_tokens(tokens, text):\r\n  File \"nltk/tokenize/util.py\", line 258, in align_tokens\r\n    raise ValueError('substring \"{}\" not found in \"{}\"'.format(token, sentence))\r\nValueError: substring \"''\" not found in \"''good muffins\"\"\r\n```\r\na working fix could be:\r\n```\r\nline 190 in tokenize.treebank.py:\r\nmatched = [m.group() for m in re.finditer(r\"\\`\\`|(?<!^)\\'\\'|^\\'\\'(?=\\s)|\\\"\", text)]\r\n```\r\ni don't like the extra look arounds, but at least this wouldn't change anything about the current implementation. happy to submit a merge req if it's helpful!"}
{"number": 2068, "owner": "timgianitsos", "title": "PunktLanguageVars becomes immutable after methods are invoked", "body": "The instance variables of PunktLanguageVars are initialized with a faulty design pattern that leads to buggy behavior when using the sentence tokenizer. The two culprits are `def word_tokenize(self, s):` and `def period_context_re(self):` in PunktLanguageVars. Both of these methods have the following construction.\r\n```\r\ntry:\r\n    return self._some_instance_variable\r\nexcept:\r\n    <initialize self._some_instance_variable>\r\n    return self._some_instance_variable\r\n```\r\nThis is a faulty way of initializing an instance variable because it only allows you to modify the variable before the method is called. After the method is called, the variable cannot be modified, and it's not initially obvious why (modifying a variable is done through changing `PunktLanguageVars.sent_end_chars` via [the documentation](http://wiki.apertium.org/wiki/Sentence_segmenting#NLTK_Punkt)). \r\n\r\nObserve the following usage of `PunktSentenceTokenizer` which uses an internal `PunktLanguageVars` instance (keep in mind that the initial default value for `PunktLanguageVars.sent_end_chars` is `('.', '?', '!')`).\r\n```\r\n#1\r\n>>> from nltk.tokenize.punkt import *; s = 'test test test test test. test test test; test test.'; p = PunktSentenceTokenizer()\r\n>>> PunktLanguageVars.sent_end_chars = ('.', ';')\r\n>>> p.tokenize(s)\r\n['test test test test test.', 'test test test;', 'test test.']\r\n\r\n#2\r\n>>> from nltk.tokenize.punkt import *; s = 'test test test test test. test test test; test test.'; p = PunktSentenceTokenizer()\r\n>>> p.tokenize(s)\r\n['test test test test test.', 'test test test; test test.']\r\n>>> PunktLanguageVars.sent_end_chars = ('.', ';')\r\n>>> p.tokenize(s)\r\n['test test test test test.', 'test test test; test test.']\r\n```\r\nWhy in the second example does changing `PunktLanguageVars.sent_end_chars` have no effect on recognizing the semi colon as a sentence ending mark, but it does have an effect in the first? Because `p.tokenize()` is called beforehand in the second example. Somewhere in `tokenize()` one of the two culprit methods is called on the internal `PunktLanguageVars` instance, thereby making it immutable.\r\n\r\nThis could be remedied by making the instance variables of `PunktLanguageVars` public instead of private so that they didn't need to be affected obliquely through setting `PunktLanguageVars.sent_end_chars`. Public access should be necessary anyway to give the users ability to modify the regular expressions in `PunktLanguageVars` for parsing the plethora of languages they are dealing with.\r\n\r\nAnother remedy is just generating the variables each time one of the methods is called instead of recalling a cached version in the `try` block."}
{"number": 2055, "owner": "PanderMusubi", "title": "List installed language support", "body": "By downloading `punkt`, several languages are support, such as:\r\n* czech\r\n* danish\r\n* dutch\r\n* english\r\n* estonian\r\n* finnish\r\n* french\r\n* german\r\n* greek\r\n* italian\r\n* norwegian\r\n* polish\r\n* portuguese\r\n* slovene\r\n* spanish\r\n* swedish\r\n* turkish\r\n\r\nHow can, in Python, this list of language names be queried from NLTK? If that is not possible in a generic way, could this please be made into a feature request?"}
{"number": 2038, "owner": "xamboon", "title": "How to add your own tags for POS tagging over default taggers using nltk?", "body": "import nltk.tag, nltk.data\r\ntagger_path = '/home/amit/nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle'\r\ndefault_tagger = nltk.data.load(tagger_path)\r\ntagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)\r\ntagged=tagger.tag(text)\r\n#model is a dict which has the required tags ,\"tagged\" gives tags according to default_tagger but I want to put tags to text from model dict .Please , explain me what is wrong here ?"}
{"number": 2034, "owner": "alvations", "title": "Better Panlex Swadesh", "body": "From https://github.com/nltk/nltk_data/issues/117 , we can have a better Panlex swadesh list interface that also reads the language ID files (which might be useful for nlp, e.g. language ID).\r\n\r\nMaybe an interface in https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordlist.py#L31 might be:\r\n\r\n```python\r\nfrom __future__ import print_function\r\nfrom collections import namedtuple, defaultdict\r\nimport re\r\nfrom six import string_types\r\n\r\nPanlexLanguage = namedtuple('PanlexLanguage',\r\n                          ['panlex_uid',  # (1) PanLex UID\r\n                           'iso639',      # (2) ISO 639 language code\r\n                           'iso639_type', # (3) ISO 639 language type, see README\r\n                           'script',      # (4) normal scripts of expressions\r\n                           'name',        # (5) PanLex default name\r\n                           'langvar_uid'  # (6) UID of the language variety in which the default name is an expression\r\n                           ])\r\n\r\nclass PanlexSwadeshCorpusReader(WordListCorpusReader):\r\n    \"\"\"\r\n    This is a class to read the PanLex Swadesh list from\r\n\r\n    David Kamholz, Jonathan Pool, and Susan M. Colowick (2014).\r\n    PanLex: Building a Resource for Panlingual Lexical Translation.\r\n    In LREC. http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf\r\n\r\n    License: CC0 1.0 Universal\r\n    https://creativecommons.org/publicdomain/zero/1.0/legalcode\r\n    \"\"\"\r\n    def __init__(self, *args, **kwargs):\r\n        super(PanlexSwadeshCorpusReader, self).__init__(*args, **kwargs)\r\n        # Find the swadesh size using the fileids' path.\r\n        self.swadesh_size = re.match(r'swadesh([0-9].*)\\/', self.fileids()[0]).group(1)\r\n        self._languages = {lang.panlex_uid:lang for lang in self.get_languages()}\r\n        self._macro_langauges = self.get_macrolanguages()\r\n\r\n    def license(self):\r\n        print('CC0 1.0 Universal')\r\n\r\n    def readme(self):\r\n        print(self.raw('README'))\r\n\r\n    def language_codes(self):\r\n        return self._languages.keys()\r\n\r\n    def get_languages(self):\r\n        for line in self.raw('langs{}.txt'.format(self.swadesh_size)).split('\\n'):\r\n            if not line.strip(): # Skip empty lines.\r\n                continue\r\n            yield PanlexLanguage(*line.strip().split('\\t'))\r\n\r\n    def get_macrolanguages(self):\r\n        macro_langauges = defaultdict(list)\r\n        for lang in self._languages.values():\r\n            macro_langauges[lang.iso639].append(lang.panlex_uid)\r\n        return macro_langauges\r\n\r\n    def words_by_lang(self, lang_code):\r\n        \"\"\"\r\n        :return: a list of list(str)\r\n        \"\"\"\r\n        fileid = 'swadesh{}/{}.txt'.format(self.swadesh_size, lang_code)\r\n        return [concept.split('\\t') for concept in self.words(fileid)]\r\n\r\n    def words_by_iso639(self, iso63_code):\r\n        \"\"\"\r\n        :return: a list of list(str)\r\n        \"\"\"\r\n        fileids = ['swadesh{}/{}.txt'.format(self.swadesh_size, lang_code)\r\n                   for lang_code in self._macro_langauges[iso63_code]]\r\n        return [concept.split('\\t') for fileid in fileids for concept in self.words(fileid)]\r\n\r\n    def entries(self, fileids=None):\r\n        \"\"\"\r\n        :return: a tuple of words for the specified fileids.\r\n        \"\"\"\r\n        if not fileids:\r\n            fileids = self.fileids()\r\n\r\n        wordlists = [self.words(f) for f in fileids]\r\n        return list(zip(*wordlists))\r\n\r\n```\r\n\r\nAnd then on https://github.com/nltk/nltk/blob/develop/nltk/corpus/__init__.py#L199\r\n\r\n```python\r\nfrom nltk.corpus.util import LazyCorpusLoader\r\nfrom nltk.corpus.reader.wordlist import PanlexSwadeshCorpusReader\r\n\r\nswadesh110 = LazyCorpusLoader(\r\n    'panlex_swadesh', PanlexSwadeshCorpusReader, r'swadesh110/.*\\.txt', encoding='utf8')\r\nswadesh207 = LazyCorpusLoader(\r\n    'panlex_swadesh', PanlexSwadeshCorpusReader, swadesh207/.*\\.txt', encoding='utf8')\r\n```"}
{"number": 2032, "owner": "AyBeeNLP", "title": "TrigramCollocationFinder nbest method exception", "body": "While trying to determine trigrams using the TrigramCollocation finder an exception was received with following details:\r\nfrom nltk.collocations import TrigramCollocationFinder\r\nfrom nltk.metrics import TrigramAssocMeasures\r\n\r\nwordList = ['borrower', 'borrower', 'borrower', 'borrower', 'borrower', 'borrower', 'page']\r\n\r\ntcf = TrigramCollocationFinder.from_words(wordList)\r\ntcf.apply_freq_filter(3)\r\nfor word1, word2, word3 in tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 8):\r\n    print ( (word1,word2,word3))\r\n\r\nException Traceback:\r\nTraceback (most recent call last):\r\n  File \"D:/Apps/Python/HelloPython/WorExploreTest.py\", line 19, in <module>\r\n    for word1, word2, word3 in tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 8):\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\collocations.py\", line 125, in nbest\r\n    return [p for p, s in self.score_ngrams(score_fn)[:n]]\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\collocations.py\", line 121, in score_ngrams\r\n    return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\collocations.py\", line 113, in _score_ngrams\r\n    score = self.score_ngram(score_fn, *tup)\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\collocations.py\", line 255, in score_ngram\r\n    n_all)\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\metrics\\association.py\", line 143, in likelihood_ratio\r\n    for obs, exp in zip(cont, cls._expected_values(cont))))\r\n  File \"D:\\Apps\\Python\\HelloPython\\venv\\lib\\site-packages\\nltk\\metrics\\association.py\", line 143, in <genexpr>\r\n    for obs, exp in zip(cont, cls._expected_values(cont))))\r\nValueError: math domain error\r\n\r\nAny assistance would be highly appreciated.\r\nThanks a lot"}
{"number": 2020, "owner": "f0lie", "title": "Type hinting / annotation (PEP 484)?", "body": "Hello folks,\r\n\r\nRecently [numpy](https://github.com/numpy/numpy/issues/7370) added some type hinting for their types. Tools like mypy is suppose to make it easier to test large codebases.  There is a recent [pycon](https://www.youtube.com/watch?v=pMgmKJyWKn8) talk about using mypy in the real world. Since nltk is pretty old and large, I figure it might be useful here.\r\n\r\nI tried use tools like Monkeytype to generate some stubs for the codebase. But I find it hard to add type hints to a project's code if you aren't familiar with it.\r\n\r\nIs there any interest in exploring this route?\r\n\r\nThanks,"}
{"number": 2017, "owner": "alvations", "title": "dataclasses", "body": "[Dataclasses](https://www.python.org/dev/peps/pep-0557/) are good and more powerful than `namedtuples` c.f. https://github.com/nltk/nltk/issues/530\r\n\r\nIt's pretty apt for most classes we have in `nltk` (not restricted to `nltk.corpus`) and one natural application is to re-write `CorpusReader`. I think after adopting `dataclasses`, creating a custom new corpus reader would be a lot easier for users. \r\n\r\nReferences:\r\n\r\n - https://hackernoon.com/a-brief-tour-of-python-3-7-data-classes-22ee5e046517\r\n - https://www.youtube.com/watch?v=T-TwcmT6Rcw "}
{"number": 2016, "owner": "The-Gupta", "title": "Stanford CoreNLP 'coref' resolution model", "body": "Does NLTK have Stanford CoreNLP 'coref' resolution model?"}
{"number": 2015, "owner": "agodbehere", "title": "Verbnet corpus is out of date", "body": "The nltk data index (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml) points `verbnet` to version 2.1. The latest `verbnet` definition is 3.2. \r\n\r\nThe latest version has updated frame descriptions that provide much more information about the phrasal structure. For example, the primary description of a frame from class `future_having-13.3` in the latest version is `NP V NP-Dative NP`, describing the frame's structure as (noun-phrase, verb, noun-phrase(dative), noun-phrase) while in version 2.1 it just reads `Dative`. "}
{"number": 2014, "owner": "TristanJM", "title": "Importing NLTK breaks multiprocessing", "body": "Using `multiprocessing.Pool().map` to train Keras models concurrently.\r\n\r\nAs soon as I add the import for `nltk` the shell freezes with no exceptions, requiring the terminal window to be force closed.\r\n\r\n> python 2.7.10 (and 2.7.15)\r\nnltk 3.2.5\r\nkeras 2.1.5\r\ntensorflow 1.4.0\r\n\r\nThe exact point of failure is\r\n```python\r\nfrom keras.models import Sequential\r\nfrom keras.layers.recurrent import LSTM\r\n\r\nmodel = Sequential()\r\nprint('im printed')\r\nmodel.add(LSTM(...))     # <----\r\nprint('doesnt get here')\r\n```\r\nWorks as soon as `import nltk` is removed from the project (a completely different py module), so I'm guessing it's a problem with NLTK and not Keras.\r\n\r\nAnyone got any ideas? Possibly related to https://github.com/nltk/nltk/issues/947?\r\n\r\n"}
{"number": 2011, "owner": "hitvoice", "title": "TweetTokenizer causes UnicodeEncodeError when input string is valid", "body": "```python\r\nfrom nltk.tokenize import TweetTokenizer\r\ntext = \"and i'm always happy when i saw his smile \ud83d\ude0d\ud83d\ude02&#55357;\ud83d\ude0d\"\r\ntknz = TweetTokenizer()\r\nprint(' '.join(tknz.tokenize(text)))\r\n```\r\nThe result will be:\r\n```\r\nUnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 46: surrogates not allowed\r\n```\r\nThe input string is valid, and the output of TweetTokenizer has UnicodeEncodeError. This is not expected.\r\n\r\nTesting environments:\r\n- Ubuntu Server 16.04\r\n- python 3.6\r\n- nltk 3.2.5"}
{"number": 2008, "owner": "alvations", "title": "Better PunktTrainer", "body": "While trying to retrain a sentence tokenizer model with `PunktTokenizer`, the NLTK code took up >200GB of RAM and a lot of swap and doesn't seem to end after 2 days of training. \r\n\r\n```python\r\nimport pickle\r\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer\r\n\r\ntokenizer = PunktSentenceTokenizer()\r\n\r\nwith open('wikipedia-en.txt') as fin:\r\n    text = fin.read()\r\n\r\ntokenizer.train(text)\r\n\r\nwith open('wiki-en.pkl') as fout:\r\n    pickle.dump(tokenizer, fout)\r\n```\r\n\r\nThe code seem inefficient for the massive data set we have now to train better Punkt models. \r\n\r\nLet's keep this issue to track Punkt related issue until we get a better version of the algorithm."}
{"number": 2005, "owner": "oxinabox", "title": "Neither  `word_tokenize` nor `TreebankWordTokenizer`  matchs the original Penn Word Tokenizer", "body": "Right now,\r\nNLTK has 2 tokenizers that are very similar to  Robert McIntyre's original  treebank tokenizer.\r\nThose are:\r\n\r\n  - `nltk.tokenize.treebank.TreebankWordTokenizer()`\r\n     -  a default instance of the type\r\n - and `nltk.tokenize._treebank_word_tokenizer`   \r\n    - a particular instance of that type **behind** `word_tokenize`.\r\n    - It has modifications for better unicode support given [here](https://github.com/nltk/nltk/blob/081a5307b319ccc641272d2daeb53389598d0828/nltk/tokenize/__init__.py#L108-L110)\r\n\r\nAccording to [this comment](https://github.com/nltk/nltk/pull/1437#issuecomment-234164667),\r\nthe reason the modifications were not simple made directly to the `TreebankWordTokenizer`,\r\nwas so that a default instance would continue to match the behaviour of McIntyre's original.\r\n\r\n\r\nHowever, a default instance already does not match.\r\nThis is illustrated by tokenizing: _\"\"I cannot cannot work under these conditions!\"\"_\r\n\r\n - McIntyre's original gives: \r\n      - `[\"I\", \"can\", \"not\", \"cannot\", \"work\", \"under\", \"these\", \"conditions\", \"!\"]`\r\n - `TreebankWordTokenizer()` and `word_tokenize` gives:\r\n      -   `[\"I\", \"can\", \"not\", \"can\", \"not\", \"work\", \"under\", \"these\", \"conditions\", \"!\"]`\r\n\r\n\r\nAs such `TreebankWordTokenizer()` already does not match the behaviour of the original.\r\nSo either:\r\n\r\n - Matching the original **does not** matter. In which chase merging the unicode support into the class makes sense.\r\n - Or: matching the original *does* matter, in which case `TreebankWordTokenizer()` needs to be amended.\r\n"}
{"number": 2004, "owner": "oxinabox", "title": "`word_tokenize` could handled URL's better", "body": "Here is what it currently does for URLs.\r\n\r\n```\r\nIn [2]: import nltk\r\n\r\nIn [3]: nltk.word_tokenize(\"http://example.com\")\r\nOut[3]: ['http', ':', '//example.com']\r\n\r\nIn [4]: nltk.word_tokenize(\"http://example.com/dir/file.html\")\r\nOut[4]: ['http', ':', '//example.com/dir/file.html']\r\n\r\nIn [5]: nltk.word_tokenize(\"example@example.com\")\r\nOut[5]: ['example', '@', 'example.com']\r\n```\r\n\r\nI'm not sure what it should do.\r\nBut I feel like the answer is _not that_\r\n\r\nI'm pretty sure one can recognise URLs with a regex.\r\nI think the answer might be to just pick them out at the start,\r\nand preform no splitting on them.\r\n\r\n\r\nObs. the treeback tokenizer  that `word_tokenize`, is based on doesn't handle URLs because they were not a thing. But we have to move with the times, no?"}
{"number": 1995, "owner": "alvations", "title": "word_tokenize keeps the opening single quotes and doesn't pad it with space", "body": "`word_tokenize` keeps the opening single quotes and doesn't pad it with space, this is to make sure that the clitics get tokenized as `'ll`, `'ve', etc. \r\n\r\nThe original treebank tokenizer has the same behavior but Stanford CoreNLP doesn't. It looks like some additional regex was put in to make sure that the opening single quotes get padded with spaces if it isn't followed by clitics. \r\n\r\nThere should be a non-capturing regex to catch the non-clitics and pad the space. \r\n\r\nDetails on https://stackoverflow.com/questions/49499770/nltk-word-tokenizer-treats-ending-single-quote-as-a-separate-word/49506436#49506436"}
{"number": 1980, "owner": "alvations", "title": "span_tokenize rtype should be standardized", "body": "Currently, there are different Tokenizer classes having the `span_tokenize()` function that returns a mix of generator/list outputs:\r\n\r\n| Tokenizer | rtype |\r\n|:-|:-|\r\n|[CharTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/simple.py#L68)| generator|\r\n|[LineTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/simple.py#L123)| generator|\r\n|[StringTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/api.py#L75) | generator|\r\n|[RegexpTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/regexp.py#L133) | generator |\r\n|[PunktSentenceTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py#L1273)|  list | \r\n|[TreebankWordTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py#L147)| list |\r\n\r\n`span_tokenize` rtype should be standardized, either all list or all generator. Related tests should be standardized too. \r\n\r\nQuestion raised comes from https://stackoverflow.com/questions/49290827/span-tokenize-gives-generator-object-as-output"}
{"number": 1979, "owner": "shuaihuaiyi", "title": "nltk.stem.arlstem.ARLSTem#pref(token) returns None", "body": "Is this a bug or feature? Have you missed a return statement at the end of this function?"}
{"number": 1978, "owner": "alvations", "title": "Consistent pos argument between wn.synsets() and WordNetLemmatizer.lemmatize()", "body": "Currently, there's some inconsistency of how POS is treated in `wn.synsets()` and `WordNetLemmatizer.lemmatize()`, e.g. \r\n\r\n```python\r\n>>> from nltk.corpus import wordnet as wn\r\n>>> from nltk.stem import WordNetLemmatizer\r\n>>> wnl = WordNetLemmatizer()\r\n\r\n# Accepts None and let pos be underspecified.\r\n>>> wn.synsets('running', pos=None)\r\n[Synset('run.n.05'), Synset('run.n.07'), Synset('running.n.03'), Synset('running.n.04'), Synset('track.n.11'), Synset('run.v.01'), Synset('scat.v.01'), Synset('run.v.03'), Synset('operate.v.01'), Synset('run.v.05'), Synset('run.v.06'), Synset('function.v.01'), Synset('range.v.01'), Synset('campaign.v.01'), Synset('play.v.18'), Synset('run.v.11'), Synset('tend.v.01'), Synset('run.v.13'), Synset('run.v.14'), Synset('run.v.15'), Synset('run.v.16'), Synset('prevail.v.03'), Synset('run.v.18'), Synset('run.v.19'), Synset('carry.v.15'), Synset('run.v.21'), Synset('guide.v.05'), Synset('run.v.23'), Synset('run.v.24'), Synset('run.v.25'), Synset('run.v.26'), Synset('run.v.27'), Synset('run.v.28'), Synset('run.v.29'), Synset('run.v.30'), Synset('run.v.31'), Synset('run.v.32'), Synset('run.v.33'), Synset('run.v.34'), Synset('ply.v.03'), Synset('hunt.v.01'), Synset('race.v.02'), Synset('move.v.13'), Synset('melt.v.01'), Synset('ladder.v.01'), Synset('run.v.41'), Synset('running.a.01'), Synset('running.s.02'), Synset('running.a.03'), Synset('running.a.04'), Synset('linear.s.05'), Synset('running.s.06')]\r\n\r\n\r\n# Doesn't accept None and raise a KeyError\r\n>>> wnl.lemmatize('running', pos=None)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/stem/wordnet.py\", line 40, in lemmatize\r\n    lemmas = wordnet._morphy(word, pos)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\", line 1774, in _morphy\r\n    exceptions = self._exception_map[pos]\r\nKeyError: None\r\n```\r\n\r\nI'm not sure how to allow `None` to `WordNetLemmatizer.lemmatize()` though.\r\n\r\n**What should be the expected behavior of `pos=None`, default to `pos='n'`?** If so, then we can make changes at https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L39:\r\n\r\n```python\r\nfrom nltk.corpus.reader.wordnet import NOUN\r\n...\r\n\r\n    def lemmatize(self, word, pos=None):\r\n        pos = NOUN if pos == None else pos\r\n        lemmas = wordnet._morphy(word, pos)\r\n        return min(lemmas, key=len) if lemmas else word\r\n```\r\n\r\n"}
{"number": 1971, "owner": "altescy", "title": "RecursionError in PorterStemmer._is_consonant", "body": "RecursionError occurs in [`PorterStemmer._is_consonant`](https://github.com/nltk/nltk/blob/5a63a62cfc2bdd9d35d863d1c583942e3e6baf6b/nltk/stem/porter.py#L126) when trying to stem a word containing 'yyyyyyy....'.\r\nIf the length of 'yyyyyyy....' is larger than the maximum recursion depth, PorterStemmer cannot stem the word.\r\n\r\n```python\r\n>> from nltk.stem import PorterStemmer\r\n>>> stemmer = PorterStemmer()\r\n>>> stemmer.stem('y'*10000)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 667, in stem\r\n    stem = self._step1c(stem)\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 421, in _step1c\r\n    else original_condition\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 268, in _apply_rule_list\r\n    if condition is None or condition(stem):\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 411, in nltk_condition\r\n    return len(stem) > 1 and self._is_consonant(stem, len(stem) - 1)\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 144, in _is_consonant\r\n    return (not self._is_consonant(word, i - 1))\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 144, in _is_consonant\r\n    return (not self._is_consonant(word, i - 1))\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 144, in _is_consonant\r\n    return (not self._is_consonant(word, i - 1))\r\n  [Previous line repeated 990 more times]\r\n  File \"/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py\", line 140, in _is_consonant\r\n    if word[i] == 'y':\r\nRecursionError: maximum recursion depth exceeded in comparison\r\n>>>\r\n```\r\n"}
{"number": 1970, "owner": "marcevrard", "title": "WordNet: Synsets' relations are absent from their corresponding lemmas", "body": "In WordNet, it seems to me that the relations relevant for a given synset should be relevant for its corresponding lemmas as well. In the `nltk` implementation, all semantic relations implemented as methods for synsets (e.g., `hypernyms()`) are indeed also available for lemmas. Though for example, the lemma `trick.n.01.trick` `hypernyms` method does not return anything, while its synset's does.\r\n\r\nUsing the lemma method:\r\n```python\r\n>>> trick_lemma = wnt.get_synset('trick.n.01').lemmas()[0]\r\n>>> print(trick_lemma.hypernyms())\r\n[]\r\n```\r\nUsing the the synset method instead:\r\n```python\r\n>>> print(trick_lemma.synset().hypernyms())\r\n[Synset('device.n.03')]\r\n>>> print(trick_lemma.synset().hypernyms()[0].lemmas())\r\n[Lemma('device.n.03.device'), Lemma('device.n.03.gimmick'), Lemma('device.n.03.twist')]\r\n```\r\n\r\nExpected behavior of the lemma method:\r\n```python\r\n>>> print(trick_lemma.hypernyms())\r\n[Lemma('device.n.03.device'), Lemma('device.n.03.gimmick'), Lemma('device.n.03.twist')]\r\n```\r\n\r\nVersions:\r\n```\r\nnltk 3.2.5\r\nPython 3.6.4\r\nUbuntu 17.10\r\n```"}
{"number": 1968, "owner": "PanderMusubi", "title": "Incorrect word_tokenize for Dutch", "body": "I have observed the following incorrect word_tokenize for Dutch. The characters `'` and `\u2019` are very often used unbalanced to represent an apostrophe that is part of correct word. It will chopped up words when tokenizing on it. The in the code below `tmp` is the correct tokenization.\r\n\r\n    from nltk import word_tokenize\r\n    from pprint import pprint\r\n    \r\n    sentence = \"Test alinea's alinea\u2019s \u2018smaak\u2019 's-Hertogenbosch \u2019s-Hertogenbosch test.\"\r\n    print(\"sentence:\", sentence)\r\n    pprint(word_tokenize(sentence, language=\"dutch\"))\r\n    tmp = \"\"\r\n    opened = False\r\n    for char in sentence:\r\n    \tif char == \"'\":\r\n    \t\ttmp += \"\u03b1\"\r\n    \telif char == \"\u2018\":\r\n    \t\topened = True\r\n    \t\ttmp += char\r\n    \telif char == \"\u2019\":\r\n    \t\tif opened:\r\n    \t\t\topened = False\r\n    \t\t\ttmp += char\r\n    \t\telse:\r\n    \t\t\ttmp += \"\u03b2\"\r\n    \telse:\r\n    \t\ttmp += char\r\n    print()\r\n    print(\"tmp:\", tmp)\r\n    result = []\r\n    for word in word_tokenize(tmp, language=\"dutch\"):\r\n    \tresult.append(word.replace(\"\u03b1\", \"'\").replace(\"\u03b2\", \"\u2019\"))\r\n    pprint(result)\r\n\r\nwith output:\r\n\r\n    sentence: Test alinea's alinea\u2019s \u2018smaak\u2019 's-Hertogenbosch \u2019s-Hertogenbosch test.\r\n    ['Test',\r\n     'alinea',\r\n     \"'s\",\r\n     'alinea',\r\n     '\u2019',\r\n     's',\r\n     '\u2018',\r\n     'smaak',\r\n     '\u2019',\r\n     \"'s-Hertogenbosch\",\r\n     '\u2019',\r\n     's-Hertogenbosch',\r\n     'test',\r\n     '.']\r\n    \r\n    tmp: Test alinea\u03b1s alinea\u03b2s \u2018smaak\u2019 \u03b1s-Hertogenbosch \u03b2s-Hertogenbosch test.\r\n    ['Test',\r\n     \"alinea's\",\r\n     'alinea\u2019s',\r\n     '\u2018',\r\n     'smaak',\r\n     '\u2019',\r\n     \"'s-Hertogenbosch\",\r\n     '\u2019s-Hertogenbosch',\r\n     'test',\r\n     '.']"}
{"number": 1967, "owner": "mrelich", "title": "Different results using tag vs. tag_sents", "body": "I was having an issue tagging short sentences and I tracked the issue to this line: https://github.com/nltk/nltk/blob/cdaa7dd4e60251390a82e42456edb359191ea6e8/nltk/tag/stanford.py#L190\r\n\r\nThe full explanation can be found [here](https://stackoverflow.com/questions/48998936/nltk-tag-tag-sents-give-different-results).  I think that the executable is not recognizing this argument (eg. swap false for true) when run inside of subprocess.  It can be fixed it by removing the `\\\"`, however I think what we actually want is for this argument to be `true`.  The reason is that if it's `false`, the NER tagger is treating everything as a single line, which means that you will get inconsistent results from tagging single sentences (using `tag(...)`) vs. batch tagging (using `tag_sents(...)`).\r\n\r\nIf you still think it should be `false`, can I recommend to add a parameter to make this configurable?  I am happy to make this update and commit."}
{"number": 1963, "owner": "satyammittal", "title": "word_tokenize not tokenizing strings containing ','", "body": "I tried\r\n```\r\nfrom nltk.tokenize import word_tokenize\r\na=\"g, a, b, c, 123, g32,12 123121 {1}\"\r\nword_tokenize(a)\r\n```\r\n**Output I am getting:** \r\n['g', ',', 'a', ',', 'b', ',', 'c', ',', '123', ',', 'g32,12', '123121', '{', '1', '}']\r\n**Output should be**\r\n['g', ',', 'a', ',', 'b', ',', 'c', ',', '123', ',', 'g32',',','12', '123121', '{', '1', '}']"}
{"number": 1962, "owner": "alvations", "title": "Better CONLLCorpusReader", "body": "Alexis has an improved version of the CONLL corpus reader on https://stackoverflow.com/a/46875652/610569\r\n\r\nIt'll be good if we incorporate these changes by either:\r\n\r\n - retaining the name `BetterConllCorpusReader` and inherit from `ConllCorpusReader`\r\n - refactoring the `ConllCorpusReader` and remove the hard-coded columns and a more flexible `self._get_iob_words`"}
{"number": 1954, "owner": "drevicko", "title": "Casual tokeniser allows newline in elipses and phone number tokens", "body": "This may be intended behaviour, but if so, it would be good to point it out in the docs as it is ONLY possible in these two cases and it is a common assumption that tokens do not contain newline characters.\r\n\r\nIt may be appropriate to disallow newlines in these tokens. This can be done by, [for example](https://stackoverflow.com/a/3469155/420867), using `[^\\S\\r\\n]` in place of `\\s` in the relevant regexes.\r\n\r\nIt is due to `\\s` appearing in the regexes for [elipses](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L168) and [phone numbers](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L130)."}
{"number": 1953, "owner": "drevicko", "title": "tweet tokenizer has US-specific phone number regex", "body": "The [phone number regex](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L125) is ONLY for united states phone numbers. Standard international numbers are not recognised at all.\r\n\r\nFurther, this regex is (causing problems with long numbers in general)[https://github.com/nltk/nltk/issues/1799].\r\n\r\nCreating a regex capable of recognising ALL phone numbers (including other countries and other international prefixes) will likely be problematic, as it will likely match other numbers not intended as phone numbers. \r\n\r\nAn immediate band-aid remedy would be to add an option to not process phone numbers and a caveat that it is only for US numbers. \r\n\r\nA more serious attempt could be to (optionally!) use [libphonenumber](https://github.com/googlei18n/libphonenumber) or something similar. Having a relatively ad-hoc regex specific to just one country seems a bit inappropriate here (only [around 1/5 of Twitter users are in the US](https://www.statista.com/statistics/274564/monthly-active-twitter-users-in-the-united-states/))."}
{"number": 1944, "owner": "villmow", "title": "Wordnet: Comparison of  a _WordNetObject fails if other object is not a _WordNetObject.", "body": "Problem: As the title says, the comparison of a _WordNetObject fails if compared to something that doesn't have the attribute `.name`. \r\n\r\nSolution: Catch `AttributeError` and return `False`\r\n\r\nSee output log:\r\n```bash \r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-217-bacea08cca5b> in <module>()\r\n    107 d = 0\r\n    108 for a,b,c in cleaned_entities:\r\n--> 109     if a == 'INVALID':\r\n    110         d += 1\r\n    111 #         for pos, synset in syn_e1:\r\n\r\n.../nltk/corpus/reader/wordnet.py in __eq__(self, other)\r\n    196 \r\n    197     def __eq__(self, other):\r\n--> 198         return self._name == other._name\r\n    199 \r\n    200     def __ne__(self, other):\r\n\r\nAttributeError: 'str' object has no attribute '_name'\r\n```"}
{"number": 1941, "owner": "psukys", "title": "WordNet unclosed files", "body": "When using wordnet, Python complains about unclosed files, i.e.:\r\n```\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1107: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/lexnames'>\r\n  for i, line in enumerate(self.open('lexnames')):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.adj'>\r\n  for i, line in enumerate(self.open('index.%s' % suffix)):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.adv'>\r\n  for i, line in enumerate(self.open('index.%s' % suffix)):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.noun'>\r\n  for i, line in enumerate(self.open('index.%s' % suffix)):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.verb'>\r\n  for i, line in enumerate(self.open('index.%s' % suffix)):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/adj.exc'>\r\n  for line in self.open('%s.exc' % suffix):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/adv.exc'>\r\n  for line in self.open('%s.exc' % suffix):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/noun.exc'>\r\n  for line in self.open('%s.exc' % suffix):\r\nvenv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/verb.exc'>\r\n  for line in self.open('%s.exc' % suffix):\r\n```\r\n"}
{"number": 1939, "owner": "shlomihod", "title": "Translation Probabilities do not sum to 1 in IBM1 Translation Model", "body": "After running IBM1 Translation model in the given code, I've summed the translation probabilites (the probability for a target word given source word), and it doesn't sum to 1:\r\n\r\n```python\r\nfrom nltk.translate import IBMModel1, AlignedSent\r\nimport numpy as np\r\n\r\nsrc_sentences = [['a', '.'], ['b', 'b', '.']]\r\ntrg_sentences = [['X', '.'], ['Z', '.']]\r\n\r\nbitext = [AlignedSent(t, s) for s,t in zip(src_sentences, trg_sentences)]\r\nibm1 = IBMModel1(bitext, 1)\r\n\r\nsum([ibm1.translation_table[t]['b'] for t in ibm1.translation_table])\r\n1.3333333333333333\r\n```\r\n\r\nI guess it is because here:\r\nhttps://github.com/nltk/nltk/blob/7bbc00e908087b60325db30526872306e767de13/nltk/translate/ibm_model.py#L336-L340\r\nThe iteration only on the counts, and not all the vocabulary. So (src_word, trg_word) pairs that don't appear in the same sentence pairs, won't get updated and will stay with the initial uniform probabilities"}
{"number": 1931, "owner": "alvations", "title": "An interface to the 1M sense tagged inventory", "body": "There's a nice corpus of 1M WordNet tagged instances of MultiUN corpus produced in http://www.comp.nus.edu.sg/~kaveh/papers/wsd-conll15.pdf \r\n\r\nIf anyone is up for a challenge, It'll be a nice addition to have a reader for this corpus in NLTK.\r\n\r\nThe data source is on http://www.comp.nus.edu.sg/~nlp/corpora.html"}
{"number": 1929, "owner": "alvations", "title": "Adding popular and book nltk_data during installation process", "body": "It is possible to enforce `nltk_data` downloads inside the `setup.py`. And maybe that'll remove some gotchas when people use functions that requires default models the first time and they would need to do `nltk.download(...)` (e.g. `sent_tokenize`, `word_tokenize`, `pos_tag`). \r\n\r\nSome useful hints to implement this:\r\n\r\n - https://stackoverflow.com/questions/26799894/installing-nltk-data-in-setup-py-script\r\n - https://stackoverflow.com/questions/21915469/python-setuptools-install-requires-is-ignored-when-overriding-cmdclass\r\n - https://stackoverflow.com/questions/20194565/running-custom-setuptools-build-during-install\r\n - https://blog.niteoweb.com/setuptools-run-custom-code-in-setup-py/\r\n\r\n"}
{"number": 1921, "owner": "LBenzahia", "title": "Improving Snowball Arabic stemmer in Nltk", "body": "I've opened this issue to add improvements of snowball ArabicStemmer in [nltk snowball ArabicStemmer](https://github.com/nltk/nltk/blob/e0b882177421498a1a49e42d4b83c51bfa451f3d/nltk/stem/snowball.py#L297) from the [original algorithm](https://github.com/assem-ch/arabicstemmer) \r\n\r\n"}
{"number": 1920, "owner": "acattle", "title": "BllipParser: AttributeError: 'str' object has no attribute 'decode' on Python 3.5", "body": "When calling next(BllipParser.parse()) on Python 3.5, I receive the following error:\r\n\r\nFile \"/home/andrew/.local/lib/python3.5/site-packages/nltk/parse/bllip.py\", line 169, in parse\r\n    _ensure_ascii(sentence)\r\n  File \"/home/andrew/.local/lib/python3.5/site-packages/nltk/parse/bllip.py\", line 101, in _ensure_ascii\r\n    word.decode('ascii')\r\nAttributeError: 'str' object has no attribute 'decode'\r\n\r\n\r\nRelated to https://github.com/nltk/nltk/issues/507\r\n\r\nNLTK v3.2.5\r\nPython v3.5.2\r\nOS: Ubuntu (16.04.3) running on WSL (16299.125)"}
{"number": 1917, "owner": "vincealdrin", "title": "How to modify requests module timeout when using corenlp?", "body": "I want to parse long text using corenlp so the request tooks longer than the default timeout which is 60 seconds. Is it possible to change it?\r\n\r\n![image](https://user-images.githubusercontent.com/12383137/34068405-d42d0a40-e274-11e7-9519-679967a1e086.png)\r\n"}
{"number": 1913, "owner": "aziyamehboob", "title": "TypeError: sequence item 352: expected str instance, NoneType found", "body": "i am new in nltk and chunking.i dont know where i am getting this error and why.i tagged the corpus and then i was trying to do chunking in that but i am getting the type error. here is the code:\r\n```\r\ndef load_corpus():\r\n    corpus_root = os.path.abspath('../nlp1/dumpfiles')\r\n    mycorpus = nltk.corpus.reader.TaggedCorpusReader(corpus_root,'.*')\r\n    return mycorpus.tagged_sents()\r\n\r\ndef sents_chunks(tagg_sents, pos_tag_pattern):\r\n    chunk_freq_dict = defaultdict(int)\r\n    chunker = nltk.RegexpParser(pos_tag_pattern)\r\n    for sent in tagg_sents:\r\n        a =filter(bool, sent) \r\n        print(type(a))\r\n        print(sent)\r\n        for chk in chunker.parse(sent).subtrees():\r\n            if str(chk).startswith('(NP'):\r\n                phrase = chk.__unicode__()[4:-1]\r\n                #print(phrase)\r\n                if '\\n' in phrase:\r\n                    phrase = ' '.join(phrase.split())\r\n                    #print(phrase)\r\n                chunk_freq_dict[phrase] += 1\r\n    #print(chunk_freq_dict)\r\n    return chunk_freq_dict\r\n```"}
{"number": 1903, "owner": "workflowsguy", "title": "Consider typographical quotes when splitting sentences", "body": "When using various tools that build upon NLTK to split sentences, I keep running into the problem that texts with typographical quotes like chevrons (\u00bb\u00ab) or guillemets (\u00ab\u00bb) are not handled correctly.\r\n\r\nE.g. with a text like:\r\n\r\n`\u00bbDie milit\u00e4rische Welt ist doch manchmal recht unsch\u00f6n. Wie froh bin ich da, dass ich Dich habe, die Du so viel anders bist als die meisten Menschen.\u00ab Das hatte Robert Scholl am 1. Februar 1916 an Lina M\u00fcller geschrieben, die bei ihren Eltern in K\u00fcnzelsau Urlaub machte.`\r\n\r\nsentences two and three are not split: \r\n\r\n```\r\n\u00bbDie milit\u00e4rische Welt ist doch manchmal recht unsch\u00f6n.\r\nWie froh bin ich da, dass ich Dich habe, die Du so viel anders bist als die meisten Menschen.\u00ab Das hatte Robert Scholl am 1. Februar 1916 an Lina M\u00fcller geschrieben, die bei ihren Eltern in K\u00fcnzelsau Urlaub machte.\r\n```\r\n\r\nThe maintainers of those tools always point back to NLTK as the culprit when I report this issue. (They all seem to use the \"Punkt\" tokenizer for this.)\r\n\r\nSo it would be great if the NLTK framework could address this issue.\r\n\r\n(Tested with NLTK 3.2.5)"}
{"number": 1892, "owner": "alvations", "title": "Deprecating and removing Stanford API support ", "body": "Stanford CoreNLP has released their official Python wrapper https://github.com/stanfordnlp/python-stanford-corenlp \r\n\r\nShould we remove the support for the Stanford API in NLTK and replace it with `NotImplementedError`? "}
{"number": 1891, "owner": "DavidNemeskey", "title": "prob_t_a_given_s() only computes half of what it should", "body": "According to the Jurafsky & Martin book, but also the [original paper](http://www.aclweb.org/anthology/J93-2003) (eq. 5), _P(F,A|E)_ is not just the product of the word probabilities, but it should also be weighted by the alignment probability (_eps / (l + 1)^m_). This is missing from the implementation of `prob_t_a_given_s()`, which, as it is, actually just computes _P(F|A,E)_. So either\r\n- the missing code should be added to it, or\r\n- it should be renamed to `prob_t_given_a_s()`. "}
{"number": 1890, "owner": "DavidNemeskey", "title": "CFG.fromstring cannot handle epsilons", "body": "There are times when one wants to include an empty terminal, such as:\r\n```\r\nS -> A B\r\nA -> 'a'\r\nB -> 'b' | ''\r\n```\r\n\r\nHowever, when trying to read it via `CFG.fromstring`, I get\r\n```\r\nERROR:root:An unexpected error occurred while tokenizing input\r\n```\r\n\r\nInterestingly, `Tree` has no problems with empty terminals:\r\n```Python\r\ntree = Tree('S', [\r\n    Tree('A', ['a']),\r\n    Tree('B', [''])\r\n])\r\ntree.pos()\r\n```\r\nprints\r\n```Python\r\n[('a', 'A'), ('', 'B')]\r\n```"}
{"number": 1887, "owner": "DavidNemeskey", "title": "Tree drawing doesn't work on a headless server", "body": "`nltk.tree.Tree` uses the Tcl (tkinter) backend to draw trees. Unfortunately, that doesn't seem to work on a headless server, which is the setup I use: run Jupyter on the server and access it via the browser from another machine.\r\n\r\nIf `python3-tk` is not installed, I get\r\n```\r\nImportError: No module named '_tkinter', please install the python3-tk package\r\n```\r\n\r\nIf it is, I get\r\n```\r\nTclError: no display name and no $DISPLAY environment variable\r\n```\r\n\r\nHowever, `dot` works in such a setup, so it would be preferable if it was used as the backend instead of tk. Also, it could be used to implement `_repr_svg_()` on `Tree`, which would also be a very welcome addition, as svg is a much better format for these kinds of things than png."}
{"number": 1885, "owner": "DavidNemeskey", "title": "AlignedSent display broken", "body": "If there is a source word that doesn't appear in the target sentence (or vice versa), `AlignedSent` cannot be displayed at all:\r\n- `_repr_svg_` throws `TypeError: list indices must be integers or slices, not NoneType`\r\n- `__str__` throws `TypeError: %d format: a number is required, not NoneType`"}
{"number": 1884, "owner": "DavidNemeskey", "title": "chomsky_normal_form() for grammars", "body": "`nltk.tree.Tree` has a `chomsky_normal_form()` function, but grammars don't. Since CNF is a form of the **grammar**, it should, also."}
{"number": 1883, "owner": "DavidNemeskey", "title": "Tree.chomsky_normal_form() throws AttributeError", "body": "More specifically, `AttributeError: 'str' object has no attribute 'label'`. This happens when the grammar doesn't have preterminals, e.g.\r\n```Python\r\nnltk.CFG.fromstring(\"\"\"\r\nS -> '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'\r\nS -> S '+' S\r\nS -> S '-' S\r\nS -> S '*' S\r\nS -> S '/' S\r\nS -> '(' S ')'\r\n\"\"\")\r\n```"}
{"number": 1878, "owner": "mariehane", "title": "Identifying chemical entities with OSCAR4", "body": "I'm interested in hearing whether this is something you would be interested in adding.\r\n\r\n[OSCAR4](https://bitbucket.org/wwmm/oscar4/wiki/Home) is an open source software toolkit for identifying chemical entities in a text. It also includes a tokeniser. \r\nThe accompanying publication is here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3205045."}
{"number": 1876, "owner": "tomirio619", "title": "POS tagging after Tokenization using CoreNLP classes", "body": "Hello,\r\n\r\nI want to use the `CoreNLPTagger` to tokenize and POS-tag a big corpus.\r\nHowever, there is no option to specify additional properties to the `raw_tag_sents` method in the `CoreNLPTagger` (in contrary to the `tokenize` method in `CoreNLPTokenizer`, which lets you specify additional properties). Therefore I'm not able to tell the tokenizer to e.g. not normalize the brackets and other stuff.\r\n\r\nFor example, I want to use the following tokenization options:\r\n```python\r\nadditional_properties = {\r\n            'tokenize.options': 'ptb3Escaping=false, unicodeQuotes=true, splitHyphenated=true, normalizeParentheses=false, normalizeOtherBrackets=false',\r\n            'annotators': 'tokenize, ssplit, pos'\r\n        }\r\n```\r\n\r\nUsing the tokenizer before the tagger does also not work, as this will revert any of the additional options you set in the tokenizer.\r\n\r\nI may be doing something wrong here, but I hope you can help me.\r\n\r\nThanks\r\n\r\n"}
{"number": 1875, "owner": "dimazest", "title": "CoreNLP documentation", "body": "Since the CoreNLP client code is more or less stable is actually being tried every now and then it's time to write detailed documentation!\r\n\r\nRelevant issues:\r\n * #1510\r\n * https://github.com/nltk/nltk/issues/1759#issuecomment-340853721\r\n * #1758\r\n * #1876\r\n * https://github.com/nltk/nltk/issues/1876#issuecomment-343877499\r\n\r\nWould it worth extending the NLTK book to include an explanation how to use CoreNLP tools."}
{"number": 1872, "owner": "stevenbird", "title": "Renovating CorpusReader and CorpusView", "body": "@rmalouf has renovated NLTK's CorpusView class in https://github.com/nltk/nltk/pull/1867, which is pulled into the https://github.com/nltk/nltk/tree/new-corpus-view branch.\r\n\r\nI'm opening this issue to host feedback.\r\n"}
{"number": 1860, "owner": "k0pernicus", "title": "[Question] Add custom Regex to improve date recognition for tokenizer and POS tagger", "body": "I am trying to recognizing a simple kind of date (\"XX/XX/XXXX\") in my Python3 code.\r\nFor that, I would like to create a regex, and to add a tag for this one: \"DATE\" (for example). \r\n\r\nThis is the code I wrote:\r\n```\r\n# Recognize month/day/year\r\npatterns = [(r'\\d{2}/\\d{2}/\\d{4}', 'DATE')]\r\n# Build a tagger\r\nreg_tagger = nltk.RegexpTagger(patterns)\r\ndefault_tagger = nltk.data.load(\"taggers/maxent_treebank_pos_tagger/english.pickle\")\r\n# Build a tagger that add reg_tagger in an existing tagger (MaxEnt)\r\ntagger = nltk.UnigramTagger(model=reg_tagger, backoff=default_tagger)\r\n# Tag the words in each sentence\r\ntags = [tagger.tag(_s) for _s in sentences]\r\n```\r\n\r\nUnfortunately, I got this error:\r\n```\r\n>>> python3.6 scripts.py examples/090003ea802cef84.txt\r\nTraceback (most recent call last):\r\n  File \"scripts.py\", line 202, in <module>\r\n    tags = get_tags_from_sentences(sentences)\r\n  File \"scripts.py\", line 50, in get_tags_from_sentences\r\n    tags = [tagger.tag(_s) for _s in sentences]\r\n  File \"scripts.py\", line 50, in <listcomp>\r\n    tags = [tagger.tag(_s) for _s in sentences]\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py\", line 63, in tag\r\n    tags.append(self.tag_one(tokens, i, tags))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py\", line 83, in tag_one\r\n    tag = tagger.choose_tag(tokens, index, history)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py\", line 142, in choose_tag\r\n    return self._context_to_tag.get(context)\r\nAttributeError: 'RegexpTagger' object has no attribute 'get'\r\n```\r\n\r\nSo, my question is: is it possible to add a custom regex (for my case, to recognize dates) in a default tagger, please?\r\n\r\nThanks a lot"}
{"number": 1859, "owner": "chaseireland", "title": "HiddenMarkovModelTrainer train_unsupervised - TypeError", "body": "Hello, \r\n\r\nI'm trying to use the `train_unsupervised` method from the `nltk.hmm` module, however, it's throwing the following TypeError:\r\n`TypeError: Can't instantiate abstract class RandomProbDist with abstract methods max`\r\n\r\nHere is the code I'm using:\r\n```\r\nimport nltk\r\n\r\ntrainer = nltk.hmm.HiddenMarkovModelTrainer()\r\ntagger = trainer.train_unsupervised([\r\n    ('a', ''),\r\n    ('b', ''),\r\n    ('c', '')\r\n])\r\n```\r\n\r\nHere's the full error:\r\n`Traceback (most recent call last):\r\n  File \"test.py\", line 10, in <module>\r\n    ('c', '')\r\n  File \"/Library/Python/2.7/site-packages/nltk/tag/hmm.py\", line 932, in train_unsupervised\r\n    priors = RandomProbDist(self._states)\r\nTypeError: Can't instantiate abstract class RandomProbDist with abstract methods max`\r\n\r\nNLTK version 3.2.5; tested on Python 2.7.10 and Python 2.7.14.\r\n\r\nAm I doing something silly wrong? \r\n\r\nThank you very much."}
{"number": 1853, "owner": "jacksonllee", "title": "Test failures in wordnet.doctest", "body": "From the latest https://github.com/nltk/nltk/commit/0477ceb2a3ab9f4c0abc0f800a827c92e8d48623 of the `develop` branch, I ran tests locally and got three failures from `wordnet.doctest`:\r\n\r\n```\r\n1) FAIL: Doctest: wordnet.doctest\r\n----------------------------------------------------------------------\r\n   Traceback (most recent call last):\r\n    /usr/lib/python3.5/doctest.py line 2190 in runTest\r\n      raise self.failureException(self.format_failure(new.getvalue()))\r\n   AssertionError: Failed doctest test for wordnet.doctest\r\n     File \"<path-to-nltk>/nltk/nltk/test/wordnet.doctest\", line 0\r\n   \r\n   ----------------------------------------------------------------------\r\n   File \"<path-to-nltk>/nltk/nltk/test/wordnet.doctest\", line 50, in wordnet.doctest\r\n   Failed example:\r\n       sorted(wn.langs()) # doctest: +NORMALIZE_WHITESPACE\r\n   Expected:\r\n       ['als', 'arb', 'bul', 'cat', 'cmn', 'dan', 'ell', 'eng', 'eus', 'fas', 'fin', 'fra', 'glg', 'heb', 'hrv', 'ind', \r\n       'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'qcn', 'slv', 'spa', 'swe', 'tha', 'zsm']\r\n   Got:\r\n       ['als', 'arb', 'cat', 'cmn', 'dan', 'eng', 'eng', 'eus', 'fas', 'fin', 'fra', 'fre', 'glg', 'heb', 'ind', 'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'spa', 'tha', 'zsm']\r\n   ----------------------------------------------------------------------\r\n   File \"<path-to-nltk>/nltk/nltk/test/wordnet.doctest\", line 64, in wordnet.doctest\r\n   Failed example:\r\n       wn.lemmas('cane', lang='ita') # doctest: +NORMALIZE_WHITESPACE\r\n   Expected:\r\n       [Lemma('dog.n.01.cane'), Lemma('cramp.n.02.cane'), Lemma('hammer.n.01.cane'), Lemma('bad_person.n.01.cane'), \r\n       Lemma('incompetent.n.01.cane')]\r\n   Got:\r\n       [Lemma('dog.n.01.cane'), Lemma('hammer.n.01.cane'), Lemma('cramp.n.02.cane'), Lemma('bad_person.n.01.cane'), Lemma('incompetent.n.01.cane')]\r\n   ----------------------------------------------------------------------\r\n   File \"<path-to-nltk>/nltk/nltk/test/wordnet.doctest\", line 79, in wordnet.doctest\r\n   Failed example:\r\n       len(wordnet.all_lemma_names(pos='n', lang='jpn'))\r\n   Expected:\r\n       64797\r\n   Got:\r\n       66027\r\n```\r\n\r\nThe exact same three failures were observed with `tox -e py27` and `tox -e py35` (shown above is from py35).\r\n\r\nTwo peculiarities:\r\n1. Inconsistency between nltk/nltk and nltk/nltk.github.com: The third failure for `len(wordnet.all_lemma_names(pos='n', lang='jpn'))` shows that [`wordnet.doctest` says 64797](https://github.com/nltk/nltk/blob/0477ceb2a3ab9f4c0abc0f800a827c92e8d48623/nltk/test/wordnet.doctest#L80), but the test got 66027, which is exactly what the NLTK how-to page says from [the underlying HTML](https://github.com/nltk/nltk.github.com/blob/fbd8429d14f9e681b0877c7a3d64e31d6144367b/howto/wordnet.html#L423). This makes me think there's reason to believe what I've observed is real, but...\r\n2. Curiously, these test failures are _not_ observed on the Jenkin CI builds, e.g., in [this recent run](https://nltk.ci.cloudbees.com/job/pull_request_tests/442/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/console) associated with #1848 that I'm working on.\r\n\r\nBecause of these, I've hesitated to submit a PR to fix what seems to be straightforward, in case there's more story behind and I've missed something."}
{"number": 1851, "owner": "alvations", "title": "Missing corpus reader for PIL", "body": "The PIL data was added quite some time ago from https://github.com/nltk/nltk/commit/ef144a94479de5536fc860c00f4f14e0a62787e4 but there's no corpus reader for it. \r\n\r\nReported on https://stackoverflow.com/q/46650761/610569 \r\n\r\nSimply using the XMLCorpusReader is not enough too:\r\n\r\n```\r\n>>> from nltk.corpus.reader import XMLCorpusReader\r\n>>> from nltk.corpus.util import LazyCorpusLoader\r\n>>> LazyCorpusLoader('pil',  XMLCorpusReader, r'(?!\\.).*\\.sgml')\r\n<XMLCorpusReader in '.../corpora/pil' (not loaded yet)>\r\n\r\n>>> pil.raw()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py\", line 84, in raw\r\n    return concat([self.open(f).read() for f in fileids])\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py\", line 84, in <listcomp>\r\n    return concat([self.open(f).read() for f in fileids])\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/data.py\", line 1110, in read\r\n    chars = self._read(size)\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/data.py\", line 1380, in _read\r\n    chars, bytes_decoded = self._incr_decode(bytes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/data.py\", line 1411, in _incr_decode\r\n    return self.decode(bytes, 'strict')\r\n  File \"/usr/lib/python3.5/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 9209: invalid start byte\r\n\r\n>>> pil.words()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py\", line 66, in words\r\n    elt = self.xml(fileid)\r\n  File \"/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py\", line 47, in xml\r\n    raise TypeError('Expected a single file identifier string')\r\nTypeError: Expected a single file identifier string\r\n\r\n```\r\n"}
{"number": 1846, "owner": "alvations", "title": "Deep Learning + NLTK", "body": "This issue to start a discussion on how we can provide deep learning features to NLTK. The topics can be and are not limited to:\r\n\r\n - What kind of DL features do we want to see in NLTK?\r\n - Would adding DL features add value to the exist ecology of Python NLP OSS and NLTK users? \r\n - What level of education vs practicality should we balance for DL features in NLTK?\r\n\r\nPlease feel free to discuss on this issue!\r\n"}
{"number": 1824, "owner": "xrtang", "title": "punkt.PunktSentenceTokenizer() for Chinese", "body": "I use the following code to train punkt for Chinese, but it doesn't produce desired result:\r\n\r\n```python\r\ninput_str_cn = \"\u53f0\u6e7e\u4e4b\u6240\u4ee5\u51fa\u73b0\u8fd9\u79cd\u5371\u673a\uff0c\u662f\u53f0\u6e7e\u4e0d\u4f46\u957f\u5e74\u4f4e\u85aa\uff0c\u4e14\u4e0d\u77e5\u8fdc\u666f\u5728\u54ea\u91cc\u300220\u4e16\u7eaa90\u5e74\u4ee3\uff0c\u53f0\u6e7e\u7684\u5927\u5b66\u6bd5\u4e1a\u751f\u8d77\u85aa\u4e0d\u5230\u65b0\u53f0\u5e013\u4e07\u5143\uff08\u7ea6\u5408\u4eba\u6c11\u5e016594\u5143\uff09\uff0c\u5230\u4e86\u4eca\u5929\uff0c\u4f9d\u7136\u5982\u6b64\u3002\"\r\n\r\n# import punkt\r\nimport nltk.tokenize.punkt\r\n\r\n# Make a new Tokenizer\r\ntokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\r\n\r\n# Read in training corpus \r\nimport codecs\r\n\r\ntrain_file = \"D:/CL/comp_ling/data/dushu_1999_2008/1999.txt\"\r\ntext = codecs.open(train_file, \"r\", \"gb18030\").read()\r\n\r\n# Train tokenizer\r\ntokenizer.sent_end_chars = ('\uff01','\uff1f','\u3002','\u201d')\r\nfor sent_end in tokenizer.sent_end_chars:\r\n    print sent_end\r\ntokenizer.train(text)\r\n\r\n# Dump pickled tokenizer\r\nimport pickle\r\nout = open(\"chinese.pickle\",\"wb\")\r\npickle.dump(tokenizer, out)\r\nout.close()\r\n\r\n# To use the tokenizer\r\nwith open(\"chinese.pickle\") as infile:\r\n    tokenizer_new = pickle.load(infile)\r\nsents = tokenizer_new.tokenize(input_str_cn)\r\nfor s in sents:\r\n    print s\r\n```\r\n\r\nThe produced result is as follows:\r\n\r\n\r\n> \"\u53f0\u6e7e\u4e4b\u6240\u4ee5\u51fa\u73b0\u8fd9\u79cd\u5371\u673a\uff0c\u662f\u53f0\u6e7e\u4e0d\u4f46\u957f\u5e74\u4f4e\u85aa\uff0c\u4e14\u4e0d\u77e5\u8fdc\u666f\u5728\u54ea\u91cc\u300220\u4e16\u7eaa90\u5e74\u4ee3\uff0c\u53f0\u6e7e\u7684\u5927\u5b66\u6bd5\u4e1a\u751f\u8d77\u85aa\u4e0d\u5230\u65b0\u53f0\u5e013\u4e07\u5143\uff08\u7ea6\u5408\u4eba\u6c11\u5e016594\u5143\uff09\uff0c\u5230\u4e86\u4eca\u5929\uff0c\u4f9d\u7136\u5982\u6b64\u3002\"\r\n\r\n\r\nIt seems that the `sent_end_chars` does not work here. I have checked the encoding. There's no problem with that. Could anyone help with it? Thanks."}
{"number": 1822, "owner": "michalstepniewski", "title": "problem with discourse tester with maltParser and RegexpTagger", "body": "When trying to apply discourse tester as in:\r\nhttp://www.nltk.org/book/ch10.html (5.2)\r\n\r\nI ran into a problem:\r\n\r\n>>> rc = DrtGlueReadingCommand(depparser=MaltParser(tagger=tagger))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: __init__() takes at least 2 arguments (2 given)\r\n>>> \r\n"}
{"number": 1819, "owner": "wangjack1221", "title": "Bug in nltk.tokenize.texttiling", "body": "In the source code `class TextTilingTokenizer`, the bug maybe exist during filting stopword. \r\n\r\n`ts.wrdindex_list = [wi for wi in ts.wrdindex_list\r\n                                if wi[0] not in self.stopwords]`\r\nAs shown above, this code intends to filter stopword from ts.wrdindex_list ,but ...wi[0] is the combination of the word and pos, not the word itself. So I'm afraid this code is invalid and can't remove any stopword from ts.wrdindex_list.\r\n\r\nPlease verify this bug...Thanks : )"}
{"number": 1800, "owner": "justinkterry", "title": "One Letter Typo in English Stoplist", "body": "The last element returned by \r\n```\r\nfrom nltk.corpus import stopwords\r\nstopwords.words('english')\r\n```\r\nis 'wouldn' when I believe it should be wouldn't"}
{"number": 1799, "owner": "alvations", "title": "TweetTokenizer quirk with long numbers", "body": "When there's a number token that's >8 length, the `TweetTokenizer` tries to iteratively parse the first 8-10 digits as a phone number if it matches the phone number regex on https://github.com/alvations/nltk/blob/develop/nltk/tokenize/casual.py#L125\r\n\r\nMaybe, there should be an option to allow users to keep long digits and not try to recognize the first part as a number. \r\n\r\nOr better yet, if there's regex fix that can prevent the number from getting split if it's longer than 10. \r\n\r\nDetails on https://stackoverflow.com/questions/45425946/tokenize-in-nltk-tweettokenizer-returning-integers-by-splitting/45431922#45431922"}
{"number": 1786, "owner": "norberte", "title": "'STANFORD_CORENLP' environment variable showing up twice ", "body": "Hello there,\r\nI am not sure if it was intended to be developed this way, but this is how nltk/nltk/parse/stanford.py looks like now:\r\n\r\n`stanford_jar = max(\r\n            find_jar_iter(\r\n                self._JAR, path_to_jar,\r\n                env_vars=('STANFORD_PARSER', 'STANFORD_CORENLP'),\r\n                searchpath=(), url=_stanford_url,\r\n                verbose=verbose, is_regex=True\r\n            ),\r\n            key=lambda model_path: os.path.dirname(model_path)\r\n        )\r\n\r\n        model_jar=max(\r\n            find_jar_iter(\r\n                self._MODEL_JAR_PATTERN, path_to_models_jar,\r\n                env_vars=('STANFORD_MODELS', 'STANFORD_CORENLP'),\r\n                searchpath=(), url=_stanford_url,\r\n                verbose=verbose, is_regex=True\r\n            ),\r\n            key=lambda model_path: os.path.dirname(model_path)\r\n        )\r\n`\r\n\r\n Notice how 'STANFORD_CORENLP' shows up twice as 2 paths for 2 different purposes. That is line 48 and 58 in the original file.\r\nIs this an issue ?  'STANFORD_CORENLP' referring to stanford-corenlp.jar and stanford-corenlp-models.jar ?"}
{"number": 1785, "owner": "alvations", "title": "Fringe cases in MT eval metrics", "body": "There are several fringe that still bugs the MT evaluation metrics in `nltk.translate`. \r\n\r\nThe BLEU related issues are mostly resolved in #1330. But similar issues happens in RIBES and CHRF too:\r\n\r\n - `ribes_score.py`\r\n   - https://github.com/nltk/nltk/blob/develop/nltk/translate/ribes_score.py#L290 and https://github.com/nltk/nltk/blob/develop/nltk/translate/ribes_score.py#L320 are subjected to ZeroDivisionError when the no. of possible ngram pairs is 0\r\n\r\n - `chrf_score.py`\r\n   - The interface for the references for other scores supports multi-reference by default while ChrF score supports single reference. It should be standardize to accommodate multi-reference\r\n      - But in the case of multi-reference score, there's no indication of which reference to choose in ChrF, we might need to contact the author to understand how to handle this.\r\n"}
{"number": 1783, "owner": "alvations", "title": "Documentation of how and what of NLTK pre-trained models", "body": "There are several pre-trained models that NLTK provides and it is unclear\r\n \r\n - what the models are trained on \r\n - how the models are trained\r\n\r\nThese pre-trained models include:\r\n\r\n - `sent_tokenize`: Punkt sentence tokenizers trained on _____ using  `nltk.tokenize.punkt.PunktSentenceTokenizer` with ____ settings/parameters \r\n\r\n - `pos_tag`: @honnibal 's perceptron POS tagger trained on ____  using `nltk.tag.perceptron.PerceptronTagger` with ____ settings/parameters \r\n\r\n - `ne_tag`: Named entity tagger trained on ____ (is it ACE? If so, which ACE?) using `nltk.classify.maxent.MaxentClassifier` with ____ settings/parameters?\r\n\r\nIt would be great if anyone knows about the ____ information above and help answer this issue. And it'll be awesome if it gets documented somewhere so that we avoid another wave of https://news.ycombinator.com/item?id=10173669 and https://explosion.ai/blog/dead-code-should-be-buried ;P"}
{"number": 1778, "owner": "alvations", "title": "PorterStemmer seems to be stemming \"this\" -> \"thi\"", "body": "I'm not sure whether it's the expected output but NLTK PorterStemmer is giving different output as compared to https://pypi.python.org/pypi/stemming/1.0\r\n\r\nFrom NLTK:\r\n\r\n```python\r\n>>> from nltk.stem import PorterStemmer\r\n>>> porter = PorterStemmer()\r\n>>> porter.stem('this')\r\nu'thi'\r\n```\r\n\r\nFrom `stemming`\r\n\r\n```python\r\n>>> from stemming.porter2 import stem\r\n>>> stem('this') \r\n'this'\r\n```"}
{"number": 1776, "owner": "alvations", "title": "Fixing LGTM alerts ", "body": "#1775 raised a PR from [188 alerts from lgtm.com](https://lgtm.com/projects/g/nltk/nltk/alerts/)\r\n\r\nPossibly, it'll be a good thing to look into each one of them. "}
{"number": 1772, "owner": "azarezade", "title": "Adding Persian language support", "body": "I wonder if there is any Persian NLP tool available in the nltk.\r\nSpecifically, I want Persian semantic parser."}
{"number": 1767, "owner": "hyzhak", "title": "Got error `TypeError: not all arguments converted during string formatting` for tree.pretty_print()", "body": "# Env\r\n- Python 3.6.1 |Anaconda 4.4.0 (64-bit) (official docker from continuumio/anaconda3:4.4.0 https://github.com/ContinuumIO/docker-images)\r\n- Jupyter and IPython 5.3.0\r\n- nltk=3.2.3\r\n\r\n# Example:\r\n```python\r\nimport nltk\r\nchunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\r\nchunkParser = nltk.RegexpParser(chunkGram)\r\ntagged = [('Tonight', 'NN'), \r\n          ('we', 'PRP'), \r\n          ('are', 'VBP'), \r\n          ('comforted', 'VBN'), \r\n          ('by', 'IN'), \r\n          ('the', 'DT'), \r\n          ('hope', 'NN'), \r\n          ('of', 'IN'), \r\n          ('a', 'DT'), \r\n          ('glad', 'JJ'), \r\n          ('reunion', 'NN'), \r\n          ('with', 'IN'), \r\n          ('the', 'DT'), \r\n          ('husband', 'NN'), \r\n          ('who', 'WP'), \r\n          ('was', 'VBD'), \r\n          ('taken', 'VBN'), \r\n          ('so', 'RB'), \r\n          ('long', 'RB'), \r\n          ('ago', 'RB'), \r\n          (',', ','), \r\n          ('and', 'CC'), \r\n          ('we', 'PRP'), \r\n          ('are', 'VBP'), \r\n          ('grateful', 'JJ'), \r\n          ('for', 'IN'), \r\n          ('the', 'DT'), \r\n          ('good', 'JJ'), \r\n          ('life', 'NN'), \r\n          ('of', 'IN'), \r\n          ('Coretta', 'NNP'), \r\n          ('Scott', 'NNP'), \r\n          ('King', 'NNP'), \r\n          ('.', '.')]\r\nchunked = chunkParser.parse(tagged)\r\nchunked.pprint()\r\nchunked.pretty_print()\r\n```\r\n\r\n# Result\r\n\r\n```\r\n(S\r\n  Tonight/NN\r\n  we/PRP\r\n  are/VBP\r\n  comforted/VBN\r\n  by/IN\r\n  the/DT\r\n  hope/NN\r\n  of/IN\r\n  a/DT\r\n  glad/JJ\r\n  reunion/NN\r\n  with/IN\r\n  the/DT\r\n  husband/NN\r\n  who/WP\r\n  was/VBD\r\n  taken/VBN\r\n  so/RB\r\n  long/RB\r\n  ago/RB\r\n  ,/,\r\n  and/CC\r\n  we/PRP\r\n  are/VBP\r\n  grateful/JJ\r\n  for/IN\r\n  the/DT\r\n  good/JJ\r\n  life/NN\r\n  of/IN\r\n  (Chunk Coretta/NNP Scott/NNP King/NNP)\r\n  ./.)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-286ecc5b3fd6> in <module>()\r\n     38 chunked = chunkParser.parse(tagged)\r\n     39 chunked.pprint()\r\n---> 40 chunked.pretty_print()\r\n\r\n/opt/conda/lib/python3.6/site-packages/nltk/tree.py in pretty_print(self, sentence, highlight, stream, **kwargs)\r\n    697         \"\"\"\r\n    698         from nltk.treeprettyprinter import TreePrettyPrinter\r\n--> 699         print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),\r\n    700               file=stream)\r\n    701 \r\n\r\n/opt/conda/lib/python3.6/site-packages/nltk/treeprettyprinter.py in __init__(self, tree, sentence, highlight)\r\n     95                             if not isinstance(b, Tree):\r\n     96                                 a[n] = len(sentence)\r\n---> 97                                 sentence.append('%s' % b)\r\n     98         self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(\r\n     99                 tree, sentence, highlight)\r\n\r\nTypeError: not all arguments converted during string formatting\r\n```"}
{"number": 1765, "owner": "hyzhak", "title": "How could I inline nltk graph to jupyter notebook?", "body": "I have already asked this question on [stackoverflow](https://stackoverflow.com/questions/44880337/use-tkinter-for-nltk-draw-inside-of-jupyter-notebook) without any luck and decide to duplicate it here.\r\n\r\nAccording to [the sources](https://github.com/nltk/nltk/blob/develop/nltk/draw/tree.py#L856) of `nltk` it draws graph by `tkinter (GUI)` but I need to inline this graph to `jupyter notebook`. And I'm trying to do it inside of official docker from [anaconda3](https://github.com/ContinuumIO/docker-images) in other words I don't need any popup GUI here but just image inside of notebook, that should be render on server side by nltk lib.\r\n\r\nHow could I overcome this by nltk? Maybe there is third party libs which could help there? \r\n\r\nSources of my try is [here](https://github.com/hyzhak/nltk-experiments/blob/master/main.ipynb) - the last 18th cell.\r\n\r\n```python\r\nchunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\r\nchunkParser = nltk.RegexpParser(chunkGram)\r\n\r\nfor i in tokenized_text[:5]:\r\n    words = nltk.word_tokenize(i)\r\n    tagged = nltk.pos_tag(words)\r\n    chunked = chunkParser.parse(tagged)\r\n    chunked.draw()\r\n```\r\n\r\nPS:\r\nin the same time matplotlib inline by itself works like a charm. Could I use matplotlib for graph rendering?\r\n\r\nThanks!"}
{"number": 1758, "owner": "f0lie", "title": "Error 403 when connecting to corenlp hosted locally", "body": "Hello,\r\n\r\nWhen I started up a server via my terminal and tried to connect to it with a python client, I got 403 errors. The odd part was that my browser could connect to the server without problems.\r\n\r\nI found out that my issue was in my proxy. When my client tried to connect to connect to 'http://localhost:9000' it passed the url to my work's proxy. The proxy didn't recognize url and returned a 403 error.\r\n\r\nThis issue didn't affect my browser because my browser was using a pac file which excluded localhost from the proxy. Since I was connecting directly to the proxy via environment variables, my terminal was running into issues.\r\n\r\nThe solution was to set `trust_env` to False. This makes the client connect without using proxy. The client would refuse to import any environment variables.\r\n\r\n```\r\nnlp = nltk.parse.corenlp.CoreNLPParser(\"http://localhost:9000\")\r\nnlp.session.trust_env = False\r\n```\r\nI am guessing most people setup their corenlp server locally. Maybe there could be some sort of warning. The class could check if there is a proxy url and if the session is pointed to a local host, then warn users that issues may happen. Maybe a more isolated way is to catch 403 exceptions, check the proxy url and localhost, and then raise a exception with advice.\r\n\r\nI managed to fix the issue but I am posting an issue to help others in the same boat.\r\n\r\nThanks,\r\n"}
{"number": 1756, "owner": "oxinabox", "title": "Fulfilling the requirements of the Treebank Tokenizer License", "body": "I am concerned about the Treebank Tokenizer's use, by its License terms.\r\n\r\n\r\nThe source for the [tokeniser (via waybackmachine)](https://web.archive.org/web/20130804202913/http://www.cis.upenn.edu/~treebank/tokenizer.sed) includes the text:\r\n\r\n> by Robert MacIntyre, University of Pennsylvania, late 1995.  \r\n> If this wasn't such a trivial program, I'd include all that stuff about\r\n no warrantee, free use, etc. from the GNU General Public License.  If you\r\n want to be picky, assume that all of its terms apply.  Okay?\r\n\r\nThis is really problematic, AFAICT.\r\nIt is an awful crayon license, \r\nneither its meaning nor its precise intent is clear.\r\nUnder a literal interpretation, one is most restricted when not being picky,\r\nand still very restricted if not being picky\r\n\r\nI asked about it on [OpenSource StackExchange](https://opensource.stackexchange.com/q/5612/934)\r\n\r\n> As I see it, one has two choices:\r\n>  - **Do not be picky:** then this has no license and so can not be incorporated into any open source project. All rights remain with Robert MacIntyre.\r\n>  - **Be picky:** this is *effectively* under *some* version of the GPL, it can be incorporated only into things that can incorporate GPL works. Since this work is from 1995, it means it is either GPL 1, or GPL 2, is intended.\r\n\r\n\r\n\r\n\r\nAs far as I can tell, that reasoning is correct, and applies to NLTK's usage.\r\nAs NLTK is Apache 2 Licensed, incorporating GPL works is not (normally considered) possible. \r\n\r\n\r\n\r\nI can't find any further reference to the tokenizer or to Robert MacIntyre. [LICENSE](https://github.com/nltk/nltk/blob/develop/LICENSE.txt)  nor in [AUTHORS.md](https://github.com/nltk/nltk/blob/develop/AUTHORS.md).\r\n\r\n\r\n"}
{"number": 1750, "owner": "albertauyeung", "title": "span_tokenize failed when sentence contains double quotation", "body": "If we feed in a sentence with double quotation into TreebankWordTokenizer's span_tokenize function, there will be errors. Probably this is because the function sends the raw string input along with the tokenized string to the align_tokens function, without considering that the tokenize function would replace double quotation marks with something else."}
{"number": 1748, "owner": "parikhabhi007", "title": "Incorrect Part of Speech classification", "body": "I was using NLTK library for one of our NLP based project and where I needed the Part-of-Speech tag of the sentences, string and tokens. So, before using the NLTK library, I went on to understand it's working before implementing it and faced the following error or bug: \r\n\r\n- The pos tag for \"Indemnify\" was recognized correctly as \"VB\" (Verb).\r\n- But, the pos tag for \"indemnify\" (here, the first letter is in small-case) was recognized incorrectly as \"NN\" (Noun). \r\n\r\nPlease find the attached piece of code stating the same. Is it a fault from our end or a bug in the library? I would really like to solve the issue and use the library in my project. \r\n\r\n![selection_125](https://user-images.githubusercontent.com/24873215/26872764-52dc3770-4b95-11e7-87b7-71ed782ada46.png)\r\n"}
{"number": 1745, "owner": "nschneid", "title": "FrameNet corpus reader: rewrite howto", "body": "[This page](http://www.nltk.org/howto/framenet.html) is out of date. Rewrite borrowing examples from https://arxiv.org/abs/1703.07438."}
{"number": 1740, "owner": "Bsmil3y", "title": "Errors in the Norwegian tokenizer", "body": "Hi!\r\n\r\nWhile working on my master in language technology I discovered some errors in the Norwegian tokenizers:\r\n\r\nnltk.word_tokenize(\u201cHello NLTK\u201d, \u201cnorwegian\u201d)\r\nnltk.sent_tokenize(\u201cMy name. Is bob.\u201d, \u201cnorwegian\u201d)\r\n\r\nWhen these errors are fixed the sentence tokenizer fail 7-8 times less, while the word tokenizer will fail 6-7 times less (for Norwegian). I was wondering if this is something I could integrate with NLTK? If yes, how would you want it integrated?\r\n\r\nFor instance, I could place the patch file in the tokenize folder and call it like this in the tokenize init file: \r\n```\r\nimport patch\r\n\r\ndef sent_tokenize(text, language='english'):\r\n    if language='norwegian':\r\n        tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\r\n        return patch.sent_patch(tokenizer.tokenize(text))\r\n    else:\r\n        tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\r\n        return tokenizer.tokenize(text)\r\n```\r\nHowever I realize this might become messy if every language wanted a slot. Any other questions, I can elaborate, please ask ^.^\r\n\r\n:bob\r\n"}
{"number": 1728, "owner": "Sofwath", "title": "Add new language (dv-MV)", "body": "I would like to request to add Dhivehi (dv-MV) spoken in Maldives to NLTK"}
{"number": 1724, "owner": "alvations", "title": "Better WordNet Reader", "body": "This is a proposal to improve the WordNet interface in terms of code quality, functionalities and speed. Please feel free to add to this issues about current wordnet reader in `nltk` and how and what should be improved. \r\n\r\n-----\r\n\r\nSuggestions / Issues\r\n====\r\n\r\n- Are there still issues with the [`_morphy()` function](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68)?  c.f. http://stackoverflow.com/questions/33594721/why-nltk-lemmatization-has-wrong-output-even-if-verb-exc-has-added-right-value\r\n\r\n- What is the best params settings for `lowest_common_subsumers` in `wup_similarity`? See https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L899 \r\n\r\n- Should the list of exceptions in the Princeton WordNet be extended? c.f. http://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet \r\n\r\n- [Add in the option to manually add a new root node; this will be useful for verb similarity as there exist multiple verb taxonomies](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1946)\r\n\r\n- Allow direct offset + pos query, i.e. `wn.synset('dog.n.01') == wn.synset('2084071-n')`.\r\n\r\n- Pre-compute similarities for all Synsets. \r\n  - We know that the Information Content (IC) based similarities scores should have been frozen by now since any new WordNet updates/extensions won't affect it so it's possible to precompute these and save them. That'll speed up the similarity matching quite drastically. \r\n  - As for the path related similarities, they would change with WordNet versions but its nice to still have them pre-computed respective to the WordNet versions from 3.0 onwards.\r\n\r\n\r\n\r\nCode \r\n====\r\n\r\n- Would `float('inf')` be a better approach than `_INF = 1e300` at https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68 ? \r\n\r\n- Should encoding be a \"set-able\" parameter for the WordNetCorpusReader? https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1052  \r\n\r\n- Should imports be at the top of the script unless it's an `@abstractmethod` accessible from outside of the `CorpusReader` object? e.g.   \r\n  - At `langs()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1145\r\n  - At `closure()`  https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L537\r\n\r\n- Proper indentation should be preferred for `try-except`/`if-else` and one-liner shortcuts, e.g. https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1165\r\n\r\n- `get_version()` should be called once at initialization and not be repeated called at `get_root()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L423\r\n"}
{"number": 1708, "owner": "Chris00", "title": "Megam URL incorrect", "body": "The URL given [in the wiki for Megam](https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#megam-mega-model-optimization-package) is incorrect, the right URL is http://hal3.name/megam/"}
{"number": 1706, "owner": "gening", "title": "Tree.fromstring(): expected u'end-of-string' but got ')' when a node is ')'", "body": "Issue:\r\n>>> s = '(S \\\\))'\r\n>>> Tree.fromstring(s)\r\nValueError: Tree.read(): expected u'end-of-string' but got ')' at index 4.\r\n\"(S \\\\))\"\r\n........^\r\n\r\n>>> s = '(S \\\\()'\r\n>>> Tree.fromstring(s)\r\nValueError: Tree.read(): expected u')' but got u'end-of-string' at index 5.\r\n\"(S \\\\()\"\r\n.......^\r\n\r\nSolution:\r\nEscaping '(' and ')' should be allowed.\r\n"}
{"number": 1700, "owner": "doweaver", "title": "sys.platform.startswith('win') is not a reliable way of detecting the OS", "body": "'ve been doing some work with the NLTK using IronPython and Windows, and discovered we'll often run into trouble with file paths. It appears that we mostly use the following to determine whether we're on Windows:\r\n\r\n`windows = sys.platform.startswith('win')`\r\n\r\nUnfortunately, IronPython has a sys.platform of \"cli\". I haven't been able to dig up historical reasons for this, but I have identified several alternate ways to identify the platform:\r\n\r\n```\r\n>>> os.name\r\n'nt'\r\n>>> platform.system()\r\n'Windows'\r\n```\r\n\r\nBoth of these appear to be more reliable than \"sys.platform\". Does anyone know if there's a particular reason these aren't being used? If there are no objections, I can create a pull request with the changes."}
{"number": 1694, "owner": "chrisspen", "title": "Tree pretty_print() fails on a tree containing features", "body": "If I try to call tree.pretty_print() on a tree containing FeatStructNonterminal() built from a feature-grammar, it throws the exception:\r\n\r\n    Traceback (most recent call last):\r\n      File \"test.py\", line 22, in <module>\r\n        tree.pretty_print()\r\n      File \"myproject/.env/local/lib/python2.7/site-packages/nltk/tree.py\", line 695, in pretty_print\r\n        print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),\r\n      File \"myproject/.env/local/lib/python2.7/site-packages/nltk/treeprettyprinter.py\", line 373, in text\r\n        label = label.split('\\n')\r\n    AttributeError: 'FeatStructNonterminal' object has no attribute 'split'\r\n\r\nI'm using `nltk==3.2.2`."}
{"number": 1689, "owner": "ExplodingCabbage", "title": "wordnet.doctest contains references to bugs by number, but they're not issues from this GitHub repo", "body": "See https://github.com/nltk/nltk/blob/3.2.2/nltk/test/wordnet.doctest. We've got references like:\r\n\r\n> Bug 284: instance hypernyms not used in similarity calculations\r\n\r\nand\r\n\r\n> Issue 541: add domains to wordnet\r\n\r\nThese certainly ain't references to issues opened here at https://github.com/nltk/nltk though; #284 and #541 are both non-WordNet-related issues.\r\n\r\nI'm guessing these issue numbers refer to the issue tracker of some other project hosting platform that historically hosted NLTK - although I have no idea where that was and will need someone familiar with the project's history like @stevenbird to point me in the right direction. If those issues are still visible on the internet, then we should replace the issue numbers with full URLs to the issues; otherwise I'll try to get Wayback Machine links."}
{"number": 1656, "owner": "stevenbird", "title": "New version of scipy causes fisher test to break", "body": "Our [CI Server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/(root)/metrics_doctest/metrics_doctest/) reports this failing test:\r\n\r\n```\r\nFile \"/scratch/jenkins/workspace/nltk/TOXENV/py35-jenkins/jdk/jdk8latestOnlineInstall/nltk/test/metrics.doctest\", line 242, in metrics.doctest\r\nFailed example:\r\n    bam.fisher(20, (42, 20), N) > bam.fisher(20, (41, 27), N)\r\nExpected:\r\n    False\r\nGot:\r\n    True\r\n```\r\n\r\n@pierpaolo points out that this test failure coincided with a new release of scipy. The failure goes away when we modify tox.ini to specify the previous version:\r\n\r\n```\r\npip install -I 'scipy < 0.19'\r\n; pip install -I scipy\r\n```\r\n"}
{"number": 1622, "owner": "prateekkrjain", "title": "ZeroDivisionError: In BigramAssocMeasures", "body": "I got an error \"ZeroDivisionError: float division by zero\". In nltk/metrics/association.py at line number 213 in phi_sq method."}
{"number": 1619, "owner": "ExplodingCabbage", "title": "Update omw data", "body": "It's definitely at least a little out of date (or perhaps was incomplete at the time of being added into NLTK); per http://compling.hss.ntu.edu.sg/omw/, there is a Romanian (language code 'ron') WordNet that is part of OMW but it's not part of NLTK's OMW corpus:\r\n\r\n```\r\n>>> nltk.corpus.wordnet.langs()\r\n['eng', 'als', 'arb', 'bul', 'cat', 'cmn', 'dan', 'ell', 'eus', 'fas', 'fin', 'fra', 'glg', 'heb', 'hrv', 'ind', 'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'qcn', 'slv', 'spa', 'swe', 'tha', 'zsm']\r\n```"}
{"number": 1612, "owner": "mpertierra", "title": "Link to C&C Tools/Boxer is broken", "body": "The link (http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Subversion) provided here (https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#cc-toolsboxer) is broken. Is there anywhere else to download these tools?"}
{"number": 1611, "owner": "ExplodingCabbage", "title": "Support extended open multilingual wordnet", "body": "Currently, NLTK supports the [open multilingual wordnet](http://compling.hss.ntu.edu.sg/omw/) (requires `nltk.download('omw')`). However, it doesn't (unless I'm missing something) yet support the [extended open multilingual wordnet](http://compling.hss.ntu.edu.sg/omw/summx.html)."}
{"number": 1489, "owner": "alvations", "title": "Reimplementing Monolingual Word Aligner in NLTK", "body": "The [Monolingual Word Aligner](https://github.com/ma-sultan/monolingual-word-aligner) is an important tool in the [Semantic Textual Similarity](http://alt.qcri.org/semeval2017/task1/) task.\n\n@amalinovskiy created a fork at https://github.com/amalinovskiy/upf-cobalt but the Stanford parser and JSON RPC is still based on outdated code.\n\n**May I suggest that we try to re-implment it within NLTK?** The version on @ma-sultan repo is rather old and it's based on an older implementation some other Stanford CoreNLP wrapper outside of NLTK and also older NLTK versions for tokenization, etc. \n\nShould this be under `nltk.sem`?\n"}
{"number": 1384, "owner": "brendanpatrickmurphy", "title": "Japanese Corpus readers do not return properly formatted (word, tag) tuples", "body": "There are still problems with how POS Tagging works for these corpora. This afternoon, I loaded up JEITA, and called jeita.tagged_words(). The problem is that the second half of each tuple in JEITA doesn't contain a tag that is easy to test against a POS-tagger. The second half of each tuple contains both orthographic information (each word in the corpus has a spelling for each syllabary in Japanese) and the tag information, so a word tagged as a noun won't have the same tag as another word tagged as a noun. This leads to quite a few problems when testing a tagger against the corpus. \n\nOpen up one of the .chasen files and you'll see what I mean. Here's line 6 of a0010.chasen in the jeita.zip file. \n\n\u51fa\u308b  \u30c7\u30eb  \u51fa\u308b  \u52d5\u8a5e-\u81ea\u7acb   \u4e00\u6bb5  \u57fa\u672c\u5f62\n\nThere's four ( or maybe five) elements here. The first three are ways of writing the word /deru/, and the last is the tag (verb, transitive, group 1, plain form.) \n\nSo I wrote this loop: \n\nfor sent in tagged_sents:\n    for(word, tag) in sent:\n        print(word)\n        print(tag)\n\nAnd here's some sample output: \n\n\u51fa\u308b\n\u30c7\u30eb  \u51fa\u308b  \u52d5\u8a5e-\u81ea\u7acb   \u4e00\u6bb5  \u57fa\u672c\u5f62\n\nAs you can see, the tag includes two forms of orthography, which throws things off.\n\n(Also, as a side note, it would be really great if we could have a \"simple\" pos tag version of these files, which didn't include some of the additional categories like \"plain form\" or which group (ichidan/godan) the verb belonged too, since I don't think a lot of parsers care too much about which is which, but doing this would probably take help from a Japanese fluent individual.) \n\nI can check again with KNBC, the other Japanese corpus included in NLTK, but it does even funkier things with tags last I checked.\n"}
{"number": 1324, "owner": "josepvalls", "title": "ParentedTree breaks deepcopy", "body": "Can deepcopy Tree but not ParentedTree, not sure if may be related to this: https://github.com/nltk/nltk/issues/130\n\n> > > from nltk.tree import Tree,ParentedTree\n> > > t1 = Tree.fromstring(\"(TOP (S (NP (NNP Bell,)) (NP (NP (DT a) (NN company)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN based) (PP (IN in) (NP (NNP LA,)))))))) (VP (VBZ makes) (CC and) (VBZ distributes) (NP (NN computer))) (. products.)))\")\n> > > t2 = copy.deepcopy(t1)\n> > > t3 = ParentedTree.fromstring(\"(TOP (S (NP (NNP Bell,)) (NP (NP (DT a) (NN company)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN based) (PP (IN in) (NP (NNP LA,)))))))) (VP (VBZ makes) (CC and) (VBZ distributes) (NP (NN computer))) (. products.)))\")\n> > > import copy\n> > > t4 = copy.deepcopy(t3)\n> > > Traceback (most recent call last):\n> > >   File \"<input>\", line 1, in <module>\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 190, in deepcopy\n> > >     y = _reconstruct(x, rv, 1, memo)\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 351, in _reconstruct\n> > >     item = deepcopy(item, memo)\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 190, in deepcopy\n> > >     y = _reconstruct(x, rv, 1, memo)\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 351, in _reconstruct\n> > >     item = deepcopy(item, memo)\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 190, in deepcopy\n> > >     y = _reconstruct(x, rv, 1, memo)\n> > >   File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py\", line 352, in _reconstruct\n> > >     y.append(item)\n> > >   File \"/usr/local/lib/python2.7/site-packages/nltk/tree.py\", line 1061, in append\n> > >     self._setparent(child, len(self))\n> > >   File \"/usr/local/lib/python2.7/site-packages/nltk/tree.py\", line 1221, in _setparent\n> > >     raise ValueError('Can not insert a subtree that already '\n> > > ValueError: Can not insert a subtree that already has a parent.\n"}
{"number": 1317, "owner": "alvations", "title": "Training the default perceptron tagger with bigger corpus", "body": "The default pickle for the `PerceptronTagger` is nice but it's a little too small for realistic usage:\n\n``` python\n>>> from nltk import PerceptronTagger\n>>> len(PerceptronTagger(load=True).tagdict)\n1549\n```\n\nWould it be possible to retrain a model on something like the full Penn TreeBank or BNC? Is the model then releasable on NLTK? \n"}
{"number": 1305, "owner": "arendu", "title": "confusing maxent formula.", "body": "I dont understand this formula from  [ nltk.classify.maxent](http://www.nltk.org/_modules/nltk/classify/maxent.html)\n\n> ```\n>                     dotprod(weights, encode(fs,label))\n>   prob(fs|label) = ---------------------------------------------------\n>                  sum(dotprod(weights, encode(fs,l)) for l in labels)\n> ```\n\nIsin't maxent same as logistic regression classifier? So the correct formula for p(fs| label) should be:\n\n> ```\n>    p(fs|label) =         exp( dotprod(weights, encode(fs,label))\n>                          --------------------------------------------------------------\n>                    sum( exp( dotprod(weights, encode(f,label)) for f in all_fs)\n> ```\n\nThere seems to be 2 mistakes in the original formula, is should have been p(label|fs) and the dot product should be exponentiated.\n\nAm I missing something?\n"}
{"number": 1248, "owner": "DuyguA", "title": "Turkish language extension", "body": "Hello all,\nI'm a Turkish NLP developer and I'd like to add Turkish language libraries including a morphological analyzer and a parser. Would anyone be interested?\n"}
{"number": 1214, "owner": "nschneid", "title": "Incorporate more accurate sentence-splitter, tokenizer, and/or lemmatizer for English?", "body": "Among open issues, we have (not an exhaustive list):\n- #135 complains about the sentence tokenizer\n- #1210, #948 complain about word tokenizer behavior\n- #78 asks for the tokenizer to provide offsets to the original string\n- #742 raises some of the foibles of the WordNet lemmatizer. #1196 discusses some counterintuitive behavior and how it might be fixed if POS tags with tense and number were provided to disambiguate.\n\nI'm not an expert on these tasks but I know that [Rebecca Dridan](https://scholar.google.com/citations?user=sAHhxYoAAAAJ&hl=en&oi=ao), for instance, has recently published methods for some of them. Given that segmentation and lemmatization are so widely used for preprocessing, state-of-the-art methods may deserve some attention.\n\nThe issue of genre (news/edited text, informal web text, tweets/SMS) is important as well: hence the separate [Twitter tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py).\n"}
{"number": 1157, "owner": "leondz", "title": "Relax PoS requirement for ConllCorpusReader", "body": "The ConllCorpusReader _require()s a PoS field for many functions, but this seems unnecessary in practice. Many corpora are in fact distributed chunked for e.g. NEs without a PoS field, leaving the determination of this intermediate piece of information down to the end user.\n\nCan this _require()ment thus be relaxed? See e.g. from nltk/corpus/reader/conll.py:\n\n```\ndef tagged_sents(self, fileids=None, tagset=None):\n    self._require(self.WORDS, self.POS)\n```\n"}
