,Unnamed: 0,Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense,HotScore,nltk,spacy,gensim,stanford-nlp,scikit-learn
23335,23335,58636587,1,,,2019-10-31 03:34:11,,16,7533,know bert ha max length limit token acticle ha length much bigger token text bert used,2019-11-05 22:56:47,2020-09-02 12:11:33,use bert long text classification,nlp text-classification bert-language-model,5,0,3.0,,,CC BY-SA 4.0,9.251026761830646e-05,False,False,False,False,False
22010,22010,57062456,1,,,2019-07-16 17:17:22,,12,17412,reimplementing text speech project facing function call stack kera scratch graph error decoder part network architecture deep voice paper using kera tf google colab code decoder kera model test data show error message going problem fc dun know pas fc output first loop second loop answer would appreciated,2019-08-07 17:41:15,2020-08-06 09:45:12,function call stack kera scratch graph error,python tensorflow keras nlp tensorflow2.0,7,1,3.0,,,CC BY-SA 4.0,6.473421167921858e-05,False,False,False,False,False
84,84,8897593,1,8897648.0,,2012-01-17 15:51:09,,217,181701,looking working nlp project programming language though python preference want take two document determine similar,2019-02-19 17:11:44,2020-05-13 06:14:59,compute similarity two text document,nlp,10,1,179.0,,,CC BY-SA 4.0,5.614956484972162e-05,False,False,False,False,False
7579,7579,34870614,1,34877590.0,,2016-01-19 07:14:40,,161,68344,understand duty function like lookup table mean return parameter corresponding id id instance model use find correspond embedding,2018-07-13 12:19:33,2018-08-05 11:52:02,doe tf nn embedding lookup function,python tensorflow deep-learning word-embedding natural-language-processing,8,1,60.0,,,CC BY-SA 3.0,5.1850114092178725e-05,False,False,False,False,False
25369,25369,60492839,1,60493083.0,,2020-03-02 16:20:07,,11,3518,using huggingface transformer package access pretrained model use case need functionality english arabic using bert base multilingual cased pretrained model need able compare similarity sentence using something cosine similarity use first need get embedding vector sentence compute cosine similarity firstly best way extratc semantic embedding bert model would taking last hidden state model fed sentence suffice secondly sufficient way get embeddings sentence another problem embedding vector different length depending length original sentence shape output value order compute two vector cosine similarity need length could something naive first summing across still work option,2020-03-02 16:25:55,2020-03-03 09:31:58,compare sentence similarity using embeddings bert,python vector nlp cosine-similarity huggingface-transformers,2,0,5.0,,,CC BY-SA 4.0,4.444430359993805e-05,False,False,False,False,False
17223,17223,50747947,1,,,2018-06-07 18:29:18,,39,47583,checked pytorch tutorial question similar one stackoverflow get confused doe embedding pytorch embedding make similar word closer need give sentence lookup table need code model,2019-08-29 16:34:17,2020-08-29 13:44:55,embedding pytorch,pytorch word-embedding,4,0,14.0,,,CC BY-SA 4.0,4.297356986464917e-05,False,False,False,False,False
8580,8580,36952763,1,36956440.0,,2016-04-30 08:45:50,,48,93639,using anaconda python window training language model using kera exmaple according kera documentation method return history callback ha history attribute containing list successive loss metric training model run get error return model history training model code update issue wa following first defined callback option called print return even though ran iteration,2017-03-10 15:21:49,2020-10-17 15:23:03,return history validation loss kera,python neural-network nlp deep-learning keras,10,2,12.0,,,CC BY-SA 3.0,4.197199211712986e-05,False,False,False,False,False
11634,11634,42821330,1,44891281.0,,2017-03-15 21:49:19,,35,11035,restore original text kera imdb dataset want restore imdb original text kera imdb dataset first load kera imdb dataset returned sequence word index found imdb get word index method return word index dictionary like create make converting create index word dictionary tried restore original text like following good english know sentence something strange happened restore original text,2017-07-12 08:30:30,2020-10-09 02:36:06,restore original text kera imdb dataset,python machine-learning neural-network nlp keras,9,0,6.0,,,CC BY-SA 3.0,3.930853484120595e-05,False,False,False,False,False
5080,5080,29760935,1,,,2015-04-21 00:46:52,,75,60711,generated vector list token large document using word vec given sentence possible get vector sentence vector token sentence,,2019-12-09 15:37:08,get vector sentence word vec token sentence,word2vec,9,0,55.0,,,CC BY-SA 3.0,3.3002327293183156e-05,False,False,False,False,False
18116,18116,51956000,1,51956230.0,,2018-08-21 20:08:48,,69,43736,occasion circumstance require u following invariably chant mantra le understand total effect figure one doe separately regardless much research including obviously documentation think ever seen one without doe circumstance would use either one without simply combined something like apology missing something obvious pretty new,2018-12-25 11:38:07,2019-06-22 11:09:42,doe kera tokenizer method exactly,python keras nlp,3,1,23.0,,,CC BY-SA 4.0,3.150686172923832e-05,False,False,False,False,False
12094,12094,43396572,1,43399308.0,,2017-04-13 15:44:02,,61,47817,tried build cnn one layer problem indeed compilator say valueerror error checking model input expected conv input dimension got array shape code,,2020-01-05 19:06:42,dimension shape conv,python keras text-classification keras-layer,5,0,29.0,,,CC BY-SA 3.0,3.060417309938013e-05,False,False,False,False,False
20673,20673,55382596,1,55416944.0,,2019-03-27 16:52:34,,32,9129,seen nlp model bert utilize wordpiece tokenization wordpiece split token like mentioned cover wider spectrum vocabulary oov word someone please help explain wordpiece tokenization actually done handle effectively help rare oov word,,2020-04-03 11:50:17,wordpiece tokenization helpful effectively deal rare word problem nlp,nlp word-embedding,1,0,10.0,,,CC BY-SA 4.0,2.475758802878529e-05,False,False,False,False,False
22783,22783,57984502,1,,,2019-09-18 03:05:58,,13,2953,want analyse text google compute server google cloud platform gcp using word vec model however un compressed word vec model http mccormickml com google pretrained word vec model python gb take time download manually upload cloud instance way access pre trained word vec model google compute server without uploading,2019-09-18 10:06:45,2019-10-16 09:10:58,access use google pre trained word vec model without manually downloading model,python google-cloud-platform nlp google-compute-engine word2vec,3,3,1.0,,,CC BY-SA 4.0,2.4335163065684792e-05,False,False,False,False,False
14778,14778,47388497,1,47393191.0,,2017-11-20 09:16:27,,25,10239,difference dialogflow bot framework v rasa nlu bot framework open source framework available market nlp support,,2020-07-29 12:35:55,difference dialogflow bot framework v rasa nlu bot framework,nlp open-source chatbot dialogflow-es rasa-nlu,4,2,13.0,,,CC BY-SA 3.0,2.2198213217726088e-05,False,False,False,False,False
15112,15112,47818669,1,47819500.0,,2017-12-14 17:05:21,,29,5918,tried understand rasa official documentation rasa core rasa nlu able deduce much able understand rasa core used guide flow conversation rasa nlu understand process text extract information entity second thing example build chatbot rasa core well rasa nlu used build chatbot understand difference two approach follow one could please help understand better way,2019-03-05 10:10:26,2020-04-30 15:47:01,difference rasa core rasa nlu,nlp artificial-intelligence chatbot rasa-nlu rasa-core,4,0,8.0,,,CC BY-SA 4.0,2.170296282266765e-05,False,False,False,False,False
19045,19045,53403306,1,53403392.0,,2018-11-20 23:47:12,,11,1648,example want get,2018-11-20 23:58:25,2020-07-31 07:20:19,batch convert sentence length mask pytorch,nlp pytorch,3,0,4.0,,,CC BY-SA 4.0,2.134247038280103e-05,False,False,False,False,False
13627,13627,45735070,1,45737582.0,,2017-08-17 12:25:32,,49,24983,trained sentiment classifier model using kera library following step broadly convert text corpus sequence using tokenizer object class build model using model fit method evaluate model scoring using model wa able save model file load file however found way save tokenizer object file without process corpus every time need score even single sentence way around,2017-12-30 13:35:34,2019-05-13 00:35:34,kera text preprocessing saving tokenizer object file scoring,machine-learning neural-network nlp deep-learning keras,4,0,12.0,,,CC BY-SA 3.0,1.967211536248635e-05,False,False,False,False,False
10341,10341,40511562,1,40512652.0,,2016-11-09 16:20:16,,34,24884,new tensorflow running deep learning assignment udacity ipython notebook link ha error please help fix thank,2016-11-10 22:21:24,2020-08-13 10:52:50,tensorflow module object ha attribute global variable initializer,python tensorflow deep-learning word2vec,4,0,5.0,,,CC BY-SA 3.0,1.8324755207053638e-05,False,False,False,False,False
17560,17560,51235118,1,51235358.0,,2018-07-08 18:53:29,,19,9316,currently working kera model ha embedding layer first layer order visualize relationship similarity word need function return mapping word vector every element vocabulary e g love way,2018-09-05 18:42:56,2020-09-09 11:47:20,get word vector kera embedding layer,python dictionary keras keras-layer word-embedding,1,0,6.0,,,CC BY-SA 4.0,1.8170057847797337e-05,False,False,False,False,False
1727,1727,19790188,1,,,2013-11-05 13:32:22,,46,30936,english language ha couple contraction instance sometimes cause headache natural language processing python library expand contraction,2018-08-23 15:37:27,2020-06-23 22:00:08,expanding english language contraction python,python nlp text-processing,9,1,19.0,,,CC BY-SA 3.0,1.7642879924354458e-05,False,False,False,False,False
12698,12698,44238154,1,44239754.0,,2017-05-29 08:43:37,,21,20621,two attention used seq seq module two different attention introduced multiplicative additive attention tensorflow documentation difference,2019-12-13 04:51:31,2020-07-27 15:22:53,difference luong attention bahdanau attention,tensorflow deep-learning nlp attention-model,4,0,9.0,,,CC BY-SA 3.0,1.6927763261164608e-05,False,False,False,False,False
11651,11651,42711144,1,42735403.0,,2017-03-10 05:47:27,,15,17267,pytorch installed machine whenever try following get following error install torchtext,2018-08-22 12:01:32,2020-06-10 12:51:00,install torchtext,python nlp deep-learning pytorch natural-language-processing,7,0,3.0,,,CC BY-SA 3.0,1.6443788706847647e-05,False,False,False,False,False
12210,12210,43510778,1,48162961.0,,2017-04-20 05:17:39,,28,7978,wa recently working data set used abbreviation various word example seem consistency term convention used e sometimes used vowel sometimes trying build mapping object like one abbreviation corresponding word without complete corpus comprehensive list term e abbreviation could introduced explicitly known simplicity sake say restricted stuff would find gym could anything basically look left hand side example kind model could processing brain term relating abbreviation corresponding full text label idea stopped taking first last letter finding dictionary assign priori probability based context since large number morpheme without marker indicates end word see possible split updated also idea combine couple string metric algorithm like match rating algorithm determine set related term calculate levenshtein distance word set target abbreviation however still dark come abbreviation word master dictionary basically inferring word construction may naive bayes model could help concerned error precision caused using algorithm invalid model training process help appreciated really stuck one,2018-01-08 22:03:03,2020-04-23 12:01:14,python intuit word abbreviated text using nlp,python machine-learning nlp abbreviation,4,1,12.0,,,CC BY-SA 3.0,1.6349714523784126e-05,False,False,False,False,False
15454,15454,48432300,1,,,2018-01-24 21:55:23,,16,5810,currently using kera tokenizer create word index matching word index imported glove dictionary create embedding matrix however problem seems defeat one advantage using word vector embedding since using trained model prediction run new word tokenizer word index remove sequence way still use tokenizer transform sentence array still use much word glove dictionary instead one show training text edit upon contemplation guess one option would add text text text tokenizer fit includes list key glove dictionary though might mess statistic want use tf idf either preferable way different better approach,2018-01-25 00:21:46,2020-06-19 07:44:59,using kera tokenizer new word training set,python machine-learning nlp deep-learning keras,3,4,8.0,,,CC BY-SA 3.0,1.5701892775994487e-05,False,False,False,False,False
18413,18413,52352522,1,52353721.0,,2018-09-16 08:52:31,,13,3912,currently developing text classification tool using kera work work fine got validation accuracy wrap head around exactly convolution layer work text data hyper parameter use following sentence input data maximum word sentence le padding added vocabulary size amount sentence training embedding vecor length many relation word ha word embeddings batch size matter question number label class simple model made complicated structure strangely work better even without using lstm main question hyper parameter use conv layer following input data max word count word embeddings dimension doe mean scan first word completely discarding rest set filter max amount word sentence example image instance input data http joxi ru krdgdbbiebypja first step convoulution layer stride http joxi ru lb c dwkor second step stride http joxi ru brrg ij ra layer repeat time correct get say th word sentence thus information lost,2018-09-16 11:45:52,2020-06-28 10:07:19,doe kera convolution layer work word embeddings text classification problem filter kernel size hyperparameter,python tensorflow keras conv-neural-network word-embedding,1,0,11.0,,,CC BY-SA 4.0,1.5576911494095447e-05,False,False,False,False,False
778,778,15173225,1,15174569.0,,2013-03-02 10:06:29,,71,97948,python tf idf cosine find document similarity possible calculate document similarity using tf idf cosine without importing external library way calculate cosine similarity string,2017-12-12 14:59:12,2020-03-27 23:03:38,calculate cosine similarity given sentence string,python string nlp similarity cosine-similarity,6,3,54.0,,,CC BY-SA 3.0,1.5406139090563978e-05,False,False,False,False,False
9048,9048,37889914,1,37937675.0,,2016-06-17 20:30:07,,45,10663,currently trying understand architecture behind word vec neural net learning algorithm representing word vector based context reading tomas mikolov paper came across defines projection layer even though term widely used referred word vec find precise definition actually neural net context question neural net context projection layer name given hidden layer whose link previous node share weight unit actually activation function kind another resource also refers broadly problem found tutorial also refers projection layer around page,2017-11-18 21:20:45,2020-07-16 07:00:04,projection layer context neural network,machine-learning nlp neural-network word2vec,3,2,21.0,,,CC BY-SA 3.0,1.4995253187842136e-05,False,False,False,False,False
699,699,9294926,1,9344555.0,,2012-02-15 14:12:06,,131,24952,io email client email contains date time location text becomes hyperlink possible create appointment look map simply tapping link work email english language also love feature would like understand naive way would many regular expression run however going scale well work specific language date format etc think apple must using concept machine learning extract entity pm pm h h etc idea apple able extract entity quickly email client machine learning algorithm would apply accomplish task,2012-09-30 20:36:35,2017-07-31 21:41:03,doe apple find date time address email,machine-learning nlp information-extraction named-entity-recognition,6,2,114.0,,,CC BY-SA 3.0,1.4657817404422949e-05,False,False,False,False,False
3926,3926,27860652,1,27864657.0,,2015-01-09 12:31:25,,86,35613,reading paper trouble understanding concept negative sampling http arxiv org pdf v pdf anyone help please,,2020-02-14 09:22:45,word vec negative sampling layman term,machine-learning nlp word2vec,3,1,52.0,,,CC BY-SA 3.0,1.4617801401956417e-05,False,False,False,False,False
1221,1221,9637278,1,,,2012-03-09 16:10:20,,32,46971,trying use tm package r perform text analysis tied following problem character valid like exclude invalid character analysis either within r importing file processing tried using iconv convert file utf exclude anything converted follows pointed batch convert latin file utf using iconv still get error appreciate help,2017-05-23 11:47:22,2020-06-06 10:11:23,r tm package invalid input utf towcs,r utf-8 iconv text-mining,14,0,16.0,,,CC BY-SA 3.0,1.3829433638905313e-05,False,False,False,False,False
14796,14796,47485216,1,53470422.0,,2017-11-25 11:03:49,,21,7820,thought output input value following layer could skip computation something doe work example actual output number random however thought output,2018-11-25 18:13:23,2020-04-08 13:50:35,doe mask zero kera embedding layer work,python machine-learning keras word-embedding,2,3,8.0,,,CC BY-SA 4.0,1.292423495154226e-05,False,False,False,False,False
13445,13445,45394949,1,,,2017-07-29 23:24:19,,11,11188,want understand meant dimensionality word embeddings embed word form matrix nlp task role doe dimensionality play visual example help understand concept,2018-01-03 01:32:13,2019-12-30 13:06:24,dimensionality word embeddings,nlp terminology dimensionality-reduction word-embedding,6,1,6.0,,,CC BY-SA 3.0,1.246277623725445e-05,False,False,False,False,False
9168,9168,38045290,1,39190391.0,,2016-06-27 03:02:43,,32,16521,result two different summary system sys sys reference summary evaluated bleu rouge problem rouge score sys wa higher sys rouge rouge rouge rouge rouge l rouge su bleu score sys wa le bleu score sys quite much question rouge bleu based n gram measure similar summary system summary human difference result evaluation like main different rouge v bleu explain issue,2020-06-24 09:31:49,2020-06-24 09:31:49,text summarization evaluation bleu v rouge,nlp text-processing rouge bleu,3,0,10.0,,,CC BY-SA 4.0,1.2407075883009403e-05,False,False,False,False,False
